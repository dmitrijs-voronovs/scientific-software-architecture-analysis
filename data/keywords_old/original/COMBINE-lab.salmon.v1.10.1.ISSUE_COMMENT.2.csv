id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:1076,Availability,echo,echo,1076,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:1626,Availability,echo,echo,1626,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327:117,Usability,simpl,simple,117,"Hi @k3yavi, ; So there is no issue when I manually add 2 pairs. Is there a max amount of input files? ; Below is the simple bash script to organize the data and find the correct files; ```; #!/bin/bash; #this script calls alevin for multiple library pairs of files . #where salmon located; salmon=""/usr/local/bin/salmon"". index=""path/to/gencode_annot/AlevinIndex/""; echo ${index}. #output folder path; output_folder=""path/to/alevin_outputTest""; echo ${output_folder}. #where the raw files are; samples_folder=""path/to/Raw_data/Sample_cells/"". cd ${samples_folder}. sample1=$(ls *R1*.fastq.gz -p | grep -v / | tr '\n' ' ') #this gives us a space seperated list of all the R1 files; #this is from the alevin tutorial ""Often, a single library may be split into multiple FASTA/Q files. Also, sometimes one may wish to quantify multiple; #replicates or samples together, treating them as if they are one library""; echo ""Value of sample1:""; echo ${sample1}. echo ""Value of sample2:""; sample2=${sample1//R1/R2} #switch the R1 with R2 to find the second pair for ALL (//) occurances; echo ${sample2}. tgMap=""path/to/gencode.primary_assembly.v29.tsv""; #this is a transcript --> gene map tsv file. Can create this using tximport. whitelist=""path/to/my_barcode.tsv""; #a list of true barcodes; salmonCommand=""${salmon} alevin -i $index -lISR -1 ${sample1} -2 ${sample2} -p 8 --celseq2 --dumpCsvCounts -o ${output_folder}/quantSC --tgMap ${tgMap} --whitelist ${whitelist}""; #--numCellBootstraps 100; #numCellBootstraps args -- generate a mean and varience for cell x; #dumpCSVcounts - dumps cell v. transcripts count matrix in csv format; echo ${salmonCommand}; if ${salmonCommand}; then; touch ${output_folder}/qauntSC_complete.txt; else; touch ${output_folder}/quantSC_failed_to_complete.txt; fi; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446199327
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446208541:124,Testability,test,tested,124,"The bash script looks good to me, and I am not aware of any hard limit on the number of files as input. However, I just did tested on 24 files as an input and it seems to work. Hard to tell what's wrong, without being able to replicate the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446208541
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446212203:72,Availability,down,downstream,72,"I have already run them all (successfully) separately as pairs, but for downstream analysis I need them to be a single library, so I thought it would be simpler to run them as multiple input files..?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446212203
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446212203:153,Usability,simpl,simpler,153,"I have already run them all (successfully) separately as pairs, but for downstream analysis I need them to be a single library, so I thought it would be simpler to run them as multiple input files..?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446212203
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446230657:16,Availability,down,down,16,"I have narrowed down the issue, the first 8 files are fine, adding a 9th reproduces the issue. . Will send you a link with a subset of the files",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446230657
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446442085:493,Deployability,release,release,493,"Hi @rbenel ,; Thanks a lot for bringing this to our attention and forwarding the relevant data to replicate the issue.; Apparently it was an extremely complicated and rare corner case but thanks to @rob-p we were able to resolve it. Since you are using Alevin by compiling from source our latest commit (https://github.com/COMBINE-lab/salmon/commit/c3eeec93d00a9b66bffb7a470723ddb6b4d8bf0d) on the develop branch should solve your issue. We will eventually merge the fix to master in the next release or as a hot-fix sometime later in the future.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446442085
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:601,Deployability,upgrade,upgrades,601,"Hi @k3yavi, ; My local repository now contains the latest commit, and the run proceeds past the `processed X Million barcodes` however, I have been stuck at `Analyzed 95 cells (100% of all)` for the past few hours.. I am sorry this is giving you guys such issues :( . I also tried this on `cat *R1*.fq.gz` of the files, and had the same issue.; [alevin.log](https://github.com/COMBINE-lab/salmon/files/2672819/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/2672821/salmon_quant.log). ```; Logs will be written to path/to/alevin_outputSingleLibrary/quantSC/logs; Check for upgrades manually at https://combine-lab.github.io/salmon; [2018-12-12 15:07:42.022] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; ### alevin (dscRNA-seq quantification) v0.12.1; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ index ] => { /path/to/gencode_annot/AlevinIndex/ }; ### [ libType ] => { ISR }; ### [ mates1 ] => { 12_CTTGTA_L001_R1_001.fastq.gz 12_CTTGTA_L001_R1_002.fastq.gz 12_CTTGTA_L001_R1_003.fastq.gz 12_CTTGTA_L001_R1_004.fastq.gz 12_CTTGTA_L001_R1_005.fastq.gz 12_CTTGTA_L001_R1_006.fastq.gz 12_CTTGTA_L001_R1_007.fastq.gz 12_CTTGTA_L001_R1_008.fastq.gz 12_CTTGTA_L001_R1_009.fastq.gz 12_CTTGTA_L001_R1_010.fastq.gz 12_CTTGTA_L002_R1_001.fastq.gz 12_CTTGTA_L002_R1_002.fastq.gz 12_CTTGTA_L002_R1_003.fastq.gz 12_CTTGTA_L002_R1_004.fastq.gz 12_CTTGTA_L002_R1_005.fastq.gz 12_CTTGTA_L002_R1_006.fastq.gz 12_CTTGTA_L002_R1_007.fastq.gz 12_CTTGTA_L002_R1_008.fastq.gz 12_CTTGTA_L002_R1_009.fastq.gz 12_CTTGTA_L002_R1_010.fastq.gz }; ### [ mates2 ] => { 12_CTTGTA_L001_R2_001.fastq.gz 12_CTTGTA_L001_R2_002.fastq.gz 12_CTTGTA_L001_R2_003.fastq.gz 12_CTTGTA_L001_R2_004.fastq.gz 12_CTTGTA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:4189,Performance,Load,Loading,4189,"cessed 74 Million barcodes. [2018-12-12 15:08:51.135] [alevinLog] [info] Done barcode density calculation.; [2018-12-12 15:08:51.135] [alevinLog] [info] # Barcodes Used: 74376522 / 74376522.; [2018-12-12 15:08:51.141] [alevinLog] [info] Done importing white-list Barcodes; [2018-12-12 15:08:51.141] [alevinLog] [warning] Skipping 1 Barcodes with 0 reads; Assuming this is the required behavior.; [2018-12-12 15:08:51.141] [alevinLog] [info] Total 95 white-listed Barcodes; [2018-12-12 15:08:51.144] [alevinLog] [info] Done populating Z matrix; [2018-12-12 15:08:51.146] [alevinLog] [info] Done indexing Barcodes; [2018-12-12 15:08:51.146] [alevinLog] [info] Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog] [info] Used Barcodes except Whitelist: 1864; [2018-12-12 15:08:51.272] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2018-12-12 15:08:51.272] [alevinLog] [info] parsing read library format; [2018-12-12 15:08:51.375] [stderrLog] [info] Loading Suffix Array ; [2018-12-12 15:08:51.272] [jointLog] [info] There is 1 library.; [2018-12-12 15:08:51.375] [jointLog] [info] Loading Quasi index; [2018-12-12 15:08:51.375] [jointLog] [info] Loading 32-bit quasi index; [2018-12-12 15:09:10.216] [stderrLog] [info] Loading Transcript Info ; [2018-12-12 15:09:15.719] [stderrLog] [info] Loading Rank-Select Bit Array; [2018-12-12 15:09:16.330] [stderrLog] [info] There were 205,870 set bits in the bit array; [2018-12-12 15:09:16.343] [stderrLog] [info] Computing transcript lengths; [2018-12-12 15:09:16.343] [stderrLog] [info] Waiting to finish loading hash; [2018-12-12 15:09:21.460] [stderrLog] [info] Done loading index; [2018-12-12 15:09:21.460] [jointLog] [info] done; [2018-12-12 15:09:21.460] [jointLog] [info] Index contained 205,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.66",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:4321,Performance,Load,Loading,4321,"5] [alevinLog] [info] # Barcodes Used: 74376522 / 74376522.; [2018-12-12 15:08:51.141] [alevinLog] [info] Done importing white-list Barcodes; [2018-12-12 15:08:51.141] [alevinLog] [warning] Skipping 1 Barcodes with 0 reads; Assuming this is the required behavior.; [2018-12-12 15:08:51.141] [alevinLog] [info] Total 95 white-listed Barcodes; [2018-12-12 15:08:51.144] [alevinLog] [info] Done populating Z matrix; [2018-12-12 15:08:51.146] [alevinLog] [info] Done indexing Barcodes; [2018-12-12 15:08:51.146] [alevinLog] [info] Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog] [info] Used Barcodes except Whitelist: 1864; [2018-12-12 15:08:51.272] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2018-12-12 15:08:51.272] [alevinLog] [info] parsing read library format; [2018-12-12 15:08:51.375] [stderrLog] [info] Loading Suffix Array ; [2018-12-12 15:08:51.272] [jointLog] [info] There is 1 library.; [2018-12-12 15:08:51.375] [jointLog] [info] Loading Quasi index; [2018-12-12 15:08:51.375] [jointLog] [info] Loading 32-bit quasi index; [2018-12-12 15:09:10.216] [stderrLog] [info] Loading Transcript Info ; [2018-12-12 15:09:15.719] [stderrLog] [info] Loading Rank-Select Bit Array; [2018-12-12 15:09:16.330] [stderrLog] [info] There were 205,870 set bits in the bit array; [2018-12-12 15:09:16.343] [stderrLog] [info] Computing transcript lengths; [2018-12-12 15:09:16.343] [stderrLog] [info] Waiting to finish loading hash; [2018-12-12 15:09:21.460] [stderrLog] [info] Done loading index; [2018-12-12 15:09:21.460] [jointLog] [info] done; [2018-12-12 15:09:21.460] [jointLog] [info] Index contained 205,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.666] [jointLog] [info] Thread saw mini-batch with a maximum of 5.34% zero probability fragments; [2018-12-12 15:12:07.677] [jointLog] ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:4386,Performance,Load,Loading,4386,"-12 15:08:51.141] [alevinLog] [info] Done importing white-list Barcodes; [2018-12-12 15:08:51.141] [alevinLog] [warning] Skipping 1 Barcodes with 0 reads; Assuming this is the required behavior.; [2018-12-12 15:08:51.141] [alevinLog] [info] Total 95 white-listed Barcodes; [2018-12-12 15:08:51.144] [alevinLog] [info] Done populating Z matrix; [2018-12-12 15:08:51.146] [alevinLog] [info] Done indexing Barcodes; [2018-12-12 15:08:51.146] [alevinLog] [info] Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog] [info] Used Barcodes except Whitelist: 1864; [2018-12-12 15:08:51.272] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2018-12-12 15:08:51.272] [alevinLog] [info] parsing read library format; [2018-12-12 15:08:51.375] [stderrLog] [info] Loading Suffix Array ; [2018-12-12 15:08:51.272] [jointLog] [info] There is 1 library.; [2018-12-12 15:08:51.375] [jointLog] [info] Loading Quasi index; [2018-12-12 15:08:51.375] [jointLog] [info] Loading 32-bit quasi index; [2018-12-12 15:09:10.216] [stderrLog] [info] Loading Transcript Info ; [2018-12-12 15:09:15.719] [stderrLog] [info] Loading Rank-Select Bit Array; [2018-12-12 15:09:16.330] [stderrLog] [info] There were 205,870 set bits in the bit array; [2018-12-12 15:09:16.343] [stderrLog] [info] Computing transcript lengths; [2018-12-12 15:09:16.343] [stderrLog] [info] Waiting to finish loading hash; [2018-12-12 15:09:21.460] [stderrLog] [info] Done loading index; [2018-12-12 15:09:21.460] [jointLog] [info] done; [2018-12-12 15:09:21.460] [jointLog] [info] Index contained 205,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.666] [jointLog] [info] Thread saw mini-batch with a maximum of 5.34% zero probability fragments; [2018-12-12 15:12:07.677] [jointLog] [info] Thread saw mini-batch with a maximum of 5.48% zero probabilit",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:4459,Performance,Load,Loading,4459,"; [2018-12-12 15:08:51.141] [alevinLog] [warning] Skipping 1 Barcodes with 0 reads; Assuming this is the required behavior.; [2018-12-12 15:08:51.141] [alevinLog] [info] Total 95 white-listed Barcodes; [2018-12-12 15:08:51.144] [alevinLog] [info] Done populating Z matrix; [2018-12-12 15:08:51.146] [alevinLog] [info] Done indexing Barcodes; [2018-12-12 15:08:51.146] [alevinLog] [info] Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog] [info] Used Barcodes except Whitelist: 1864; [2018-12-12 15:08:51.272] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2018-12-12 15:08:51.272] [alevinLog] [info] parsing read library format; [2018-12-12 15:08:51.375] [stderrLog] [info] Loading Suffix Array ; [2018-12-12 15:08:51.272] [jointLog] [info] There is 1 library.; [2018-12-12 15:08:51.375] [jointLog] [info] Loading Quasi index; [2018-12-12 15:08:51.375] [jointLog] [info] Loading 32-bit quasi index; [2018-12-12 15:09:10.216] [stderrLog] [info] Loading Transcript Info ; [2018-12-12 15:09:15.719] [stderrLog] [info] Loading Rank-Select Bit Array; [2018-12-12 15:09:16.330] [stderrLog] [info] There were 205,870 set bits in the bit array; [2018-12-12 15:09:16.343] [stderrLog] [info] Computing transcript lengths; [2018-12-12 15:09:16.343] [stderrLog] [info] Waiting to finish loading hash; [2018-12-12 15:09:21.460] [stderrLog] [info] Done loading index; [2018-12-12 15:09:21.460] [jointLog] [info] done; [2018-12-12 15:09:21.460] [jointLog] [info] Index contained 205,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.666] [jointLog] [info] Thread saw mini-batch with a maximum of 5.34% zero probability fragments; [2018-12-12 15:12:07.677] [jointLog] [info] Thread saw mini-batch with a maximum of 5.48% zero probability fragments. [2018-12-12 15:12:07.721] [jointLog] [info] Computed 173,36",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:4530,Performance,Load,Loading,4530," 0 reads; Assuming this is the required behavior.; [2018-12-12 15:08:51.141] [alevinLog] [info] Total 95 white-listed Barcodes; [2018-12-12 15:08:51.144] [alevinLog] [info] Done populating Z matrix; [2018-12-12 15:08:51.146] [alevinLog] [info] Done indexing Barcodes; [2018-12-12 15:08:51.146] [alevinLog] [info] Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog] [info] Used Barcodes except Whitelist: 1864; [2018-12-12 15:08:51.272] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2018-12-12 15:08:51.272] [alevinLog] [info] parsing read library format; [2018-12-12 15:08:51.375] [stderrLog] [info] Loading Suffix Array ; [2018-12-12 15:08:51.272] [jointLog] [info] There is 1 library.; [2018-12-12 15:08:51.375] [jointLog] [info] Loading Quasi index; [2018-12-12 15:08:51.375] [jointLog] [info] Loading 32-bit quasi index; [2018-12-12 15:09:10.216] [stderrLog] [info] Loading Transcript Info ; [2018-12-12 15:09:15.719] [stderrLog] [info] Loading Rank-Select Bit Array; [2018-12-12 15:09:16.330] [stderrLog] [info] There were 205,870 set bits in the bit array; [2018-12-12 15:09:16.343] [stderrLog] [info] Computing transcript lengths; [2018-12-12 15:09:16.343] [stderrLog] [info] Waiting to finish loading hash; [2018-12-12 15:09:21.460] [stderrLog] [info] Done loading index; [2018-12-12 15:09:21.460] [jointLog] [info] done; [2018-12-12 15:09:21.460] [jointLog] [info] Index contained 205,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.666] [jointLog] [info] Thread saw mini-batch with a maximum of 5.34% zero probability fragments; [2018-12-12 15:12:07.677] [jointLog] [info] Thread saw mini-batch with a maximum of 5.48% zero probability fragments. [2018-12-12 15:12:07.721] [jointLog] [info] Computed 173,365 rich equivalence classes for further processing; [2018-12-12 15:12:07.7",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:4790,Performance,load,loading,4790,"] Done indexing Barcodes; [2018-12-12 15:08:51.146] [alevinLog] [info] Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog] [info] Used Barcodes except Whitelist: 1864; [2018-12-12 15:08:51.272] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2018-12-12 15:08:51.272] [alevinLog] [info] parsing read library format; [2018-12-12 15:08:51.375] [stderrLog] [info] Loading Suffix Array ; [2018-12-12 15:08:51.272] [jointLog] [info] There is 1 library.; [2018-12-12 15:08:51.375] [jointLog] [info] Loading Quasi index; [2018-12-12 15:08:51.375] [jointLog] [info] Loading 32-bit quasi index; [2018-12-12 15:09:10.216] [stderrLog] [info] Loading Transcript Info ; [2018-12-12 15:09:15.719] [stderrLog] [info] Loading Rank-Select Bit Array; [2018-12-12 15:09:16.330] [stderrLog] [info] There were 205,870 set bits in the bit array; [2018-12-12 15:09:16.343] [stderrLog] [info] Computing transcript lengths; [2018-12-12 15:09:16.343] [stderrLog] [info] Waiting to finish loading hash; [2018-12-12 15:09:21.460] [stderrLog] [info] Done loading index; [2018-12-12 15:09:21.460] [jointLog] [info] done; [2018-12-12 15:09:21.460] [jointLog] [info] Index contained 205,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.666] [jointLog] [info] Thread saw mini-batch with a maximum of 5.34% zero probability fragments; [2018-12-12 15:12:07.677] [jointLog] [info] Thread saw mini-batch with a maximum of 5.48% zero probability fragments. [2018-12-12 15:12:07.721] [jointLog] [info] Computed 173,365 rich equivalence classes for further processing; [2018-12-12 15:12:07.721] [jointLog] [info] Counted 27,831,508 total reads in the equivalence classes ; [2018-12-12 15:12:07.721] [jointLog] [warning] Found 31347 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:4854,Performance,load,loading,4854,"Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog] [info] Used Barcodes except Whitelist: 1864; [2018-12-12 15:08:51.272] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2018-12-12 15:08:51.272] [alevinLog] [info] parsing read library format; [2018-12-12 15:08:51.375] [stderrLog] [info] Loading Suffix Array ; [2018-12-12 15:08:51.272] [jointLog] [info] There is 1 library.; [2018-12-12 15:08:51.375] [jointLog] [info] Loading Quasi index; [2018-12-12 15:08:51.375] [jointLog] [info] Loading 32-bit quasi index; [2018-12-12 15:09:10.216] [stderrLog] [info] Loading Transcript Info ; [2018-12-12 15:09:15.719] [stderrLog] [info] Loading Rank-Select Bit Array; [2018-12-12 15:09:16.330] [stderrLog] [info] There were 205,870 set bits in the bit array; [2018-12-12 15:09:16.343] [stderrLog] [info] Computing transcript lengths; [2018-12-12 15:09:16.343] [stderrLog] [info] Waiting to finish loading hash; [2018-12-12 15:09:21.460] [stderrLog] [info] Done loading index; [2018-12-12 15:09:21.460] [jointLog] [info] done; [2018-12-12 15:09:21.460] [jointLog] [info] Index contained 205,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.666] [jointLog] [info] Thread saw mini-batch with a maximum of 5.34% zero probability fragments; [2018-12-12 15:12:07.677] [jointLog] [info] Thread saw mini-batch with a maximum of 5.48% zero probability fragments. [2018-12-12 15:12:07.721] [jointLog] [info] Computed 173,365 rich equivalence classes for further processing; [2018-12-12 15:12:07.721] [jointLog] [info] Counted 27,831,508 total reads in the equivalence classes ; [2018-12-12 15:12:07.721] [jointLog] [warning] Found 31347 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is too large; [2018-12-12 15:12:07.721] [jointLog] [info] Mapping rate = 37",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:5992,Performance,optimiz,optimizer,5992,"05,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.666] [jointLog] [info] Thread saw mini-batch with a maximum of 5.34% zero probability fragments; [2018-12-12 15:12:07.677] [jointLog] [info] Thread saw mini-batch with a maximum of 5.48% zero probability fragments. [2018-12-12 15:12:07.721] [jointLog] [info] Computed 173,365 rich equivalence classes for further processing; [2018-12-12 15:12:07.721] [jointLog] [info] Counted 27,831,508 total reads in the equivalence classes ; [2018-12-12 15:12:07.721] [jointLog] [warning] Found 31347 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is too large; [2018-12-12 15:12:07.721] [jointLog] [info] Mapping rate = 37.4197%. [2018-12-12 15:12:07.721] [jointLog] [info] finished quantifyLibrary(); [2018-12-12 15:12:07.904] [alevinLog] [info] Starting optimizer. Analyzed 7 cells (7% of all).; Analyzed 8 cells (8% of all).; Analyzed 9 cells (9% of all).; Analyzed 10 cells (11% of all).; Analyzed 11 cells (12% of all).; Analyzed 12 cells (13% of all).; Analyzed 13 cells (14% of all).; Analyzed 14 cells (15% of all).; Analyzed 15 cells (16% of all).; Analyzed 16 cells (17% of all).; Analyzed 17 cells (18% of all).; Analyzed 18 cells (19% of all).; Analyzed 19 cells (20% of all).; Analyzed 20 cells (21% of all).; Analyzed 21 cells (22% of all).; Analyzed 22 cells (23% of all).; Analyzed 23 cells (24% of all).; Analyzed 24 cells (25% of all).; Analyzed 25 cells (26% of all).; Analyzed 26 cells (27% of all).; Analyzed 27 cells (28% of all).; Analyzed 28 cells (29% of all).; Analyzed 29 cells (31% of all).; Analyzed 30 cells (32% of all).; Analyzed 31 cells (33% of all).; Analyzed 32 cells (34% of all).; Analyzed 33 cells (35% of all).; Analyzed 34 cells (36% of all).; Analyzed 35 cells (37% of all).; Analyzed 36 cells (38% of a",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:2606,Security,validat,validateMappings,2606,".fastq.gz }; ### [ mates2 ] => { 12_CTTGTA_L001_R2_001.fastq.gz 12_CTTGTA_L001_R2_002.fastq.gz 12_CTTGTA_L001_R2_003.fastq.gz 12_CTTGTA_L001_R2_004.fastq.gz 12_CTTGTA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002_R2_002.fastq.gz 12_CTTGTA_L002_R2_003.fastq.gz 12_CTTGTA_L002_R2_004.fastq.gz 12_CTTGTA_L002_R2_005.fastq.gz 12_CTTGTA_L002_R2_006.fastq.gz 12_CTTGTA_L002_R2_007.fastq.gz 12_CTTGTA_L002_R2_008.fastq.gz 12_CTTGTA_L002_R2_009.fastq.gz 12_CTTGTA_L002_R2_010.fastq.gz }; ### [ threads ] => { 8 }; ### [ celseq2 ] => { }; ### [ dumpCsvCounts ] => { }; ### [ output ] => { /path/to/alevin_outputSingleLibrary/quantSC }; ### [ tgMap ] => { /path/to/gencode_annot/gencode.primary_assembly.v29.tsv }; ### [ whitelist ] => { /path/to/salmon/my_barcode.tsv }. [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2018-12-12 15:07:42.022] [jointLog] [info] Using default value of 0.8 for minScoreFraction in Alevin; [2018-12-12 15:07:42.028] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 74 Million barcodes. [2018-12-12 15:08:51.135] [alevinLog] [info] Done barcode density calculation.; [2018-12-12 15:08:51.135] [alevinLog] [info] # Barcodes Used: 74376522 / 74376522.; [2018-12-12 15:08:51.141] [alevinLog] [info] Done importing white-list Barcodes; [2018-12-12 15:08:51.141] [alevinLog] [warning] Skipping 1 Barcodes with 0 reads; Assuming this is the required behavior.; [2018-12-12 15:08:5",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:2768,Security,validat,validateMappings,2768,"TA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002_R2_002.fastq.gz 12_CTTGTA_L002_R2_003.fastq.gz 12_CTTGTA_L002_R2_004.fastq.gz 12_CTTGTA_L002_R2_005.fastq.gz 12_CTTGTA_L002_R2_006.fastq.gz 12_CTTGTA_L002_R2_007.fastq.gz 12_CTTGTA_L002_R2_008.fastq.gz 12_CTTGTA_L002_R2_009.fastq.gz 12_CTTGTA_L002_R2_010.fastq.gz }; ### [ threads ] => { 8 }; ### [ celseq2 ] => { }; ### [ dumpCsvCounts ] => { }; ### [ output ] => { /path/to/alevin_outputSingleLibrary/quantSC }; ### [ tgMap ] => { /path/to/gencode_annot/gencode.primary_assembly.v29.tsv }; ### [ whitelist ] => { /path/to/salmon/my_barcode.tsv }. [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2018-12-12 15:07:42.022] [jointLog] [info] Using default value of 0.8 for minScoreFraction in Alevin; [2018-12-12 15:07:42.028] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 74 Million barcodes. [2018-12-12 15:08:51.135] [alevinLog] [info] Done barcode density calculation.; [2018-12-12 15:08:51.135] [alevinLog] [info] # Barcodes Used: 74376522 / 74376522.; [2018-12-12 15:08:51.141] [alevinLog] [info] Done importing white-list Barcodes; [2018-12-12 15:08:51.141] [alevinLog] [warning] Skipping 1 Barcodes with 0 reads; Assuming this is the required behavior.; [2018-12-12 15:08:51.141] [alevinLog] [info] Total 95 white-listed Barcodes; [2018-12-12 15:08:51.144] [alevinLog] [info] Done populating Z matrix; [2018-12-12 15:08:51.146] [alevinL",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:2918,Security,validat,validateMappings,2918,"TTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002_R2_002.fastq.gz 12_CTTGTA_L002_R2_003.fastq.gz 12_CTTGTA_L002_R2_004.fastq.gz 12_CTTGTA_L002_R2_005.fastq.gz 12_CTTGTA_L002_R2_006.fastq.gz 12_CTTGTA_L002_R2_007.fastq.gz 12_CTTGTA_L002_R2_008.fastq.gz 12_CTTGTA_L002_R2_009.fastq.gz 12_CTTGTA_L002_R2_010.fastq.gz }; ### [ threads ] => { 8 }; ### [ celseq2 ] => { }; ### [ dumpCsvCounts ] => { }; ### [ output ] => { /path/to/alevin_outputSingleLibrary/quantSC }; ### [ tgMap ] => { /path/to/gencode_annot/gencode.primary_assembly.v29.tsv }; ### [ whitelist ] => { /path/to/salmon/my_barcode.tsv }. [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2018-12-12 15:07:42.022] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2018-12-12 15:07:42.022] [jointLog] [info] Using default value of 0.8 for minScoreFraction in Alevin; [2018-12-12 15:07:42.028] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 74 Million barcodes. [2018-12-12 15:08:51.135] [alevinLog] [info] Done barcode density calculation.; [2018-12-12 15:08:51.135] [alevinLog] [info] # Barcodes Used: 74376522 / 74376522.; [2018-12-12 15:08:51.141] [alevinLog] [info] Done importing white-list Barcodes; [2018-12-12 15:08:51.141] [alevinLog] [warning] Skipping 1 Barcodes with 0 reads; Assuming this is the required behavior.; [2018-12-12 15:08:51.141] [alevinLog] [info] Total 95 white-listed Barcodes; [2018-12-12 15:08:51.144] [alevinLog] [info] Done populating Z matrix; [2018-12-12 15:08:51.146] [alevinLog] [info] Done indexing Barcodes; [2018-12-12 15:08:51.146] [alevinLog] [info] Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:4798,Security,hash,hash,4798,"] Done indexing Barcodes; [2018-12-12 15:08:51.146] [alevinLog] [info] Total Unique barcodes found: 4096; [2018-12-12 15:08:51.146] [alevinLog] [info] Used Barcodes except Whitelist: 1864; [2018-12-12 15:08:51.272] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2018-12-12 15:08:51.272] [alevinLog] [info] parsing read library format; [2018-12-12 15:08:51.375] [stderrLog] [info] Loading Suffix Array ; [2018-12-12 15:08:51.272] [jointLog] [info] There is 1 library.; [2018-12-12 15:08:51.375] [jointLog] [info] Loading Quasi index; [2018-12-12 15:08:51.375] [jointLog] [info] Loading 32-bit quasi index; [2018-12-12 15:09:10.216] [stderrLog] [info] Loading Transcript Info ; [2018-12-12 15:09:15.719] [stderrLog] [info] Loading Rank-Select Bit Array; [2018-12-12 15:09:16.330] [stderrLog] [info] There were 205,870 set bits in the bit array; [2018-12-12 15:09:16.343] [stderrLog] [info] Computing transcript lengths; [2018-12-12 15:09:16.343] [stderrLog] [info] Waiting to finish loading hash; [2018-12-12 15:09:21.460] [stderrLog] [info] Done loading index; [2018-12-12 15:09:21.460] [jointLog] [info] done; [2018-12-12 15:09:21.460] [jointLog] [info] Index contained 205,870 targets. processed 0 Million fragments; processed 1 Million fragments; processed 1 Million fragments; ..............; processed 74 Million fragments; hits: 111594303, hits per frag: 1.50848[2018-12-12 15:12:07.666] [jointLog] [info] Thread saw mini-batch with a maximum of 5.34% zero probability fragments; [2018-12-12 15:12:07.677] [jointLog] [info] Thread saw mini-batch with a maximum of 5.48% zero probability fragments. [2018-12-12 15:12:07.721] [jointLog] [info] Computed 173,365 rich equivalence classes for further processing; [2018-12-12 15:12:07.721] [jointLog] [info] Counted 27,831,508 total reads in the equivalence classes ; [2018-12-12 15:12:07.721] [jointLog] [warning] Found 31347 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:353,Testability,log,log,353,"Hi @k3yavi, ; My local repository now contains the latest commit, and the run proceeds past the `processed X Million barcodes` however, I have been stuck at `Analyzed 95 cells (100% of all)` for the past few hours.. I am sorry this is giving you guys such issues :( . I also tried this on `cat *R1*.fq.gz` of the files, and had the same issue.; [alevin.log](https://github.com/COMBINE-lab/salmon/files/2672819/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/2672821/salmon_quant.log). ```; Logs will be written to path/to/alevin_outputSingleLibrary/quantSC/logs; Check for upgrades manually at https://combine-lab.github.io/salmon; [2018-12-12 15:07:42.022] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; ### alevin (dscRNA-seq quantification) v0.12.1; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ index ] => { /path/to/gencode_annot/AlevinIndex/ }; ### [ libType ] => { ISR }; ### [ mates1 ] => { 12_CTTGTA_L001_R1_001.fastq.gz 12_CTTGTA_L001_R1_002.fastq.gz 12_CTTGTA_L001_R1_003.fastq.gz 12_CTTGTA_L001_R1_004.fastq.gz 12_CTTGTA_L001_R1_005.fastq.gz 12_CTTGTA_L001_R1_006.fastq.gz 12_CTTGTA_L001_R1_007.fastq.gz 12_CTTGTA_L001_R1_008.fastq.gz 12_CTTGTA_L001_R1_009.fastq.gz 12_CTTGTA_L001_R1_010.fastq.gz 12_CTTGTA_L002_R1_001.fastq.gz 12_CTTGTA_L002_R1_002.fastq.gz 12_CTTGTA_L002_R1_003.fastq.gz 12_CTTGTA_L002_R1_004.fastq.gz 12_CTTGTA_L002_R1_005.fastq.gz 12_CTTGTA_L002_R1_006.fastq.gz 12_CTTGTA_L002_R1_007.fastq.gz 12_CTTGTA_L002_R1_008.fastq.gz 12_CTTGTA_L002_R1_009.fastq.gz 12_CTTGTA_L002_R1_010.fastq.gz }; ### [ mates2 ] => { 12_CTTGTA_L001_R2_001.fastq.gz 12_CTTGTA_L001_R2_002.fastq.gz 12_CTTGTA_L001_R2_003.fastq.gz 12_CTTGTA_L001_R2_004.fastq.gz 12_CTTGTA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:417,Testability,log,log,417,"Hi @k3yavi, ; My local repository now contains the latest commit, and the run proceeds past the `processed X Million barcodes` however, I have been stuck at `Analyzed 95 cells (100% of all)` for the past few hours.. I am sorry this is giving you guys such issues :( . I also tried this on `cat *R1*.fq.gz` of the files, and had the same issue.; [alevin.log](https://github.com/COMBINE-lab/salmon/files/2672819/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/2672821/salmon_quant.log). ```; Logs will be written to path/to/alevin_outputSingleLibrary/quantSC/logs; Check for upgrades manually at https://combine-lab.github.io/salmon; [2018-12-12 15:07:42.022] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; ### alevin (dscRNA-seq quantification) v0.12.1; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ index ] => { /path/to/gencode_annot/AlevinIndex/ }; ### [ libType ] => { ISR }; ### [ mates1 ] => { 12_CTTGTA_L001_R1_001.fastq.gz 12_CTTGTA_L001_R1_002.fastq.gz 12_CTTGTA_L001_R1_003.fastq.gz 12_CTTGTA_L001_R1_004.fastq.gz 12_CTTGTA_L001_R1_005.fastq.gz 12_CTTGTA_L001_R1_006.fastq.gz 12_CTTGTA_L001_R1_007.fastq.gz 12_CTTGTA_L001_R1_008.fastq.gz 12_CTTGTA_L001_R1_009.fastq.gz 12_CTTGTA_L001_R1_010.fastq.gz 12_CTTGTA_L002_R1_001.fastq.gz 12_CTTGTA_L002_R1_002.fastq.gz 12_CTTGTA_L002_R1_003.fastq.gz 12_CTTGTA_L002_R1_004.fastq.gz 12_CTTGTA_L002_R1_005.fastq.gz 12_CTTGTA_L002_R1_006.fastq.gz 12_CTTGTA_L002_R1_007.fastq.gz 12_CTTGTA_L002_R1_008.fastq.gz 12_CTTGTA_L002_R1_009.fastq.gz 12_CTTGTA_L002_R1_010.fastq.gz }; ### [ mates2 ] => { 12_CTTGTA_L001_R2_001.fastq.gz 12_CTTGTA_L001_R2_002.fastq.gz 12_CTTGTA_L001_R2_003.fastq.gz 12_CTTGTA_L001_R2_004.fastq.gz 12_CTTGTA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:437,Testability,log,log,437,"Hi @k3yavi, ; My local repository now contains the latest commit, and the run proceeds past the `processed X Million barcodes` however, I have been stuck at `Analyzed 95 cells (100% of all)` for the past few hours.. I am sorry this is giving you guys such issues :( . I also tried this on `cat *R1*.fq.gz` of the files, and had the same issue.; [alevin.log](https://github.com/COMBINE-lab/salmon/files/2672819/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/2672821/salmon_quant.log). ```; Logs will be written to path/to/alevin_outputSingleLibrary/quantSC/logs; Check for upgrades manually at https://combine-lab.github.io/salmon; [2018-12-12 15:07:42.022] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; ### alevin (dscRNA-seq quantification) v0.12.1; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ index ] => { /path/to/gencode_annot/AlevinIndex/ }; ### [ libType ] => { ISR }; ### [ mates1 ] => { 12_CTTGTA_L001_R1_001.fastq.gz 12_CTTGTA_L001_R1_002.fastq.gz 12_CTTGTA_L001_R1_003.fastq.gz 12_CTTGTA_L001_R1_004.fastq.gz 12_CTTGTA_L001_R1_005.fastq.gz 12_CTTGTA_L001_R1_006.fastq.gz 12_CTTGTA_L001_R1_007.fastq.gz 12_CTTGTA_L001_R1_008.fastq.gz 12_CTTGTA_L001_R1_009.fastq.gz 12_CTTGTA_L001_R1_010.fastq.gz 12_CTTGTA_L002_R1_001.fastq.gz 12_CTTGTA_L002_R1_002.fastq.gz 12_CTTGTA_L002_R1_003.fastq.gz 12_CTTGTA_L002_R1_004.fastq.gz 12_CTTGTA_L002_R1_005.fastq.gz 12_CTTGTA_L002_R1_006.fastq.gz 12_CTTGTA_L002_R1_007.fastq.gz 12_CTTGTA_L002_R1_008.fastq.gz 12_CTTGTA_L002_R1_009.fastq.gz 12_CTTGTA_L002_R1_010.fastq.gz }; ### [ mates2 ] => { 12_CTTGTA_L001_R2_001.fastq.gz 12_CTTGTA_L001_R2_002.fastq.gz 12_CTTGTA_L001_R2_003.fastq.gz 12_CTTGTA_L001_R2_004.fastq.gz 12_CTTGTA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:507,Testability,log,log,507,"Hi @k3yavi, ; My local repository now contains the latest commit, and the run proceeds past the `processed X Million barcodes` however, I have been stuck at `Analyzed 95 cells (100% of all)` for the past few hours.. I am sorry this is giving you guys such issues :( . I also tried this on `cat *R1*.fq.gz` of the files, and had the same issue.; [alevin.log](https://github.com/COMBINE-lab/salmon/files/2672819/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/2672821/salmon_quant.log). ```; Logs will be written to path/to/alevin_outputSingleLibrary/quantSC/logs; Check for upgrades manually at https://combine-lab.github.io/salmon; [2018-12-12 15:07:42.022] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; ### alevin (dscRNA-seq quantification) v0.12.1; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ index ] => { /path/to/gencode_annot/AlevinIndex/ }; ### [ libType ] => { ISR }; ### [ mates1 ] => { 12_CTTGTA_L001_R1_001.fastq.gz 12_CTTGTA_L001_R1_002.fastq.gz 12_CTTGTA_L001_R1_003.fastq.gz 12_CTTGTA_L001_R1_004.fastq.gz 12_CTTGTA_L001_R1_005.fastq.gz 12_CTTGTA_L001_R1_006.fastq.gz 12_CTTGTA_L001_R1_007.fastq.gz 12_CTTGTA_L001_R1_008.fastq.gz 12_CTTGTA_L001_R1_009.fastq.gz 12_CTTGTA_L001_R1_010.fastq.gz 12_CTTGTA_L002_R1_001.fastq.gz 12_CTTGTA_L002_R1_002.fastq.gz 12_CTTGTA_L002_R1_003.fastq.gz 12_CTTGTA_L002_R1_004.fastq.gz 12_CTTGTA_L002_R1_005.fastq.gz 12_CTTGTA_L002_R1_006.fastq.gz 12_CTTGTA_L002_R1_007.fastq.gz 12_CTTGTA_L002_R1_008.fastq.gz 12_CTTGTA_L002_R1_009.fastq.gz 12_CTTGTA_L002_R1_010.fastq.gz }; ### [ mates2 ] => { 12_CTTGTA_L001_R2_001.fastq.gz 12_CTTGTA_L001_R2_002.fastq.gz 12_CTTGTA_L001_R2_003.fastq.gz 12_CTTGTA_L001_R2_004.fastq.gz 12_CTTGTA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:518,Testability,Log,Logs,518,"Hi @k3yavi, ; My local repository now contains the latest commit, and the run proceeds past the `processed X Million barcodes` however, I have been stuck at `Analyzed 95 cells (100% of all)` for the past few hours.. I am sorry this is giving you guys such issues :( . I also tried this on `cat *R1*.fq.gz` of the files, and had the same issue.; [alevin.log](https://github.com/COMBINE-lab/salmon/files/2672819/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/2672821/salmon_quant.log). ```; Logs will be written to path/to/alevin_outputSingleLibrary/quantSC/logs; Check for upgrades manually at https://combine-lab.github.io/salmon; [2018-12-12 15:07:42.022] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; ### alevin (dscRNA-seq quantification) v0.12.1; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ index ] => { /path/to/gencode_annot/AlevinIndex/ }; ### [ libType ] => { ISR }; ### [ mates1 ] => { 12_CTTGTA_L001_R1_001.fastq.gz 12_CTTGTA_L001_R1_002.fastq.gz 12_CTTGTA_L001_R1_003.fastq.gz 12_CTTGTA_L001_R1_004.fastq.gz 12_CTTGTA_L001_R1_005.fastq.gz 12_CTTGTA_L001_R1_006.fastq.gz 12_CTTGTA_L001_R1_007.fastq.gz 12_CTTGTA_L001_R1_008.fastq.gz 12_CTTGTA_L001_R1_009.fastq.gz 12_CTTGTA_L001_R1_010.fastq.gz 12_CTTGTA_L002_R1_001.fastq.gz 12_CTTGTA_L002_R1_002.fastq.gz 12_CTTGTA_L002_R1_003.fastq.gz 12_CTTGTA_L002_R1_004.fastq.gz 12_CTTGTA_L002_R1_005.fastq.gz 12_CTTGTA_L002_R1_006.fastq.gz 12_CTTGTA_L002_R1_007.fastq.gz 12_CTTGTA_L002_R1_008.fastq.gz 12_CTTGTA_L002_R1_009.fastq.gz 12_CTTGTA_L002_R1_010.fastq.gz }; ### [ mates2 ] => { 12_CTTGTA_L001_R2_001.fastq.gz 12_CTTGTA_L001_R2_002.fastq.gz 12_CTTGTA_L001_R2_003.fastq.gz 12_CTTGTA_L001_R2_004.fastq.gz 12_CTTGTA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422:585,Testability,log,logs,585,"Hi @k3yavi, ; My local repository now contains the latest commit, and the run proceeds past the `processed X Million barcodes` however, I have been stuck at `Analyzed 95 cells (100% of all)` for the past few hours.. I am sorry this is giving you guys such issues :( . I also tried this on `cat *R1*.fq.gz` of the files, and had the same issue.; [alevin.log](https://github.com/COMBINE-lab/salmon/files/2672819/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/2672821/salmon_quant.log). ```; Logs will be written to path/to/alevin_outputSingleLibrary/quantSC/logs; Check for upgrades manually at https://combine-lab.github.io/salmon; [2018-12-12 15:07:42.022] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; ### alevin (dscRNA-seq quantification) v0.12.1; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ index ] => { /path/to/gencode_annot/AlevinIndex/ }; ### [ libType ] => { ISR }; ### [ mates1 ] => { 12_CTTGTA_L001_R1_001.fastq.gz 12_CTTGTA_L001_R1_002.fastq.gz 12_CTTGTA_L001_R1_003.fastq.gz 12_CTTGTA_L001_R1_004.fastq.gz 12_CTTGTA_L001_R1_005.fastq.gz 12_CTTGTA_L001_R1_006.fastq.gz 12_CTTGTA_L001_R1_007.fastq.gz 12_CTTGTA_L001_R1_008.fastq.gz 12_CTTGTA_L001_R1_009.fastq.gz 12_CTTGTA_L001_R1_010.fastq.gz 12_CTTGTA_L002_R1_001.fastq.gz 12_CTTGTA_L002_R1_002.fastq.gz 12_CTTGTA_L002_R1_003.fastq.gz 12_CTTGTA_L002_R1_004.fastq.gz 12_CTTGTA_L002_R1_005.fastq.gz 12_CTTGTA_L002_R1_006.fastq.gz 12_CTTGTA_L002_R1_007.fastq.gz 12_CTTGTA_L002_R1_008.fastq.gz 12_CTTGTA_L002_R1_009.fastq.gz 12_CTTGTA_L002_R1_010.fastq.gz }; ### [ mates2 ] => { 12_CTTGTA_L001_R2_001.fastq.gz 12_CTTGTA_L001_R2_002.fastq.gz 12_CTTGTA_L001_R2_003.fastq.gz 12_CTTGTA_L001_R2_004.fastq.gz 12_CTTGTA_L001_R2_005.fastq.gz 12_CTTGTA_L001_R2_006.fastq.gz 12_CTTGTA_L001_R2_007.fastq.gz 12_CTTGTA_L001_R2_008.fastq.gz 12_CTTGTA_L001_R2_009.fastq.gz 12_CTTGTA_L001_R2_010.fastq.gz 12_CTTGTA_L002_R2_001.fastq.gz 12_CTTGTA_L002",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/329#issuecomment-446668422
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060:475,Integrability,depend,depends,475,"Hi @Anto007,. Sounds like an interesting experiment! A couple of questions: (1) are you quantifying the meta-transcriptome or the metagenomes? What I mean is, are your target sequences the specific genes from the microbes, or the entire microbial genomes? Is the sequencing data RNA-seq from sequencing the mixture of expressed gene transcripts, or DNA-seq of the microbes? This will have an effect on how you expect reads to be generated. The effect of `--minScoreFraction` depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, `0.9` is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the `--minScoreFraction` you want to set is the one such that ; x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that:. x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 . so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Finally, I'd typically avoid using `--mimicStrictBT2`, since those are pretty harsh parameters. Of course, you could try mapping both with and without that flag and see how it affects your mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060:1465,Safety,avoid,avoid,1465,"Hi @Anto007,. Sounds like an interesting experiment! A couple of questions: (1) are you quantifying the meta-transcriptome or the metagenomes? What I mean is, are your target sequences the specific genes from the microbes, or the entire microbial genomes? Is the sequencing data RNA-seq from sequencing the mixture of expressed gene transcripts, or DNA-seq of the microbes? This will have an effect on how you expect reads to be generated. The effect of `--minScoreFraction` depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, `0.9` is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the `--minScoreFraction` you want to set is the one such that ; x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that:. x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 . so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Finally, I'd typically avoid using `--mimicStrictBT2`, since those are pretty harsh parameters. Of course, you could try mapping both with and without that flag and see how it affects your mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060:1282,Usability,simpl,simple,1282,"Hi @Anto007,. Sounds like an interesting experiment! A couple of questions: (1) are you quantifying the meta-transcriptome or the metagenomes? What I mean is, are your target sequences the specific genes from the microbes, or the entire microbial genomes? Is the sequencing data RNA-seq from sequencing the mixture of expressed gene transcripts, or DNA-seq of the microbes? This will have an effect on how you expect reads to be generated. The effect of `--minScoreFraction` depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, `0.9` is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the `--minScoreFraction` you want to set is the one such that ; x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that:. x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 . so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Finally, I'd typically avoid using `--mimicStrictBT2`, since those are pretty harsh parameters. Of course, you could try mapping both with and without that flag and see how it affects your mapping rate.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-447589060
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-874634736:274,Performance,optimiz,optimize,274,"Sorry for the late response. According to the developer, the --meta flag is well-suited to metagenomics. Unlike handling RNA data, the use of this flag changes the initialization conditions of the EM algorithm and turns off Salmon's rich equivalence classes so as to better optimize Salmon for handling metagenomic (DNA) data. You can read more here- https://gitter.im/COMBINE-lab/salmon?at=589f11106b2d8dd5522e0ff1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-874634736
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869:82,Integrability,depend,depends,82,"@rob-p can you elaborate on this a bit more: . > The effect of --minScoreFraction depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, 0.9 is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the --minScoreFraction you want to set is the one such that x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that: x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Would the two parameter sets mentioned above have the same effect assuming read length 100?. Also, it says Alevin has a default minScoreFraction of 0.87. Would it be safe to assume differentiating between isoforms with Alevin is a similar problem to differentiating between orthologous genes in metagenomics/transcriptomics?. Which parameters would be relevant to control for this?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869:1206,Safety,safe,safe,1206,"@rob-p can you elaborate on this a bit more: . > The effect of --minScoreFraction depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, 0.9 is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the --minScoreFraction you want to set is the one such that x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that: x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Would the two parameter sets mentioned above have the same effect assuming read length 100?. Also, it says Alevin has a default minScoreFraction of 0.87. Would it be safe to assume differentiating between isoforms with Alevin is a similar problem to differentiating between orthologous genes in metagenomics/transcriptomics?. Which parameters would be relevant to control for this?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869
https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869:880,Usability,simpl,simple,880,"@rob-p can you elaborate on this a bit more: . > The effect of --minScoreFraction depends, to some extent, on how you set the match/mismatch/gap parameters. With the default parameters, 0.9 is actually higher than 90% sequence identity, because the default mismatch penalty is twice the match score. If you assume only matches and mismatches, then the --minScoreFraction you want to set is the one such that x * (match_score * read_length) <= (match_score * read_length) - (m * read_length * match_score) + (m * read_length * mismatch_penalty), where m is the mismatch fraction (0.1 in your case). So, for example, if the match_score is 2 and mismatch penalty is -4, and the read length is 100, you want to set it so that: x * 200 = 200 - (0.1 * 100 * 2) + (0.1 * 100 * -4) = 200 - 20 - 40 = 140 so, the appropriate x would be ~0.7. Of course, if you want to make the calculation simple, you can set the match score to 1 and mismatch penalty to 0, and then the interpretation (modulo gaps) is straightforward (and 0.9 means what you want). Would the two parameter sets mentioned above have the same effect assuming read length 100?. Also, it says Alevin has a default minScoreFraction of 0.87. Would it be safe to assume differentiating between isoforms with Alevin is a similar problem to differentiating between orthologous genes in metagenomics/transcriptomics?. Which parameters would be relevant to control for this?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/330#issuecomment-2249029869
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:135,Availability,down,download,135,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:214,Availability,error,error,214,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:93,Deployability,install,install,93,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:181,Deployability,release,releases,181,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:261,Deployability,install,installed,261,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:333,Integrability,depend,dependencies,333,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917:128,Usability,simpl,simply,128,"Hi @evofish,. Unless you have a particular reason to build from source, it is much easier to install salmon via bioconda, or to simply download our pre-compiled executable from the releases page. Nonetheless, your error stems from not having the `curl` program installed, which is used by the build system to automatically fetch all dependencies.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447689917
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447960701:58,Deployability,install,install,58,"Thanks for the fast reply, after many tries I was able to install it with Bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447960701
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447963407:101,Availability,error,errors,101,"Sure, thanks for letting me know you were able to install via bioconda. I'll try to make these build errors more informative.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447963407
https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447963407:50,Deployability,install,install,50,"Sure, thanks for letting me know you were able to install via bioconda. I'll try to make these build errors more informative.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/331#issuecomment-447963407
https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-450929631:527,Availability,robust,robust,527,"Hi @qifei9,. Thanks for the question and for pointing out the need for update in the docs. Regarding your first question, both approaches (3) and (2) seem reasonable to me. I would *not* try approach (1) as this will eliminate the benefit of the stranded library for the targets where you do know the orientation. For approach (2) , I'd either use `--validateMappings` or at least set `--rangeFactorizationBins 4` (the former implies the latter). As for what value to set for `--incompatPrior`, the effect should be reasonably robust across a range of values, the question is how unlikely _a priori_ would you expect a mapping not in `ISR` orientation to be if you also observed a mapping in `ISR` ... probably very unlikely (you could try e.g. 1e-10 or some such). Approach 3 is also also reasonable, though what you might consider doing is looking at the abundances for these opposite strands of the same sequence post quantification --- you should generally see that one of the two has a non-zero expression, or at least one orientation should have a much higher expression than the other (for expressed transcripts, at least, this might give you evidence as to the true strand of origin). Regarding your second point, the changelog is correct. In recent versions of salmon, `--incompatPrior` is 0 by default. We'll update the documentation accordingly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-450929631
https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-450929631:71,Deployability,update,update,71,"Hi @qifei9,. Thanks for the question and for pointing out the need for update in the docs. Regarding your first question, both approaches (3) and (2) seem reasonable to me. I would *not* try approach (1) as this will eliminate the benefit of the stranded library for the targets where you do know the orientation. For approach (2) , I'd either use `--validateMappings` or at least set `--rangeFactorizationBins 4` (the former implies the latter). As for what value to set for `--incompatPrior`, the effect should be reasonably robust across a range of values, the question is how unlikely _a priori_ would you expect a mapping not in `ISR` orientation to be if you also observed a mapping in `ISR` ... probably very unlikely (you could try e.g. 1e-10 or some such). Approach 3 is also also reasonable, though what you might consider doing is looking at the abundances for these opposite strands of the same sequence post quantification --- you should generally see that one of the two has a non-zero expression, or at least one orientation should have a much higher expression than the other (for expressed transcripts, at least, this might give you evidence as to the true strand of origin). Regarding your second point, the changelog is correct. In recent versions of salmon, `--incompatPrior` is 0 by default. We'll update the documentation accordingly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-450929631
https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-450929631:1319,Deployability,update,update,1319,"Hi @qifei9,. Thanks for the question and for pointing out the need for update in the docs. Regarding your first question, both approaches (3) and (2) seem reasonable to me. I would *not* try approach (1) as this will eliminate the benefit of the stranded library for the targets where you do know the orientation. For approach (2) , I'd either use `--validateMappings` or at least set `--rangeFactorizationBins 4` (the former implies the latter). As for what value to set for `--incompatPrior`, the effect should be reasonably robust across a range of values, the question is how unlikely _a priori_ would you expect a mapping not in `ISR` orientation to be if you also observed a mapping in `ISR` ... probably very unlikely (you could try e.g. 1e-10 or some such). Approach 3 is also also reasonable, though what you might consider doing is looking at the abundances for these opposite strands of the same sequence post quantification --- you should generally see that one of the two has a non-zero expression, or at least one orientation should have a much higher expression than the other (for expressed transcripts, at least, this might give you evidence as to the true strand of origin). Regarding your second point, the changelog is correct. In recent versions of salmon, `--incompatPrior` is 0 by default. We'll update the documentation accordingly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-450929631
https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-450929631:351,Security,validat,validateMappings,351,"Hi @qifei9,. Thanks for the question and for pointing out the need for update in the docs. Regarding your first question, both approaches (3) and (2) seem reasonable to me. I would *not* try approach (1) as this will eliminate the benefit of the stranded library for the targets where you do know the orientation. For approach (2) , I'd either use `--validateMappings` or at least set `--rangeFactorizationBins 4` (the former implies the latter). As for what value to set for `--incompatPrior`, the effect should be reasonably robust across a range of values, the question is how unlikely _a priori_ would you expect a mapping not in `ISR` orientation to be if you also observed a mapping in `ISR` ... probably very unlikely (you could try e.g. 1e-10 or some such). Approach 3 is also also reasonable, though what you might consider doing is looking at the abundances for these opposite strands of the same sequence post quantification --- you should generally see that one of the two has a non-zero expression, or at least one orientation should have a much higher expression than the other (for expressed transcripts, at least, this might give you evidence as to the true strand of origin). Regarding your second point, the changelog is correct. In recent versions of salmon, `--incompatPrior` is 0 by default. We'll update the documentation accordingly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-450929631
https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-1331883878:69,Deployability,update,updated,69,"Hi @rob-p, I believe the docs for `--incompatPrior` still need to be updated to reflect the default behaviour.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/332#issuecomment-1331883878
https://github.com/COMBINE-lab/salmon/issues/333#issuecomment-452491311:339,Deployability,release,release,339,"HI @mariaolaaksonen ,; Thanks for raising the issue and using Alevin with 1.3M dataset.; Can you check if your issue has the same behavior as in https://github.com/COMBINE-lab/salmon/issues/329, i.e. Alevin is stuck after processing a multiple of 4 number of barcodes?; We have already fixed the issue but it's not in the master or in the release `v0.12.0` of salmon. . As a fast resolution, we'd recommend compiling salmon from source using the develop branch. If you can wait for sometime, we'd release a new version with the hot-fix soon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/333#issuecomment-452491311
https://github.com/COMBINE-lab/salmon/issues/333#issuecomment-452491311:497,Deployability,release,release,497,"HI @mariaolaaksonen ,; Thanks for raising the issue and using Alevin with 1.3M dataset.; Can you check if your issue has the same behavior as in https://github.com/COMBINE-lab/salmon/issues/329, i.e. Alevin is stuck after processing a multiple of 4 number of barcodes?; We have already fixed the issue but it's not in the master or in the release `v0.12.0` of salmon. . As a fast resolution, we'd recommend compiling salmon from source using the develop branch. If you can wait for sometime, we'd release a new version with the hot-fix soon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/333#issuecomment-452491311
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666:405,Performance,load,load,405,"Hi @BenLangmead!. Thanks for the formal feature request. This is, indeed, a great idea, and something I've been interested in for quite a while. As far as I can tell, the main impediment to this is the hash table (https://github.com/greg7mdp/sparsepp) used in the index. The suffix array used by the mapping algorithm (by virtue of simply being a flat array of either 32 or 64-bit integers) is trivial to load via shared memory, as is the flat representation of the concatenated text itself. The bitvector and rank data structure that separate individual transcript sequences might be a bit trickier, but is also small enough to exist per-process. However, it's unclear to me if there is an easy or straightforward way to have the hash table reside in shared memory, and this is usually the single largest element of the index. As I mentioned, this is a feature that I've thought would be very useful for quite a while, and I'm interested in seeing it implemented. If you have any suggestions on what might be the best approach, I'm all 👂s.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666:202,Security,hash,hash,202,"Hi @BenLangmead!. Thanks for the formal feature request. This is, indeed, a great idea, and something I've been interested in for quite a while. As far as I can tell, the main impediment to this is the hash table (https://github.com/greg7mdp/sparsepp) used in the index. The suffix array used by the mapping algorithm (by virtue of simply being a flat array of either 32 or 64-bit integers) is trivial to load via shared memory, as is the flat representation of the concatenated text itself. The bitvector and rank data structure that separate individual transcript sequences might be a bit trickier, but is also small enough to exist per-process. However, it's unclear to me if there is an easy or straightforward way to have the hash table reside in shared memory, and this is usually the single largest element of the index. As I mentioned, this is a feature that I've thought would be very useful for quite a while, and I'm interested in seeing it implemented. If you have any suggestions on what might be the best approach, I'm all 👂s.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666:731,Security,hash,hash,731,"Hi @BenLangmead!. Thanks for the formal feature request. This is, indeed, a great idea, and something I've been interested in for quite a while. As far as I can tell, the main impediment to this is the hash table (https://github.com/greg7mdp/sparsepp) used in the index. The suffix array used by the mapping algorithm (by virtue of simply being a flat array of either 32 or 64-bit integers) is trivial to load via shared memory, as is the flat representation of the concatenated text itself. The bitvector and rank data structure that separate individual transcript sequences might be a bit trickier, but is also small enough to exist per-process. However, it's unclear to me if there is an easy or straightforward way to have the hash table reside in shared memory, and this is usually the single largest element of the index. As I mentioned, this is a feature that I've thought would be very useful for quite a while, and I'm interested in seeing it implemented. If you have any suggestions on what might be the best approach, I'm all 👂s.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666:332,Usability,simpl,simply,332,"Hi @BenLangmead!. Thanks for the formal feature request. This is, indeed, a great idea, and something I've been interested in for quite a while. As far as I can tell, the main impediment to this is the hash table (https://github.com/greg7mdp/sparsepp) used in the index. The suffix array used by the mapping algorithm (by virtue of simply being a flat array of either 32 or 64-bit integers) is trivial to load via shared memory, as is the flat representation of the concatenated text itself. The bitvector and rank data structure that separate individual transcript sequences might be a bit trickier, but is also small enough to exist per-process. However, it's unclear to me if there is an easy or straightforward way to have the hash table reside in shared memory, and this is usually the single largest element of the index. As I mentioned, this is a feature that I've thought would be very useful for quite a while, and I'm interested in seeing it implemented. If you have any suggestions on what might be the best approach, I'm all 👂s.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-455905666
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-1416422342:206,Performance,load,load,206,"+1. Trying to quantify ~2000 Smart-Seq2 samples. Currently takes about 5 days on a single node doing 1 cell at a time. Perhaps an easier way to implement this would be to provide a batch mode such that you load the index once and then serially quantify a batch of N samples within the same process. This would save the significant overhead of having to load the index for each sample (~50-75% of the total per-sample processing time). As a bonus, the batch mode could spit out a single transcript x sample matrix so you wouldn't have to run `quantmerge` separately.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-1416422342
https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-1416422342:353,Performance,load,load,353,"+1. Trying to quantify ~2000 Smart-Seq2 samples. Currently takes about 5 days on a single node doing 1 cell at a time. Perhaps an easier way to implement this would be to provide a batch mode such that you load the index once and then serially quantify a batch of N samples within the same process. This would save the significant overhead of having to load the index for each sample (~50-75% of the total per-sample processing time). As a bonus, the batch mode could spit out a single transcript x sample matrix so you wouldn't have to run `quantmerge` separately.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/335#issuecomment-1416422342
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496238223:422,Availability,ERROR,ERROR,422,"This issue is not only the case for ensembl, but also for gencode v29. Following the [tutorial](https://gist.github.com/k3yavi/c501705ed2d29b12b0d10cf78b3ed001), building an index from *ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_29/gencode.v29.pc_transcripts.fa.gz* and *txp2gene.tsv* from *ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_29/gencode.v29.annotation.gtf.gz* results in *ERROR: Txp to Gene Map not found for 98665 transcripts. ExitingNULL*. A suggested solution: have an option during building the index so that it will only include transcripts also present in a pre-generated txp2gene.tsv (what @sarahhcarl did manually).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496238223
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496246321:644,Testability,log,log,644,"Hey @alexvpickering ,; I agree, it does makes sense to index the transcript and gene relationship while creating the salmon indexing but I don't see how it solves this problem i.e. if the idea is to ignore the transcripts which doesn't having transcript to gene mapping then it may bias the analysis as we are considering lesser number of transcripts than known; having said that an argument about unknown splice junction can still be made. regarding the link for `gencode_29`, may I ask what version of salmon you are using ? Because I just tried to run alevin with the links you forwarded and it works fine for me. If possible forwarding the log will help too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496246321
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496251814:57,Testability,log,logs,57,"Hey @k3yavi,; salmon version is 0.13.1. I don't have the logs at the moment as I am trying the ensembl-based solution suggested by @sarahhcarl using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz and equivalent ensembl version for cdna index. After doing so I will reproduce the issue and add the log file here. What about instead of filtering the index based on txp2gene, having some way of padding missing transcripts in txp2gene (e.g. just mapping from the indexes ENST to the same ENST)? I don't understand the algorithm enough to know the best solution - this just seems like an internal detail that the end user should not have to resolve/abandon alevin as a result of. A seperate issue for the [tutorial](https://combine-lab.github.io/alevin-tutorial/2018/setting-up-resources/) that doesn't apply to the accompanying [gist](https://gist.github.com/k3yavi/c501705ed2d29b12b0d10cf78b3ed001): the index is generated using gencode.**v28**.pc_transcripts.fa.gz and the txp2gene.tsv is generated using gencode.**v26**.primary_assembly.annotation.gtf. Perhaps I generated the txp2gene.tsv incorrectly? The bioawk instructions from the tutorial had to be altered (I'm guessing because of version differences). ```bash; bioawk --version; awk version 20110810. # command from tutorial; bioawk -c gff '$feature==""transcript"" {print $group}' <(gunzip -c gencode.v29.annotation.gtf.gz) | awk -F ' ' '{print substr($4,2,length($4)-3) ""\t"" substr($2,2,length($2)-3)}' - > txp2gene.tsv. bioawk: illegal field $(), name ""group""; input record number 7, file /dev/fd/63; source line number 1. # how I modified it; bioawk -c gff '$feature==""transcript"" {print $attribute}' <(gunzip -c gencode.v29.annotation.gtf.gz) | awk -F ' ' '{print substr($4,2,length($4)-3) ""\t"" substr($2,2,length($2)-3)}' - > txp2gene.tsv. cat txp2gene.tsv | head -n 5; ENST00000456328.2 ENSG00000223972.5; ENST00000450305.2 ENSG00000223972.5; ENST00000488147.1 ENSG00000227232.5; ENST00000619216.1 ENSG00000278267.1; ENST00000473358.1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496251814
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496251814:301,Testability,log,log,301,"Hey @k3yavi,; salmon version is 0.13.1. I don't have the logs at the moment as I am trying the ensembl-based solution suggested by @sarahhcarl using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz and equivalent ensembl version for cdna index. After doing so I will reproduce the issue and add the log file here. What about instead of filtering the index based on txp2gene, having some way of padding missing transcripts in txp2gene (e.g. just mapping from the indexes ENST to the same ENST)? I don't understand the algorithm enough to know the best solution - this just seems like an internal detail that the end user should not have to resolve/abandon alevin as a result of. A seperate issue for the [tutorial](https://combine-lab.github.io/alevin-tutorial/2018/setting-up-resources/) that doesn't apply to the accompanying [gist](https://gist.github.com/k3yavi/c501705ed2d29b12b0d10cf78b3ed001): the index is generated using gencode.**v28**.pc_transcripts.fa.gz and the txp2gene.tsv is generated using gencode.**v26**.primary_assembly.annotation.gtf. Perhaps I generated the txp2gene.tsv incorrectly? The bioawk instructions from the tutorial had to be altered (I'm guessing because of version differences). ```bash; bioawk --version; awk version 20110810. # command from tutorial; bioawk -c gff '$feature==""transcript"" {print $group}' <(gunzip -c gencode.v29.annotation.gtf.gz) | awk -F ' ' '{print substr($4,2,length($4)-3) ""\t"" substr($2,2,length($2)-3)}' - > txp2gene.tsv. bioawk: illegal field $(), name ""group""; input record number 7, file /dev/fd/63; source line number 1. # how I modified it; bioawk -c gff '$feature==""transcript"" {print $attribute}' <(gunzip -c gencode.v29.annotation.gtf.gz) | awk -F ' ' '{print substr($4,2,length($4)-3) ""\t"" substr($2,2,length($2)-3)}' - > txp2gene.tsv. cat txp2gene.tsv | head -n 5; ENST00000456328.2 ENSG00000223972.5; ENST00000450305.2 ENSG00000223972.5; ENST00000488147.1 ENSG00000227232.5; ENST00000619216.1 ENSG00000278267.1; ENST00000473358.1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496251814
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:226,Availability,error,error,226,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:311,Availability,error,error,311,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:596,Availability,ERROR,ERROR,596,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:119,Deployability,release,release-,119,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:317,Integrability,message,message,317,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:246,Testability,log,logs,246,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:274,Testability,log,log,274,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:358,Testability,log,log,358,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:422,Testability,log,log,422,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:442,Testability,log,log,442,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062:512,Testability,log,log,512,"I tried using Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz for the annotation file and ftp://ftp.ensembl.org/pub/release-94/fasta/homo_sapiens/cdna/Homo_sapiens.GRCh38.cdna.all.fa.gz to build the index but got a similar error. Here are the logs (I modified the salmon log a bit because it didn't have the error message that printed to stdout). [alevin.log](https://github.com/COMBINE-lab/salmon/files/3224429/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3224440/salmon_quant.log). One curious thing that I noticed:. ```; Index contained 175,775 targets; ...; ERROR: Txp to Gene Map not found for 175775 transcripts; ```; It seems to not be finding any of the transcripts? This was also the case for the gencode attempt that I made previously.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496258062
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496266217:303,Availability,error,error,303,"Ah yes, you are right - I did not have the `--gencode` flag for salmon indexing. Thank you @k3yavi! It is working perfectly now. . Perhaps some sort of quick grep of the index file name for `'gencode'` or within the file for multiple `'|'`s should be done to check if it's gencode followed by a warning/error if the `--gencode` flag is missing? When running `salmon alevin`, it also might be nice to fail sooner if possible (instead of after processing barcodes/fragments - can take a long time). If you agree/think this is easy enough I can open separate issues. The ensembl files do seem to be inconsistent about subversion. Here is the cdna file for the index and the GTF:; ```bash; zcat Homo_sapiens.GRCh38.cdna.all.fa.gz | awk 'NR % 2 {print $1 ""\t"" $4}' | head -n 5; >ENST00000632684.1 gene:ENSG00000282431.1; >ENST00000434970.2 gene:ENSG00000237235.2; >ENST00000448914.1 gene:ENSG00000228985.1; >ENST00000415118.1 gene:ENSG00000223997.1; >ENST00000631435.1 gene:ENSG00000282253.1. # for Homo_sapiens.GRCh38.94.gtf.gz; bioawk -c gff '$feature==""transcript"" {print $attribute}' <(gunzip -c Homo_sapiens.GRCh38.94.gtf.gz) | awk -F ' ' '{print substr($6,2,length($6)-3) ""\t"" substr($2,2,length($2)-3)}' | head -n 5; ENST00000456328 ENSG00000223972; ENST00000450305 ENSG00000223972; ENST00000488147 ENSG00000227232; ENST00000619216 ENSG00000278267; ENST00000473358 ENSG00000243485. # for Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz; bioawk -c gff '$feature==""transcript"" {print $attribute}' <(gunzip -c Homo_sapiens.GRCh38.94.chr_patch_hapl_scaff.gtf.gz) | awk -F ' ' '{print substr($6,2,length($6)-3) ""\t"" substr($2,2,length($2)-3)}' | head -n 5; ENST00000456328 ENSG00000223972; ENST00000450305 ENSG00000223972; ENST00000488147 ENSG00000227232; ENST00000619216 ENSG00000278267; ENST00000473358 ENSG00000243485; ```. For the gencode index and GTF. ```bash; bioawk -c gff '$feature==""transcript"" {print $attribute}' <(gunzip -c gencode.v29.annotation.gtf.gz) | awk -F ' ' '{print substr($4,2,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496266217
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496280290:125,Availability,failure,failure,125,"Glad to hear that. Yes, I agree we can put some sort of filter while indexing to make the pipeline less painful in case of a failure. Regarding the transcript to gene map file parsing before the CB correction, we plan to make that into the next release which we plan to merge soon. I'll update you once we have the new version ready.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496280290
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496280290:90,Deployability,pipeline,pipeline,90,"Glad to hear that. Yes, I agree we can put some sort of filter while indexing to make the pipeline less painful in case of a failure. Regarding the transcript to gene map file parsing before the CB correction, we plan to make that into the next release which we plan to merge soon. I'll update you once we have the new version ready.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496280290
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496280290:245,Deployability,release,release,245,"Glad to hear that. Yes, I agree we can put some sort of filter while indexing to make the pipeline less painful in case of a failure. Regarding the transcript to gene map file parsing before the CB correction, we plan to make that into the next release which we plan to merge soon. I'll update you once we have the new version ready.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496280290
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496280290:287,Deployability,update,update,287,"Glad to hear that. Yes, I agree we can put some sort of filter while indexing to make the pipeline less painful in case of a failure. Regarding the transcript to gene map file parsing before the CB correction, we plan to make that into the next release which we plan to merge soon. I'll update you once we have the new version ready.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-496280290
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-498040517:73,Availability,failure,failure,73,"Hi @alexvpickering ,. With the latest release we have moved the pipeline failure with `txp2gene` file before parsing the CB .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-498040517
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-498040517:38,Deployability,release,release,38,"Hi @alexvpickering ,. With the latest release we have moved the pipeline failure with `txp2gene` file before parsing the CB .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-498040517
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-498040517:64,Deployability,pipeline,pipeline,64,"Hi @alexvpickering ,. With the latest release we have moved the pipeline failure with `txp2gene` file before parsing the CB .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-498040517
https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-576069305:227,Availability,ERROR,ERROR,227,"Hi @k3yavi Is there any way that an option can be added to ignore missing transcripts? I had to modify the gencode v30 transcriptome fasta and GTF to include fusion transcripts... but this seems to have created an issue:; ```; ERROR: Txp to Gene Map not found for 9 transcripts. Exiting; ```; I don't think ignoring 9 transcripts should be a big deal. Is there any way to accomplish that? Or any other workaround? . Thank you,; Henry. Edit: Figured out a work around!; -- For anyone who is having a similar issue--. I figured out that the issue was a set of transcripts in my fasta file that were not in the trp2gene file. . To find the conflicts in my annotations, I had to grep the headers from my fasta file and direct the output into a new .txt file. I read the new file into R as a ""|"" delimited table and used gsub() to remove the "">"" from the first column. Then I used vim to search and delete all the lines of the fasta file which were missing from the trp2gene file using `?searchText` --> `ma` (marks the start of the delete) --> `d'a` (deletes from `ma` to the current cursor location).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/336#issuecomment-576069305
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-457573475:5,Availability,Down,Downgraded,5,"Hi,; Downgraded to version 0.11.3 of salmon using conda and the segmentation fault doesn't occur anymore. So must be an issue in the conda build of version 0.12.0 of salmon. ; Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-457573475
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-457573475:77,Availability,fault,fault,77,"Hi,; Downgraded to version 0.11.3 of salmon using conda and the segmentation fault doesn't occur anymore. So must be an issue in the conda build of version 0.12.0 of salmon. ; Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-457573475
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-457575844:128,Availability,down,down,128,"Thanks for the info, @annajbott! Can you mention what OS and version you see the segfault on? This will help trying to track it down.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-457575844
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-457575858:128,Availability,down,down,128,"Thanks for the info, @annajbott! Can you mention what OS and version you see the segfault on? This will help trying to track it down.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-457575858
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458145886:34,Availability,fault,fault,34,"Hi @rob-p, The version we see the fault on is Red Hat Enterprise Release 6.10 (Scientific linux). I have a version of ubuntu that I can test on but not sure when I can get around to it at the moment.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458145886
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458145886:65,Deployability,Release,Release,65,"Hi @rob-p, The version we see the fault on is Red Hat Enterprise Release 6.10 (Scientific linux). I have a version of ubuntu that I can test on but not sure when I can get around to it at the moment.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458145886
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458145886:136,Testability,test,test,136,"Hi @rob-p, The version we see the fault on is Red Hat Enterprise Release 6.10 (Scientific linux). I have a version of ubuntu that I can test on but not sure when I can get around to it at the moment.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458145886
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458159405:163,Testability,test,test,163,"Thanks for your help. @TomSmithCGAT suggested that it could be our cluster, as he had issues he could resolve with the same cluster. I have a second cluster I can test on. I will do testing on this first and see if I can come to the bottom of it and will get back to you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458159405
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458159405:182,Testability,test,testing,182,"Thanks for your help. @TomSmithCGAT suggested that it could be our cluster, as he had issues he could resolve with the same cluster. I have a second cluster I can test on. I will do testing on this first and see if I can come to the bottom of it and will get back to you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458159405
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685:71,Availability,error,error,71,"For clarification: From memory, using the same cluster, I had the same error at the same stage but only with particular data sets and confirmed this was not an issue of available memory. @k3yavi may remember some more of the details but we never got to the bottom of it. . @Acribbs Testing on another cluster would be a good idea in case this is a very specific cluster configuration issue",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685:169,Availability,avail,available,169,"For clarification: From memory, using the same cluster, I had the same error at the same stage but only with particular data sets and confirmed this was not an issue of available memory. @k3yavi may remember some more of the details but we never got to the bottom of it. . @Acribbs Testing on another cluster would be a good idea in case this is a very specific cluster configuration issue",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685:370,Deployability,configurat,configuration,370,"For clarification: From memory, using the same cluster, I had the same error at the same stage but only with particular data sets and confirmed this was not an issue of available memory. @k3yavi may remember some more of the details but we never got to the bottom of it. . @Acribbs Testing on another cluster would be a good idea in case this is a very specific cluster configuration issue",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685:370,Modifiability,config,configuration,370,"For clarification: From memory, using the same cluster, I had the same error at the same stage but only with particular data sets and confirmed this was not an issue of available memory. @k3yavi may remember some more of the details but we never got to the bottom of it. . @Acribbs Testing on another cluster would be a good idea in case this is a very specific cluster configuration issue",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685:282,Testability,Test,Testing,282,"For clarification: From memory, using the same cluster, I had the same error at the same stage but only with particular data sets and confirmed this was not an issue of available memory. @k3yavi may remember some more of the details but we never got to the bottom of it. . @Acribbs Testing on another cluster would be a good idea in case this is a very specific cluster configuration issue",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458160685
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458197495:211,Modifiability,config,config,211,"Yep, it aligns with https://github.com/COMBINE-lab/salmon/issues/328, where the user confirms it works out for him in a local computer while failing on his cluster environment. We should figure out this cluster config which is making `alevin` segfault as its becoming recurrent issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458197495
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:283,Availability,error,error,283,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:716,Availability,avail,available,716,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:758,Availability,avail,available,758,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:6144,Availability,avail,available,6144,"[alevinLog] [info] Done importing order of barcodes ""quants_mat_rows.txt"" file.; [2019-01-29 09:55:59.107] [alevinLog] [info] Total 138 barcodes found; [2019-01-29 09:55:59.107] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] Starting to make feature Matrix; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making regular featues; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making feature Matrix; [2019-01-29 09:55:59.123] [alevinLog] [info] Finished white listing; [2019-01-29 09:55:59.126] [alevinLog] [info] Finished optimizer; ``` . Concat fastq:; ```; salmon alevin -l ISR -1 big.fastq.1.gz -2 big.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:56:37.731] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:56:37.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 2 Million barcodes. [2019-01-29 09:56:43.029",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:6186,Availability,avail,available,6186," [alevinLog] [info] Total 138 barcodes found; [2019-01-29 09:55:59.107] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] Starting to make feature Matrix; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making regular featues; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making feature Matrix; [2019-01-29 09:55:59.123] [alevinLog] [info] Finished white listing; [2019-01-29 09:55:59.126] [alevinLog] [info] Finished optimizer; ``` . Concat fastq:; ```; salmon alevin -l ISR -1 big.fastq.1.gz -2 big.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:56:37.731] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:56:37.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 2 Million barcodes. [2019-01-29 09:56:43.029] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:56:43.029] [alevinLog] [info] # Ba",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:10240,Availability,fault,fault,10240,"ng 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:57:36.601] [jointLog] [info] Mapping rate = 8.94141%. [2019-01-29 09:57:36.601] [jointLog] [info] finished quantifyLibrary(). Analyzed 293 cells (100% of all).; [2019-01-29 09:57:40.090] [alevinLog] [info] Total 206902 UMI after deduplicating.; [2019-01-29 09:57:40.091] [alevinLog] [warning] Skipped 71 barcodes due to No mapped read; [2019-01-29 09:57:40.110] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-01-29 09:57:40.176] [alevinLog] [info] Starting Import of the gene count matrix.; [2019-01-29 09:57:41.168] [alevinLog] [info] Done Importing gene count matrix for dimension 222x19879; [2019-01-29 09:57:41.168] [alevinLog] [info] Starting dumping cell v gene counts in csv format; Segmentation fault (core dumped); ```. I then installed through conda salmon=0.12.0. Both times it failed with core dump.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:32,Deployability,install,installed,32,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:621,Deployability,UPGRADE,UPGRADE,621,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:809,Deployability,release,releases,809,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:878,Deployability,upgrade,upgrade,878,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:6049,Deployability,UPGRADE,UPGRADE,6049,"[alevinLog] [info] Done importing order of barcodes ""quants_mat_rows.txt"" file.; [2019-01-29 09:55:59.107] [alevinLog] [info] Total 138 barcodes found; [2019-01-29 09:55:59.107] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] Starting to make feature Matrix; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making regular featues; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making feature Matrix; [2019-01-29 09:55:59.123] [alevinLog] [info] Finished white listing; [2019-01-29 09:55:59.126] [alevinLog] [info] Finished optimizer; ``` . Concat fastq:; ```; salmon alevin -l ISR -1 big.fastq.1.gz -2 big.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:56:37.731] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:56:37.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 2 Million barcodes. [2019-01-29 09:56:43.029",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:6237,Deployability,release,releases,6237,"rna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] Starting to make feature Matrix; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making regular featues; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making feature Matrix; [2019-01-29 09:55:59.123] [alevinLog] [info] Finished white listing; [2019-01-29 09:55:59.126] [alevinLog] [info] Finished optimizer; ``` . Concat fastq:; ```; salmon alevin -l ISR -1 big.fastq.1.gz -2 big.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:56:37.731] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:56:37.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 2 Million barcodes. [2019-01-29 09:56:43.029] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:56:43.029] [alevinLog] [info] # Barcodes Used: 2695632 / 2712324.; [2019-01-29 09:56:52.900] [alevinLog] [info] Knee found left b",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:6306,Deployability,upgrade,upgrade,6306,"rna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] Starting to make feature Matrix; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making regular featues; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making feature Matrix; [2019-01-29 09:55:59.123] [alevinLog] [info] Finished white listing; [2019-01-29 09:55:59.126] [alevinLog] [info] Finished optimizer; ``` . Concat fastq:; ```; salmon alevin -l ISR -1 big.fastq.1.gz -2 big.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:56:37.731] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:56:37.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 2 Million barcodes. [2019-01-29 09:56:43.029] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:56:43.029] [alevinLog] [info] # Barcodes Used: 2695632 / 2712324.; [2019-01-29 09:56:52.900] [alevinLog] [info] Knee found left b",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:10273,Deployability,install,installed,10273,"ng 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:57:36.601] [jointLog] [info] Mapping rate = 8.94141%. [2019-01-29 09:57:36.601] [jointLog] [info] finished quantifyLibrary(). Analyzed 293 cells (100% of all).; [2019-01-29 09:57:40.090] [alevinLog] [info] Total 206902 UMI after deduplicating.; [2019-01-29 09:57:40.091] [alevinLog] [warning] Skipped 71 barcodes due to No mapped read; [2019-01-29 09:57:40.110] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-01-29 09:57:40.176] [alevinLog] [info] Starting Import of the gene count matrix.; [2019-01-29 09:57:41.168] [alevinLog] [info] Done Importing gene count matrix for dimension 222x19879; [2019-01-29 09:57:41.168] [alevinLog] [info] Starting dumping cell v gene counts in csv format; Segmentation fault (core dumped); ```. I then installed through conda salmon=0.12.0. Both times it failed with core dump.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:2747,Performance,Load,Loading,2747,"4:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Learned InvCov: 114.535 normfactor: 147.323; [2019-01-29 09:55:04.817] [alevinLog] [info] Total 289(has 190 low confidence) barcodes; [2019-01-29 09:55:04.822] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:55:04.855] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:55:04.855] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:55:04.855] [alevinLog] [info] Total Unique barcodes found: 70316; [2019-01-29 09:55:04.855] [alevinLog] [info] Used Barcodes except Whitelist: 184; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equival",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:2879,Performance,Load,Loading,2879,"ry at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Learned InvCov: 114.535 normfactor: 147.323; [2019-01-29 09:55:04.817] [alevinLog] [info] Total 289(has 190 low confidence) barcodes; [2019-01-29 09:55:04.822] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:55:04.855] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:55:04.855] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:55:04.855] [alevinLog] [info] Total Unique barcodes found: 70316; [2019-01-29 09:55:04.855] [alevinLog] [info] Used Barcodes except Whitelist: 184; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence class",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:2944,Performance,Load,Loading,2944,"ted Boundary at 99 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Learned InvCov: 114.535 normfactor: 147.323; [2019-01-29 09:55:04.817] [alevinLog] [info] Total 289(has 190 low confidence) barcodes; [2019-01-29 09:55:04.822] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:55:04.855] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:55:04.855] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:55:04.855] [alevinLog] [info] Total Unique barcodes found: 70316; [2019-01-29 09:55:04.855] [alevinLog] [info] Used Barcodes except Whitelist: 184; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence classes ; [2019-01-29 09:55:54.754] [jointLog] [warning] Only 80300 fragm",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:3017,Performance,Load,Loading,3017,"ed InvCov: 114.535 normfactor: 147.323; [2019-01-29 09:55:04.817] [alevinLog] [info] Total 289(has 190 low confidence) barcodes; [2019-01-29 09:55:04.822] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:55:04.855] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:55:04.855] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:55:04.855] [alevinLog] [info] Total Unique barcodes found: 70316; [2019-01-29 09:55:04.855] [alevinLog] [info] Used Barcodes except Whitelist: 184; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence classes ; [2019-01-29 09:55:54.754] [jointLog] [warning] Only 80300 fragments were mapped, but the number of burn-in fragments was set to 5000000",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:3088,Performance,Load,Loading,3088,"og] [info] Total 289(has 190 low confidence) barcodes; [2019-01-29 09:55:04.822] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:55:04.855] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:55:04.855] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:55:04.855] [alevinLog] [info] Total Unique barcodes found: 70316; [2019-01-29 09:55:04.855] [alevinLog] [info] Used Barcodes except Whitelist: 184; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence classes ; [2019-01-29 09:55:54.754] [jointLog] [warning] Only 80300 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:3347,Performance,load,loading,3347,"o] Done indexing Barcodes; [2019-01-29 09:55:04.855] [alevinLog] [info] Total Unique barcodes found: 70316; [2019-01-29 09:55:04.855] [alevinLog] [info] Used Barcodes except Whitelist: 184; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence classes ; [2019-01-29 09:55:54.754] [jointLog] [warning] Only 80300 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:55:54.754] [jointLog] [info] Mapping rate = 8.80342%. [2019-01-29 09:55:54.754] [jointLog] [info] finished quantifyLibrary(). Analyzed 289 cells (100% of all).; [2019-01-29 09:55:56.858] [alevinLog] [info] Total 72037 UMI after",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:3537,Performance,load,loading,3537,"84; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence classes ; [2019-01-29 09:55:54.754] [jointLog] [warning] Only 80300 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:55:54.754] [jointLog] [info] Mapping rate = 8.80342%. [2019-01-29 09:55:54.754] [jointLog] [info] finished quantifyLibrary(). Analyzed 289 cells (100% of all).; [2019-01-29 09:55:56.858] [alevinLog] [info] Total 72037 UMI after deduplicating.; [2019-01-29 09:55:56.858] [alevinLog] [warning] Skipped 151 barcodes due to No mapped read; [2019-01-29 09:55:56.876] [alevinLog] [info] Clearing EqMap; Might take some ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:3675,Performance,optimiz,optimizer,3675,"arsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence classes ; [2019-01-29 09:55:54.754] [jointLog] [warning] Only 80300 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:55:54.754] [jointLog] [info] Mapping rate = 8.80342%. [2019-01-29 09:55:54.754] [jointLog] [info] finished quantifyLibrary(). Analyzed 289 cells (100% of all).; [2019-01-29 09:55:56.858] [alevinLog] [info] Total 72037 UMI after deduplicating.; [2019-01-29 09:55:56.858] [alevinLog] [warning] Skipped 151 barcodes due to No mapped read; [2019-01-29 09:55:56.876] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-01-29 09:55:56.917] [alevinLog] [info] Starting Import of the gene count matrix.; [2019-01-29 09:55:57.130] [alevinLog] [info] Done ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:5806,Performance,optimiz,optimizer,5806,"gene counts in csv format; 0.00215799	7.4911e-08	0.000194712	11697.8	; 0.00705206	1.19109e-07	30039.7	29692.8	; [2019-01-29 09:55:59.105] [alevinLog] [info] Finished dumping csv counts; [2019-01-29 09:55:59.106] [alevinLog] [info] Starting white listing; [2019-01-29 09:55:59.107] [alevinLog] [info] Done importing order of barcodes ""quants_mat_rows.txt"" file.; [2019-01-29 09:55:59.107] [alevinLog] [info] Total 138 barcodes found; [2019-01-29 09:55:59.107] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] Starting to make feature Matrix; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making regular featues; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making feature Matrix; [2019-01-29 09:55:59.123] [alevinLog] [info] Finished white listing; [2019-01-29 09:55:59.126] [alevinLog] [info] Finished optimizer; ``` . Concat fastq:; ```; salmon alevin -l ISR -1 big.fastq.1.gz -2 big.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] =>",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:8146,Performance,Load,Loading,8146,"029] [alevinLog] [info] # Barcodes Used: 2695632 / 2712324.; [2019-01-29 09:56:52.900] [alevinLog] [info] Knee found left boundary at 692 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Gauss Corrected Boundary at 100 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Learned InvCov: 114.414 normfactor: 148.807; [2019-01-29 09:56:53.219] [alevinLog] [info] Total 293(has 193 low confidence) barcodes; [2019-01-29 09:56:53.224] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:56:53.254] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:56:53.255] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:56:53.255] [alevinLog] [info] Total Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equiv",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:8278,Performance,Load,Loading,8278,"at 692 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Gauss Corrected Boundary at 100 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Learned InvCov: 114.414 normfactor: 148.807; [2019-01-29 09:56:53.219] [alevinLog] [info] Total 293(has 193 low confidence) barcodes; [2019-01-29 09:56:53.224] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:56:53.254] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:56:53.255] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:56:53.255] [alevinLog] [info] Total Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence cl",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:8343,Performance,Load,Loading,8343," Boundary at 100 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Learned InvCov: 114.414 normfactor: 148.807; [2019-01-29 09:56:53.219] [alevinLog] [info] Total 293(has 193 low confidence) barcodes; [2019-01-29 09:56:53.224] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:56:53.254] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:56:53.255] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:56:53.255] [alevinLog] [info] Total Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 f",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:8416,Performance,Load,Loading,8416," InvCov: 114.414 normfactor: 148.807; [2019-01-29 09:56:53.219] [alevinLog] [info] Total 293(has 193 low confidence) barcodes; [2019-01-29 09:56:53.224] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:56:53.254] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:56:53.255] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:56:53.255] [alevinLog] [info] Total Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 500",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:8487,Performance,Load,Loading,8487,"] [info] Total 293(has 193 low confidence) barcodes; [2019-01-29 09:56:53.224] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:56:53.254] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:56:53.255] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:56:53.255] [alevinLog] [info] Total Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:8746,Performance,load,loading,8746," Done indexing Barcodes; [2019-01-29 09:56:53.255] [alevinLog] [info] Total Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:57:36.601] [jointLog] [info] Mapping rate = 8.94141%. [2019-01-29 09:57:36.601] [jointLog] [info] finished quantifyLibrary(). Analyzed 293 cells (100% of all).; [2019-01-29 09:57:40.090] [alevinLog] [info] Total 206902 UMI ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:8810,Performance,load,loading,8810,"otal Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:57:36.601] [jointLog] [info] Mapping rate = 8.94141%. [2019-01-29 09:57:36.601] [jointLog] [info] finished quantifyLibrary(). Analyzed 293 cells (100% of all).; [2019-01-29 09:57:40.090] [alevinLog] [info] Total 206902 UMI after deduplicating.; [2019-01-29 09:57:40.091] [alevinLog] [warning] S",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:9075,Performance,optimiz,optimizer,9075,"rsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:57:36.601] [jointLog] [info] Mapping rate = 8.94141%. [2019-01-29 09:57:36.601] [jointLog] [info] finished quantifyLibrary(). Analyzed 293 cells (100% of all).; [2019-01-29 09:57:40.090] [alevinLog] [info] Total 206902 UMI after deduplicating.; [2019-01-29 09:57:40.091] [alevinLog] [warning] Skipped 71 barcodes due to No mapped read; [2019-01-29 09:57:40.110] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-01-29 09:57:40.176] [alevinLog] [info] Starting Import of the gene count matrix.; [2019-01-29 09:57:41.168] [alevinLog] [info] Do",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:3355,Security,hash,hash,3355,"o] Done indexing Barcodes; [2019-01-29 09:55:04.855] [alevinLog] [info] Total Unique barcodes found: 70316; [2019-01-29 09:55:04.855] [alevinLog] [info] Used Barcodes except Whitelist: 184; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:55:09.968] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:55:16.908] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:55:19.931] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:55:19.931] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:55:41.122] [jointLog] [info] done; [2019-01-29 09:55:41.122] [jointLog] [info] Index contained 80,511 targets; [2019-01-29 09:55:41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence classes ; [2019-01-29 09:55:54.754] [jointLog] [warning] Only 80300 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:55:54.754] [jointLog] [info] Mapping rate = 8.80342%. [2019-01-29 09:55:54.754] [jointLog] [info] finished quantifyLibrary(). Analyzed 289 cells (100% of all).; [2019-01-29 09:55:56.858] [alevinLog] [info] Total 72037 UMI after",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:8754,Security,hash,hash,8754," Done indexing Barcodes; [2019-01-29 09:56:53.255] [alevinLog] [info] Total Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:57:36.601] [jointLog] [info] Mapping rate = 8.94141%. [2019-01-29 09:57:36.601] [jointLog] [info] finished quantifyLibrary(). Analyzed 293 cells (100% of all).; [2019-01-29 09:57:40.090] [alevinLog] [info] Total 206902 UMI ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:923,Testability,Log,Logs,923,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:958,Testability,log,logs,958,"Just a bit more information:. I installed through conda salmon=0.11.3 and executed command on two different fastq files. The first one was on a single lane of the data and the second was on a concatenated file across 4 lanes. I managed to run the single lane file but got a seg dump error for the ""big""er file. Both times it seems to output the correct files. . Single lane:; ```; salmon alevin -l ISR -1 hgmm_100_S1_L001_001.fastq.1.gz -2 hgmm_100_S1_L001_001.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.81",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:6351,Testability,Log,Logs,6351,"9.107] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] Starting to make feature Matrix; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making regular featues; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making feature Matrix; [2019-01-29 09:55:59.123] [alevinLog] [info] Finished white listing; [2019-01-29 09:55:59.126] [alevinLog] [info] Finished optimizer; ``` . Concat fastq:; ```; salmon alevin -l ISR -1 big.fastq.1.gz -2 big.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:56:37.731] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:56:37.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 2 Million barcodes. [2019-01-29 09:56:43.029] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:56:43.029] [alevinLog] [info] # Barcodes Used: 2695632 / 2712324.; [2019-01-29 09:56:52.900] [alevinLog] [info] Knee found left boundary at 692 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Gauss Corrected Bounda",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:6386,Testability,log,logs,6386,"ovided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] Starting to make feature Matrix; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making regular featues; [2019-01-29 09:55:59.115] [alevinLog] [info] Done making feature Matrix; [2019-01-29 09:55:59.123] [alevinLog] [info] Finished white listing; [2019-01-29 09:55:59.126] [alevinLog] [info] Finished optimizer; ``` . Concat fastq:; ```; salmon alevin -l ISR -1 big.fastq.1.gz -2 big.fastq.2.gz --chromium -i geneset.dir/geneset_coding_exons.salmon.index/ -o salmon.dir/ --tgMap transcript2geneMap.tsv --dumpCsvCounts; Version Info: ### PLEASE UPGRADE SALMON ###; ### A newer version of Salmon with important bug fixes and improvements is available. ####; ###; The newest version, available at https://github.com/COMBINE-lab/salmon/releases; contains new features, improvements, and bug fixes; please upgrade at your; earliest convenience.; ###; Logs will be written to salmon.dir/logs; ### alevin (dscRNA-seq quantification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:56:37.731] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:56:37.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 2 Million barcodes. [2019-01-29 09:56:43.029] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:56:43.029] [alevinLog] [info] # Barcodes Used: 2695632 / 2712324.; [2019-01-29 09:56:52.900] [alevinLog] [info] Knee found left boundary at 692 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Gauss Corrected Boundary at 100 ; [2019-01-29 09:56:53.219] [alevin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:2023,Usability,Learn,Learned,2023,gram ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { hgmm_100_S1_L001_001.fastq.1.gz }; ### [ mates2 ] => { hgmm_100_S1_L001_001.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:54:57.898] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:54:57.916] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 0 Million barcodes. [2019-01-29 09:54:59.693] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:54:59.693] [alevinLog] [info] # Barcodes Used: 902561 / 912145.; [2019-01-29 09:55:04.490] [alevinLog] [info] Knee found left boundary at 391 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Gauss Corrected Boundary at 99 ; [2019-01-29 09:55:04.817] [alevinLog] [info] Learned InvCov: 114.535 normfactor: 147.323; [2019-01-29 09:55:04.817] [alevinLog] [info] Total 289(has 190 low confidence) barcodes; [2019-01-29 09:55:04.822] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:55:04.855] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:55:04.855] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:55:04.855] [alevinLog] [info] Total Unique barcodes found: 70316; [2019-01-29 09:55:04.855] [alevinLog] [info] Used Barcodes except Whitelist: 184; [2019-01-29 09:55:04.882] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:55:04.882] [alevinLog] [info] parsing read library format; [2019-01-29 09:55:05.014] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:55:04.882] [jointLog] [info] There is 1 library.; [2019-01-29 09:55:05.012] [jointLog] [info] Loading Quasi index; [2019-01-29 09:55:05.013] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:55:06.105] [stderrLog] [info] Load,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:4498,Usability,Clear,Clearing,4498,":41.122] [stderrLog] [info] Done loading index. processed 0 Million fragments; hits: 161433, hits per frag: 0.32698. [2019-01-29 09:55:54.788] [alevinLog] [info] Starting optimizer; [2019-01-29 09:55:54.742] [jointLog] [info] Computed 6,346 rich equivalence classes for further processing; [2019-01-29 09:55:54.742] [jointLog] [info] Counted 80,300 total reads in the equivalence classes ; [2019-01-29 09:55:54.754] [jointLog] [warning] Only 80300 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:55:54.754] [jointLog] [info] Mapping rate = 8.80342%. [2019-01-29 09:55:54.754] [jointLog] [info] finished quantifyLibrary(). Analyzed 289 cells (100% of all).; [2019-01-29 09:55:56.858] [alevinLog] [info] Total 72037 UMI after deduplicating.; [2019-01-29 09:55:56.858] [alevinLog] [warning] Skipped 151 barcodes due to No mapped read; [2019-01-29 09:55:56.876] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-01-29 09:55:56.917] [alevinLog] [info] Starting Import of the gene count matrix.; [2019-01-29 09:55:57.130] [alevinLog] [info] Done Importing gene count matrix for dimension 138x19879; [2019-01-29 09:55:57.130] [alevinLog] [info] Starting dumping cell v gene counts in csv format; 0.00215799	7.4911e-08	0.000194712	11697.8	; 0.00705206	1.19109e-07	30039.7	29692.8	; [2019-01-29 09:55:59.105] [alevinLog] [info] Finished dumping csv counts; [2019-01-29 09:55:59.106] [alevinLog] [info] Starting white listing; [2019-01-29 09:55:59.107] [alevinLog] [info] Done importing order of barcodes ""quants_mat_rows.txt"" file.; [2019-01-29 09:55:59.107] [alevinLog] [info] Total 138 barcodes found; [2019-01-29 09:55:59.107] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-01-29 09:55:59.107] [alevinLog] [info] St",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:7420,Usability,Learn,Learned,7420,ntification) v0.11.3; ### [ program ] => salmon ; ### [ command ] => alevin ; ### [ libType ] => { ISR }; ### [ mates1 ] => { big.fastq.1.gz }; ### [ mates2 ] => { big.fastq.2.gz }; ### [ chromium ] => { }; ### [ index ] => { geneset.dir/geneset_coding_exons.salmon.index/ }; ### [ output ] => { salmon.dir/ }; ### [ tgMap ] => { transcript2geneMap.tsv }; ### [ dumpCsvCounts ] => { }. [2019-01-29 09:56:37.731] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-01-29 09:56:37.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 2 Million barcodes. [2019-01-29 09:56:43.029] [alevinLog] [info] Done barcode density calculation.; [2019-01-29 09:56:43.029] [alevinLog] [info] # Barcodes Used: 2695632 / 2712324.; [2019-01-29 09:56:52.900] [alevinLog] [info] Knee found left boundary at 692 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Gauss Corrected Boundary at 100 ; [2019-01-29 09:56:53.219] [alevinLog] [info] Learned InvCov: 114.414 normfactor: 148.807; [2019-01-29 09:56:53.219] [alevinLog] [info] Total 293(has 193 low confidence) barcodes; [2019-01-29 09:56:53.224] [alevinLog] [info] Done True Barcode Sampling; [2019-01-29 09:56:53.254] [alevinLog] [info] Done populating Z matrix; [2019-01-29 09:56:53.255] [alevinLog] [info] Done indexing Barcodes; [2019-01-29 09:56:53.255] [alevinLog] [info] Total Unique barcodes found: 125401; [2019-01-29 09:56:53.255] [alevinLog] [info] Used Barcodes except Whitelist: 1256; [2019-01-29 09:56:53.281] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-01-29 09:56:53.281] [alevinLog] [info] parsing read library format; [2019-01-29 09:56:53.412] [stderrLog] [info] Loading Suffix Array ; [2019-01-29 09:56:53.281] [jointLog] [info] There is 1 library.; [2019-01-29 09:56:53.410] [jointLog] [info] Loading Quasi index; [2019-01-29 09:56:53.411] [jointLog] [info] Loading 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Lo,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722:9901,Usability,Clear,Clearing,9901,"ng 32-bit quasi index; [2019-01-29 09:56:54.551] [stderrLog] [info] Loading Transcript Info ; [2019-01-29 09:56:54.826] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-01-29 09:56:54.883] [stderrLog] [info] There were 80,511 set bits in the bit array; [2019-01-29 09:56:54.908] [stderrLog] [info] Computing transcript lengths; [2019-01-29 09:56:54.908] [stderrLog] [info] Waiting to finish loading hash; [2019-01-29 09:57:09.336] [stderrLog] [info] Done loading index; [2019-01-29 09:57:09.336] [jointLog] [info] done; [2019-01-29 09:57:09.336] [jointLog] [info] Index contained 80,511 targets. processed 2 Million fragments; hits: 812181, hits per frag: 0.326777. [2019-01-29 09:57:36.647] [alevinLog] [info] Starting optimizer; [2019-01-29 09:57:36.587] [jointLog] [info] Computed 12,933 rich equivalence classes for further processing; [2019-01-29 09:57:36.587] [jointLog] [info] Counted 242,520 total reads in the equivalence classes ; [2019-01-29 09:57:36.601] [jointLog] [warning] Only 242520 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2019-01-29 09:57:36.601] [jointLog] [info] Mapping rate = 8.94141%. [2019-01-29 09:57:36.601] [jointLog] [info] finished quantifyLibrary(). Analyzed 293 cells (100% of all).; [2019-01-29 09:57:40.090] [alevinLog] [info] Total 206902 UMI after deduplicating.; [2019-01-29 09:57:40.091] [alevinLog] [warning] Skipped 71 barcodes due to No mapped read; [2019-01-29 09:57:40.110] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-01-29 09:57:40.176] [alevinLog] [info] Starting Import of the gene count matrix.; [2019-01-29 09:57:41.168] [alevinLog] [info] Done Importing gene count matrix for dimension 222x19879; [2019-01-29 09:57:41.168] [alevinLog] [info] Starting dumping cell v gene counts in csv format; Segmentation fault (core dumped); ```. I then installed through conda salmon=0.12.0. Both times it failed with core dump.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458481722
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458998076:130,Availability,error,error,130,"So the plot thickens, it seems like this is only happening for certain data. Unfortunately I cant share the data that creates the error because its commercially sensitive (I don't have permission, which is annoying) so I am looking for other data that will reproduce the issue. I have just ran a huge 10X PBMC dataset and it ran without any errors, so its unlikely due to any memory issue. Will let you know when I have data that can recreate this error",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458998076
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458998076:341,Availability,error,errors,341,"So the plot thickens, it seems like this is only happening for certain data. Unfortunately I cant share the data that creates the error because its commercially sensitive (I don't have permission, which is annoying) so I am looking for other data that will reproduce the issue. I have just ran a huge 10X PBMC dataset and it ran without any errors, so its unlikely due to any memory issue. Will let you know when I have data that can recreate this error",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458998076
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458998076:448,Availability,error,error,448,"So the plot thickens, it seems like this is only happening for certain data. Unfortunately I cant share the data that creates the error because its commercially sensitive (I don't have permission, which is annoying) so I am looking for other data that will reproduce the issue. I have just ran a huge 10X PBMC dataset and it ran without any errors, so its unlikely due to any memory issue. Will let you know when I have data that can recreate this error",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-458998076
https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-466424466:156,Availability,error,error,156,It seems to be working now with lab-generated data. Can't seem to recreate the issue. So will close the issue until we find more data that returns the same error. Thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/337#issuecomment-466424466
https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-464872433:42,Integrability,protocol,protocol,42,Just here to upvote support for inDrop v2 protocol 👍,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-464872433
https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-474551107:314,Deployability,integrat,integrate,314,"I'm interested in this as well. Note that the (Harvard) inDrops v3 library protocol differs from v2, and is documented here: https://iccb.med.harvard.edu/files/iccb/files/sequencing_indrops_libraries_02_28_18.pdf. For reference, bcbio supports inDrops and Dropseq barcodes, but it'd be great it if we could better integrate this workflow with alevin: https://bcbio-nextgen.readthedocs.io/en/latest/contents/pipelines.html",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-474551107
https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-474551107:407,Deployability,pipeline,pipelines,407,"I'm interested in this as well. Note that the (Harvard) inDrops v3 library protocol differs from v2, and is documented here: https://iccb.med.harvard.edu/files/iccb/files/sequencing_indrops_libraries_02_28_18.pdf. For reference, bcbio supports inDrops and Dropseq barcodes, but it'd be great it if we could better integrate this workflow with alevin: https://bcbio-nextgen.readthedocs.io/en/latest/contents/pipelines.html",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-474551107
https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-474551107:75,Integrability,protocol,protocol,75,"I'm interested in this as well. Note that the (Harvard) inDrops v3 library protocol differs from v2, and is documented here: https://iccb.med.harvard.edu/files/iccb/files/sequencing_indrops_libraries_02_28_18.pdf. For reference, bcbio supports inDrops and Dropseq barcodes, but it'd be great it if we could better integrate this workflow with alevin: https://bcbio-nextgen.readthedocs.io/en/latest/contents/pipelines.html",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-474551107
https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-474551107:314,Integrability,integrat,integrate,314,"I'm interested in this as well. Note that the (Harvard) inDrops v3 library protocol differs from v2, and is documented here: https://iccb.med.harvard.edu/files/iccb/files/sequencing_indrops_libraries_02_28_18.pdf. For reference, bcbio supports inDrops and Dropseq barcodes, but it'd be great it if we could better integrate this workflow with alevin: https://bcbio-nextgen.readthedocs.io/en/latest/contents/pipelines.html",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/339#issuecomment-474551107
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458713282:62,Testability,test,test,62,not sure what's the best way to share the 34G fastqs for your test.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458713282
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458718943:574,Testability,log,log,574,"I think I see the issue, and we might not actually need the data.; The issue being the list of CBs in `whitelist.txt` is too many. It seems the data is very noisy and `knee` finding algorithm is failing. I'd suggest to use `--dumpFeatures` flag with alevin (you can also do `--noQuant` to stop alevin before mapping). This flag will generate `raw_cb_frequency.txt` file i.e. the frequency of all the observed CB and making the histogram will help visually find if there is possible knee in the data, generally it is in a real experiment. re: the mapping rate, in the alevin log you'll observe there were 50% of the CB which were thrown away if you use `forceCells 3000` w/o whitelist, basically alevin is taking top `3000` cells into consideration and throwing everything away. While the rise in mapping rate when externally provided with whitelist is actually by allowing more CB to go through. The two options `forceCells` and `whitelist` wan't intended to be use simultaneously because if provided with external whitelist alevin assumes the user is confident with the set of CBs, in your case this list is huge (~700k). Are you using the 10x whitelist from their website? If yes, then it's not what alevin expect with `--whitelist` option. One work around here would be to look at the CB frequency histogram and making a file with only the CB sequences which you thing are above knee. Providing that file as the `--whitelist` is the intended use case for alevin. Hope this makes sense.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458718943
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458788769:34,Availability,down,downloaded,34,"yes, I am using the 10x whitelist downloaded from their website. Thanks for clarifying it. so should I not use --whitelist at all and let alevin determine what BCs to use? 50% of reads throwing away seems to be too much. what percentage do you observe? Thanks! without whitelist and use --expectCells 3000 gives me error] Can't find right Boundary. I should do --dumpFeatures and --noQuant to get the whitelist first?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458788769
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458788769:315,Availability,error,error,315,"yes, I am using the 10x whitelist downloaded from their website. Thanks for clarifying it. so should I not use --whitelist at all and let alevin determine what BCs to use? 50% of reads throwing away seems to be too much. what percentage do you observe? Thanks! without whitelist and use --expectCells 3000 gives me error] Can't find right Boundary. I should do --dumpFeatures and --noQuant to get the whitelist first?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458788769
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458793327:188,Deployability,pipeline,pipeline,188,"Hi @crazyhottommy ,. Reading a bit in https://github.com/COMBINE-lab/salmon/issues/245 and https://github.com/COMBINE-lab/salmon/issues/284 would definitely help understand more about the pipeline. _summary:_ it's possible the 50% mapping read is due to alevin throwing away CB. In this case, we normally suggest to try running alevin in the following order:; 1.) `--expectCells X`: alevin will look for local knee threshold near to top X barcodes. Alevin can still here based on the CB frequency distribution.; 2.) `--forceCells X`: run alevin with `--noQuant --dumpFeature` mode and extract the frequency histogram of the CBs present in the histogram. Try to manually found knee in the descending sorted CB frequency histogram and figure out X. alevin will use top X barcodes as specified by the user.; 3.) If there is other tools like `cellranger` already been run on the data, then alevin can directly consume their predicted CB sequence using `--whitelist` option. This is different file, not the 727k file from 10x, and is different for each experiment .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458793327
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458793327:920,Safety,predict,predicted,920,"Hi @crazyhottommy ,. Reading a bit in https://github.com/COMBINE-lab/salmon/issues/245 and https://github.com/COMBINE-lab/salmon/issues/284 would definitely help understand more about the pipeline. _summary:_ it's possible the 50% mapping read is due to alevin throwing away CB. In this case, we normally suggest to try running alevin in the following order:; 1.) `--expectCells X`: alevin will look for local knee threshold near to top X barcodes. Alevin can still here based on the CB frequency distribution.; 2.) `--forceCells X`: run alevin with `--noQuant --dumpFeature` mode and extract the frequency histogram of the CBs present in the histogram. Try to manually found knee in the descending sorted CB frequency histogram and figure out X. alevin will use top X barcodes as specified by the user.; 3.) If there is other tools like `cellranger` already been run on the data, then alevin can directly consume their predicted CB sequence using `--whitelist` option. This is different file, not the 727k file from 10x, and is different for each experiment .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-458793327
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:122,Deployability,pipeline,pipeline,122,"Hi @k3yavi ,. I'm using alevin to process 10X V3 data and encountered similar problem with this issue. I've tried run the pipeline using default whitelisting by alevin, `--whitelist barcode.txt` which from cellranger v.3.1.0 run (including 7938 barcodes), `--expectCells 10000`, and `--expectCells 30000`. But no matter how I change the parameter, the log shows that there are always about 50% percent reads has been thrown away, and the mapping rate was between 18.7%-19.1%. . the salmon version is `salmon 1.4.0`; the reference genome is sequenced by ourselves, and it's a plant.; my reads layout is paired end 150bp, . > R1: ; @A00582:424:HJYLGDSXY:3:1101:1090:1000 1:N:0:ACCGGCTC; TAACCAGGTCGAGTGAGTATTTAAGGCGCGCGGCGCACCAACGCACTCCCAACAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA; > +; FFFFFFFFFFFFFFFFFFFFFFFFFFFF,,:,,FF:,,,::FF,,:F,,,,,,F:,,,:,::FF::::::,FFF:F:FF:FFFFFFF::FF::FF,F:F:FF:F,FFFF,:FF,FFFFF:,FF:::FF:FFF:FF:FF:FFFFFFFFFF:; > R2:; @A00582:424:HJYLGDSXY:3:1101:1090:1000 2:N:0:ACCGGCTC; NCCTAGAAGCAGCCACCCTTGAAAGAGTGCGTAATAGCTCACTGATCGAGCGCTCTTGCGCCGAAGATGAACGGGGCTAAGCGATCTGCCGAAGCTGTGGGATGTAAAAATACATCGGTAGGGGAGCGTTCCGCCTTAGAGAGAAGCCTC; > +; #FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFF::FFFFFFFFFF:FFFFFFFFFFFFF:FFFFFF:FFFFFFF:. Here is the logs. ## Default setting ; `salmon alevin -l ISR -1 ../clean/sample_S1_L001_R1_001.fastq -2 ../clean/sample_S1_L001_R2_001.fastq --chromiumV3 -i ../../dna/00.ref_genome/sample/salmon_index_allmRNA -p 40 -o test_alevin_allmRNA --tgMap ../../dna/00.ref_genome/sample/alltxp2gene.tsv`. > [2021-01-25 16:22:09.565] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-25 16:22:09.615] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-25 16:22:09.620] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-25 16:22:09.631] [alevinLog] [info",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:8729,Deployability,pipeline,pipeline,8729,"5.251] [alevinLog] [info] Done True Barcode Sampling; [2021-01-23 11:07:56.200] [alevinLog] [info] Total **49.0191% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-23 11:07:57.144] [alevinLog] [info] Done populating Z matrix; [2021-01-23 11:07:57.172] [alevinLog] [info] Total 35787 CB got sequence corrected; [2021-01-23 11:07:57.177] [alevinLog] [info] Done indexing Barcodes; [2021-01-23 11:07:57.177] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-23 11:07:57.177] [alevinLog] [info] Used Barcodes except Whitelist: 35219; [2021-01-23 11:07:57.360] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; .... > {; ""total_reads"": 188934609,; ""reads_with_N"": 0,; ""noisy_cb_reads"": 92614076,; ""noisy_umi_reads"": 17028,; ""used_reads"": 96303505,; ""mapping_rate"": 19.451325087824434,; ""reads_in_eqclasses"": 36750285,; ""total_cbs"": 3896665,; ""used_cbs"": 47725,; ""initial_whitelist"": 11511,; ""low_conf_cbs"": 995,; ""num_features"": 5,; ""no_read_mapping_cbs"": 70,; ""final_num_cbs"": 8324,; ""deduplicated_umis"": 19613485,; ""mean_umis_per_cell"": 2356,; ""mean_genes_per_cell"": 1120; }. ## I'm wondering that ; 1. how can I use as much reads as possible and improve the mapping rate.; 2. will the 150bp reads R1 affect the pipeline, and if it will, how can I make it to 28bp. By the way, the cellranger result shows that reads map to Transcriptome is low, but reads mapped to Genome is 85%. Reads Mapped to Genome | 85.2%; -- | --; Reads Mapped Confidently to Genome | 45.8%; Reads Mapped Confidently to Intergenic Regions | 11.0%; Reads Mapped Confidently to Intronic Regions | 4.2%; Reads Mapped Confidently to Exonic Regions | 30.6%; Reads Mapped Confidently to Transcriptome | 25.3%; Reads Mapped Antisense to Gene | 0.9%. Estimated Number of Cells | 7,938; -- | --; Fraction Reads in Cells | 73.1%; Mean Reads per Cell | 23,801; Median Genes per Cell | 1,076; Total Genes Detected | 17,492; Median UMI Counts per Cell | 2,155. Best wishes,; Matthew",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:3392,Performance,optimiz,optimizer,3392,"ng it; [2021-01-25 16:27:05.707] [alevinLog] [info] Learned InvCov: 556.394 normfactor: 9159.58; [2021-01-25 16:27:05.707] [alevinLog] [info] Total 222(has 201 low confidence) barcodes; [2021-01-25 16:27:06.573] [alevinLog] [info] Done True Barcode Sampling; [2021-01-25 16:27:07.383] [alevinLog] [warning] Total **96.7029% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-25 16:27:07.412] [alevinLog] [info] Done populating Z matrix; [2021-01-25 16:27:07.414] [alevinLog] [info] Total 3667 CB got sequence corrected; [2021-01-25 16:27:07.414] [alevinLog] [info] Done indexing Barcodes; [2021-01-25 16:27:07.414] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-25 16:27:07.414] [alevinLog] [info] Used Barcodes except Whitelist: 3667; [2021-01-25 16:27:07.498] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-25 16:27:07.498] [alevinLog] [info] parsing read library format; [2021-01-25 16:30:54.542] [alevinLog] [info] Starting optimizer; [2021-01-25 16:30:54.782] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2021-01-25 16:30:54.782] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 1350278.00 UMI after deduplicating.; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 30909 BiDirected Edges.; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 8817 UniDirected Edges.; [2021-01-25 16:30:55.969] [alevinLog] [info] Clearing EqMap; Might take some time.; [2021-01-25 16:30:56.294] [alevinLog] [warning] Num High confidence barcodes too less 20 < 90.Can't performing whitelisting; Skipping; [2021-01-25 16:30:56.297] [alevinLog] [info] Finished optimizer. ## with `--exceptCells 7000`; > [2021-01-21 09:24:45.891] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-21 09:24:45.942] [alevinLog] [info] Filled with 43030 txp to gene ent",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:4055,Performance,perform,performing,4055,"6665; [2021-01-25 16:27:07.414] [alevinLog] [info] Used Barcodes except Whitelist: 3667; [2021-01-25 16:27:07.498] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-25 16:27:07.498] [alevinLog] [info] parsing read library format; [2021-01-25 16:30:54.542] [alevinLog] [info] Starting optimizer; [2021-01-25 16:30:54.782] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2021-01-25 16:30:54.782] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 1350278.00 UMI after deduplicating.; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 30909 BiDirected Edges.; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 8817 UniDirected Edges.; [2021-01-25 16:30:55.969] [alevinLog] [info] Clearing EqMap; Might take some time.; [2021-01-25 16:30:56.294] [alevinLog] [warning] Num High confidence barcodes too less 20 < 90.Can't performing whitelisting; Skipping; [2021-01-25 16:30:56.297] [alevinLog] [info] Finished optimizer. ## with `--exceptCells 7000`; > [2021-01-21 09:24:45.891] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-21 09:24:45.942] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-21 09:24:45.947] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-21 09:24:45.967] [alevinLog] [info] Processing barcodes files (if Present); [2021-01-21 09:33:35.885] [alevinLog] [info] Done barcode density calculation.; [2021-01-21 09:33:35.885] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-21 09:33:37.337] [alevinLog] [info] Total 10016(has 1000 low confidence) barcodes; [2021-01-21 09:33:38.202] [alevinLog] [info] Done True Barcode Sampling; [2021-01-21 09:33:39.137] [alevinLog] [warning] Total **52.0343% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-21 09:33:39.960] [alevinLog] [info] Done p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:4144,Performance,optimiz,optimizer,4144,"sed Barcodes except Whitelist: 3667; [2021-01-25 16:27:07.498] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-25 16:27:07.498] [alevinLog] [info] parsing read library format; [2021-01-25 16:30:54.542] [alevinLog] [info] Starting optimizer; [2021-01-25 16:30:54.782] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2021-01-25 16:30:54.782] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 1350278.00 UMI after deduplicating.; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 30909 BiDirected Edges.; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 8817 UniDirected Edges.; [2021-01-25 16:30:55.969] [alevinLog] [info] Clearing EqMap; Might take some time.; [2021-01-25 16:30:56.294] [alevinLog] [warning] Num High confidence barcodes too less 20 < 90.Can't performing whitelisting; Skipping; [2021-01-25 16:30:56.297] [alevinLog] [info] Finished optimizer. ## with `--exceptCells 7000`; > [2021-01-21 09:24:45.891] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-21 09:24:45.942] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-21 09:24:45.947] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-21 09:24:45.967] [alevinLog] [info] Processing barcodes files (if Present); [2021-01-21 09:33:35.885] [alevinLog] [info] Done barcode density calculation.; [2021-01-21 09:33:35.885] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-21 09:33:37.337] [alevinLog] [info] Total 10016(has 1000 low confidence) barcodes; [2021-01-21 09:33:38.202] [alevinLog] [info] Done True Barcode Sampling; [2021-01-21 09:33:39.137] [alevinLog] [warning] Total **52.0343% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-21 09:33:39.960] [alevinLog] [info] Done populating Z matrix; [2021-01-21 09:33:39.989] [alevin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:5643,Performance,optimiz,optimizer,5643,ion.; [2021-01-21 09:33:35.885] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-21 09:33:37.337] [alevinLog] [info] Total 10016(has 1000 low confidence) barcodes; [2021-01-21 09:33:38.202] [alevinLog] [info] Done True Barcode Sampling; [2021-01-21 09:33:39.137] [alevinLog] [warning] Total **52.0343% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-21 09:33:39.960] [alevinLog] [info] Done populating Z matrix; [2021-01-21 09:33:39.989] [alevinLog] [info] Total 34923 CB got sequence corrected; [2021-01-21 09:33:39.994] [alevinLog] [info] Done indexing Barcodes; [2021-01-21 09:33:39.994] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-21 09:33:39.994] [alevinLog] [info] Used Barcodes except Whitelist: 34503; [2021-01-21 09:33:40.718] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-21 09:33:40.718] [alevinLog] [info] parsing read library format; [2021-01-21 09:48:11.430] [alevinLog] [info] Starting optimizer; [2021-01-21 09:48:12.160] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2021-01-21 09:48:12.160] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 19031525.00 UMI after deduplicating.; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 454402 BiDirected Edges.; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 113688 UniDirected Edges.; [2021-01-21 09:48:36.288] [alevinLog] [warning] Skipped 44 barcodes due to No mapped read; [2021-01-21 09:48:36.307] [alevinLog] [info] Clearing EqMap; Might take some time.; [2021-01-21 09:48:41.314] [alevinLog] [info] Starting white listing of 9971 cells; [2021-01-21 09:48:41.314] [alevinLog] [info] Starting to make feature Matrix; [2021-01-21 09:48:41.337] [alevinLog] [info] Done making feature Matrix; [2021-01-21 09:48:41.557] [alevinLog] [info] Finished white listing; [2021-01-21 09:48:41.580] [alevinLog] [,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:6658,Performance,optimiz,optimizer,6658,"imizer; [2021-01-21 09:48:12.160] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2021-01-21 09:48:12.160] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 19031525.00 UMI after deduplicating.; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 454402 BiDirected Edges.; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 113688 UniDirected Edges.; [2021-01-21 09:48:36.288] [alevinLog] [warning] Skipped 44 barcodes due to No mapped read; [2021-01-21 09:48:36.307] [alevinLog] [info] Clearing EqMap; Might take some time.; [2021-01-21 09:48:41.314] [alevinLog] [info] Starting white listing of 9971 cells; [2021-01-21 09:48:41.314] [alevinLog] [info] Starting to make feature Matrix; [2021-01-21 09:48:41.337] [alevinLog] [info] Done making feature Matrix; [2021-01-21 09:48:41.557] [alevinLog] [info] Finished white listing; [2021-01-21 09:48:41.580] [alevinLog] [info] Finished optimizer. > {; ""total_reads"": 188934609,; ""reads_with_N"": 0,; ""noisy_cb_reads"": 98310747,; ""noisy_umi_reads"": 16600,; ""used_reads"": 90607262,; ""mapping_rate"": 18.89108045842464,; ""reads_in_eqclasses"": 35691789,; ""total_cbs"": 3896665,; ""used_cbs"": 44518,; ""initial_whitelist"": 9015,; ""low_conf_cbs"": 1000,; ""num_features"": 5,; ""no_read_mapping_cbs"": 44,; ""final_num_cbs"": 6765,; ""deduplicated_umis"": 19031525,; ""mean_umis_per_cell"": 2813,; ""mean_genes_per_cell"": 1315; }. ## My best result with `--exceptCells 30000`; > ...; [2021-01-23 11:07:52.910] [alevinLog] [info] Done barcode density calculation.; [2021-01-23 11:07:52.910] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-23 11:07:54.387] [alevinLog] [info] Total 12507(has 995 low confidence) barcodes; [2021-01-23 11:07:55.251] [alevinLog] [info] Done True Barcode Sampling; [2021-01-23 11:07:56.200] [alevinLog] [info] Total **49.0191% reads will be thrown away** because of noisy Cellular barcodes.; [2021-0",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:2340,Safety,Predict,Prediction,2340,". Here is the logs. ## Default setting ; `salmon alevin -l ISR -1 ../clean/sample_S1_L001_R1_001.fastq -2 ../clean/sample_S1_L001_R2_001.fastq --chromiumV3 -i ../../dna/00.ref_genome/sample/salmon_index_allmRNA -p 40 -o test_alevin_allmRNA --tgMap ../../dna/00.ref_genome/sample/alltxp2gene.tsv`. > [2021-01-25 16:22:09.565] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-25 16:22:09.615] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-25 16:22:09.620] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-25 16:22:09.631] [alevinLog] [info] Processing barcodes files (if Present); [2021-01-25 16:26:35.067] [alevinLog] [info] Done barcode density calculation.; [2021-01-25 16:26:35.067] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-25 16:26:42.979] [alevinLog] [info] Knee found left boundary at 21; [2021-01-25 16:27:05.707] [alevinLog] [warning] Gauss Prediction 4969 Too far from knee prediction skipping it; [2021-01-25 16:27:05.707] [alevinLog] [info] Learned InvCov: 556.394 normfactor: 9159.58; [2021-01-25 16:27:05.707] [alevinLog] [info] Total 222(has 201 low confidence) barcodes; [2021-01-25 16:27:06.573] [alevinLog] [info] Done True Barcode Sampling; [2021-01-25 16:27:07.383] [alevinLog] [warning] Total **96.7029% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-25 16:27:07.412] [alevinLog] [info] Done populating Z matrix; [2021-01-25 16:27:07.414] [alevinLog] [info] Total 3667 CB got sequence corrected; [2021-01-25 16:27:07.414] [alevinLog] [info] Done indexing Barcodes; [2021-01-25 16:27:07.414] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-25 16:27:07.414] [alevinLog] [info] Used Barcodes except Whitelist: 3667; [2021-01-25 16:27:07.498] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-25 16:27:07.498] [alevinLog] [info] parsing read library format; [2021-01-25 16:30:54.542]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:2374,Safety,predict,prediction,2374,". Here is the logs. ## Default setting ; `salmon alevin -l ISR -1 ../clean/sample_S1_L001_R1_001.fastq -2 ../clean/sample_S1_L001_R2_001.fastq --chromiumV3 -i ../../dna/00.ref_genome/sample/salmon_index_allmRNA -p 40 -o test_alevin_allmRNA --tgMap ../../dna/00.ref_genome/sample/alltxp2gene.tsv`. > [2021-01-25 16:22:09.565] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-25 16:22:09.615] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-25 16:22:09.620] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-25 16:22:09.631] [alevinLog] [info] Processing barcodes files (if Present); [2021-01-25 16:26:35.067] [alevinLog] [info] Done barcode density calculation.; [2021-01-25 16:26:35.067] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-25 16:26:42.979] [alevinLog] [info] Knee found left boundary at 21; [2021-01-25 16:27:05.707] [alevinLog] [warning] Gauss Prediction 4969 Too far from knee prediction skipping it; [2021-01-25 16:27:05.707] [alevinLog] [info] Learned InvCov: 556.394 normfactor: 9159.58; [2021-01-25 16:27:05.707] [alevinLog] [info] Total 222(has 201 low confidence) barcodes; [2021-01-25 16:27:06.573] [alevinLog] [info] Done True Barcode Sampling; [2021-01-25 16:27:07.383] [alevinLog] [warning] Total **96.7029% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-25 16:27:07.412] [alevinLog] [info] Done populating Z matrix; [2021-01-25 16:27:07.414] [alevinLog] [info] Total 3667 CB got sequence corrected; [2021-01-25 16:27:07.414] [alevinLog] [info] Done indexing Barcodes; [2021-01-25 16:27:07.414] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-25 16:27:07.414] [alevinLog] [info] Used Barcodes except Whitelist: 3667; [2021-01-25 16:27:07.498] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-25 16:27:07.498] [alevinLog] [info] parsing read library format; [2021-01-25 16:30:54.542]",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:9383,Safety,Detect,Detected,9383,"5.251] [alevinLog] [info] Done True Barcode Sampling; [2021-01-23 11:07:56.200] [alevinLog] [info] Total **49.0191% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-23 11:07:57.144] [alevinLog] [info] Done populating Z matrix; [2021-01-23 11:07:57.172] [alevinLog] [info] Total 35787 CB got sequence corrected; [2021-01-23 11:07:57.177] [alevinLog] [info] Done indexing Barcodes; [2021-01-23 11:07:57.177] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-23 11:07:57.177] [alevinLog] [info] Used Barcodes except Whitelist: 35219; [2021-01-23 11:07:57.360] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; .... > {; ""total_reads"": 188934609,; ""reads_with_N"": 0,; ""noisy_cb_reads"": 92614076,; ""noisy_umi_reads"": 17028,; ""used_reads"": 96303505,; ""mapping_rate"": 19.451325087824434,; ""reads_in_eqclasses"": 36750285,; ""total_cbs"": 3896665,; ""used_cbs"": 47725,; ""initial_whitelist"": 11511,; ""low_conf_cbs"": 995,; ""num_features"": 5,; ""no_read_mapping_cbs"": 70,; ""final_num_cbs"": 8324,; ""deduplicated_umis"": 19613485,; ""mean_umis_per_cell"": 2356,; ""mean_genes_per_cell"": 1120; }. ## I'm wondering that ; 1. how can I use as much reads as possible and improve the mapping rate.; 2. will the 150bp reads R1 affect the pipeline, and if it will, how can I make it to 28bp. By the way, the cellranger result shows that reads map to Transcriptome is low, but reads mapped to Genome is 85%. Reads Mapped to Genome | 85.2%; -- | --; Reads Mapped Confidently to Genome | 45.8%; Reads Mapped Confidently to Intergenic Regions | 11.0%; Reads Mapped Confidently to Intronic Regions | 4.2%; Reads Mapped Confidently to Exonic Regions | 30.6%; Reads Mapped Confidently to Transcriptome | 25.3%; Reads Mapped Antisense to Gene | 0.9%. Estimated Number of Cells | 7,938; -- | --; Fraction Reads in Cells | 73.1%; Mean Reads per Cell | 23,801; Median Genes per Cell | 1,076; Total Genes Detected | 17,492; Median UMI Counts per Cell | 2,155. Best wishes,; Matthew",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:352,Testability,log,log,352,"Hi @k3yavi ,. I'm using alevin to process 10X V3 data and encountered similar problem with this issue. I've tried run the pipeline using default whitelisting by alevin, `--whitelist barcode.txt` which from cellranger v.3.1.0 run (including 7938 barcodes), `--expectCells 10000`, and `--expectCells 30000`. But no matter how I change the parameter, the log shows that there are always about 50% percent reads has been thrown away, and the mapping rate was between 18.7%-19.1%. . the salmon version is `salmon 1.4.0`; the reference genome is sequenced by ourselves, and it's a plant.; my reads layout is paired end 150bp, . > R1: ; @A00582:424:HJYLGDSXY:3:1101:1090:1000 1:N:0:ACCGGCTC; TAACCAGGTCGAGTGAGTATTTAAGGCGCGCGGCGCACCAACGCACTCCCAACAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA; > +; FFFFFFFFFFFFFFFFFFFFFFFFFFFF,,:,,FF:,,,::FF,,:F,,,,,,F:,,,:,::FF::::::,FFF:F:FF:FFFFFFF::FF::FF,F:F:FF:F,FFFF,:FF,FFFFF:,FF:::FF:FFF:FF:FF:FFFFFFFFFF:; > R2:; @A00582:424:HJYLGDSXY:3:1101:1090:1000 2:N:0:ACCGGCTC; NCCTAGAAGCAGCCACCCTTGAAAGAGTGCGTAATAGCTCACTGATCGAGCGCTCTTGCGCCGAAGATGAACGGGGCTAAGCGATCTGCCGAAGCTGTGGGATGTAAAAATACATCGGTAGGGGAGCGTTCCGCCTTAGAGAGAAGCCTC; > +; #FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFF::FFFFFFFFFF:FFFFFFFFFFFFF:FFFFFF:FFFFFFF:. Here is the logs. ## Default setting ; `salmon alevin -l ISR -1 ../clean/sample_S1_L001_R1_001.fastq -2 ../clean/sample_S1_L001_R2_001.fastq --chromiumV3 -i ../../dna/00.ref_genome/sample/salmon_index_allmRNA -p 40 -o test_alevin_allmRNA --tgMap ../../dna/00.ref_genome/sample/alltxp2gene.tsv`. > [2021-01-25 16:22:09.565] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-25 16:22:09.615] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-25 16:22:09.620] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-25 16:22:09.631] [alevinLog] [info",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:1377,Testability,log,logs,1377,"are always about 50% percent reads has been thrown away, and the mapping rate was between 18.7%-19.1%. . the salmon version is `salmon 1.4.0`; the reference genome is sequenced by ourselves, and it's a plant.; my reads layout is paired end 150bp, . > R1: ; @A00582:424:HJYLGDSXY:3:1101:1090:1000 1:N:0:ACCGGCTC; TAACCAGGTCGAGTGAGTATTTAAGGCGCGCGGCGCACCAACGCACTCCCAACAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA; > +; FFFFFFFFFFFFFFFFFFFFFFFFFFFF,,:,,FF:,,,::FF,,:F,,,,,,F:,,,:,::FF::::::,FFF:F:FF:FFFFFFF::FF::FF,F:F:FF:F,FFFF,:FF,FFFFF:,FF:::FF:FFF:FF:FF:FFFFFFFFFF:; > R2:; @A00582:424:HJYLGDSXY:3:1101:1090:1000 2:N:0:ACCGGCTC; NCCTAGAAGCAGCCACCCTTGAAAGAGTGCGTAATAGCTCACTGATCGAGCGCTCTTGCGCCGAAGATGAACGGGGCTAAGCGATCTGCCGAAGCTGTGGGATGTAAAAATACATCGGTAGGGGAGCGTTCCGCCTTAGAGAGAAGCCTC; > +; #FF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFFFFFFFFFF::FFFFFFFFFF:FFFFFFFFFFFFF:FFFFFF:FFFFFFF:. Here is the logs. ## Default setting ; `salmon alevin -l ISR -1 ../clean/sample_S1_L001_R1_001.fastq -2 ../clean/sample_S1_L001_R2_001.fastq --chromiumV3 -i ../../dna/00.ref_genome/sample/salmon_index_allmRNA -p 40 -o test_alevin_allmRNA --tgMap ../../dna/00.ref_genome/sample/alltxp2gene.tsv`. > [2021-01-25 16:22:09.565] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-25 16:22:09.615] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-25 16:22:09.620] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-25 16:22:09.631] [alevinLog] [info] Processing barcodes files (if Present); [2021-01-25 16:26:35.067] [alevinLog] [info] Done barcode density calculation.; [2021-01-25 16:26:35.067] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-25 16:26:42.979] [alevinLog] [info] Knee found left boundary at 21; [2021-01-25 16:27:05.707] [alevinLog] [warning] Gauss Prediction 4969 Too far from knee",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:2443,Usability,Learn,Learned,2443,"ple_S1_L001_R1_001.fastq -2 ../clean/sample_S1_L001_R2_001.fastq --chromiumV3 -i ../../dna/00.ref_genome/sample/salmon_index_allmRNA -p 40 -o test_alevin_allmRNA --tgMap ../../dna/00.ref_genome/sample/alltxp2gene.tsv`. > [2021-01-25 16:22:09.565] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-25 16:22:09.615] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-25 16:22:09.620] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-25 16:22:09.631] [alevinLog] [info] Processing barcodes files (if Present); [2021-01-25 16:26:35.067] [alevinLog] [info] Done barcode density calculation.; [2021-01-25 16:26:35.067] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-25 16:26:42.979] [alevinLog] [info] Knee found left boundary at 21; [2021-01-25 16:27:05.707] [alevinLog] [warning] Gauss Prediction 4969 Too far from knee prediction skipping it; [2021-01-25 16:27:05.707] [alevinLog] [info] Learned InvCov: 556.394 normfactor: 9159.58; [2021-01-25 16:27:05.707] [alevinLog] [info] Total 222(has 201 low confidence) barcodes; [2021-01-25 16:27:06.573] [alevinLog] [info] Done True Barcode Sampling; [2021-01-25 16:27:07.383] [alevinLog] [warning] Total **96.7029% reads will be thrown away** because of noisy Cellular barcodes.; [2021-01-25 16:27:07.412] [alevinLog] [info] Done populating Z matrix; [2021-01-25 16:27:07.414] [alevinLog] [info] Total 3667 CB got sequence corrected; [2021-01-25 16:27:07.414] [alevinLog] [info] Done indexing Barcodes; [2021-01-25 16:27:07.414] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-25 16:27:07.414] [alevinLog] [info] Used Barcodes except Whitelist: 3667; [2021-01-25 16:27:07.498] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-25 16:27:07.498] [alevinLog] [info] parsing read library format; [2021-01-25 16:30:54.542] [alevinLog] [info] Starting optimizer; [2021-01-25 16:30:54.782] [alevinLog] ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:3916,Usability,Clear,Clearing,3916," corrected; [2021-01-25 16:27:07.414] [alevinLog] [info] Done indexing Barcodes; [2021-01-25 16:27:07.414] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-25 16:27:07.414] [alevinLog] [info] Used Barcodes except Whitelist: 3667; [2021-01-25 16:27:07.498] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-25 16:27:07.498] [alevinLog] [info] parsing read library format; [2021-01-25 16:30:54.542] [alevinLog] [info] Starting optimizer; [2021-01-25 16:30:54.782] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2021-01-25 16:30:54.782] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 1350278.00 UMI after deduplicating.; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 30909 BiDirected Edges.; [2021-01-25 16:30:55.950] [alevinLog] [info] Total 8817 UniDirected Edges.; [2021-01-25 16:30:55.969] [alevinLog] [info] Clearing EqMap; Might take some time.; [2021-01-25 16:30:56.294] [alevinLog] [warning] Num High confidence barcodes too less 20 < 90.Can't performing whitelisting; Skipping; [2021-01-25 16:30:56.297] [alevinLog] [info] Finished optimizer. ## with `--exceptCells 7000`; > [2021-01-21 09:24:45.891] [alevinLog] [info] Found 43030 transcripts(+0 decoys, +0 short and +0 duplicate names in the index); [2021-01-21 09:24:45.942] [alevinLog] [info] Filled with 43030 txp to gene entries; [2021-01-21 09:24:45.947] [alevinLog] [info] Found all transcripts to gene mappings; [2021-01-21 09:24:45.967] [alevinLog] [info] Processing barcodes files (if Present); [2021-01-21 09:33:35.885] [alevinLog] [info] Done barcode density calculation.; [2021-01-21 09:33:35.885] [alevinLog] [info] # Barcodes Used: 188934609 / 188934609.; [2021-01-21 09:33:37.337] [alevinLog] [info] Total 10016(has 1000 low confidence) barcodes; [2021-01-21 09:33:38.202] [alevinLog] [info] Done True Barcode Sampling; [2021-01-21 09:33:39.137",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567:6262,Usability,Clear,Clearing,6262,"33:39.994] [alevinLog] [info] Total Unique barcodes found: 3896665; [2021-01-21 09:33:39.994] [alevinLog] [info] Used Barcodes except Whitelist: 34503; [2021-01-21 09:33:40.718] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify; [2021-01-21 09:33:40.718] [alevinLog] [info] parsing read library format; [2021-01-21 09:48:11.430] [alevinLog] [info] Starting optimizer; [2021-01-21 09:48:12.160] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2021-01-21 09:48:12.160] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 19031525.00 UMI after deduplicating.; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 454402 BiDirected Edges.; [2021-01-21 09:48:36.288] [alevinLog] [info] Total 113688 UniDirected Edges.; [2021-01-21 09:48:36.288] [alevinLog] [warning] Skipped 44 barcodes due to No mapped read; [2021-01-21 09:48:36.307] [alevinLog] [info] Clearing EqMap; Might take some time.; [2021-01-21 09:48:41.314] [alevinLog] [info] Starting white listing of 9971 cells; [2021-01-21 09:48:41.314] [alevinLog] [info] Starting to make feature Matrix; [2021-01-21 09:48:41.337] [alevinLog] [info] Done making feature Matrix; [2021-01-21 09:48:41.557] [alevinLog] [info] Finished white listing; [2021-01-21 09:48:41.580] [alevinLog] [info] Finished optimizer. > {; ""total_reads"": 188934609,; ""reads_with_N"": 0,; ""noisy_cb_reads"": 98310747,; ""noisy_umi_reads"": 16600,; ""used_reads"": 90607262,; ""mapping_rate"": 18.89108045842464,; ""reads_in_eqclasses"": 35691789,; ""total_cbs"": 3896665,; ""used_cbs"": 44518,; ""initial_whitelist"": 9015,; ""low_conf_cbs"": 1000,; ""num_features"": 5,; ""no_read_mapping_cbs"": 44,; ""final_num_cbs"": 6765,; ""deduplicated_umis"": 19031525,; ""mean_umis_per_cell"": 2813,; ""mean_genes_per_cell"": 1315; }. ## My best result with `--exceptCells 30000`; > ...; [2021-01-23 11:07:52.910] [alevinLog] [info] Done barcode density calculation.; [2021-",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/340#issuecomment-766311567
https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900:189,Availability,error,error,189,"Hi @rob-p,. thank you for your quick answer. As mentioned already, I started with the full set of options (at least I thought so) and then reduced them to the minimal case to reproduce the error for reporting the problem.; I was mislead by the term 'unrecognized option' and didn't expect the program to ""forget"" options from other modes. But now that you stated that this is the case, I realized that the '-a' in front of the BAM file name was missing, which I overlooked before. After adding it the program at least started to run. (Although it ran into another crirical error, appearantly misinterpreting chromosome and supercontig names from the BAM file header generated by STAR as transcript names.). Maybe it would be a good idea to distinguish in the error messages between 'unrecognised' and 'inappropriate' options to provide a better clue to the user what went wrong.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900
https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900:573,Availability,error,error,573,"Hi @rob-p,. thank you for your quick answer. As mentioned already, I started with the full set of options (at least I thought so) and then reduced them to the minimal case to reproduce the error for reporting the problem.; I was mislead by the term 'unrecognized option' and didn't expect the program to ""forget"" options from other modes. But now that you stated that this is the case, I realized that the '-a' in front of the BAM file name was missing, which I overlooked before. After adding it the program at least started to run. (Although it ran into another crirical error, appearantly misinterpreting chromosome and supercontig names from the BAM file header generated by STAR as transcript names.). Maybe it would be a good idea to distinguish in the error messages between 'unrecognised' and 'inappropriate' options to provide a better clue to the user what went wrong.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900
https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900:759,Availability,error,error,759,"Hi @rob-p,. thank you for your quick answer. As mentioned already, I started with the full set of options (at least I thought so) and then reduced them to the minimal case to reproduce the error for reporting the problem.; I was mislead by the term 'unrecognized option' and didn't expect the program to ""forget"" options from other modes. But now that you stated that this is the case, I realized that the '-a' in front of the BAM file name was missing, which I overlooked before. After adding it the program at least started to run. (Although it ran into another crirical error, appearantly misinterpreting chromosome and supercontig names from the BAM file header generated by STAR as transcript names.). Maybe it would be a good idea to distinguish in the error messages between 'unrecognised' and 'inappropriate' options to provide a better clue to the user what went wrong.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900
https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900:139,Energy Efficiency,reduce,reduced,139,"Hi @rob-p,. thank you for your quick answer. As mentioned already, I started with the full set of options (at least I thought so) and then reduced them to the minimal case to reproduce the error for reporting the problem.; I was mislead by the term 'unrecognized option' and didn't expect the program to ""forget"" options from other modes. But now that you stated that this is the case, I realized that the '-a' in front of the BAM file name was missing, which I overlooked before. After adding it the program at least started to run. (Although it ran into another crirical error, appearantly misinterpreting chromosome and supercontig names from the BAM file header generated by STAR as transcript names.). Maybe it would be a good idea to distinguish in the error messages between 'unrecognised' and 'inappropriate' options to provide a better clue to the user what went wrong.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900
https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900:765,Integrability,message,messages,765,"Hi @rob-p,. thank you for your quick answer. As mentioned already, I started with the full set of options (at least I thought so) and then reduced them to the minimal case to reproduce the error for reporting the problem.; I was mislead by the term 'unrecognized option' and didn't expect the program to ""forget"" options from other modes. But now that you stated that this is the case, I realized that the '-a' in front of the BAM file name was missing, which I overlooked before. After adding it the program at least started to run. (Although it ran into another crirical error, appearantly misinterpreting chromosome and supercontig names from the BAM file header generated by STAR as transcript names.). Maybe it would be a good idea to distinguish in the error messages between 'unrecognised' and 'inappropriate' options to provide a better clue to the user what went wrong.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-462761900
https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-469843098:51,Availability,error,error,51,"Thanks @molwizard,. We've added a more informative error message in 0.13.0. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-469843098
https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-469843098:57,Integrability,message,message,57,"Thanks @molwizard,. We've added a more informative error message in 0.13.0. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/343#issuecomment-469843098
https://github.com/COMBINE-lab/salmon/issues/345#issuecomment-466130305:286,Testability,test,test,286,"Hi @rob-p,. Thanks for the prompt response. I think I may have realised my mistake. It seems like a silly mistake where I wasn't, in fact, using the same version of salmon for the indexing and quantification. . Also yes they would have been running on different machines. I will try to test this and get back to you.; Thanks again",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/345#issuecomment-466130305
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469108517:223,Integrability,message,messages,223,"Hi @asher1234,. Thanks for the detailed bug report. The detection of the library type as `MU` certainly does raise some flags as that is not something that would be expected. Moreover, in the v0.12.0 log you posted, we see messages like:. ```; Thread saw mini-batch with a maximum of 90.16% zero probability fragments; ```. Which means that e.g. ~90% of the fragments, even though they map, are being assigned a 0 probability under the model (because of e.g. incompatibility with the library type). Would you be able to share one of these samples and the reference transcriptome against which you are quantifying? Also, do things look any different if you force the library type to be something more common (e.g. `-l IU`)?. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469108517
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469108517:56,Safety,detect,detection,56,"Hi @asher1234,. Thanks for the detailed bug report. The detection of the library type as `MU` certainly does raise some flags as that is not something that would be expected. Moreover, in the v0.12.0 log you posted, we see messages like:. ```; Thread saw mini-batch with a maximum of 90.16% zero probability fragments; ```. Which means that e.g. ~90% of the fragments, even though they map, are being assigned a 0 probability under the model (because of e.g. incompatibility with the library type). Would you be able to share one of these samples and the reference transcriptome against which you are quantifying? Also, do things look any different if you force the library type to be something more common (e.g. `-l IU`)?. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469108517
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469108517:200,Testability,log,log,200,"Hi @asher1234,. Thanks for the detailed bug report. The detection of the library type as `MU` certainly does raise some flags as that is not something that would be expected. Moreover, in the v0.12.0 log you posted, we see messages like:. ```; Thread saw mini-batch with a maximum of 90.16% zero probability fragments; ```. Which means that e.g. ~90% of the fragments, even though they map, are being assigned a 0 probability under the model (because of e.g. incompatibility with the library type). Would you be able to share one of these samples and the reference transcriptome against which you are quantifying? Also, do things look any different if you force the library type to be something more common (e.g. `-l IU`)?. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469108517
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:565,Performance,load,load,565,"Hi Rob,. Appreciate the response. . The link for the data should be in here:; https://drive.google.com/open?id=1SDqMrHLx-cbfvD4HDmpxcHitqO3B1ZLR; [https://drive.google.com/open?id=1SDqMrHLx-cbfvD4HDmpxcHitqO3B1ZLR](url); https://drive.google.com/drive/folders/1SDqMrHLx-cbfvD4HDmpxcHitqO3B1ZLR?usp=sharing. I've tried forcing the library type for both version 0.8 and 0.12 as follows:. **For version 0.8 with raw reads**; #!/bin/bash; #SBATCH -N 1; #SBATCH -c 8; #SBATCH --mem=10G; #SBATCH --mail-use=tarun2@illinois.edu; #SBATCH -J Salmon; #SBATCH -a 1-24. module load Salmon/0.8.2-IGB-gcc-4.9.4-Python-2.7.13. line=$(sed -n -e ""$SLURM_ARRAY_TASK_ID p"" ~/source/BLBnew.txt). salmon quant -i ~/data/genome/MSU7_transcript.index -l IU \; -1 ~/data/raw-data/BLB/${line}1.fastq.gz \; -2 ~/data/raw-data/BLB/${line}2.fastq.gz --numBootstraps=30 \; -p 12 -o ~/results/salmon_quant_Sheng_IU_old/${line} --seqBias --gcBias. The EffectiveLength is again the same (250) for all genes across all samples:; Name Length EffectiveLength TPM NumReads; LOC_Os01g01010.1 3017 250 28.8836 527.392; LOC_Os01g01010.2 2218 250 1.84062 33.6083; LOC_Os01g01019.1 1127 250 0.0547668 1; LOC_Os01g01030.1 2464 250 4.43611 81; LOC_Os01g01040.4 1524 250 0.941635 17.1935; LOC_Os01g01040.1 2508 250 11.5632 211.135; LOC_Os01g01040.2 2482 250 8.02082 146.454; LOC_Os01g01040.3 2583 250 8.55554 156.218; LOC_Os01g01050.1 2039 250 17.2333 314.667. The mapping rate is again similar for all samples:; [2019-03-04 04:42:18.872] [jointLog] [info] parsing read library format; [2019-03-04 04:42:18.872] [jointLog] [info] There is 1 library.; [2019-03-04 04:42:18.928] [jointLog] [info] Loading Quasi index; [2019-03-04 04:42:18.929] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 04:42:28.958] [jointLog] [info] done; [2019-03-04 04:42:28.958] [jointLog] [info] Index contained 66153 targets; [2019-03-04 04:44:08.443] [fileLog] [info]; At end of round 0; ==================; Observed 18861231 total fragments (18861231 in mo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:1651,Performance,Load,Loading,1651,"ce/BLBnew.txt). salmon quant -i ~/data/genome/MSU7_transcript.index -l IU \; -1 ~/data/raw-data/BLB/${line}1.fastq.gz \; -2 ~/data/raw-data/BLB/${line}2.fastq.gz --numBootstraps=30 \; -p 12 -o ~/results/salmon_quant_Sheng_IU_old/${line} --seqBias --gcBias. The EffectiveLength is again the same (250) for all genes across all samples:; Name Length EffectiveLength TPM NumReads; LOC_Os01g01010.1 3017 250 28.8836 527.392; LOC_Os01g01010.2 2218 250 1.84062 33.6083; LOC_Os01g01019.1 1127 250 0.0547668 1; LOC_Os01g01030.1 2464 250 4.43611 81; LOC_Os01g01040.4 1524 250 0.941635 17.1935; LOC_Os01g01040.1 2508 250 11.5632 211.135; LOC_Os01g01040.2 2482 250 8.02082 146.454; LOC_Os01g01040.3 2583 250 8.55554 156.218; LOC_Os01g01050.1 2039 250 17.2333 314.667. The mapping rate is again similar for all samples:; [2019-03-04 04:42:18.872] [jointLog] [info] parsing read library format; [2019-03-04 04:42:18.872] [jointLog] [info] There is 1 library.; [2019-03-04 04:42:18.928] [jointLog] [info] Loading Quasi index; [2019-03-04 04:42:18.929] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 04:42:28.958] [jointLog] [info] done; [2019-03-04 04:42:28.958] [jointLog] [info] Index contained 66153 targets; [2019-03-04 04:44:08.443] [fileLog] [info]; At end of round 0; ==================; Observed 18861231 total fragments (18861231 in most recent round). [2019-03-04 04:44:08.442] [jointLog] [info] Computed 48502 rich equivalence classes for further processing; [2019-03-04 04:44:08.442] [jointLog] [info] Counted 17308442 total reads in the equivalence classes; [2019-03-04 04:44:08.450] [jointLog] [info] Mapping rate = 91.7673%. [2019-03-04 04:44:08.450] [jointLog] [info] finished quantifyLibrary(). **For version 0.12**; #!/bin/bash; #SBATCH -N 1; #SBATCH -c 8; #SBATCH --mem=10G; #SBATCH --mail-use=tarun2@illinois.edu; #SBATCH -J Salmon; #SBATCH -a 1-24. module load Salmon/0.12.0-IGB-gcc-8.2.0. line=$(sed -n -e ""$SLURM_ARRAY_TASK_ID p"" ~/source/BLBnew.txt). salmon quant -i ~/data/genome",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:1716,Performance,Load,Loading,1716,"l IU \; -1 ~/data/raw-data/BLB/${line}1.fastq.gz \; -2 ~/data/raw-data/BLB/${line}2.fastq.gz --numBootstraps=30 \; -p 12 -o ~/results/salmon_quant_Sheng_IU_old/${line} --seqBias --gcBias. The EffectiveLength is again the same (250) for all genes across all samples:; Name Length EffectiveLength TPM NumReads; LOC_Os01g01010.1 3017 250 28.8836 527.392; LOC_Os01g01010.2 2218 250 1.84062 33.6083; LOC_Os01g01019.1 1127 250 0.0547668 1; LOC_Os01g01030.1 2464 250 4.43611 81; LOC_Os01g01040.4 1524 250 0.941635 17.1935; LOC_Os01g01040.1 2508 250 11.5632 211.135; LOC_Os01g01040.2 2482 250 8.02082 146.454; LOC_Os01g01040.3 2583 250 8.55554 156.218; LOC_Os01g01050.1 2039 250 17.2333 314.667. The mapping rate is again similar for all samples:; [2019-03-04 04:42:18.872] [jointLog] [info] parsing read library format; [2019-03-04 04:42:18.872] [jointLog] [info] There is 1 library.; [2019-03-04 04:42:18.928] [jointLog] [info] Loading Quasi index; [2019-03-04 04:42:18.929] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 04:42:28.958] [jointLog] [info] done; [2019-03-04 04:42:28.958] [jointLog] [info] Index contained 66153 targets; [2019-03-04 04:44:08.443] [fileLog] [info]; At end of round 0; ==================; Observed 18861231 total fragments (18861231 in most recent round). [2019-03-04 04:44:08.442] [jointLog] [info] Computed 48502 rich equivalence classes for further processing; [2019-03-04 04:44:08.442] [jointLog] [info] Counted 17308442 total reads in the equivalence classes; [2019-03-04 04:44:08.450] [jointLog] [info] Mapping rate = 91.7673%. [2019-03-04 04:44:08.450] [jointLog] [info] finished quantifyLibrary(). **For version 0.12**; #!/bin/bash; #SBATCH -N 1; #SBATCH -c 8; #SBATCH --mem=10G; #SBATCH --mail-use=tarun2@illinois.edu; #SBATCH -J Salmon; #SBATCH -a 1-24. module load Salmon/0.12.0-IGB-gcc-8.2.0. line=$(sed -n -e ""$SLURM_ARRAY_TASK_ID p"" ~/source/BLBnew.txt). salmon quant -i ~/data/genome/MSU7new_transcript.index -l IU \; -1 ~/results/trimmingSheng/${line",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:2534,Performance,load,load,2534,"mat; [2019-03-04 04:42:18.872] [jointLog] [info] There is 1 library.; [2019-03-04 04:42:18.928] [jointLog] [info] Loading Quasi index; [2019-03-04 04:42:18.929] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 04:42:28.958] [jointLog] [info] done; [2019-03-04 04:42:28.958] [jointLog] [info] Index contained 66153 targets; [2019-03-04 04:44:08.443] [fileLog] [info]; At end of round 0; ==================; Observed 18861231 total fragments (18861231 in most recent round). [2019-03-04 04:44:08.442] [jointLog] [info] Computed 48502 rich equivalence classes for further processing; [2019-03-04 04:44:08.442] [jointLog] [info] Counted 17308442 total reads in the equivalence classes; [2019-03-04 04:44:08.450] [jointLog] [info] Mapping rate = 91.7673%. [2019-03-04 04:44:08.450] [jointLog] [info] finished quantifyLibrary(). **For version 0.12**; #!/bin/bash; #SBATCH -N 1; #SBATCH -c 8; #SBATCH --mem=10G; #SBATCH --mail-use=tarun2@illinois.edu; #SBATCH -J Salmon; #SBATCH -a 1-24. module load Salmon/0.12.0-IGB-gcc-8.2.0. line=$(sed -n -e ""$SLURM_ARRAY_TASK_ID p"" ~/source/BLBnew.txt). salmon quant -i ~/data/genome/MSU7new_transcript.index -l IU \; -1 ~/results/trimmingSheng/${line}1.paired.fastq \; -2 ~/results/trimmingSheng/${line}2.paired.fastq --numBootstraps=30 \; -p 12 -o ~/results/salmon_quant_Sheng_IU/${line} --seqBias --gcBias --validateMappings. There are no estimate and reads generated when invokin the library type IU:; Name Length EffectiveLength TPM NumReads; LOC_Os01g01010.1 3017 3017.000 0.000000 0.000; LOC_Os01g01010.2 2218 2218.000 0.000000 0.000; LOC_Os01g01019.1 1127 1127.000 0.000000 0.000; LOC_Os01g01030.1 2464 2464.000 0.000000 0.000; LOC_Os01g01040.4 1524 1524.000 0.000000 0.000; LOC_Os01g01040.1 2508 2508.000 0.000000 0.000; LOC_Os01g01040.2 2482 2482.000 0.000000 0.000; LOC_Os01g01040.3 2583 2583.000 0.000000 0.000; LOC_Os01g01050.1 2039 2039.000 0.000000 0.000. [2019-03-04 01:24:12.788] [jointLog] [info] Fragment incompatibility prior below thresho",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:4218,Performance,Load,Loading,4218,"0.4 1524 1524.000 0.000000 0.000; LOC_Os01g01040.1 2508 2508.000 0.000000 0.000; LOC_Os01g01040.2 2482 2482.000 0.000000 0.000; LOC_Os01g01040.3 2583 2583.000 0.000000 0.000; LOC_Os01g01050.1 2039 2039.000 0.000000 0.000. [2019-03-04 01:24:12.788] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-03-04 01:24:12.788] [jointLog] [info] parsing read library format; [2019-03-04 01:24:12.788] [jointLog] [info] There is 1 library.; [2019-03-04 01:24:12.852] [jointLog] [info] Loading Quasi index; [2019-03-04 01:24:12.852] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 01:24:19.703] [jointLog] [info] done; [2019-03-04 01:24:19.704] [jointLog] [info] Index contained 66,004 targets; [2019-03-04 01:25:14.064] [jointLog] [info] Thread saw mini-batch with a maximum of 91.10% zero probability fragments; [2019-03-04 01:25:14.075] [jointLog] [info] Thread saw mini-batch with a maximum of 90.58% zero probability fragments; [2019-03-04 01:25:14.085] [jointLog] [info] Thread saw mini-batch with a maximum of 90.64% zero probability fragments; [2019-03-04 01:25:14.089] [jointLog] [info] Thread saw mini-batch with a maximum of 91.08% zero probability fragments; [2019-03-04 01:25:14.091] [jointLog] [info] Thread saw mini-batch with a maximum of 90.72% zero probability fragments; [2019-03-04 01:25:14.093] [jointLog] [info] Thread saw mini-batch with a maximum of 90.78% zero probability fragments; [2019-03-04 01:25:14.102] [jointLog] [info] Thread saw mini-batch with a maxi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:4283,Performance,Load,Loading,4283,"0000 0.000; LOC_Os01g01040.2 2482 2482.000 0.000000 0.000; LOC_Os01g01040.3 2583 2583.000 0.000000 0.000; LOC_Os01g01050.1 2039 2039.000 0.000000 0.000. [2019-03-04 01:24:12.788] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-03-04 01:24:12.788] [jointLog] [info] parsing read library format; [2019-03-04 01:24:12.788] [jointLog] [info] There is 1 library.; [2019-03-04 01:24:12.852] [jointLog] [info] Loading Quasi index; [2019-03-04 01:24:12.852] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 01:24:19.703] [jointLog] [info] done; [2019-03-04 01:24:19.704] [jointLog] [info] Index contained 66,004 targets; [2019-03-04 01:25:14.064] [jointLog] [info] Thread saw mini-batch with a maximum of 91.10% zero probability fragments; [2019-03-04 01:25:14.075] [jointLog] [info] Thread saw mini-batch with a maximum of 90.58% zero probability fragments; [2019-03-04 01:25:14.085] [jointLog] [info] Thread saw mini-batch with a maximum of 90.64% zero probability fragments; [2019-03-04 01:25:14.089] [jointLog] [info] Thread saw mini-batch with a maximum of 91.08% zero probability fragments; [2019-03-04 01:25:14.091] [jointLog] [info] Thread saw mini-batch with a maximum of 90.72% zero probability fragments; [2019-03-04 01:25:14.093] [jointLog] [info] Thread saw mini-batch with a maximum of 90.78% zero probability fragments; [2019-03-04 01:25:14.102] [jointLog] [info] Thread saw mini-batch with a maximum of 90.60% zero probability fragments; [2019-03-04 01:25:14.102] ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:2889,Security,validat,validateMappings,2889,"ained 66153 targets; [2019-03-04 04:44:08.443] [fileLog] [info]; At end of round 0; ==================; Observed 18861231 total fragments (18861231 in most recent round). [2019-03-04 04:44:08.442] [jointLog] [info] Computed 48502 rich equivalence classes for further processing; [2019-03-04 04:44:08.442] [jointLog] [info] Counted 17308442 total reads in the equivalence classes; [2019-03-04 04:44:08.450] [jointLog] [info] Mapping rate = 91.7673%. [2019-03-04 04:44:08.450] [jointLog] [info] finished quantifyLibrary(). **For version 0.12**; #!/bin/bash; #SBATCH -N 1; #SBATCH -c 8; #SBATCH --mem=10G; #SBATCH --mail-use=tarun2@illinois.edu; #SBATCH -J Salmon; #SBATCH -a 1-24. module load Salmon/0.12.0-IGB-gcc-8.2.0. line=$(sed -n -e ""$SLURM_ARRAY_TASK_ID p"" ~/source/BLBnew.txt). salmon quant -i ~/data/genome/MSU7new_transcript.index -l IU \; -1 ~/results/trimmingSheng/${line}1.paired.fastq \; -2 ~/results/trimmingSheng/${line}2.paired.fastq --numBootstraps=30 \; -p 12 -o ~/results/salmon_quant_Sheng_IU/${line} --seqBias --gcBias --validateMappings. There are no estimate and reads generated when invokin the library type IU:; Name Length EffectiveLength TPM NumReads; LOC_Os01g01010.1 3017 3017.000 0.000000 0.000; LOC_Os01g01010.2 2218 2218.000 0.000000 0.000; LOC_Os01g01019.1 1127 1127.000 0.000000 0.000; LOC_Os01g01030.1 2464 2464.000 0.000000 0.000; LOC_Os01g01040.4 1524 1524.000 0.000000 0.000; LOC_Os01g01040.1 2508 2508.000 0.000000 0.000; LOC_Os01g01040.2 2482 2482.000 0.000000 0.000; LOC_Os01g01040.3 2583 2583.000 0.000000 0.000; LOC_Os01g01050.1 2039 2039.000 0.000000 0.000. [2019-03-04 01:24:12.788] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of range factorizatio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:3637,Security,validat,validateMappings,3637," -i ~/data/genome/MSU7new_transcript.index -l IU \; -1 ~/results/trimmingSheng/${line}1.paired.fastq \; -2 ~/results/trimmingSheng/${line}2.paired.fastq --numBootstraps=30 \; -p 12 -o ~/results/salmon_quant_Sheng_IU/${line} --seqBias --gcBias --validateMappings. There are no estimate and reads generated when invokin the library type IU:; Name Length EffectiveLength TPM NumReads; LOC_Os01g01010.1 3017 3017.000 0.000000 0.000; LOC_Os01g01010.2 2218 2218.000 0.000000 0.000; LOC_Os01g01019.1 1127 1127.000 0.000000 0.000; LOC_Os01g01030.1 2464 2464.000 0.000000 0.000; LOC_Os01g01040.4 1524 1524.000 0.000000 0.000; LOC_Os01g01040.1 2508 2508.000 0.000000 0.000; LOC_Os01g01040.2 2482 2482.000 0.000000 0.000; LOC_Os01g01040.3 2583 2583.000 0.000000 0.000; LOC_Os01g01050.1 2039 2039.000 0.000000 0.000. [2019-03-04 01:24:12.788] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-03-04 01:24:12.788] [jointLog] [info] parsing read library format; [2019-03-04 01:24:12.788] [jointLog] [info] There is 1 library.; [2019-03-04 01:24:12.852] [jointLog] [info] Loading Quasi index; [2019-03-04 01:24:12.852] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 01:24:19.703] [jointLog] [info] done; [2019-03-04 01:24:19.704] [jointLog] [info] Index contained 66,004 targets; [2019-03-04 01:25:14.064] [jointLog] [info] Thread saw mini-batch with a maximum of 91.10% zero probability fragments; [2019-03-04 01:25:14.075] [jointLog] [info] Thread saw mini-batch with a maximum of 90.58",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:3799,Security,validat,validateMappings,3799,"raps=30 \; -p 12 -o ~/results/salmon_quant_Sheng_IU/${line} --seqBias --gcBias --validateMappings. There are no estimate and reads generated when invokin the library type IU:; Name Length EffectiveLength TPM NumReads; LOC_Os01g01010.1 3017 3017.000 0.000000 0.000; LOC_Os01g01010.2 2218 2218.000 0.000000 0.000; LOC_Os01g01019.1 1127 1127.000 0.000000 0.000; LOC_Os01g01030.1 2464 2464.000 0.000000 0.000; LOC_Os01g01040.4 1524 1524.000 0.000000 0.000; LOC_Os01g01040.1 2508 2508.000 0.000000 0.000; LOC_Os01g01040.2 2482 2482.000 0.000000 0.000; LOC_Os01g01040.3 2583 2583.000 0.000000 0.000; LOC_Os01g01050.1 2039 2039.000 0.000000 0.000. [2019-03-04 01:24:12.788] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-03-04 01:24:12.788] [jointLog] [info] parsing read library format; [2019-03-04 01:24:12.788] [jointLog] [info] There is 1 library.; [2019-03-04 01:24:12.852] [jointLog] [info] Loading Quasi index; [2019-03-04 01:24:12.852] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 01:24:19.703] [jointLog] [info] done; [2019-03-04 01:24:19.704] [jointLog] [info] Index contained 66,004 targets; [2019-03-04 01:25:14.064] [jointLog] [info] Thread saw mini-batch with a maximum of 91.10% zero probability fragments; [2019-03-04 01:25:14.075] [jointLog] [info] Thread saw mini-batch with a maximum of 90.58% zero probability fragments; [2019-03-04 01:25:14.085] [jointLog] [info] Thread saw mini-batch with a maximum of 90.64% zero probability fragments; [2019-03-04 01",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256:3949,Security,validat,validateMappings,3949,"n the library type IU:; Name Length EffectiveLength TPM NumReads; LOC_Os01g01010.1 3017 3017.000 0.000000 0.000; LOC_Os01g01010.2 2218 2218.000 0.000000 0.000; LOC_Os01g01019.1 1127 1127.000 0.000000 0.000; LOC_Os01g01030.1 2464 2464.000 0.000000 0.000; LOC_Os01g01040.4 1524 1524.000 0.000000 0.000; LOC_Os01g01040.1 2508 2508.000 0.000000 0.000; LOC_Os01g01040.2 2482 2482.000 0.000000 0.000; LOC_Os01g01040.3 2583 2583.000 0.000000 0.000; LOC_Os01g01050.1 2039 2039.000 0.000000 0.000. [2019-03-04 01:24:12.788] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-03-04 01:24:12.788] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-03-04 01:24:12.788] [jointLog] [info] parsing read library format; [2019-03-04 01:24:12.788] [jointLog] [info] There is 1 library.; [2019-03-04 01:24:12.852] [jointLog] [info] Loading Quasi index; [2019-03-04 01:24:12.852] [jointLog] [info] Loading 32-bit quasi index; [2019-03-04 01:24:19.703] [jointLog] [info] done; [2019-03-04 01:24:19.704] [jointLog] [info] Index contained 66,004 targets; [2019-03-04 01:25:14.064] [jointLog] [info] Thread saw mini-batch with a maximum of 91.10% zero probability fragments; [2019-03-04 01:25:14.075] [jointLog] [info] Thread saw mini-batch with a maximum of 90.58% zero probability fragments; [2019-03-04 01:25:14.085] [jointLog] [info] Thread saw mini-batch with a maximum of 90.64% zero probability fragments; [2019-03-04 01:25:14.089] [jointLog] [info] Thread saw mini-batch with a maximum of 91.08% zero probability fragments; [2019-03-04 01:25:14.091] [jointLog] [info] Thr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469215256
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469412869:67,Testability,log,log,67,"Hi @asher1234,. Thanks. I'll try and grab the data now. The 0.12.0 log here is quite informative. It looks like the problem is that none of the reads are making through the likelihood filter, which explains why you see the output you do. I'll take a look and see if there is a clear reason why. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469412869
https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469412869:277,Usability,clear,clear,277,"Hi @asher1234,. Thanks. I'll try and grab the data now. The 0.12.0 log here is quite informative. It looks like the problem is that none of the reads are making through the likelihood filter, which explains why you see the output you do. I'll take a look and see if there is a clear reason why. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/346#issuecomment-469412869
https://github.com/COMBINE-lab/salmon/issues/347#issuecomment-469750859:127,Security,validat,validate,127,"Hi @red-plant,. This is actually an issue that is a result of the range-factorized equivalence classes that are induced by the validate mappings option. We noticed this side-effect of range-factorization in our own testing, and the issue causing it was fixed in 0.13.0. However, it is worth noting that `--validateMappings` will generally map reads in a much more sensitive way than the default quasi-mapping, and so it is likely that if a read maps to one allele, it will also map to the other but with a lower alignment score (which the algorithm accounts for during quantification). If you really only want to consider the best mappings for a read, and not weight read assignments by alignment score, then you can use the `--hardFilter` option that is also introduced in 0.13.0. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/347#issuecomment-469750859
https://github.com/COMBINE-lab/salmon/issues/347#issuecomment-469750859:306,Security,validat,validateMappings,306,"Hi @red-plant,. This is actually an issue that is a result of the range-factorized equivalence classes that are induced by the validate mappings option. We noticed this side-effect of range-factorization in our own testing, and the issue causing it was fixed in 0.13.0. However, it is worth noting that `--validateMappings` will generally map reads in a much more sensitive way than the default quasi-mapping, and so it is likely that if a read maps to one allele, it will also map to the other but with a lower alignment score (which the algorithm accounts for during quantification). If you really only want to consider the best mappings for a read, and not weight read assignments by alignment score, then you can use the `--hardFilter` option that is also introduced in 0.13.0. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/347#issuecomment-469750859
https://github.com/COMBINE-lab/salmon/issues/347#issuecomment-469750859:215,Testability,test,testing,215,"Hi @red-plant,. This is actually an issue that is a result of the range-factorized equivalence classes that are induced by the validate mappings option. We noticed this side-effect of range-factorization in our own testing, and the issue causing it was fixed in 0.13.0. However, it is worth noting that `--validateMappings` will generally map reads in a much more sensitive way than the default quasi-mapping, and so it is likely that if a read maps to one allele, it will also map to the other but with a lower alignment score (which the algorithm accounts for during quantification). If you really only want to consider the best mappings for a read, and not weight read assignments by alignment score, then you can use the `--hardFilter` option that is also introduced in 0.13.0. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/347#issuecomment-469750859
https://github.com/COMBINE-lab/salmon/issues/347#issuecomment-469752257:71,Performance,perform,perform,71,"Thanks Dr Patro,; Updating now, In my simulations weighted assignments perform quite better than 'best mappings' for ASE, so will stick with that. ; Best.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/347#issuecomment-469752257
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:82,Availability,echo,echo,82,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:797,Availability,ERROR,ERROR,797,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:805,Availability,Error,Error,805,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:897,Availability,error,error,897,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1109,Availability,error,error,1109,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1170,Availability,error,error,1170,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1579,Availability,failure,failure,1579,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:607,Deployability,pipeline,pipeline,607,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1182,Performance,load,loading,1182,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:58,Testability,log,log,58,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:89,Testability,Test,Testing,89,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:107,Testability,Test,Testing,107,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:476,Testability,test,test,476,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:546,Testability,test,test,546,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:565,Testability,test,test,565,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:588,Testability,test,test,588,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:662,Testability,test,tests,662,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1526,Testability,log,log,1526,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264:1497,Usability,resume,resume,1497,"This is failing on our local drone CI during runtime. The log output is :. ```; + echo ""[Testing quant]""; [Testing quant]; + ./.drone/test_quant.sh; Holy build box activated; Prefix: /hbb_exe; CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; LDFLAGS: -L/hbb_exe/lib -static-libstdc++; STATICLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_CFLAGS: -g -O2 -fvisibility=hidden -I/hbb_exe/include ; SHLIB_LDFLAGS: -L/hbb_exe/lib -static-libstdc++; [Drone test] current path : /drone/src/github.com/COMBINE-lab/salmon; [Drone test] making quant test directory; [Drone test] run nextflow pipeline; N E X T F L O W ~ version 0.29.1; Launching `tests/test_quant.nf` [curious_gilbert] - revision: 4f25b30301; [warm up] executor > local; [91/922fac] Submitted process > buildIndex; ERROR ~ Error executing process > 'buildIndex'; Caused by:; Process `buildIndex` terminated with an error exit status (127); Command executed:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon index -t Homo_sapiens.GRCh37.75.cdna.pc.fa -i nfindex; Command exit status:; 127; Command output:; (empty); Command error:; /drone/src/github.com/COMBINE-lab/salmon/bin/salmon: error while loading shared libraries: libjemalloc.so.2: cannot open shared object file: No such file or directory; Work dir:; /drone/src/github.com/COMBINE-lab/salmon/work/91/922facec25da43edd4a2ce82f2289d; Tip: when you have fixed the problem you can continue the execution appending to the nextflow command line the option `-resume`; -- Check '.nextflow.log' file for detail; ```. So, it seems to be due to failure to find the dynamic shared library for jemalloc. Any idea why that might be?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472495264
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:20,Availability,error,error,20,"This in the compile error I get for the Staden package, FYI:. ```; ❯ make VERBOSE=1 [12:30:35]; /usr/local/Cellar/cmake/3.13.4/bin/cmake -S/Users/gabriel/Projects/salmon-0.13.1 -B/Users/gabriel/Projects/salmon-0.13.1/build --check-build-system CMakeFiles/Makefile.cmake 0; /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_progress_start /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/progress.marks; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/Makefile2 all; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libcereal.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3069,Availability,error,error,3069,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3155,Availability,error,error,3155,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3354,Availability,Error,Error,3354,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3409,Availability,Error,Error,3409,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3434,Availability,Error,Error,3434,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:2159,Deployability,install,install,2159,".dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C com",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:2403,Deployability,install,install,2403,"ake CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMa",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:669,Integrability,depend,depend,669,"This in the compile error I get for the Staden package, FYI:. ```; ❯ make VERBOSE=1 [12:30:35]; /usr/local/Cellar/cmake/3.13.4/bin/cmake -S/Users/gabriel/Projects/salmon-0.13.1 -B/Users/gabriel/Projects/salmon-0.13.1/build --check-build-system CMakeFiles/Makefile.cmake 0; /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_progress_start /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/progress.marks; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/Makefile2 all; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libcereal.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabr",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:1035,Integrability,Depend,DependInfo,1035,"e Staden package, FYI:. ```; ❯ make VERBOSE=1 [12:30:35]; /usr/local/Cellar/cmake/3.13.4/bin/cmake -S/Users/gabriel/Projects/salmon-0.13.1 -B/Users/gabriel/Projects/salmon-0.13.1/build --check-build-system CMakeFiles/Makefile.cmake 0; /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_progress_start /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/progress.marks; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/Makefile2 all; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libcereal.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/st",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:1410,Integrability,depend,depend,1410,"/build/CMakeFiles/progress.marks; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/Makefile2 all; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libcereal.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/g",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:1778,Integrability,Depend,DependInfo,1778,"nds ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libcereal.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /A",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:1953,Modifiability,config,configure,1953,"abriel/Projects/salmon-0.13.1/build/CMakeFiles/libcereal.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:2056,Modifiability,config,configure,2056,"oper/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Proje",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:2666,Modifiability,variab,variables,2666,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3058,Modifiability,config,configure,3058,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3144,Modifiability,config,configure,3144,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3246,Modifiability,config,config,3246,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3343,Modifiability,config,configure,3343,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:1942,Performance,Perform,Performing,1942,"abriel/Projects/salmon-0.13.1/build/CMakeFiles/libcereal.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libcereal.dir/build.make CMakeFiles/libcereal.dir/build; make[2]: Nothing to be done for `CMakeFiles/libcereal.dir/build'.; [ 8%] Built target libcereal; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/depend; cd /Users/gabriel/Projects/salmon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:2515,Safety,safe,safe,2515,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713:3253,Testability,log,log,3253,"almon-0.13.1/build && /usr/local/Cellar/cmake/3.13.4/bin/cmake -E cmake_depends ""Unix Makefiles"" /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1 /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build /Users/gabriel/Projects/salmon-0.13.1/build/CMakeFiles/libstadenio.dir/DependInfo.cmake --color=; /Applications/Xcode.app/Contents/Developer/usr/bin/make -f CMakeFiles/libstadenio.dir/build.make CMakeFiles/libstadenio.dir/build; [ 9%] Performing configure step for 'libstadenio'; cd /Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib && ./configure --enable-shared=no --without-libcurl --prefix=/Users/gabriel/Projects/salmon-0.13.1/external/install LDFLAGS= CFLAGS= CC=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc CXX=/Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/c++; checking for a BSD-compatible install... /usr/local/bin/ginstall -c; checking whether build environment is sane... yes; checking for a thread-safe mkdir -p... /usr/local/bin/gmkdir -p; checking for gawk... gawk; checking whether make sets $(MAKE)... yes; checking whether make supports nested variables... yes; checking whether to enable maintainer-specific portions of Makefiles... no; checking for gcc... /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/cc; checking whether the C compiler works... yes; checking for C compiler default output file name... a.out; checking for suffix of executables...; checking whether we are cross compiling... configure: error: in `/Users/gabriel/Projects/salmon-0.13.1/external/staden-io_lib':; configure: error: cannot run C compiled programs.; If you meant to cross compile, use `--host'.; See `config.log' for more details; make[2]: *** [libstadenio-prefix/src/libstadenio-stamp/libstadenio-configure] Error 1; make[1]: *** [CMakeFiles/libstadenio.dir/all] Error 2; make: *** [all] Error 2; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472500713
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472593229:121,Availability,down,download,121,"Hey @rob-p,. OK, I think the changes should pass CI now in theory. Basically I patched the `CMakeLists.txt` file to only download and build those libraries which are not found on the system. If they aren't found, then you have the same behaviour as before. Also, you can now activate the use of shared libraries by setting `USE_SHARED_LIBS=1` on cmake. This should work quite nicely for a Homebrew formula.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472593229
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472593229:79,Deployability,patch,patched,79,"Hey @rob-p,. OK, I think the changes should pass CI now in theory. Basically I patched the `CMakeLists.txt` file to only download and build those libraries which are not found on the system. If they aren't found, then you have the same behaviour as before. Also, you can now activate the use of shared libraries by setting `USE_SHARED_LIBS=1` on cmake. This should work quite nicely for a Homebrew formula.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-472593229
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473300118:110,Deployability,release,release,110,"Hi @gaberoo,. I rebased this in develop since this is where we make changes, so that this will be in the next release. There was one change I had to make. There was a place where a string variable was being checked to determine the existence of a library (e.g. `if (${SUFFARRAY_LIB})` I believe). It seems CMake interprets even the empty string as TRUE here, so I had to change this to the corresponding `FOUND` variable. Otherwise, the changes all seem to have worked beautifully. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473300118
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473300118:188,Modifiability,variab,variable,188,"Hi @gaberoo,. I rebased this in develop since this is where we make changes, so that this will be in the next release. There was one change I had to make. There was a place where a string variable was being checked to determine the existence of a library (e.g. `if (${SUFFARRAY_LIB})` I believe). It seems CMake interprets even the empty string as TRUE here, so I had to change this to the corresponding `FOUND` variable. Otherwise, the changes all seem to have worked beautifully. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473300118
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473300118:412,Modifiability,variab,variable,412,"Hi @gaberoo,. I rebased this in develop since this is where we make changes, so that this will be in the next release. There was one change I had to make. There was a place where a string variable was being checked to determine the existence of a library (e.g. `if (${SUFFARRAY_LIB})` I believe). It seems CMake interprets even the empty string as TRUE here, so I had to change this to the corresponding `FOUND` variable. Otherwise, the changes all seem to have worked beautifully. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473300118
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473479367:35,Deployability,release,release,35,"Sounds good! Once it’s in the next release, we can see if we can get the Homebrew formula working.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473479367
https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473483470:89,Deployability,update,updated,89,Thank you both very much for working on this issue! I'm looking forward to having Salmon updated in Brewsci/bio. Enjoy your weekend!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/348#issuecomment-473483470
https://github.com/COMBINE-lab/salmon/issues/349#issuecomment-472994215:691,Deployability,release,release,691,"Thanks again @jdrnevich for sharing the data for debugging purposes. For anyone seeing similar behavior or following this issue, the resolution is as follows:. The mapping rate difference for between 100bp and 150bp reads (for both single-end and paired-end) becomes very small, and consistent with the ""high"" mapping rate of ~76-79% when using salmon v0.13.1 with `--validateMappings`. Thus, the recommendation here (and in general) is to process the data using the latest version of salmon and ensuring to use the `--validateMappings` option. Also, thanks to @jdrnevich for suggesting that the importance of this feature be highlighted in the documentation to the same extent it is in the release notes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/349#issuecomment-472994215
https://github.com/COMBINE-lab/salmon/issues/349#issuecomment-472994215:368,Security,validat,validateMappings,368,"Thanks again @jdrnevich for sharing the data for debugging purposes. For anyone seeing similar behavior or following this issue, the resolution is as follows:. The mapping rate difference for between 100bp and 150bp reads (for both single-end and paired-end) becomes very small, and consistent with the ""high"" mapping rate of ~76-79% when using salmon v0.13.1 with `--validateMappings`. Thus, the recommendation here (and in general) is to process the data using the latest version of salmon and ensuring to use the `--validateMappings` option. Also, thanks to @jdrnevich for suggesting that the importance of this feature be highlighted in the documentation to the same extent it is in the release notes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/349#issuecomment-472994215
https://github.com/COMBINE-lab/salmon/issues/349#issuecomment-472994215:519,Security,validat,validateMappings,519,"Thanks again @jdrnevich for sharing the data for debugging purposes. For anyone seeing similar behavior or following this issue, the resolution is as follows:. The mapping rate difference for between 100bp and 150bp reads (for both single-end and paired-end) becomes very small, and consistent with the ""high"" mapping rate of ~76-79% when using salmon v0.13.1 with `--validateMappings`. Thus, the recommendation here (and in general) is to process the data using the latest version of salmon and ensuring to use the `--validateMappings` option. Also, thanks to @jdrnevich for suggesting that the importance of this feature be highlighted in the documentation to the same extent it is in the release notes.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/349#issuecomment-472994215
https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-582077473:197,Availability,avail,available,197,"I'm curious too if there is any specific feedback from the devs on whether using salmon on bacterial coding sequences is generally seen as appropriate or not? (sorry if this information is already available elsewhere, I've looked a bit and not seen it yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-582077473
https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-582077473:41,Usability,feedback,feedback,41,"I'm curious too if there is any specific feedback from the devs on whether using salmon on bacterial coding sequences is generally seen as appropriate or not? (sorry if this information is already available elsewhere, I've looked a bit and not seen it yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-582077473
https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-824686957:92,Safety,predict,predicted,92,"I'm wondering the same thing! Can Salmon be used for quantification with a reference set of predicted CDS? IMO this would also have advantages for quantification with de novo assembled transcriptomes, as this would alleviate problems with chimeric contigs...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/350#issuecomment-824686957
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075:166,Availability,down,downstream,166,"Hi @Munfred ,. Apologies for the delayed response.; Thanks for your very important question. We are aware of the problem and are extensively working on improving the downstream processing of the alevin output. Unfortunately, in current form there is no other direct way of loading alevin output matrix. We are thinking of alternative options like using `loompy` but it's a work in progress. We will definitely inform here once we have a simpler working version.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075:273,Performance,load,loading,273,"Hi @Munfred ,. Apologies for the delayed response.; Thanks for your very important question. We are aware of the problem and are extensively working on improving the downstream processing of the alevin output. Unfortunately, in current form there is no other direct way of loading alevin output matrix. We are thinking of alternative options like using `loompy` but it's a work in progress. We will definitely inform here once we have a simpler working version.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075:437,Usability,simpl,simpler,437,"Hi @Munfred ,. Apologies for the delayed response.; Thanks for your very important question. We are aware of the problem and are extensively working on improving the downstream processing of the alevin output. Unfortunately, in current form there is no other direct way of loading alevin output matrix. We are thinking of alternative options like using `loompy` but it's a work in progress. We will definitely inform here once we have a simpler working version.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-490091075
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-498320392:144,Deployability,release,release,144,"Hi @Munfred ,. Thanks for raising the issue. We have significantly modified the mapping algorithm and the output format of alevin in the latest release, the manuscript for changes in the mapping algorithm can be found in the [this](https://www.biorxiv.org/content/10.1101/657874v1) preprint. We have updated the relevant parsers in the python channel too. In practice we have observed significant difference in the size, although we think there is still room for improvement and we are working on improving it. Let us know how it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-498320392
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-498320392:300,Deployability,update,updated,300,"Hi @Munfred ,. Thanks for raising the issue. We have significantly modified the mapping algorithm and the output format of alevin in the latest release, the manuscript for changes in the mapping algorithm can be found in the [this](https://www.biorxiv.org/content/10.1101/657874v1) preprint. We have updated the relevant parsers in the python channel too. In practice we have observed significant difference in the size, although we think there is still room for improvement and we are working on improving it. Let us know how it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-498320392
https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-548616000:119,Testability,benchmark,benchmarks,119,"The latest recommended way is through [tximport](https://github.com/mikelove/tximport). In case you need to check more benchmarks comparing different output format, please check [EDS](https://github.com/COMBINE-lab/EDS/blob/master/README.md). Closing this issue but feel free to reopen in case you still have problems.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/354#issuecomment-548616000
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:812,Deployability,patch,patch,812,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:872,Deployability,release,release,872,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:458,Performance,optimiz,optimization,458,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:745,Performance,optimiz,optimization,745,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146:350,Usability,clear,clearly,350,"Hi @come-raczy,. Thanks for reporting this, it is addressed now in commit efe26b1ca2ced305256357e3b2e95f0e51e3376d. While the function that returns this value is called in two places `normalizeAlphas()` and `writeAbundances()`, the latter of these is actually deprecated and so is not used (we should clean up that code). So, while this value should clearly be initialized, the only potential effect here is through `normalizeAlphas()`, is called before the optimization, and which modifies the alphas that will be used for setting the _initial conditions_ of the VBEM. Therefore, the effect is likely to be limited since, even if the value of `totalCount_` was incorrectly initialized, it should only affect the initialization condition of the optimization. Thank you again for the detailed bug report, and the patch! This is now fixed in develop and will be in the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/355#issuecomment-480004146
https://github.com/COMBINE-lab/salmon/issues/356#issuecomment-481924188:95,Testability,test,testing,95,"Sure thing. Here's a small quants file. It's just the top 2500 lines of one of the files I was testing with. I double checked and this one gets cut at 17 lines (it has ""Erdr1""). [quant.genes.sf.txt](https://github.com/COMBINE-lab/salmon/files/3066421/quant.genes.sf.txt). And here is the mapping I was using:. [map.tsv.gz](https://github.com/COMBINE-lab/salmon/files/3066425/map.tsv.gz). If you need anything else, I'm happy to oblige.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/356#issuecomment-481924188
https://github.com/COMBINE-lab/salmon/issues/356#issuecomment-481945550:819,Deployability,release,release,819,"Hi @cljacobs,. Ok, so I figured it out and just pushed a fix to develop. The problem is the following. The `--genes` flag was only recently added to quant merge (actually, via a pull request). The problem is that while, in transcript-level `quant.sf` files, the `Length` field is always an integer, in gene-level `quant.genes.sf` files, the `Length` field can actually be a floating point number (because the gene length is the abundance weighted combination of transcript lengths). So the problem is that the code was parsing length as a `uint32_t`, which is right for transcript-level files but wrong for gene-level files (where it should be a `double`). This caused the parsing to fail when aggregating gene-level files in your example data. As I said, I pushed a fix to develop, and this will make it into the next release. [Here's](https://drive.google.com/file/d/1olbeygYYHvdJGDKNlmZr6F8HMI1NBgRC/view?usp=sharing) a copy of our CI-server's pre-compiled executable (for linux), in case you need this functionality ASAP and can't build from source.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/356#issuecomment-481945550
https://github.com/COMBINE-lab/salmon/issues/356#issuecomment-486109301:64,Testability,test,test,64,"Sorry for the inconvenience. I completely missed this point. My test set was too small and none were doubles. I was in the process of writting a bug fix... Anyway, thanks for the correction.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/356#issuecomment-486109301
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165:228,Integrability,protocol,protocols,228,"Hi @pinin4fjords ,. Apologies for the delayed response and thanks for your interest in Alevin.; Unfortunately, there is no one straight answer for your question. ; Other people have been using Alevin for various microwell based protocols like (CEL-seq https://github.com/COMBINE-lab/salmon/issues/269 ) but from our side we have not extensively tested alevin on non-droplet based protocols. However, we are open to provide any kind of help you may need to test the microwell-seq protocol and extend the support for alevin. If you happen to have been already testing alevin please let us know of your experience and how we can improve aleivn.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165:380,Integrability,protocol,protocols,380,"Hi @pinin4fjords ,. Apologies for the delayed response and thanks for your interest in Alevin.; Unfortunately, there is no one straight answer for your question. ; Other people have been using Alevin for various microwell based protocols like (CEL-seq https://github.com/COMBINE-lab/salmon/issues/269 ) but from our side we have not extensively tested alevin on non-droplet based protocols. However, we are open to provide any kind of help you may need to test the microwell-seq protocol and extend the support for alevin. If you happen to have been already testing alevin please let us know of your experience and how we can improve aleivn.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165:479,Integrability,protocol,protocol,479,"Hi @pinin4fjords ,. Apologies for the delayed response and thanks for your interest in Alevin.; Unfortunately, there is no one straight answer for your question. ; Other people have been using Alevin for various microwell based protocols like (CEL-seq https://github.com/COMBINE-lab/salmon/issues/269 ) but from our side we have not extensively tested alevin on non-droplet based protocols. However, we are open to provide any kind of help you may need to test the microwell-seq protocol and extend the support for alevin. If you happen to have been already testing alevin please let us know of your experience and how we can improve aleivn.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165:492,Modifiability,extend,extend,492,"Hi @pinin4fjords ,. Apologies for the delayed response and thanks for your interest in Alevin.; Unfortunately, there is no one straight answer for your question. ; Other people have been using Alevin for various microwell based protocols like (CEL-seq https://github.com/COMBINE-lab/salmon/issues/269 ) but from our side we have not extensively tested alevin on non-droplet based protocols. However, we are open to provide any kind of help you may need to test the microwell-seq protocol and extend the support for alevin. If you happen to have been already testing alevin please let us know of your experience and how we can improve aleivn.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165:345,Testability,test,tested,345,"Hi @pinin4fjords ,. Apologies for the delayed response and thanks for your interest in Alevin.; Unfortunately, there is no one straight answer for your question. ; Other people have been using Alevin for various microwell based protocols like (CEL-seq https://github.com/COMBINE-lab/salmon/issues/269 ) but from our side we have not extensively tested alevin on non-droplet based protocols. However, we are open to provide any kind of help you may need to test the microwell-seq protocol and extend the support for alevin. If you happen to have been already testing alevin please let us know of your experience and how we can improve aleivn.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165:456,Testability,test,test,456,"Hi @pinin4fjords ,. Apologies for the delayed response and thanks for your interest in Alevin.; Unfortunately, there is no one straight answer for your question. ; Other people have been using Alevin for various microwell based protocols like (CEL-seq https://github.com/COMBINE-lab/salmon/issues/269 ) but from our side we have not extensively tested alevin on non-droplet based protocols. However, we are open to provide any kind of help you may need to test the microwell-seq protocol and extend the support for alevin. If you happen to have been already testing alevin please let us know of your experience and how we can improve aleivn.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165:558,Testability,test,testing,558,"Hi @pinin4fjords ,. Apologies for the delayed response and thanks for your interest in Alevin.; Unfortunately, there is no one straight answer for your question. ; Other people have been using Alevin for various microwell based protocols like (CEL-seq https://github.com/COMBINE-lab/salmon/issues/269 ) but from our side we have not extensively tested alevin on non-droplet based protocols. However, we are open to provide any kind of help you may need to test the microwell-seq protocol and extend the support for alevin. If you happen to have been already testing alevin please let us know of your experience and how we can improve aleivn.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490089165
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490095148:96,Deployability,pipeline,pipelines,96,"Hi @k3yavi - thanks for the response. We have been looking at using Alevin to support our wider pipelines in the gene expression group at the EBI, as a generic way of quantifying droplet experiments. It has worked well for the 10X v2 studies I've tried, but I'm experiencing some trouble with the drop-seq studies I've tried thus far due to noisy barcodes. May just be bad data, but I'll post an issue or two when I'm clearer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490095148
https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490095148:418,Usability,clear,clearer,418,"Hi @k3yavi - thanks for the response. We have been looking at using Alevin to support our wider pipelines in the gene expression group at the EBI, as a generic way of quantifying droplet experiments. It has worked well for the 10X v2 studies I've tried, but I'm experiencing some trouble with the drop-seq studies I've tried thus far due to noisy barcodes. May just be bad data, but I'll post an issue or two when I'm clearer.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/358#issuecomment-490095148
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490081966:349,Performance,perform,performs,349,"Hi @wmegchel ,. Thanks for the raising the issue.; If I understand correctly, your question is Number of mapped reads != Number of deduplicated reads ? Basically, the quant matrix (and the csv matrix) represents the number of _deduplicated_ reads, which indeed should be less than number of mapped reads. Alevin consumes the reads which are mapped, performs UMI level deduplication and reports them in the `quant_mat.gz`. I am unsure about the 10x part of your question i.e.; > I was able to run the 10x PBMC4k example and there, the sum of the count matrix entries indeed fitted the reported UMI counts and mapping rate. If this is true then something is wrong, as the number of mapped reads should be much greater than (based on the number of PCR cycles) number of deduplicated reads. Hope this answer your question ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490081966
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490098177:923,Energy Efficiency,adapt,adapted,923,"Hi @k3yavi,. Thanks for the reply!. Let's take the PBMC 4K as example. Looking at the summary sheet from 10x: ; http://cf.10xgenomics.com/samples/cell-exp/2.1.0/pbmc4k/pbmc4k_web_summary.html. They detected 4,340 cells with a median UMI count of 3,866 per cell. That means ~17M UMIs in the count matrix, which is in the same order what I find with Alevin. I am not sure if/where Alevin reports the number of mapped reads (maybe it is the number of hits?), but this is not of much importance. Indeed, the total UMI count is **much** lower than the number of sequenced/mapped/barcoded reads (~190M), which is expected. However, using the `--dumpUmiGraph` option provides a file ""MappedUMI.txt"" which I assume are the number of deduplicated UMIs mapped per cell/barcode (summed over all genes). The sum of over all the barcodes = 17M in this case and the sum per barcode = the sum in the quant_mat. This does not hold for the adapted cel-seq2 protocol. sum mapped UMI != summed quant_mat.gz. I am making a mistake, or is there something wrong?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490098177
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490098177:940,Integrability,protocol,protocol,940,"Hi @k3yavi,. Thanks for the reply!. Let's take the PBMC 4K as example. Looking at the summary sheet from 10x: ; http://cf.10xgenomics.com/samples/cell-exp/2.1.0/pbmc4k/pbmc4k_web_summary.html. They detected 4,340 cells with a median UMI count of 3,866 per cell. That means ~17M UMIs in the count matrix, which is in the same order what I find with Alevin. I am not sure if/where Alevin reports the number of mapped reads (maybe it is the number of hits?), but this is not of much importance. Indeed, the total UMI count is **much** lower than the number of sequenced/mapped/barcoded reads (~190M), which is expected. However, using the `--dumpUmiGraph` option provides a file ""MappedUMI.txt"" which I assume are the number of deduplicated UMIs mapped per cell/barcode (summed over all genes). The sum of over all the barcodes = 17M in this case and the sum per barcode = the sum in the quant_mat. This does not hold for the adapted cel-seq2 protocol. sum mapped UMI != summed quant_mat.gz. I am making a mistake, or is there something wrong?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490098177
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490098177:923,Modifiability,adapt,adapted,923,"Hi @k3yavi,. Thanks for the reply!. Let's take the PBMC 4K as example. Looking at the summary sheet from 10x: ; http://cf.10xgenomics.com/samples/cell-exp/2.1.0/pbmc4k/pbmc4k_web_summary.html. They detected 4,340 cells with a median UMI count of 3,866 per cell. That means ~17M UMIs in the count matrix, which is in the same order what I find with Alevin. I am not sure if/where Alevin reports the number of mapped reads (maybe it is the number of hits?), but this is not of much importance. Indeed, the total UMI count is **much** lower than the number of sequenced/mapped/barcoded reads (~190M), which is expected. However, using the `--dumpUmiGraph` option provides a file ""MappedUMI.txt"" which I assume are the number of deduplicated UMIs mapped per cell/barcode (summed over all genes). The sum of over all the barcodes = 17M in this case and the sum per barcode = the sum in the quant_mat. This does not hold for the adapted cel-seq2 protocol. sum mapped UMI != summed quant_mat.gz. I am making a mistake, or is there something wrong?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490098177
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490098177:198,Safety,detect,detected,198,"Hi @k3yavi,. Thanks for the reply!. Let's take the PBMC 4K as example. Looking at the summary sheet from 10x: ; http://cf.10xgenomics.com/samples/cell-exp/2.1.0/pbmc4k/pbmc4k_web_summary.html. They detected 4,340 cells with a median UMI count of 3,866 per cell. That means ~17M UMIs in the count matrix, which is in the same order what I find with Alevin. I am not sure if/where Alevin reports the number of mapped reads (maybe it is the number of hits?), but this is not of much importance. Indeed, the total UMI count is **much** lower than the number of sequenced/mapped/barcoded reads (~190M), which is expected. However, using the `--dumpUmiGraph` option provides a file ""MappedUMI.txt"" which I assume are the number of deduplicated UMIs mapped per cell/barcode (summed over all genes). The sum of over all the barcodes = 17M in this case and the sum per barcode = the sum in the quant_mat. This does not hold for the adapted cel-seq2 protocol. sum mapped UMI != summed quant_mat.gz. I am making a mistake, or is there something wrong?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490098177
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490105924:704,Testability,test,tested,704,"Very strange, I'd like to start with first apologizing for relatively poor documentation for all these meta files (`MappedUMI`) and I think that's the point of confusion. The expected behavior is as follows:; * `--dumpFeature` dumps various meta files like `filtered_cb_frequency.txt` which gives number of reads in each CB after sequence correction and `MappedUMI,txt` which is a subset of reads from the `filtered_cb_frequency.txt` which are mapped by alevin and should be reflective of the mapping rate.; * `--dumpUmiGraph` dumps the internal graphical structure used by alevin for deduplication. Having said that, I was curious that the sum of count in the `MappedUMI.txt` is coming out to be 17M. I tested the same at my end and it's coming out to be ~200M . I am puzzled why the counts are so low for your run, can you please double check if alevin has finished ? It seems the behavior you are observing in the CEL-seq data is the right one. For counting the number of mapped reads use the file `MappedUMI.txt` and for counting number of deduplicated reads use `quants_mat.gz`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490105924
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490115088:347,Availability,error,error,347,"Right, that might explain it. So first of all I indeed used the `--dumpFeature` option to get the `MappedUMI.txt`, sorry for that. I will check the file once again and rerun the analysis, which I did on my machine at home. It might be that the 17M I told from the top of my head is actually incorrect. Unfortunately, the 10x crashes with a memory error on our clusters (I tested 2 different machines), for which I will open a separate thread (I think I saw someone else reporting a similar issue; will look that up). Celseq2 runs fine. I will check the analysis and the `mappedUMI.txt` a.s.a.p. I fear this issue will soon be resolved where both of us have a memory problem to deal with ;-). Cheers,; Wout",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490115088
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490115088:372,Testability,test,tested,372,"Right, that might explain it. So first of all I indeed used the `--dumpFeature` option to get the `MappedUMI.txt`, sorry for that. I will check the file once again and rerun the analysis, which I did on my machine at home. It might be that the 17M I told from the top of my head is actually incorrect. Unfortunately, the 10x crashes with a memory error on our clusters (I tested 2 different machines), for which I will open a separate thread (I think I saw someone else reporting a similar issue; will look that up). Celseq2 runs fine. I will check the analysis and the `mappedUMI.txt` a.s.a.p. I fear this issue will soon be resolved where both of us have a memory problem to deal with ;-). Cheers,; Wout",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-490115088
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-498040307:29,Deployability,release,release,29,"Hi @wmegchel ,. We have just release `v0.14.0`, we expect the memory problem to go away with the latest release. Let us know if it works out for you and feel free to close the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-498040307
https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-498040307:104,Deployability,release,release,104,"Hi @wmegchel ,. We have just release `v0.14.0`, we expect the memory problem to go away with the latest release. Let us know if it works out for you and feel free to close the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/361#issuecomment-498040307
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490143007:465,Usability,clear,clear,465,"Heya, we had the same problem with unclear knee-plots like this. We make an alternative plot that looks like these. The first is on high quality data from Allon's K562 data from the original inDrop paper; a knee plot works well on this dataset. ![image](https://user-images.githubusercontent.com/414586/57312464-9e631680-70bb-11e9-961d-5bf2c3ede38a.png). and this is from blood in the Zebrafish, the data is of less good quality. The knee plot for this data wasn't clear enough to draw a reasonable cutoff but this alternative plot makes it easier to pick the cutoff:. ![image](https://user-images.githubusercontent.com/414586/57312538-c3578980-70bb-11e9-910c-84017a5dbcde.png). These plots are made like this:. ```; barcode_plot = function(bcs, sample) {; bcs_hist = hist(log10(bcs$count), plot=FALSE, n=50); fLog = bcs_hist$count; xLog = bcs_hist$mids; y = fLog * (10^xLog) / sum(fLog * (10^xLog)); print(qplot(xLog, y) + geom_point() + theme_bw() + ggtitle(sample)); return(data.frame(x=xLog, y=y, sample=sample)); }; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490143007
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490143396:470,Availability,error,error,470,"Hi @pinin4fjords ,. That's a really nice suggestion. We had a similar idea, basically the plan was to ask the user to run the algorithm with `--dumpFeatures` which dumps the `filter_read_counts.txt` file, basically this file has the per CB level read counts after sequence correction, and visualize the distribution through small script to chose a user-defined threshold by `--forceCell` command line flag. I do see a value in giving a ballpark for some boundary in the error code, but the problem is a little circular i.e. if we had some idea about the boundary we use it anyhow and correct it later through the intelligent whitelisting. ; We are open to ideas you may have ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490143396
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:841,Availability,avail,available,841,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:1218,Availability,error,error,1218,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:1264,Availability,robust,robust,1264,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:1320,Availability,error,error,1320,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:1367,Availability,error,errors,1367,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:1394,Availability,error,error,1394,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:76,Deployability,pipeline,pipeline,76,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:102,Performance,throughput,throughput,102,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:209,Safety,avoid,avoid,209,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:1198,Safety,detect,detect,1198,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:1383,Safety,detect,detect,1383,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428:1418,Testability,log,logic,1418,"Thanks @roryk and @k3yavi . The issue we have is that we're trying to run a pipeline in a fairly high-throughput manner to get a sensible 'enough' matrix without too much manual intervention. So I'm trying to avoid anything that requires an eyeballing step, accepting that the matrix we get will be less optimal than one you'd get from manual optimisation. Where possible, our curators are extracting the expected cell numbers from publications, so sometimes I have at least a general idea of where to look for an elbow/ feature. @roryk - have you used your alternate view on the data to automatically derive cutoffs? Does it work well?. @k3yavi:. As I say, first point is that this is for cases where I have a rough idea of the target cell number- we're generally working with pre-published data (though cell numbers per run are not always available). . From https://github.com/COMBINE-lab/salmon/issues/340 I'd inferred that --expectCells gives Alevin ballpark to look for a knee within, while --forceCells is a strict cuttoff. Is that correct? . That being the case, my thought was to try --expectCells first, and failing that --forceCells. The problem is that I need to parse the STDOUT/ERR to detect the boundary error from --expectCells, which is not a very robust way of doing things. If you returned informative error codes (anything but 1) on this and other errors, I could detect the error and implement the logic I describe.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490157428
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490160480:1049,Testability,log,logger,1049,"Hi @pinin4fjords,. We have the same use case, trying to automate as much as possible; for some datasets there really isn't anything you can do; if it is super bad both methods are bad. This function does a pretty reasonable job of picking a cutoff based on that histogram:. ```; def guess_depth_cutoff(cb_histogram):; ''' Guesses at an appropriate barcode cutoff; '''; with read_cbhistogram(cb_histogram) as fh:; cb_vals = [int(p.strip().split()[1]) for p in fh]; histo = np.histogram(np.log10(cb_vals), bins=50); vals = histo[0]; edges = histo[1]; mids = np.array([(edges[i] + edges[i+1])/2 for i in range(edges.size - 1)]); wdensity = vals * (10**mids) / sum(vals * (10**mids)); baseline = np.median(wdensity); wdensity = list(wdensity); # find highest density in upper half of barcode distribution; peak = wdensity.index(max(wdensity[len(wdensity)/2:])); cutoff = None; for index, dens in reversed(list(enumerate(wdensity[1:peak]))):; if dens < 2 * baseline:; cutoff = index; break; if not cutoff:; return None; else:; cutoff = 10**mids[cutoff]; logger.info('Setting barcode cutoff to %d' % cutoff); return cutoff; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490160480
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161277:178,Availability,error,error,178,"Thanks @roryk for the code and @pinin4fjords for the suggestion.; That's correct, `--expectCells` and `--forceCells` flags are designed to use the way you described above. ; re: error codes, It's a good suggestion. Based on the timeline either we can make this into the next release of alevin or we can edit it in a different branch and then you can compile or I can give you a linux binary. Let us know what works for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161277
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161277:275,Deployability,release,release,275,"Thanks @roryk for the code and @pinin4fjords for the suggestion.; That's correct, `--expectCells` and `--forceCells` flags are designed to use the way you described above. ; re: error codes, It's a good suggestion. Based on the timeline either we can make this into the next release of alevin or we can edit it in a different branch and then you can compile or I can give you a linux binary. Let us know what works for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161277
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161823:72,Availability,robust,robust,72,"I think if you want to automate these steps though the easiest and most robust thing you could do is require everyone to tell you how many cells they captured and sequenced, and relax that number a little bit and do whatever filtering you need to do downstream to get rid of the crap on the low end; usually other quality control metrics like mitochondrial content or genes detected or whatever will filter out the garbage that leaks into the count matrix from being permissive in initial cell demultiplexing + quantification steps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161823
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161823:250,Availability,down,downstream,250,"I think if you want to automate these steps though the easiest and most robust thing you could do is require everyone to tell you how many cells they captured and sequenced, and relax that number a little bit and do whatever filtering you need to do downstream to get rid of the crap on the low end; usually other quality control metrics like mitochondrial content or genes detected or whatever will filter out the garbage that leaks into the count matrix from being permissive in initial cell demultiplexing + quantification steps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161823
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161823:374,Safety,detect,detected,374,"I think if you want to automate these steps though the easiest and most robust thing you could do is require everyone to tell you how many cells they captured and sequenced, and relax that number a little bit and do whatever filtering you need to do downstream to get rid of the crap on the low end; usually other quality control metrics like mitochondrial content or genes detected or whatever will filter out the garbage that leaks into the count matrix from being permissive in initial cell demultiplexing + quantification steps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490161823
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490163389:90,Availability,down,downstream,90,"Great- thanks @roryk , I'll give your code a try, and yes, we are applying some filtering downstream. Unfortunately we don't always have control of the metadata associated with the experiments we're handling. . @k3yavi - thank you, next release will be fine :-)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490163389
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490163389:237,Deployability,release,release,237,"Great- thanks @roryk , I'll give your code a try, and yes, we are applying some filtering downstream. Unfortunately we don't always have control of the metadata associated with the experiments we're handling. . @k3yavi - thank you, next release will be fine :-)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490163389
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490165970:98,Availability,down,downstream,98,"I agree with the @roryk's suggestion and that was indeed the motivation to have whitelisting step downstream of deduplication in Alevin.; @pinin4fjords sounds good, I will add this into the todo list for next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490165970
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490165970:209,Deployability,release,release,209,"I agree with the @roryk's suggestion and that was indeed the motivation to have whitelisting step downstream of deduplication in Alevin.; @pinin4fjords sounds good, I will add this into the todo list for next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490165970
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490391990:323,Modifiability,sandbox,sandbox,323,I made myself a plot to illustrate @roryk 's approach (hopefully got it right)- just leaving it here in case others are interested. ![compare_droplet_threshold](https://user-images.githubusercontent.com/5775915/57373557-9032fa00-7190-11e9-9cec-15b0a32f88aa.png). Code here: https://github.com/ebi-gene-expression-group/jon-sandbox/tree/master/droplet_cutoffs.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490391990
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490391990:323,Testability,sandbox,sandbox,323,I made myself a plot to illustrate @roryk 's approach (hopefully got it right)- just leaving it here in case others are interested. ![compare_droplet_threshold](https://user-images.githubusercontent.com/5775915/57373557-9032fa00-7190-11e9-9cec-15b0a32f88aa.png). Code here: https://github.com/ebi-gene-expression-group/jon-sandbox/tree/master/droplet_cutoffs.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490391990
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234:162,Availability,recover,recover,162,"Thanks- Jonathan. Yikes, that bad quality one looks like particularly bad quality, I have an example that looks like that in my failed examples. Were you able to recover usable data from it?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234:162,Safety,recover,recover,162,"Thanks- Jonathan. Yikes, that bad quality one looks like particularly bad quality, I have an example that looks like that in my failed examples. Were you able to recover usable data from it?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234:170,Usability,usab,usable,170,"Thanks- Jonathan. Yikes, that bad quality one looks like particularly bad quality, I have an example that looks like that in my failed examples. Were you able to recover usable data from it?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490510234
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490795334:73,Availability,down,downstream,73,"@k3yavi - is it possible to skip the thresholding entirely, so as to use downstream tools to remove empty barcodes instead?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490795334
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490833213:675,Availability,down,down,675,"It is, if you provide a list of CB to use through command line flag --whitelist. But again I think it's a circular problem, if you know the list of CB to use, you might have already figures out the frequency distribution of each CB by parsing the fastq. Either by using --dumpFeatures or externally may be through awk. One other option is to use --keepCBfraction it takes a number in (0, 1] , which basically tells Alevin to use X fraction of CB from the total observed. The caveat there is to figure out a decent value of X as the CB frequency distribution is a long tailed distribution and if say you provide 1 then Alevin will quantify each and every observed CB and slow down the full pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490833213
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490833213:689,Deployability,pipeline,pipeline,689,"It is, if you provide a list of CB to use through command line flag --whitelist. But again I think it's a circular problem, if you know the list of CB to use, you might have already figures out the frequency distribution of each CB by parsing the fastq. Either by using --dumpFeatures or externally may be through awk. One other option is to use --keepCBfraction it takes a number in (0, 1] , which basically tells Alevin to use X fraction of CB from the total observed. The caveat there is to figure out a decent value of X as the CB frequency distribution is a long tailed distribution and if say you provide 1 then Alevin will quantify each and every observed CB and slow down the full pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490833213
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490843243:154,Availability,down,downstream,154,"Thanks @k3yavi , --keepCBfraction isn't in the docs, so I missed it. Unless it leads to completely unfeasible run times, --keepCBfraction 1 combined with downstream filtering may be the most robust way to handle things in my high throughput situation (as alluded to by @roryk ). Is there a way of combining this with a minimum UMI count per CB to remove just the most obvious junk and hopefully somewhat limit the impact on runtimes?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490843243
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490843243:191,Availability,robust,robust,191,"Thanks @k3yavi , --keepCBfraction isn't in the docs, so I missed it. Unless it leads to completely unfeasible run times, --keepCBfraction 1 combined with downstream filtering may be the most robust way to handle things in my high throughput situation (as alluded to by @roryk ). Is there a way of combining this with a minimum UMI count per CB to remove just the most obvious junk and hopefully somewhat limit the impact on runtimes?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490843243
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490843243:230,Performance,throughput,throughput,230,"Thanks @k3yavi , --keepCBfraction isn't in the docs, so I missed it. Unless it leads to completely unfeasible run times, --keepCBfraction 1 combined with downstream filtering may be the most robust way to handle things in my high throughput situation (as alluded to by @roryk ). Is there a way of combining this with a minimum UMI count per CB to remove just the most obvious junk and hopefully somewhat limit the impact on runtimes?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490843243
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490857399:661,Availability,down,downstream,661,"Unfortunately it's getting into a little under explored territory. ; Retrospectively, it does makes sense to have such a threshold but sadly in the current version we don't have that option. If it may help, I just got some stats for 10x data and generally it's around top < 1% of the data which is useful, which should be even lower in dropseq. Although it's possible my sample size of 5 below dataset is too small. ```; useful -> total; 4k -> 1,239,476; 8k -> 1,877,718; 900 -> 657,180; 2k -> 1,653,795; 9k -> 2,812,291; ```. My guess is keeping the `keepCBFraction` even to 0.1 (i.e. 10%) would get you decent number of empty/low confidence CB to correct for downstream but you might have to explore a little. Another caveat is, having too many CB blows up the memory for downstream whitelisting of alevin and currently there is no option to disable that whitelisting, again a must have feature which is missing. Thanks for this very useful discussion, we will definitely improve/add these options into alevin with the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490857399
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490857399:774,Availability,down,downstream,774,"Unfortunately it's getting into a little under explored territory. ; Retrospectively, it does makes sense to have such a threshold but sadly in the current version we don't have that option. If it may help, I just got some stats for 10x data and generally it's around top < 1% of the data which is useful, which should be even lower in dropseq. Although it's possible my sample size of 5 below dataset is too small. ```; useful -> total; 4k -> 1,239,476; 8k -> 1,877,718; 900 -> 657,180; 2k -> 1,653,795; 9k -> 2,812,291; ```. My guess is keeping the `keepCBFraction` even to 0.1 (i.e. 10%) would get you decent number of empty/low confidence CB to correct for downstream but you might have to explore a little. Another caveat is, having too many CB blows up the memory for downstream whitelisting of alevin and currently there is no option to disable that whitelisting, again a must have feature which is missing. Thanks for this very useful discussion, we will definitely improve/add these options into alevin with the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490857399
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490857399:1026,Deployability,release,release,1026,"Unfortunately it's getting into a little under explored territory. ; Retrospectively, it does makes sense to have such a threshold but sadly in the current version we don't have that option. If it may help, I just got some stats for 10x data and generally it's around top < 1% of the data which is useful, which should be even lower in dropseq. Although it's possible my sample size of 5 below dataset is too small. ```; useful -> total; 4k -> 1,239,476; 8k -> 1,877,718; 900 -> 657,180; 2k -> 1,653,795; 9k -> 2,812,291; ```. My guess is keeping the `keepCBFraction` even to 0.1 (i.e. 10%) would get you decent number of empty/low confidence CB to correct for downstream but you might have to explore a little. Another caveat is, having too many CB blows up the memory for downstream whitelisting of alevin and currently there is no option to disable that whitelisting, again a must have feature which is missing. Thanks for this very useful discussion, we will definitely improve/add these options into alevin with the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490857399
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490859077:111,Deployability,release,release,111,Thanks @k3yavi - I think those options would really help us use Alevin in production- look forward to the next release. . I'll do some more testing in the meantime.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490859077
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490859077:140,Testability,test,testing,140,Thanks @k3yavi - I think those options would really help us use Alevin in production- look forward to the next release. . I'll do some more testing in the meantime.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490859077
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490864673:60,Deployability,release,release,60,"Sounds good, I will report back as soon as we have the next release. ; Just a quick correction the useful percentage is less than 1% (not 1-4% it's 0.1 - 0.4%), I was off by a magnitude in the percentage reported above.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490864673
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490907226:502,Availability,down,downstream,502,"Yep, that is what I was reflecting at earlier. Using `--dumpFeatures` you can get the file `filtered_cb_frequency.txt` (this is after CB seq correction, `raw_cb_frequency` is before seq correction). As the file is reverse sorted by frequency you can count the number of CB with frequency `>100` and use that for the `--forceCell`. It does makes alevin multiple pass algorithm but that was the current use case with complex dataset where it fails to find the knee. . One thing to note here would be the downstream whitelisting. If the number of CB becomes too many then it can potentially blow up the memory. However, since you are already parsing the names of the CB (in your awk script) you can pass those CB as the `--whitelist`, alevin will still sequence correct the remaining CB with 1-edit distance around them. The benefit of this is, alevin will not run downstream whitelisting as a list of whitelisted CB has been provided externally.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490907226
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490907226:862,Availability,down,downstream,862,"Yep, that is what I was reflecting at earlier. Using `--dumpFeatures` you can get the file `filtered_cb_frequency.txt` (this is after CB seq correction, `raw_cb_frequency` is before seq correction). As the file is reverse sorted by frequency you can count the number of CB with frequency `>100` and use that for the `--forceCell`. It does makes alevin multiple pass algorithm but that was the current use case with complex dataset where it fails to find the knee. . One thing to note here would be the downstream whitelisting. If the number of CB becomes too many then it can potentially blow up the memory. However, since you are already parsing the names of the CB (in your awk script) you can pass those CB as the `--whitelist`, alevin will still sequence correct the remaining CB with 1-edit distance around them. The benefit of this is, alevin will not run downstream whitelisting as a list of whitelisted CB has been provided externally.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490907226
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490914214:307,Safety,detect,detection,307,"Okay, thanks @k3yavi. Just to be clear- you're saying I should derive the whitelist from the filtered_cb_frequency rather than the raw? This is a much smaller file in the case of the bad data above (more so than I'd expect from the cb correction, 984), so I was afraid it had already been subjected to knee detection. I also note that it's also not in fact sorted by default.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490914214
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490914214:33,Usability,clear,clear,33,"Okay, thanks @k3yavi. Just to be clear- you're saying I should derive the whitelist from the filtered_cb_frequency rather than the raw? This is a much smaller file in the case of the bad data above (more so than I'd expect from the cb correction, 984), so I was afraid it had already been subjected to knee detection. I also note that it's also not in fact sorted by default.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-490914214
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:107,Availability,error,error,107,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:163,Availability,error,error,163,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:200,Availability,Error,Error,200,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:241,Availability,error,errors,241,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:254,Availability,Error,Error,254,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:324,Availability,Error,Error,324,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:383,Availability,Error,Error,383,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:28,Deployability,release,release,28,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:143,Deployability,pipeline,pipeline,143,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:751,Deployability,release,release,751,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:862,Deployability,release,release,862,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072:364,Performance,optimiz,optimization,364,"Hi @pinin4fjords ,. We have release a new version `v0.14.0`. In the latest version we have added different error codes based on what stage the pipeline fails. The error codes are as follows:. ```; 1: Error while mapping reads and/or generic errors.; 64: Error in knee estimation / Cellular Barcode sequence correction.; 74: Error while deduplicating UMI and/or EM optimization.; 84: Error while intelligent whitelisting.; ```. As we have discussed earlier, you can control the expected behavior by tweaking the following two flags.; ```; --keepCBfraction: A value in (0, 1] i..e what fraction of CB to keep for quantification.; --freqThreshold: default 10, Minimum frequency required to quantify the CB.; ```. Just a heads up, alevin with the current release will by default dump the `dumpFeatures.txt` which contains the per CB level features. Please check the release notes for more details. Closing this issue for now, but feel free to reopen if you face any issue or have question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-498040072
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-501591974:79,Deployability,release,release,79,Sorry @k3yavi - was away on leave. Seems to be lots of helpful titbits in this release- thank you.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-501591974
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-516399536:437,Availability,down,downstream,437,"Hi @k3yavi - apologies, just coming back to this with an eye to updating our pipelines, just wanted to clarify. . Just to recap, right now I'm running the previous Alevin version with `--dumpFeatures --noQuant`, filtering raw_cb_frequency.txt by barcode frequency (e.g. 10), and then using the whitelist in a further Alevin run, like `--whitelist whitelist.txt --forceCells (whitelist size)`. I then get a relaxed matrix I can filter in downstream analysis. Am I right in thinking that with the new version I can now just have a single run and say `--keepFraction 1 --freqThreshold 10`? Is the freqThreshold applied before the keepFraction?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-516399536
https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-516399536:77,Deployability,pipeline,pipelines,77,"Hi @k3yavi - apologies, just coming back to this with an eye to updating our pipelines, just wanted to clarify. . Just to recap, right now I'm running the previous Alevin version with `--dumpFeatures --noQuant`, filtering raw_cb_frequency.txt by barcode frequency (e.g. 10), and then using the whitelist in a further Alevin run, like `--whitelist whitelist.txt --forceCells (whitelist size)`. I then get a relaxed matrix I can filter in downstream analysis. Am I right in thinking that with the new version I can now just have a single run and say `--keepFraction 1 --freqThreshold 10`? Is the freqThreshold applied before the keepFraction?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/362#issuecomment-516399536
https://github.com/COMBINE-lab/salmon/issues/363#issuecomment-498039361:93,Deployability,release,release,93,"Hi @TobiTekath ,. Thanks for bringing this to our attention. We fixed this bug in the latest release. Just a heads up we have deprecated the `csv` dumping in favor of sparse `mtx` format. The format is still human readable, it's just dumping each and every point we dump only the expressed value and their coordinates in the quant matrix. We have update the python parser too which can be found [here](https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.py#L140). Closing this issue for now but feel free to reopen it if you still face the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/363#issuecomment-498039361
https://github.com/COMBINE-lab/salmon/issues/363#issuecomment-498039361:347,Deployability,update,update,347,"Hi @TobiTekath ,. Thanks for bringing this to our attention. We fixed this bug in the latest release. Just a heads up we have deprecated the `csv` dumping in favor of sparse `mtx` format. The format is still human readable, it's just dumping each and every point we dump only the expressed value and their coordinates in the quant matrix. We have update the python parser too which can be found [here](https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.py#L140). Closing this issue for now but feel free to reopen it if you still face the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/363#issuecomment-498039361
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-498316176:330,Deployability,release,released,330,"Hi @tamuanand ,. Thanks for the very interesting question.; Personally I can't comment much on the Lexogen Quantseq quantification, however, the comparison of alignment based (both STAR/Bowtie2) and alignment free methods and their impact on RNA-seq quantification is within itself a very interesting comparison. In fact. we just released a preprint today about the same, you can check it out [here](https://www.biorxiv.org/content/10.1101/657874v1).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-498316176
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-498953769:59,Availability,avail,available,59,"Thanks for sharing the preprint. Is the SA method codebase available via github or via the combine lab site https://combine-lab.github.io/software/ . I know the preprint has supplementary material and code, but it would be valuable to have something like https://combine-lab.github.io/salmon/getting_started/ and/or https://salmon.readthedocs.io/en/latest/salmon.html",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-498953769
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499297622:62,Deployability,integrat,integrated,62,"Hi @tamuanand ,; Thanks for raising this doubt. SA is already integrated into the salmon environment i.e. you just have to re index salmon using the `generateDecoyTranscriptome.sh` script from [here](https://github.com/COMBINE-lab/SalmonTools) and run salmon quant as you usually do w/ the `--validateMappings` additional command line flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499297622
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499297622:62,Integrability,integrat,integrated,62,"Hi @tamuanand ,; Thanks for raising this doubt. SA is already integrated into the salmon environment i.e. you just have to re index salmon using the `generateDecoyTranscriptome.sh` script from [here](https://github.com/COMBINE-lab/SalmonTools) and run salmon quant as you usually do w/ the `--validateMappings` additional command line flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499297622
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499297622:293,Security,validat,validateMappings,293,"Hi @tamuanand ,; Thanks for raising this doubt. SA is already integrated into the salmon environment i.e. you just have to re index salmon using the `generateDecoyTranscriptome.sh` script from [here](https://github.com/COMBINE-lab/SalmonTools) and run salmon quant as you usually do w/ the `--validateMappings` additional command line flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499297622
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499303095:1365,Availability,ping,ping,1365,"Hi @tamuanand,. Thanks @k3yavi for pointing out the major idea. Just to fill in some more details. The implementation of SA is, as Avi mentions, part of the mainline salmon code now (in the develop and master branch). We link, in the README, to some pre-constructed decoy-aware transcriptomes, but you can build your own for any organism where you have the transcriptome, the genome, and an annotation, using the script Avi linked to. There are a few ways to enable selective alignment, and the details are listed with the relevant flags in the release notes (we will be updating the documentation shortly with more detailed examples as well). Specifically, you can pass salmon the `—validateMappings` flag, which turns on selective alignment with some reasonable default parameters. You can, instead, pass the flag `—mimicBT2`, which is a meta-flag that enables selective alignment, and turns on a few other things that make the alignments more similar to the Bowtie2 parameters we discuss in the paper (e.g. it disallows orphan alignments). Finally, there is the `—mimicStrictBT2` flag, which mimics Bowtie2 parameters that disallow indels; however, we generally don’t recommend this flag unless you have a particular reason for using it. For any of these, once you’ve built a decoy-aware index, you need not do anything else special during quantification. We’ll ping back here with more details once we have more examples in place etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499303095
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499303095:545,Deployability,release,release,545,"Hi @tamuanand,. Thanks @k3yavi for pointing out the major idea. Just to fill in some more details. The implementation of SA is, as Avi mentions, part of the mainline salmon code now (in the develop and master branch). We link, in the README, to some pre-constructed decoy-aware transcriptomes, but you can build your own for any organism where you have the transcriptome, the genome, and an annotation, using the script Avi linked to. There are a few ways to enable selective alignment, and the details are listed with the relevant flags in the release notes (we will be updating the documentation shortly with more detailed examples as well). Specifically, you can pass salmon the `—validateMappings` flag, which turns on selective alignment with some reasonable default parameters. You can, instead, pass the flag `—mimicBT2`, which is a meta-flag that enables selective alignment, and turns on a few other things that make the alignments more similar to the Bowtie2 parameters we discuss in the paper (e.g. it disallows orphan alignments). Finally, there is the `—mimicStrictBT2` flag, which mimics Bowtie2 parameters that disallow indels; however, we generally don’t recommend this flag unless you have a particular reason for using it. For any of these, once you’ve built a decoy-aware index, you need not do anything else special during quantification. We’ll ping back here with more details once we have more examples in place etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499303095
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499303095:684,Security,validat,validateMappings,684,"Hi @tamuanand,. Thanks @k3yavi for pointing out the major idea. Just to fill in some more details. The implementation of SA is, as Avi mentions, part of the mainline salmon code now (in the develop and master branch). We link, in the README, to some pre-constructed decoy-aware transcriptomes, but you can build your own for any organism where you have the transcriptome, the genome, and an annotation, using the script Avi linked to. There are a few ways to enable selective alignment, and the details are listed with the relevant flags in the release notes (we will be updating the documentation shortly with more detailed examples as well). Specifically, you can pass salmon the `—validateMappings` flag, which turns on selective alignment with some reasonable default parameters. You can, instead, pass the flag `—mimicBT2`, which is a meta-flag that enables selective alignment, and turns on a few other things that make the alignments more similar to the Bowtie2 parameters we discuss in the paper (e.g. it disallows orphan alignments). Finally, there is the `—mimicStrictBT2` flag, which mimics Bowtie2 parameters that disallow indels; however, we generally don’t recommend this flag unless you have a particular reason for using it. For any of these, once you’ve built a decoy-aware index, you need not do anything else special during quantification. We’ll ping back here with more details once we have more examples in place etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499303095
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499373498:594,Availability,mask,masked,594,"Hi @k3yavi and @rob-p,. Thanks for the detailed info and pointing me to the links on your github page. One suggestion regarding semantics ([from the README page](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md)) - you might want to rethink if stating 80% homology is the correct way. I know it is prevalent in literature, etc but from a bioinformatics/computational biology expertise standpoint ""80% homology etc"" is wrong. I think you are meaning similarity/identity here when you refer to homology as written here on the README page:.; ```; to align transcriptome to an exon masked genome, with 80% homology and extracts the mapped genomic interval.; ```; Check these out: ; https://www.ncbi.nlm.nih.gov/books/NBK20255/#A23; https://twitter.com/MatthewMoscou/status/866227138575429633; http://bytesizebio.net/2009/07/15/distant-homology-and-being-a-little-pregnant/. Thanks once again,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499373498
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462:413,Energy Efficiency,adapt,adaptive,413,"Hi @tamuanand,. Thanks for the suggestion. You're right, of course, and we should change the wording in that readme. The cause of the sequence similarity is not always known, and frankly, not important for our particular application. We adopted this term as shorthand given it's common use and also because the version of MashMap used to compute these sequence-similar regions was introduced in the paper [A fast adaptive algorithm for computing whole-genome homology maps](https://academic.oup.com/bioinformatics/article/34/17/i748/5093242). In the preprint itself, we're generally careful to simply refer to these as sequence-similar regions ;).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462:413,Modifiability,adapt,adaptive,413,"Hi @tamuanand,. Thanks for the suggestion. You're right, of course, and we should change the wording in that readme. The cause of the sequence similarity is not always known, and frankly, not important for our particular application. We adopted this term as shorthand given it's common use and also because the version of MashMap used to compute these sequence-similar regions was introduced in the paper [A fast adaptive algorithm for computing whole-genome homology maps](https://academic.oup.com/bioinformatics/article/34/17/i748/5093242). In the preprint itself, we're generally careful to simply refer to these as sequence-similar regions ;).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462:594,Usability,simpl,simply,594,"Hi @tamuanand,. Thanks for the suggestion. You're right, of course, and we should change the wording in that readme. The cause of the sequence similarity is not always known, and frankly, not important for our particular application. We adopted this term as shorthand given it's common use and also because the version of MashMap used to compute these sequence-similar regions was introduced in the paper [A fast adaptive algorithm for computing whole-genome homology maps](https://academic.oup.com/bioinformatics/article/34/17/i748/5093242). In the preprint itself, we're generally careful to simply refer to these as sequence-similar regions ;).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499476462
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499542660:667,Availability,mask,masked,667,"Hi @rob-p . Thanks for getting back. I know there are lots of such instances in the literature which have wordings like ```50% homology``` etc - that's why I shared the book chapter from Eugene Koonin's book and the other references/quotes from Walter Fitch. We both cannot change what has already been published, however, when we write something ourselves, we can change the paradigm and represent things correctly. Also, the preprint paper has similar wordings that you might want to reconsider changing:. ```To obtain homologous sequences within a reference, we map the spliced transcript sequences against a version of the genome where all exon segments are hard-masked (i.e. replaced with N). We perform this mapping using MashMap 20, with segment size 500 and homology 80%. ```. Probably, change the first instance of ```homologous``` to '_identical_' and ```homology 80%``` to' _identity 80%_'. And I do not want to digress from the main issue or **take the sheen away from the great work from your group on the paper**.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499542660
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499542660:701,Performance,perform,perform,701,"Hi @rob-p . Thanks for getting back. I know there are lots of such instances in the literature which have wordings like ```50% homology``` etc - that's why I shared the book chapter from Eugene Koonin's book and the other references/quotes from Walter Fitch. We both cannot change what has already been published, however, when we write something ourselves, we can change the paradigm and represent things correctly. Also, the preprint paper has similar wordings that you might want to reconsider changing:. ```To obtain homologous sequences within a reference, we map the spliced transcript sequences against a version of the genome where all exon segments are hard-masked (i.e. replaced with N). We perform this mapping using MashMap 20, with segment size 500 and homology 80%. ```. Probably, change the first instance of ```homologous``` to '_identical_' and ```homology 80%``` to' _identity 80%_'. And I do not want to digress from the main issue or **take the sheen away from the great work from your group on the paper**.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499542660
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499544736:276,Usability,clear,clear,276,"@tamuanand,. Thank you for pointing out the relevant literature, and I definitely appreciate your clarity on this issue. Also, I completely agree with your suggested re-wordings in the manuscript, as they correct the mistaken terminology and make the overall intent even more clear. We will be sure to address this when we revise the pre-print. Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499544736
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499732849:667,Security,validat,validateMappings,667,"Hello @rob-p . While I have the luxury of catching your attention, I am going to be sneaky :) and refer you back to my original question on this thread - would like to know your response. To summarize, these are my questions:. 1. what Salmon ```quant```command line options would you recommend for QuantSeq data -- I realize you introduced 'noLengthCorrection' specifically for QuantSeq as mentioned in [Issue108](https://github.com/COMBINE-lab/salmon/issues/108) and [Issue177](https://github.com/COMBINE-lab/salmon/issues/177). - I thought this would be good one: ```salmon quant -i {input.index} -l A -1 {input.R1} -2 {input.R2} -o {output} --noLengthCorrection --validateMappings --gcBias --seqBias --posBias```. 2. likewise, what would be the command line if I chose to take SA approach (build gentrome.fa with decoys). Suggestion: once the dust has settled after the printing of this new paper, you should include these command line suggestions for QuantSeq in your tutorial/readthedocs/README sections. Thanks in advance",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-499732849
https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-545639470:25,Availability,ping,ping,25,Hello @rob-p . I want to ping your question above.; I'm sitting on a heap of new QuantSeq data and wanted to know about the commands recommended for such data - was this ever resolved?. Cheers!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/365#issuecomment-545639470
https://github.com/COMBINE-lab/salmon/issues/366#issuecomment-497391366:121,Deployability,release,release,121,Thanks again @alexvpickering .; We have fixed this in the develop branch and it will be part of the salmon from the next release which we plan to release very soon.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/366#issuecomment-497391366
https://github.com/COMBINE-lab/salmon/issues/366#issuecomment-497391366:146,Deployability,release,release,146,Thanks again @alexvpickering .; We have fixed this in the develop branch and it will be part of the salmon from the next release which we plan to release very soon.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/366#issuecomment-497391366
https://github.com/COMBINE-lab/salmon/issues/367#issuecomment-497390977:308,Availability,down,down,308,"Thanks for the kind word @alexvpickering and bringing this to our attention.; It does makes sense to have this clarified in alevin docs. We use `mRNA` for optional whitelisting at the end and you are right about aleivn using the fraction of reads from the mtRNA as a feature, so if the reads are both up and down regulated then the feature would not be very useful as it might cancel out the overall effect. May be we can use more fine grained at per gene level information as a feature but currently we have not put enough thoughts into it but we will clarify this in the alevin docs.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/367#issuecomment-497390977
https://github.com/COMBINE-lab/salmon/issues/367#issuecomment-498038312:8,Deployability,update,updated,8,"We have updated the docs. Thanks for the heads up . We are closing this issue for now,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/367#issuecomment-498038312
https://github.com/COMBINE-lab/salmon/issues/368#issuecomment-498037183:89,Deployability,release,released,89,"Hey @plbaldoni ,. Thanks for raising the issue, it does seem like a corner case. We just released `v0.14.0`, can you please check if this still an issue? If it is can you please share a minimal data so that we can replicate and work on solving this issue ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/368#issuecomment-498037183
https://github.com/COMBINE-lab/salmon/issues/368#issuecomment-498038220:67,Testability,log,log,67,"Also, it's worth noting that the number you see in the interactive log is an artifact of the way the console refreshes in asynchronous logging. That is, salmon does not think it observed that many unique fragments. You can verify this, as the sum of the NumReads column in quant.sf is an upper bound on the total mapped fragments and thus the uniquely mapped fragments. The nymber of mapped fragments is also reported in aux_info/meta_info.json.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/368#issuecomment-498038220
https://github.com/COMBINE-lab/salmon/issues/368#issuecomment-498038220:135,Testability,log,logging,135,"Also, it's worth noting that the number you see in the interactive log is an artifact of the way the console refreshes in asynchronous logging. That is, salmon does not think it observed that many unique fragments. You can verify this, as the sum of the NumReads column in quant.sf is an upper bound on the total mapped fragments and thus the uniquely mapped fragments. The nymber of mapped fragments is also reported in aux_info/meta_info.json.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/368#issuecomment-498038220
https://github.com/COMBINE-lab/salmon/issues/368#issuecomment-504560952:317,Deployability,pipeline,pipeline,317,"Thanks @k3yavi and @rob-p . . Regarding the number of mapped reads, everything looks good in aux_info/meta_info.json. That number was indeed an artifact. However, I still could not figure out why there is such a discrepancy between the read counts of these replicates. . How would it be the best way to share my data/pipeline? The data are coming from the Roadmap project (GSM1112836 and GSM916094). I am getting the fasta files from the bed files with `bedtools getfasta`, aligning them to the transcriptome with bowtie2, and then running salmon on the resulting bam files using alignment-based mode. Does it seem to be the correct way to process these data?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/368#issuecomment-504560952
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-498955579:131,Availability,error,error,131,"Hi @k3yavi ,; At first glance, the idea seems good. What does the `--end` parameter do?. I tried your idea but it gives me another error. `salmon alevin -l U -1 c1a.fastq c2a.fastq ... -2 c1b.fastq c2b.fastq ... -i salmon_index -p 10 -o levin_results --end 5 --barcodeLength 16 --umiLength 6 --chromium --tgMap dict_transcript_gene.tsv --expectCells 50`. ```; Exception : [Error reading from the FASTA/Q stream. Minimum return code for left and right read was (-2). Make sure the file is valid.]; salmon alevin was invoked improperly.; For usage information, try salmon alevin --help; Exiting.; [2019-06-05 08:20:37.952] [alevinLog] [info] Processing barcodes files (if Present) ; ```. The FASTQ file of the reads is not paired-end, maybe there's the error? Here is a sample of the CB+UMI FASTQ, and of the reads FASTQ. CB+UMI FASTQ; ```; @ERS2271611.246_AGCCTC 246 length=51; GGTGAACGAGGCGAAGAGCCTC; +ERS2271611.246_AGCCTC 246 length=51; AAAAAAAAAAAAAAAAAAAAAAAAAA; @ERS2271611.247_GTGCTA 247 length=51; GGTGAACGAGGCGAAGGTGCTA; +ERS2271611.247_GTGCTA 247 length=51; AAAAAAAAAAAAAAAAAAAAAAAAAA; @ERS2271611.248_AATAAA 248 length=51; GGTGAACGAGGCGAAGAATAAA; ```. Reads FASTQ; ```; @ERS2271611.246_AGCCTC 246 length=51; CAAATTCACATTATATGGACAGAATATAGAAAAGTCCTTTCC; +; FFFFIIIIIIIIIIIFIFFIIIIIIIIIIIFIBFFFBFFIII; @ERS2271611.247_GTGCTA 247 length=51; TTTGAATGCCTGGAAGGTTACTTACCCTCTTTAGTTACTCCT; +; FFFFIIIIIIIIIFFIIFFIIIIIIIIIIIIIIIIIIIIIII; @ERS2271611.248_AATAAA 248 length=51; GTTGAATCATACCTTTCGATTACATCTTTTAAGCAAACCCTT; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-498955579
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-498955579:373,Availability,Error,Error,373,"Hi @k3yavi ,; At first glance, the idea seems good. What does the `--end` parameter do?. I tried your idea but it gives me another error. `salmon alevin -l U -1 c1a.fastq c2a.fastq ... -2 c1b.fastq c2b.fastq ... -i salmon_index -p 10 -o levin_results --end 5 --barcodeLength 16 --umiLength 6 --chromium --tgMap dict_transcript_gene.tsv --expectCells 50`. ```; Exception : [Error reading from the FASTA/Q stream. Minimum return code for left and right read was (-2). Make sure the file is valid.]; salmon alevin was invoked improperly.; For usage information, try salmon alevin --help; Exiting.; [2019-06-05 08:20:37.952] [alevinLog] [info] Processing barcodes files (if Present) ; ```. The FASTQ file of the reads is not paired-end, maybe there's the error? Here is a sample of the CB+UMI FASTQ, and of the reads FASTQ. CB+UMI FASTQ; ```; @ERS2271611.246_AGCCTC 246 length=51; GGTGAACGAGGCGAAGAGCCTC; +ERS2271611.246_AGCCTC 246 length=51; AAAAAAAAAAAAAAAAAAAAAAAAAA; @ERS2271611.247_GTGCTA 247 length=51; GGTGAACGAGGCGAAGGTGCTA; +ERS2271611.247_GTGCTA 247 length=51; AAAAAAAAAAAAAAAAAAAAAAAAAA; @ERS2271611.248_AATAAA 248 length=51; GGTGAACGAGGCGAAGAATAAA; ```. Reads FASTQ; ```; @ERS2271611.246_AGCCTC 246 length=51; CAAATTCACATTATATGGACAGAATATAGAAAAGTCCTTTCC; +; FFFFIIIIIIIIIIIFIFFIIIIIIIIIIIFIBFFFBFFIII; @ERS2271611.247_GTGCTA 247 length=51; TTTGAATGCCTGGAAGGTTACTTACCCTCTTTAGTTACTCCT; +; FFFFIIIIIIIIIFFIIFFIIIIIIIIIIIIIIIIIIIIIII; @ERS2271611.248_AATAAA 248 length=51; GTTGAATCATACCTTTCGATTACATCTTTTAAGCAAACCCTT; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-498955579
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-498955579:751,Availability,error,error,751,"Hi @k3yavi ,; At first glance, the idea seems good. What does the `--end` parameter do?. I tried your idea but it gives me another error. `salmon alevin -l U -1 c1a.fastq c2a.fastq ... -2 c1b.fastq c2b.fastq ... -i salmon_index -p 10 -o levin_results --end 5 --barcodeLength 16 --umiLength 6 --chromium --tgMap dict_transcript_gene.tsv --expectCells 50`. ```; Exception : [Error reading from the FASTA/Q stream. Minimum return code for left and right read was (-2). Make sure the file is valid.]; salmon alevin was invoked improperly.; For usage information, try salmon alevin --help; Exiting.; [2019-06-05 08:20:37.952] [alevinLog] [info] Processing barcodes files (if Present) ; ```. The FASTQ file of the reads is not paired-end, maybe there's the error? Here is a sample of the CB+UMI FASTQ, and of the reads FASTQ. CB+UMI FASTQ; ```; @ERS2271611.246_AGCCTC 246 length=51; GGTGAACGAGGCGAAGAGCCTC; +ERS2271611.246_AGCCTC 246 length=51; AAAAAAAAAAAAAAAAAAAAAAAAAA; @ERS2271611.247_GTGCTA 247 length=51; GGTGAACGAGGCGAAGGTGCTA; +ERS2271611.247_GTGCTA 247 length=51; AAAAAAAAAAAAAAAAAAAAAAAAAA; @ERS2271611.248_AATAAA 248 length=51; GGTGAACGAGGCGAAGAATAAA; ```. Reads FASTQ; ```; @ERS2271611.246_AGCCTC 246 length=51; CAAATTCACATTATATGGACAGAATATAGAAAAGTCCTTTCC; +; FFFFIIIIIIIIIIIFIFFIIIIIIIIIIIFIBFFFBFFIII; @ERS2271611.247_GTGCTA 247 length=51; TTTGAATGCCTGGAAGGTTACTTACCCTCTTTAGTTACTCCT; +; FFFFIIIIIIIIIFFIIFFIIIIIIIIIIIIIIIIIIIIIII; @ERS2271611.248_AATAAA 248 length=51; GTTGAATCATACCTTTCGATTACATCTTTTAAGCAAACCCTT; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-498955579
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:54,Availability,error,error,54,"Hi @k3yavi ,. I've corrected the CB+UMI FASTQ and the error persists. . ```; [2019-06-06 19:23:29.734] [alevinLog] [info] Found all transcripts to gene mappings; [2019-06-06 19:23:29.749] [alevinLog] [info] Processing barcodes files (if Present) . ; processed 31 Million barcodes. [2019-06-06 19:24:55.681] [alevinLog] [info] Done barcode density calculation.; [2019-06-06 19:24:55.681] [alevinLog] [info] # Barcodes Used: 31478936 / 31478936.; [2019-06-06 19:24:55.688] [alevinLog] [info] Total 247(has 200 low confidence) barcodes; [2019-06-06 19:24:55.688] [alevinLog] [info] Done True Barcode Sampling; [2019-06-06 19:24:55.690] [alevinLog] [info] Total 0% reads will be thrown away because of noisy Cellular barcodes.; [2019-06-06 19:24:55.692] [alevinLog] [info] Done populating Z matrix; [2019-06-06 19:24:55.692] [alevinLog] [info] Done indexing Barcodes; [2019-06-06 19:24:55.692] [alevinLog] [info] Total Unique barcodes found: 50; [2019-06-06 19:24:55.692] [alevinLog] [info] Used Barcodes except Whitelist: 0; [2019-06-06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.55",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:2082,Availability,error,error,2082,"Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [join",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:1300,Performance,Load,Loading,1300,"levinLog] [info] Done barcode density calculation.; [2019-06-06 19:24:55.681] [alevinLog] [info] # Barcodes Used: 31478936 / 31478936.; [2019-06-06 19:24:55.688] [alevinLog] [info] Total 247(has 200 low confidence) barcodes; [2019-06-06 19:24:55.688] [alevinLog] [info] Done True Barcode Sampling; [2019-06-06 19:24:55.690] [alevinLog] [info] Total 0% reads will be thrown away because of noisy Cellular barcodes.; [2019-06-06 19:24:55.692] [alevinLog] [info] Done populating Z matrix; [2019-06-06 19:24:55.692] [alevinLog] [info] Done indexing Barcodes; [2019-06-06 19:24:55.692] [alevinLog] [info] Total Unique barcodes found: 50; [2019-06-06 19:24:55.692] [alevinLog] [info] Used Barcodes except Whitelist: 0; [2019-06-06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:1365,Performance,Load,Loading,1365,":55.681] [alevinLog] [info] # Barcodes Used: 31478936 / 31478936.; [2019-06-06 19:24:55.688] [alevinLog] [info] Total 247(has 200 low confidence) barcodes; [2019-06-06 19:24:55.688] [alevinLog] [info] Done True Barcode Sampling; [2019-06-06 19:24:55.690] [alevinLog] [info] Total 0% reads will be thrown away because of noisy Cellular barcodes.; [2019-06-06 19:24:55.692] [alevinLog] [info] Done populating Z matrix; [2019-06-06 19:24:55.692] [alevinLog] [info] Done indexing Barcodes; [2019-06-06 19:24:55.692] [alevinLog] [info] Total Unique barcodes found: 50; [2019-06-06 19:24:55.692] [alevinLog] [info] Used Barcodes except Whitelist: 0; [2019-06-06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies us",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:1438,Performance,Load,Loading,1438,"19-06-06 19:24:55.688] [alevinLog] [info] Total 247(has 200 low confidence) barcodes; [2019-06-06 19:24:55.688] [alevinLog] [info] Done True Barcode Sampling; [2019-06-06 19:24:55.690] [alevinLog] [info] Total 0% reads will be thrown away because of noisy Cellular barcodes.; [2019-06-06 19:24:55.692] [alevinLog] [info] Done populating Z matrix; [2019-06-06 19:24:55.692] [alevinLog] [info] Done indexing Barcodes; [2019-06-06 19:24:55.692] [alevinLog] [info] Total Unique barcodes found: 50; [2019-06-06 19:24:55.692] [alevinLog] [info] Used Barcodes except Whitelist: 0; [2019-06-06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:1506,Performance,Load,Loading,1506,"dence) barcodes; [2019-06-06 19:24:55.688] [alevinLog] [info] Done True Barcode Sampling; [2019-06-06 19:24:55.690] [alevinLog] [info] Total 0% reads will be thrown away because of noisy Cellular barcodes.; [2019-06-06 19:24:55.692] [alevinLog] [info] Done populating Z matrix; [2019-06-06 19:24:55.692] [alevinLog] [info] Done indexing Barcodes; [2019-06-06 19:24:55.692] [alevinLog] [info] Total Unique barcodes found: 50; [2019-06-06 19:24:55.692] [alevinLog] [info] Used Barcodes except Whitelist: 0; [2019-06-06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --valid",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:1577,Performance,Load,Loading,1577,"rcode Sampling; [2019-06-06 19:24:55.690] [alevinLog] [info] Total 0% reads will be thrown away because of noisy Cellular barcodes.; [2019-06-06 19:24:55.692] [alevinLog] [info] Done populating Z matrix; [2019-06-06 19:24:55.692] [alevinLog] [info] Done indexing Barcodes; [2019-06-06 19:24:55.692] [alevinLog] [info] Total Unique barcodes found: 50; [2019-06-06 19:24:55.692] [alevinLog] [info] Used Barcodes except Whitelist: 0; [2019-06-06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. ran",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:1837,Performance,load,loading,1837,"[info] Done indexing Barcodes; [2019-06-06 19:24:55.692] [alevinLog] [info] Total Unique barcodes found: 50; [2019-06-06 19:24:55.692] [alevinLog] [info] Used Barcodes except Whitelist: 0; [2019-06-06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:2028,Performance,load,loading,2028,"06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [joi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:3046,Performance,Load,Loading,3046,"lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; ```. It is interesting because the barcodes are recognized during the processing, but they don't appear in the frequency table? I don0t get that part. > Can you clarify a bit more about what you meant with: The FASTQ file of the reads is not paired-end. I mean that each of the files has all the unique reads, that is, it is not a paired-end sample where one fastq is forward and the other one is reverse. I just mentioned it in case it was necessary for the `--end 5`parameter. Thanks for the help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:3111,Performance,Load,Loading,3111,"lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; ```. It is interesting because the barcodes are recognized during the processing, but they don't appear in the frequency table? I don0t get that part. > Can you clarify a bit more about what you meant with: The FASTQ file of the reads is not paired-end. I mean that each of the files has all the unique reads, that is, it is not a paired-end sample where one fastq is forward and the other one is reverse. I just mentioned it in case it was necessary for the `--end 5`parameter. Thanks for the help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:1845,Security,hash,hash,1845,"[info] Done indexing Barcodes; [2019-06-06 19:24:55.692] [alevinLog] [info] Total Unique barcodes found: 50; [2019-06-06 19:24:55.692] [alevinLog] [info] Used Barcodes except Whitelist: 0; [2019-06-06 19:24:55.716] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-06-06 19:24:55.716] [alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:2351,Security,validat,validateMappings,2351,"[info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; ```. It is interesting because the barcodes are recognized during the processing, but they do",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:2513,Security,validat,validateMappings,2513,"2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; ```. It is interesting because the barcodes are recognized during the processing, but they don't appear in the frequency table? I don0t get that part. > Can you clarify a bit more about what you meant with: The FASTQ file of the reads is not paired-end. I mean that e",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:2685,Security,validat,validateMappings,2685,"y; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; ```. It is interesting because the barcodes are recognized during the processing, but they don't appear in the frequency table? I don0t get that part. > Can you clarify a bit more about what you meant with: The FASTQ file of the reads is not paired-end. I mean that each of the files has all the unique reads, that is, it is not a paired-end sample where one fastq is forward and the other one is reverse. I just mentioned it in c",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790:2145,Testability,log,log,2145,"alevinLog] [info] parsing read library format; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019-06-06 19:24:55.890] [stderrLog] [info] Loading Suffix Array ; [2019-06-06 19:24:56.791] [stderrLog] [info] Loading Transcript Info ; [2019-06-06 19:24:57.025] [stderrLog] [info] Loading Rank-Select Bit Array; [2019-06-06 19:24:57.061] [stderrLog] [info] There were 136,011 set bits in the bit array; [2019-06-06 19:24:57.084] [stderrLog] [info] Computing transcript lengths; [2019-06-06 19:24:57.084] [stderrLog] [info] Waiting to finish loading hash; [2019-06-06 19:25:06.552] [jointLog] [info] done; [2019-06-06 19:25:06.552] [jointLog] [info] Index contained 136,011 targets; [2019-06-06 19:25:06.552] [stderrLog] [info] Done loading index; [2019-06-06 19:25:06.728] [alevinLog] [error] Barcode not found in frequency table; ```. Salmon Quant log is this. ```; [2019-06-06 19:23:29.519] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-06-06 19:23:29.519] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-06-06 19:23:29.520] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-06-06 19:23:29.520] [jointLog] [info] Using default value of 0.87 for minScoreFraction in Alevin; Using default value of 0.6 for consensusSlack in Alevin; [2019-06-06 19:24:55.716] [jointLog] [info] There is 1 library.; [2019-06-06 19:24:55.889] [jointLog] [info] Loading Quasi index; [2019-06-06 19:24:55.889] [jointLog] [info] Loading 32-bit quasi index; [2019",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/369#issuecomment-499592790
https://github.com/COMBINE-lab/salmon/issues/370#issuecomment-499513288:739,Deployability,integrat,integrated,739,"HI @Acribbs ,. Thanks for the very interesting question.; I think the first question in my mind is, do you need intron level deduplicated counts ? If yes, then sadly concatenating the pre-mrna sequence into the transcriptome sequences is probably not a good idea, as in general, the length of intronic sequences are much longer than that of exonic sequence and it may bias alevin deduplication algorithm. However, if you don't need the number of unspliced deduplicated counts and as the nuceli scRNA-seq has more pre-mRNA data if the question is regarding the aligning to genome v transcriptome then we just proposed a solution in our latest [preprint](https://www.biorxiv.org/content/10.1101/657874v1?rss=1). The new SA method is already integrated into salmon but you may have to index the genome+transcriptome using our scripts from [here](https://github.com/COMBINE-lab/SalmonTools). Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/370#issuecomment-499513288
https://github.com/COMBINE-lab/salmon/issues/370#issuecomment-499513288:739,Integrability,integrat,integrated,739,"HI @Acribbs ,. Thanks for the very interesting question.; I think the first question in my mind is, do you need intron level deduplicated counts ? If yes, then sadly concatenating the pre-mrna sequence into the transcriptome sequences is probably not a good idea, as in general, the length of intronic sequences are much longer than that of exonic sequence and it may bias alevin deduplication algorithm. However, if you don't need the number of unspliced deduplicated counts and as the nuceli scRNA-seq has more pre-mRNA data if the question is regarding the aligning to genome v transcriptome then we just proposed a solution in our latest [preprint](https://www.biorxiv.org/content/10.1101/657874v1?rss=1). The new SA method is already integrated into salmon but you may have to index the genome+transcriptome using our scripts from [here](https://github.com/COMBINE-lab/SalmonTools). Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/370#issuecomment-499513288
https://github.com/COMBINE-lab/salmon/issues/370#issuecomment-503643808:213,Performance,optimiz,optimizations,213,"Hi @Acribbs ,. Thanks for your kind words.; Basically, modularity is very important part of the whole salmon framework. Alignment and quantification are two separate module and they are all interconnected, so the optimizations in one is automatically propagated to another. As a result, the optimizations of SA done in the alignment stage got propagated to both alevin (single-cell) and salmon (bulk) RNA-seq quantification. As a summary, from 0.14.0 onwards alevin can automatically utilize the benefits of SA if the reference is indexed following the scripts we shared in the SalmonTools repo. Hope it answers your question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/370#issuecomment-503643808
https://github.com/COMBINE-lab/salmon/issues/370#issuecomment-503643808:291,Performance,optimiz,optimizations,291,"Hi @Acribbs ,. Thanks for your kind words.; Basically, modularity is very important part of the whole salmon framework. Alignment and quantification are two separate module and they are all interconnected, so the optimizations in one is automatically propagated to another. As a result, the optimizations of SA done in the alignment stage got propagated to both alevin (single-cell) and salmon (bulk) RNA-seq quantification. As a summary, from 0.14.0 onwards alevin can automatically utilize the benefits of SA if the reference is indexed following the scripts we shared in the SalmonTools repo. Hope it answers your question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/370#issuecomment-503643808
https://github.com/COMBINE-lab/salmon/issues/371#issuecomment-501432106:307,Usability,simpl,simply,307,"Hi,; Thank you for the detailed explanation. ; For the first question, ideally you should include the alternate alleles. But they should be part of the transcriptome, instead of the genome. We expect the size of the decoys to double if alt alleles are included in the genome. This is because the decoys are simply regions of high sequence identity between a transcript and the genome. Hence, with alleles as part of the genome, each transcript will map almost equally well to alleles.; If you're running salmon in the alignment mode, the input bam/sam file should include the CIGAR string as well. There is a flag, `--noErrorModel`, to ignore the CIGAR but that is not recommended, since salmon uses the information for scoring the alignments. ; I hope that answers your concerns.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/371#issuecomment-501432106
https://github.com/COMBINE-lab/salmon/issues/373#issuecomment-501284160:34,Security,access,access,34,"Hi @PeteCausey-Freeman ,. You can access the script and the requirements [here](https://github.com/COMBINE-lab/SalmonTools). Look for `generateDecoyTranscriptome`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/373#issuecomment-501284160
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501681776:41,Testability,log,logs,41,"Hi @Ryan-Zhu ,; Can you please share the logs ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501681776
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672:422,Availability,down,downstream,422,"Thanks @Ryan-Zhu ,. A couple of thoughts.; To me the data seem a little noisy as a lot of CB/reads are thrown away due to ""knee"" thresholding.; Check https://github.com/COMBINE-lab/salmon/issues/362 if you wanna play with how to customize alevin for user-define whitelisting. Having said that this is how you can parse the data from alevin.; Alevin use 1277 CB after its knee thresholding + 638 low confidence Barcode for downstream whitelisting = total 1915 CBs.; If you check the warning in the log it says :. ```; [2019-06-12 15:07:08.152] [alevinLog] [warning] Skipped 313 barcodes due to No mapped read; ```; Basically it means out of 1915, 313 didn't had any read mapped to them, so alevin doesn't report them in the output matrix. Alevin reports 1915 - 313 = 1602 CBs both in `.mtx` and `quants_mat.gz` file. You can check the order of the CB in the `quants_mat_rows.txt` file, which has 1602 rows/CBs. If you don't provide alevin with external whitelist alevin tries to do post whitelisting of it's own. Basically out of the 1277 high confidence CBs alevin initially find out through knee it assigns 647 CBs as final whitelisted CB as found in the `whitelist.txt` file. If you wan't to subsample these CBs you have to extract the information from the `.mtx` or `quants_mat.gz` file. You can check a simple python parse of the `quants_mat.gz` file [here](https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.py#L187-L230).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672:497,Testability,log,log,497,"Thanks @Ryan-Zhu ,. A couple of thoughts.; To me the data seem a little noisy as a lot of CB/reads are thrown away due to ""knee"" thresholding.; Check https://github.com/COMBINE-lab/salmon/issues/362 if you wanna play with how to customize alevin for user-define whitelisting. Having said that this is how you can parse the data from alevin.; Alevin use 1277 CB after its knee thresholding + 638 low confidence Barcode for downstream whitelisting = total 1915 CBs.; If you check the warning in the log it says :. ```; [2019-06-12 15:07:08.152] [alevinLog] [warning] Skipped 313 barcodes due to No mapped read; ```; Basically it means out of 1915, 313 didn't had any read mapped to them, so alevin doesn't report them in the output matrix. Alevin reports 1915 - 313 = 1602 CBs both in `.mtx` and `quants_mat.gz` file. You can check the order of the CB in the `quants_mat_rows.txt` file, which has 1602 rows/CBs. If you don't provide alevin with external whitelist alevin tries to do post whitelisting of it's own. Basically out of the 1277 high confidence CBs alevin initially find out through knee it assigns 647 CBs as final whitelisted CB as found in the `whitelist.txt` file. If you wan't to subsample these CBs you have to extract the information from the `.mtx` or `quants_mat.gz` file. You can check a simple python parse of the `quants_mat.gz` file [here](https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.py#L187-L230).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672:1307,Usability,simpl,simple,1307,"Thanks @Ryan-Zhu ,. A couple of thoughts.; To me the data seem a little noisy as a lot of CB/reads are thrown away due to ""knee"" thresholding.; Check https://github.com/COMBINE-lab/salmon/issues/362 if you wanna play with how to customize alevin for user-define whitelisting. Having said that this is how you can parse the data from alevin.; Alevin use 1277 CB after its knee thresholding + 638 low confidence Barcode for downstream whitelisting = total 1915 CBs.; If you check the warning in the log it says :. ```; [2019-06-12 15:07:08.152] [alevinLog] [warning] Skipped 313 barcodes due to No mapped read; ```; Basically it means out of 1915, 313 didn't had any read mapped to them, so alevin doesn't report them in the output matrix. Alevin reports 1915 - 313 = 1602 CBs both in `.mtx` and `quants_mat.gz` file. You can check the order of the CB in the `quants_mat_rows.txt` file, which has 1602 rows/CBs. If you don't provide alevin with external whitelist alevin tries to do post whitelisting of it's own. Basically out of the 1277 high confidence CBs alevin initially find out through knee it assigns 647 CBs as final whitelisted CB as found in the `whitelist.txt` file. If you wan't to subsample these CBs you have to extract the information from the `.mtx` or `quants_mat.gz` file. You can check a simple python parse of the `quants_mat.gz` file [here](https://github.com/k3yavi/vpolo/blob/master/vpolo/alevin/parser.py#L187-L230).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501846672
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501943370:241,Deployability,release,release,241,"Hi @Ryan-Zhu ,. Thanks a lot for bringing this to our attention.; We have fixed this in the latest commit of the develop branch https://github.com/COMBINE-lab/salmon/commit/e93d6cee19c46d56d603e75097dbe17ab18e6811 and will merge in the next release . Usually the number of skipped Barcodes due to no mapped reads are relatively few that's why this corner case slipped from our testing. If it's possible for you to compile salmon from source you can use the develop branch to generate the new binary otherwise let us know we can provide a temporary linux binary until the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501943370
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501943370:576,Deployability,release,release,576,"Hi @Ryan-Zhu ,. Thanks a lot for bringing this to our attention.; We have fixed this in the latest commit of the develop branch https://github.com/COMBINE-lab/salmon/commit/e93d6cee19c46d56d603e75097dbe17ab18e6811 and will merge in the next release . Usually the number of skipped Barcodes due to no mapped reads are relatively few that's why this corner case slipped from our testing. If it's possible for you to compile salmon from source you can use the develop branch to generate the new binary otherwise let us know we can provide a temporary linux binary until the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501943370
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501943370:377,Testability,test,testing,377,"Hi @Ryan-Zhu ,. Thanks a lot for bringing this to our attention.; We have fixed this in the latest commit of the develop branch https://github.com/COMBINE-lab/salmon/commit/e93d6cee19c46d56d603e75097dbe17ab18e6811 and will merge in the next release . Usually the number of skipped Barcodes due to no mapped reads are relatively few that's why this corner case slipped from our testing. If it's possible for you to compile salmon from source you can use the develop branch to generate the new binary otherwise let us know we can provide a temporary linux binary until the next release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-501943370
https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-502230996:235,Testability,test,tested,235,"Corner cases are hard to catch. The drop-seq dataset I was probing was from 2-3 years ago and it appeared very noisy. When I tried Alevin with a morden 10x V2 dataset there was no issue at all. ; Anyway, thank you for the quick fix. I tested it and it worked well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/374#issuecomment-502230996
https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501828238:36,Testability,log,log,36,"Hi @hliu5259 ,; can you forward the log ?; There can be multiple reasons, did you gave external whitelist ? When you say 9253 samples do you mean 9253 cells ? How did you fix the number of cells ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501828238
https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501831515:16,Testability,log,log,16,"Hi, this is the log. I give the external whitelist which inculed 9185 barcodes. ; Yeah, I mean the 9253 cells. It comes from the output from the paper. . Bests,; Hongyu. > On Jun 13, 2019, at 2:33 PM, Avi Srivastava <notifications@github.com> wrote:; > ; > Hi @hliu5259 <https://github.com/hliu5259> ,; > can you forward the log ?; > There can be multiple reasons, did you gave external whitelist ? When you say 9253 samples do you mean 9253 cells ? How did you fix the number of cells ?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/COMBINE-lab/salmon/issues/375?email_source=notifications&email_token=AK7LCF6ALRVM7WPLKREVDT3P2KHI3A5CNFSM4HX4WLH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXUUVDQ#issuecomment-501828238>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AK7LCF5HE6PY2FLHJB26QO3P2KHI3ANCNFSM4HX4WLHQ>.; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501831515
https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501831515:325,Testability,log,log,325,"Hi, this is the log. I give the external whitelist which inculed 9185 barcodes. ; Yeah, I mean the 9253 cells. It comes from the output from the paper. . Bests,; Hongyu. > On Jun 13, 2019, at 2:33 PM, Avi Srivastava <notifications@github.com> wrote:; > ; > Hi @hliu5259 <https://github.com/hliu5259> ,; > can you forward the log ?; > There can be multiple reasons, did you gave external whitelist ? When you say 9253 samples do you mean 9253 cells ? How did you fix the number of cells ?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/COMBINE-lab/salmon/issues/375?email_source=notifications&email_token=AK7LCF6ALRVM7WPLKREVDT3P2KHI3A5CNFSM4HX4WLH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXUUVDQ#issuecomment-501828238>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AK7LCF5HE6PY2FLHJB26QO3P2KHI3ANCNFSM4HX4WLHQ>.; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501831515
https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501839908:33,Testability,log,log,33,I think you forgot to attach the log file ?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501839908
https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501845900:181,Testability,log,log,181,"I already attach the file. Can you see it right now?. Bests. > On Jun 13, 2019, at 3:07 PM, Avi Srivastava <notifications@github.com> wrote:; > ; > I think you forgot to attach the log file ?; > ; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub <https://github.com/COMBINE-lab/salmon/issues/375?email_source=notifications&email_token=AK7LCF6HIQSMM4UTPONU37TP2KLH5A5CNFSM4HX4WLH2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGODXUXQJA#issuecomment-501839908>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AK7LCFZKFOYIZEGYU5YOIX3P2KLH5ANCNFSM4HX4WLHQ>.; >",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/375#issuecomment-501845900
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-502818453:542,Deployability,release,release,542,"Hi @alexvpickering ,. Thanks for raising the issue. It seems #377 and #379 are connected .; Alevin is in fact suppose to output whitelist.txt file when provided with the flags you provided.; I think what's happening in your case is since `--keepCBFraction 1`, alevin is using all the CB for quantification and it couldn't find (any or very low) CB from the low confidence region needed for the whitelisting. ; Basically in the above screenshot, alevin never finished. It should have failed more gracefully, I'll make sure of that in the next release. In the meantime you can use the exit code 0 or ""Finished Optimizer"" log for successful finish. Also, try playing with the lower values for the `keepCBFracion` may be around (0.4 / 0.5) and `--freqThreshold` for changing the minimum frequency of a CB to consider, currently set to 10. You can also follow https://github.com/COMBINE-lab/salmon/issues/362 for more details.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-502818453
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-502818453:608,Performance,Optimiz,Optimizer,608,"Hi @alexvpickering ,. Thanks for raising the issue. It seems #377 and #379 are connected .; Alevin is in fact suppose to output whitelist.txt file when provided with the flags you provided.; I think what's happening in your case is since `--keepCBFraction 1`, alevin is using all the CB for quantification and it couldn't find (any or very low) CB from the low confidence region needed for the whitelisting. ; Basically in the above screenshot, alevin never finished. It should have failed more gracefully, I'll make sure of that in the next release. In the meantime you can use the exit code 0 or ""Finished Optimizer"" log for successful finish. Also, try playing with the lower values for the `keepCBFracion` may be around (0.4 / 0.5) and `--freqThreshold` for changing the minimum frequency of a CB to consider, currently set to 10. You can also follow https://github.com/COMBINE-lab/salmon/issues/362 for more details.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-502818453
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-502818453:619,Testability,log,log,619,"Hi @alexvpickering ,. Thanks for raising the issue. It seems #377 and #379 are connected .; Alevin is in fact suppose to output whitelist.txt file when provided with the flags you provided.; I think what's happening in your case is since `--keepCBFraction 1`, alevin is using all the CB for quantification and it couldn't find (any or very low) CB from the low confidence region needed for the whitelisting. ; Basically in the above screenshot, alevin never finished. It should have failed more gracefully, I'll make sure of that in the next release. In the meantime you can use the exit code 0 or ""Finished Optimizer"" log for successful finish. Also, try playing with the lower values for the `keepCBFracion` may be around (0.4 / 0.5) and `--freqThreshold` for changing the minimum frequency of a CB to consider, currently set to 10. You can also follow https://github.com/COMBINE-lab/salmon/issues/362 for more details.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-502818453
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503319334:43,Testability,log,log,43,"Is it possible for you to share the alevin log, then I can explain better the numbers ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503319334
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503326828:25,Testability,log,log,25,Thanks @k3yavi . [alevin.log](https://github.com/COMBINE-lab/salmon/files/3303575/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3303576/salmon_quant.log),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503326828
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503326828:89,Testability,log,log,89,Thanks @k3yavi . [alevin.log](https://github.com/COMBINE-lab/salmon/files/3303575/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3303576/salmon_quant.log),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503326828
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503326828:109,Testability,log,log,109,Thanks @k3yavi . [alevin.log](https://github.com/COMBINE-lab/salmon/files/3303575/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3303576/salmon_quant.log),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503326828
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503326828:179,Testability,log,log,179,Thanks @k3yavi . [alevin.log](https://github.com/COMBINE-lab/salmon/files/3303575/alevin.log); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/3303576/salmon_quant.log),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503326828
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686:879,Deployability,update,update,879,"okies, I think I see the issue. So look at the following lines in the log:; ```; [2019-06-17 21:21:44.518] [alevinLog] [info] Total 824863; [2019-06-17 21:22:47.680] [alevinLog] [info] Total Unique barcodes found: 3474567; ```; What it means is alevin found total: `3,474,567` unique CB in the whole sample and keeps `824,863` CB for further processing which is ~23% of the CB. So all the `keepCBFraction` values above 0.23 would have no effect. If you wan't to generate the `whitelist.txt`, alevin has to have some low confidence CB to learn from, so I am guessing in your case any value from 0.15-0.20 should ideally work. Having said that, I am still exploring why even setting `freqThreshold` to 0, alevin not considers all `3M` CB for processing, I guess there is some kind of filter which is coming into the picture but I might need a bit more time to explore that. I will update here once I figure it out. Thanks again for raising the issue and investing your time in improving alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686:70,Testability,log,log,70,"okies, I think I see the issue. So look at the following lines in the log:; ```; [2019-06-17 21:21:44.518] [alevinLog] [info] Total 824863; [2019-06-17 21:22:47.680] [alevinLog] [info] Total Unique barcodes found: 3474567; ```; What it means is alevin found total: `3,474,567` unique CB in the whole sample and keeps `824,863` CB for further processing which is ~23% of the CB. So all the `keepCBFraction` values above 0.23 would have no effect. If you wan't to generate the `whitelist.txt`, alevin has to have some low confidence CB to learn from, so I am guessing in your case any value from 0.15-0.20 should ideally work. Having said that, I am still exploring why even setting `freqThreshold` to 0, alevin not considers all `3M` CB for processing, I guess there is some kind of filter which is coming into the picture but I might need a bit more time to explore that. I will update here once I figure it out. Thanks again for raising the issue and investing your time in improving alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686:537,Usability,learn,learn,537,"okies, I think I see the issue. So look at the following lines in the log:; ```; [2019-06-17 21:21:44.518] [alevinLog] [info] Total 824863; [2019-06-17 21:22:47.680] [alevinLog] [info] Total Unique barcodes found: 3474567; ```; What it means is alevin found total: `3,474,567` unique CB in the whole sample and keeps `824,863` CB for further processing which is ~23% of the CB. So all the `keepCBFraction` values above 0.23 would have no effect. If you wan't to generate the `whitelist.txt`, alevin has to have some low confidence CB to learn from, so I am guessing in your case any value from 0.15-0.20 should ideally work. Having said that, I am still exploring why even setting `freqThreshold` to 0, alevin not considers all `3M` CB for processing, I guess there is some kind of filter which is coming into the picture but I might need a bit more time to explore that. I will update here once I figure it out. Thanks again for raising the issue and investing your time in improving alevin.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503344686
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503356703:40,Deployability,pipeline,pipeline,40,A couple of question before you fix the pipeline to do this though: ; * In general what fraction of the CB does SoupX expects to have from 1-10 CB ?; * In our experience. a usual setup has <1% of the High confidence CB which alevin reports at the end. If you are keeping 10% of the CB then you already have >9% of the low quality stuff. I might have to read the SoupX paper again but I feel it's a lot of low quality data to begin with.; * Can you check what was the frequency of the last CB which was reported ? Because I do see; `Skipped 330862 barcodes due to No mapped read` in the log which mean even if there was >0 reads for the CB that doesn't map to the reference and alevin end up skipping it.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503356703
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503356703:586,Testability,log,log,586,A couple of question before you fix the pipeline to do this though: ; * In general what fraction of the CB does SoupX expects to have from 1-10 CB ?; * In our experience. a usual setup has <1% of the High confidence CB which alevin reports at the end. If you are keeping 10% of the CB then you already have >9% of the low quality stuff. I might have to read the SoupX paper again but I feel it's a lot of low quality data to begin with.; * Can you check what was the frequency of the last CB which was reported ? Because I do see; `Skipped 330862 barcodes due to No mapped read` in the log which mean even if there was >0 reads for the CB that doesn't map to the reference and alevin end up skipping it.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503356703
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503359662:300,Deployability,pipeline,pipeline,300,"1) SoupX is generally working with the raw output from Cell Ranger. I beleive they use all 1-10CBs, and state something to the effect that the soup expression profile estimate is of good quality because there are such a large fractions of droplets in this range. 2) I see your point. Maybe the SoupX pipeline could be improved by using low count but high confidence CBs. 3) raw_cb_frequency.txt was also not written. Here is the last line of featureDump.txt (line 494,002):. | CB	| CorrectedReads	| MappedReads	| DeduplicatedReads |	MappingRate	| DedupRate	| MeanByMax |	NumGenesExpressed	| NumGenesOverMean	| mRnaFraction	| rRnaFraction	| ArborescenceCount |; | ------ | ------ 	| ------ 	| ------ |	 ------ 	| ------ 	| ------ |	 ------ 	| ------ 	| ------ 	| ------ 	| ------ |; |AAAAATGGTTGCTCCT|	2	|1	|1	|0.5|	0|	1	|1|	0	|1	|0|	0|. Thanks for the heads up on `whitelist.txt`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503359662
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823:720,Availability,error,error,720,"Oki, so I have updated a couple of things in the latest commit on the develop branch, which should make the things more streamlined. . * `maxNumBarcodes`: As you have initially used `maxNumBarcodes` which is by default set to 100k it means. by default alevin quantifies 100k CBs which includes both the low and high confidence CB count. You can change this number accordingly to set the universe of the top CB to quantify.; * `KeepCBFraction` : It defines what fraction of `maxNumBarcodes` to be used as the high confidence barcodes and should definitely generate the quants for. If set to 1 then everything is high confidence and the whitelisting cannot be performed. Thanks to this issue, alevin will not fail without error when there is no low confidence CB is found instead it checks if the number of low confidence CB is less than `lowRegionMinBarcodes` (default to 200), alevin will warn and not perform the whitelisting.; * `freqThreshold`: This is used to filter out most obvious cases to filter out CB with frequency less than set by the parameter (default to 10). Hope this help ! I am also testing on my end for any other potential bug. Please let me know if you get a chance to check the develop branch .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823:15,Deployability,update,updated,15,"Oki, so I have updated a couple of things in the latest commit on the develop branch, which should make the things more streamlined. . * `maxNumBarcodes`: As you have initially used `maxNumBarcodes` which is by default set to 100k it means. by default alevin quantifies 100k CBs which includes both the low and high confidence CB count. You can change this number accordingly to set the universe of the top CB to quantify.; * `KeepCBFraction` : It defines what fraction of `maxNumBarcodes` to be used as the high confidence barcodes and should definitely generate the quants for. If set to 1 then everything is high confidence and the whitelisting cannot be performed. Thanks to this issue, alevin will not fail without error when there is no low confidence CB is found instead it checks if the number of low confidence CB is less than `lowRegionMinBarcodes` (default to 200), alevin will warn and not perform the whitelisting.; * `freqThreshold`: This is used to filter out most obvious cases to filter out CB with frequency less than set by the parameter (default to 10). Hope this help ! I am also testing on my end for any other potential bug. Please let me know if you get a chance to check the develop branch .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823:658,Performance,perform,performed,658,"Oki, so I have updated a couple of things in the latest commit on the develop branch, which should make the things more streamlined. . * `maxNumBarcodes`: As you have initially used `maxNumBarcodes` which is by default set to 100k it means. by default alevin quantifies 100k CBs which includes both the low and high confidence CB count. You can change this number accordingly to set the universe of the top CB to quantify.; * `KeepCBFraction` : It defines what fraction of `maxNumBarcodes` to be used as the high confidence barcodes and should definitely generate the quants for. If set to 1 then everything is high confidence and the whitelisting cannot be performed. Thanks to this issue, alevin will not fail without error when there is no low confidence CB is found instead it checks if the number of low confidence CB is less than `lowRegionMinBarcodes` (default to 200), alevin will warn and not perform the whitelisting.; * `freqThreshold`: This is used to filter out most obvious cases to filter out CB with frequency less than set by the parameter (default to 10). Hope this help ! I am also testing on my end for any other potential bug. Please let me know if you get a chance to check the develop branch .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823:902,Performance,perform,perform,902,"Oki, so I have updated a couple of things in the latest commit on the develop branch, which should make the things more streamlined. . * `maxNumBarcodes`: As you have initially used `maxNumBarcodes` which is by default set to 100k it means. by default alevin quantifies 100k CBs which includes both the low and high confidence CB count. You can change this number accordingly to set the universe of the top CB to quantify.; * `KeepCBFraction` : It defines what fraction of `maxNumBarcodes` to be used as the high confidence barcodes and should definitely generate the quants for. If set to 1 then everything is high confidence and the whitelisting cannot be performed. Thanks to this issue, alevin will not fail without error when there is no low confidence CB is found instead it checks if the number of low confidence CB is less than `lowRegionMinBarcodes` (default to 200), alevin will warn and not perform the whitelisting.; * `freqThreshold`: This is used to filter out most obvious cases to filter out CB with frequency less than set by the parameter (default to 10). Hope this help ! I am also testing on my end for any other potential bug. Please let me know if you get a chance to check the develop branch .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823:1101,Testability,test,testing,1101,"Oki, so I have updated a couple of things in the latest commit on the develop branch, which should make the things more streamlined. . * `maxNumBarcodes`: As you have initially used `maxNumBarcodes` which is by default set to 100k it means. by default alevin quantifies 100k CBs which includes both the low and high confidence CB count. You can change this number accordingly to set the universe of the top CB to quantify.; * `KeepCBFraction` : It defines what fraction of `maxNumBarcodes` to be used as the high confidence barcodes and should definitely generate the quants for. If set to 1 then everything is high confidence and the whitelisting cannot be performed. Thanks to this issue, alevin will not fail without error when there is no low confidence CB is found instead it checks if the number of low confidence CB is less than `lowRegionMinBarcodes` (default to 200), alevin will warn and not perform the whitelisting.; * `freqThreshold`: This is used to filter out most obvious cases to filter out CB with frequency less than set by the parameter (default to 10). Hope this help ! I am also testing on my end for any other potential bug. Please let me know if you get a chance to check the develop branch .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503396823
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503620733:525,Safety,detect,detection,525,"Thanks @k3yavi - building the `develop` branch currently and will let you know if I encounter any issues. I thought of another approach to obtain at least a sample of 1-10UMI CBs:. - Run alevin with standard options, then parse `raw_cb_frequency.txt` for a sample of 1-10UMI CBs and using them as input to `--whitelist` option for additional run of alevin with `--freqThreshold 0 --maxNumBarcodes 4294967295`. `--whitelist`:. > This is an optional argument, where user can explicitly specify the whitelist CB to use for cell detection and CB sequence correction. If not given, alevin generates its own set of putative CBs. I did the above and everything completed fine. I took a sample of 50,000 1-10UMI CBs of which ~30,000 ended up in `quants_mat.mtx.gz` (I'm assuming the difference is because of CB equivalence relationships within the supplied whitelist). Of these, ~50% had counts <= 10 (I'm assuming those with more counts had CB equivalence relationships across all CBs for which they were assigned reads). Does the above approach seem reasonable?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503620733
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503624687:56,Safety,avoid,avoid,56,"Yep that sounds reasonable to me.; In case you wan't to avoid multiple round of alevin runs, the idea in the develop branch is to use `--freqThreshold 0 --maxNumBarcodes 4294967295 --keepCBFraction 0.95` i.e. maxNumBarcodes is almost infinity which will force alevin to consider all CB for processing while keeping 95% of the CB as high confidence and hopefully the last 5% would be `>200` CBs which will make alevin run whitelisting. Thanks again for testing alevin and its features !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503624687
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503624687:452,Testability,test,testing,452,"Yep that sounds reasonable to me.; In case you wan't to avoid multiple round of alevin runs, the idea in the develop branch is to use `--freqThreshold 0 --maxNumBarcodes 4294967295 --keepCBFraction 0.95` i.e. maxNumBarcodes is almost infinity which will force alevin to consider all CB for processing while keeping 95% of the CB as high confidence and hopefully the last 5% would be `>200` CBs which will make alevin run whitelisting. Thanks again for testing alevin and its features !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503624687
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503641072:1006,Deployability,release,release,1006,Hard to say - I definitely don't know the code base or algorithm details better than you. The thing that jumps to mind is separation of finding equivalent CBs/quantification and whitelisting. For example:. 1) `salmon alevin --quantifyAll`: could do no whitelisting (knee or otherwise) and output a sparse matrix of counts for all CBs (binning counts by equivalences). 2) `salmon alevin --runQC --rrna path/to/rrna.txt --mrna path/to/mrna.txt path/to/quants_mat.gz`: could just do whitelisting via whatever combination of knee and/or NB modelling on the counts from `1)`. . I think you're much better positioned to know if the above makes sense or is reasonable. One advantage of separating CB mapping/quantification and whitelisting is that it would be relatively straightforward to run automatic cell QC filtering from the output of kallisto or Cell Ranger. There seems to be a definite lack of automation in this area (only other thing I have come across is [`cellity`](https://bioconductor.org/packages/release/bioc/html/cellity.html) which seems to be more hands on).,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503641072
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503646040:129,Energy Efficiency,reduce,reduce,129,"Actually that's a very nice idea! Thanks for sharing it. If we can modularize it into multiple independent components that would reduce the overall complexity and might help differentiate the use cases. I'll definitely raise this in our next alevin meeting. Although, it might take some time to get back regarding this but I will poke back here once we have some progress. Thanks again !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-503646040
https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-645474333:272,Deployability,release,release,272,"Hi, while this issue was solved, im just a bit confused, if i want to generate a quant matrix of all CBS including those in the range of 1-10 reads for use with SoupX, how do i do this in the most streamlined way. As there has been a lot of discussion about this and many release of alevin since then. Will using FreqThreshold 0 --maxNumBarcodes 4294967295 do the trick? or do i also need to use --KeepCBFraction 1.0",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/379#issuecomment-645474333
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502788751:327,Deployability,update,updated,327,"Hi Ryan,; I was looking at the output matrix. It seems the count values are in scientific notations like e-7. Not sure how that happened and it's not happening at my end. I'll check what can be done. Thanks again for reporting this. Regarding the binary file parsing, we recently optimize the output format for Alevin and have updated the python parser. We are still working on updating R parser, will update here once stable.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502788751
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502788751:402,Deployability,update,update,402,"Hi Ryan,; I was looking at the output matrix. It seems the count values are in scientific notations like e-7. Not sure how that happened and it's not happening at my end. I'll check what can be done. Thanks again for reporting this. Regarding the binary file parsing, we recently optimize the output format for Alevin and have updated the python parser. We are still working on updating R parser, will update here once stable.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502788751
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502788751:280,Performance,optimiz,optimize,280,"Hi Ryan,; I was looking at the output matrix. It seems the count values are in scientific notations like e-7. Not sure how that happened and it's not happening at my end. I'll check what can be done. Thanks again for reporting this. Regarding the binary file parsing, we recently optimize the output format for Alevin and have updated the python parser. We are still working on updating R parser, will update here once stable.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502788751
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:177,Deployability,release,release,177,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:260,Deployability,release,release,260,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:325,Deployability,update,updated,325,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:215,Integrability,depend,dependencies,215,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416:26,Usability,feedback,feedbacks,26,"Thanks @Ryan-Zhu for your feedbacks and the suggestion.; I apologize for the trouble you had to face while working with the alevin output.; We will prepare better from the next release and try updating the external dependencies first before making an official release. ; Just wanted to give you the heads up that I have also updated the bug for the scientific notation in the `mtx` format. It's in the develop branch of alevin, if you have time please let me know if it works for you. Thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-502828416
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503677169:92,Availability,Error,Error,92,"The issue with reading the .mtx file does not appear to be fixed in develop. I still get:. `Error: readMM(): column values 'j' are not in 1:nc` with `Matrix::readMM(file)`. I think it is related to the column indexing in the .mtx file. Specifically, values should be in `1:nc` where `nc` is in the first row second column after the `%%MatrixMarket matrix coordinate real general` line. There is a value of 0 in my .mtx file so I am guessing the issue is 0 indexing instead of 1 indexing.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503677169
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503778585:318,Deployability,update,updated,318,"Hi guys,; Thanks @alexvpickering for the faster version !; There has been a lot of language switches recently (C++, Rust, python and R), and I apologize there is a minor bug in the above code, thanks to @mikelove for pointing it.; ```; get_binary <- function(id) { as.integer(head(intToBits(id), 8)) }; ```. should be updated to; ```; get_binary <- function(id) { rev( as.integer(head(intToBits(id), 8)) ) }; ```. The R functions returns least significant bit first while I was in the pythonic/C++ mode and assumed their native format of most significant bit first. Python parser was fine but please consider updating the above lines in the parser. @mikelove is double checking the code and will update the tximport package to make it more streamlined. I will keep you guys updated. re: mtx format, yea I think we designed the sparse format mainly with the motivation of saving space both in memory and disk as our per cell level inferential data grows linearly with the number of bootstraps used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503778585
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503778585:696,Deployability,update,update,696,"Hi guys,; Thanks @alexvpickering for the faster version !; There has been a lot of language switches recently (C++, Rust, python and R), and I apologize there is a minor bug in the above code, thanks to @mikelove for pointing it.; ```; get_binary <- function(id) { as.integer(head(intToBits(id), 8)) }; ```. should be updated to; ```; get_binary <- function(id) { rev( as.integer(head(intToBits(id), 8)) ) }; ```. The R functions returns least significant bit first while I was in the pythonic/C++ mode and assumed their native format of most significant bit first. Python parser was fine but please consider updating the above lines in the parser. @mikelove is double checking the code and will update the tximport package to make it more streamlined. I will keep you guys updated. re: mtx format, yea I think we designed the sparse format mainly with the motivation of saving space both in memory and disk as our per cell level inferential data grows linearly with the number of bootstraps used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503778585
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503778585:774,Deployability,update,updated,774,"Hi guys,; Thanks @alexvpickering for the faster version !; There has been a lot of language switches recently (C++, Rust, python and R), and I apologize there is a minor bug in the above code, thanks to @mikelove for pointing it.; ```; get_binary <- function(id) { as.integer(head(intToBits(id), 8)) }; ```. should be updated to; ```; get_binary <- function(id) { rev( as.integer(head(intToBits(id), 8)) ) }; ```. The R functions returns least significant bit first while I was in the pythonic/C++ mode and assumed their native format of most significant bit first. Python parser was fine but please consider updating the above lines in the parser. @mikelove is double checking the code and will update the tximport package to make it more streamlined. I will keep you guys updated. re: mtx format, yea I think we designed the sparse format mainly with the motivation of saving space both in memory and disk as our per cell level inferential data grows linearly with the number of bootstraps used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503778585
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503787604:80,Performance,load,loading,80,"> Thanks again @alexvpickering, just pushed a fix. Confirm that .mtx.gz file is loading properly now. Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-503787604
https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-505983550:44,Deployability,release,released,44,"Hey guys, just wanna give heads up, we just released `0.14.1` for various bugfixes. Please consider updating salmon to the latest.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/380#issuecomment-505983550
https://github.com/COMBINE-lab/salmon/issues/381#issuecomment-503442543:53,Integrability,protocol,protocol,53,"I mean the additional barcodes you can get with this protocol if people use CITE-seq like approaches, see https://www.10xgenomics.com/solutions/single-cell/. . On 18 June 2019 23:53:55 BST, Avi Srivastava <notifications@github.com> wrote:; >Hi @pinin4fjords ,; >; >Thanks for the question, do you mean the longer UMI/CB length or you; >are talking about some other feature ?; >; >; >; >-- ; >You are receiving this because you were mentioned.; >Reply to this email directly or view it on GitHub:; >https://github.com/COMBINE-lab/salmon/issues/381#issuecomment-503341361. -- ; Sent from my Android device with K-9 Mail. Please excuse my brevity.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/381#issuecomment-503442543
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-503627124:169,Availability,mask,maskfasta,169,"Hi @tamuanand & @k3yavi,. Actually, I think the `makefasta` bedtools command we use accepts a gff file directly (https://bedtools.readthedocs.io/en/latest/content/tools/maskfasta.html). So, it might make sense to have a flag (or automatically detect, but that can be error prone), and only run the line Avi links above if we have a GTF. If we have a BED or GFF file, we can just pass it directly to bedtools.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-503627124
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-503627124:267,Availability,error,error,267,"Hi @tamuanand & @k3yavi,. Actually, I think the `makefasta` bedtools command we use accepts a gff file directly (https://bedtools.readthedocs.io/en/latest/content/tools/maskfasta.html). So, it might make sense to have a flag (or automatically detect, but that can be error prone), and only run the line Avi links above if we have a GTF. If we have a BED or GFF file, we can just pass it directly to bedtools.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-503627124
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-503627124:243,Safety,detect,detect,243,"Hi @tamuanand & @k3yavi,. Actually, I think the `makefasta` bedtools command we use accepts a gff file directly (https://bedtools.readthedocs.io/en/latest/content/tools/maskfasta.html). So, it might make sense to have a flag (or automatically detect, but that can be error prone), and only run the line Avi links above if we have a GTF. If we have a BED or GFF file, we can just pass it directly to bedtools.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-503627124
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-503632945:135,Integrability,depend,depending,135,"Hey @rob-p ,. Thanks for your response. I agree with your suggestion - to have a flag for gff/bed and run either GTF or a BED/GFF file depending on what the end user has. In my case, I have mostly gff files and I need to do some reformatting if I have to use the current shell script.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-503632945
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549141221:9,Deployability,release,release,9,With the release of `v1.0` we don't need the gtf/gff any more. Closing this issue.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549141221
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549180089:740,Deployability,release,released,740,"@rob-p @k3yavi ; IMO, There is something missing in all the relevant documentation associated with salmon/v1.0. https://salmon.readthedocs.io/en/latest/salmon.html#preparing-transcriptome-indices-mapping-based-mode points to this https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md which still states the use of gtf/gff. also, the usage statement in generateDecoyTranscriptome.sh script [here](https://github.com/COMBINE-lab/SalmonTools/blob/master/scripts/generateDecoyTranscriptome.sh) still states the use of gtf files. The link to the preprint paper within salmon_readthedocs stills points to the June 2019 version - https://www.biorxiv.org/content/10.1101/657874v1 . Isn't there a newer version of this preprint which got released couple of days back.. Am I missing something?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549180089
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549183799:75,Deployability,update,update,75,"Hi @tamuanand ,. Thanks for pointing this out. You are right, we missed to update the salmon doc with this details. You can find the latest preprint [here](https://www.biorxiv.org/content/10.1101/657874v2). The new version don't require the GTF because now we don't generate the decoys explicitly i.e. you don't have to run mashmap. With the latest version salmon, it can consume the full genome and transcriptome without the explicit need of annotation. It's much more efficient and takes significantly less memory to align/quantify compared to other genome based method. Please checkout the preprint for more details and [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) tutorial for how to index the gentrome (genome + transcriptome) index. We will update the salmon docs too, to reflect the same.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549183799
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549183799:782,Deployability,update,update,782,"Hi @tamuanand ,. Thanks for pointing this out. You are right, we missed to update the salmon doc with this details. You can find the latest preprint [here](https://www.biorxiv.org/content/10.1101/657874v2). The new version don't require the GTF because now we don't generate the decoys explicitly i.e. you don't have to run mashmap. With the latest version salmon, it can consume the full genome and transcriptome without the explicit need of annotation. It's much more efficient and takes significantly less memory to align/quantify compared to other genome based method. Please checkout the preprint for more details and [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) tutorial for how to index the gentrome (genome + transcriptome) index. We will update the salmon docs too, to reflect the same.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549183799
https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549183799:470,Energy Efficiency,efficient,efficient,470,"Hi @tamuanand ,. Thanks for pointing this out. You are right, we missed to update the salmon doc with this details. You can find the latest preprint [here](https://www.biorxiv.org/content/10.1101/657874v2). The new version don't require the GTF because now we don't generate the decoys explicitly i.e. you don't have to run mashmap. With the latest version salmon, it can consume the full genome and transcriptome without the explicit need of annotation. It's much more efficient and takes significantly less memory to align/quantify compared to other genome based method. Please checkout the preprint for more details and [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) tutorial for how to index the gentrome (genome + transcriptome) index. We will update the salmon docs too, to reflect the same.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/384#issuecomment-549183799
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-504952404:112,Availability,error,error,112,"Hi @alexmascension ,. Thanks for raising this.; Quick question thought, the issue header say : ; > Out of Range error for txp to gene Map. But I don't see that error in the log, did you skip copying that ?; Also, I should have raised this in the previous issue too but it should not matter at least in this error case, however, you should use `-lISR` instead of `-lU` with alevin as the reads are expected to come from the reverse strand. It seems a lot of reads `91.1983%` are supposedly getting thrown away, weren't you using the whitelisted CB instead of ""knee"" thresholding ?; If possible, can you share a small set of reads, like these `even some of them fail just when starting the analysis of the cells`, on which I can replicate the issue? it'd help resolve the issue much faster.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-504952404
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-504952404:160,Availability,error,error,160,"Hi @alexmascension ,. Thanks for raising this.; Quick question thought, the issue header say : ; > Out of Range error for txp to gene Map. But I don't see that error in the log, did you skip copying that ?; Also, I should have raised this in the previous issue too but it should not matter at least in this error case, however, you should use `-lISR` instead of `-lU` with alevin as the reads are expected to come from the reverse strand. It seems a lot of reads `91.1983%` are supposedly getting thrown away, weren't you using the whitelisted CB instead of ""knee"" thresholding ?; If possible, can you share a small set of reads, like these `even some of them fail just when starting the analysis of the cells`, on which I can replicate the issue? it'd help resolve the issue much faster.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-504952404
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-504952404:307,Availability,error,error,307,"Hi @alexmascension ,. Thanks for raising this.; Quick question thought, the issue header say : ; > Out of Range error for txp to gene Map. But I don't see that error in the log, did you skip copying that ?; Also, I should have raised this in the previous issue too but it should not matter at least in this error case, however, you should use `-lISR` instead of `-lU` with alevin as the reads are expected to come from the reverse strand. It seems a lot of reads `91.1983%` are supposedly getting thrown away, weren't you using the whitelisted CB instead of ""knee"" thresholding ?; If possible, can you share a small set of reads, like these `even some of them fail just when starting the analysis of the cells`, on which I can replicate the issue? it'd help resolve the issue much faster.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-504952404
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-504952404:173,Testability,log,log,173,"Hi @alexmascension ,. Thanks for raising this.; Quick question thought, the issue header say : ; > Out of Range error for txp to gene Map. But I don't see that error in the log, did you skip copying that ?; Also, I should have raised this in the previous issue too but it should not matter at least in this error case, however, you should use `-lISR` instead of `-lU` with alevin as the reads are expected to come from the reverse strand. It seems a lot of reads `91.1983%` are supposedly getting thrown away, weren't you using the whitelisted CB instead of ""knee"" thresholding ?; If possible, can you share a small set of reads, like these `even some of them fail just when starting the analysis of the cells`, on which I can replicate the issue? it'd help resolve the issue much faster.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-504952404
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-505504105:38,Availability,error,error,38,"Hi @k3yavi,; There is no more message error. Once the message occurs, it abruptly ends and that's it. > weren't you using the whitelisted CB instead of ""knee"" thresholding?. Yeap, but in the previous dataset. In this one the CB is provided in the dataset, and I don't have to create a whitelist. . The error does not always happen. I have created other random datasets, and they do not always fail. I will try to create a small dataset which fails, for you to replicate. Is there anyway I can privately send it to you?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-505504105
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-505504105:302,Availability,error,error,302,"Hi @k3yavi,; There is no more message error. Once the message occurs, it abruptly ends and that's it. > weren't you using the whitelisted CB instead of ""knee"" thresholding?. Yeap, but in the previous dataset. In this one the CB is provided in the dataset, and I don't have to create a whitelist. . The error does not always happen. I have created other random datasets, and they do not always fail. I will try to create a small dataset which fails, for you to replicate. Is there anyway I can privately send it to you?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-505504105
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-505504105:30,Integrability,message,message,30,"Hi @k3yavi,; There is no more message error. Once the message occurs, it abruptly ends and that's it. > weren't you using the whitelisted CB instead of ""knee"" thresholding?. Yeap, but in the previous dataset. In this one the CB is provided in the dataset, and I don't have to create a whitelist. . The error does not always happen. I have created other random datasets, and they do not always fail. I will try to create a small dataset which fails, for you to replicate. Is there anyway I can privately send it to you?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-505504105
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-505504105:54,Integrability,message,message,54,"Hi @k3yavi,; There is no more message error. Once the message occurs, it abruptly ends and that's it. > weren't you using the whitelisted CB instead of ""knee"" thresholding?. Yeap, but in the previous dataset. In this one the CB is provided in the dataset, and I don't have to create a whitelist. . The error does not always happen. I have created other random datasets, and they do not always fail. I will try to create a small dataset which fails, for you to replicate. Is there anyway I can privately send it to you?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-505504105
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-506225248:80,Availability,error,error,80,"Hi!. Sorry for not answering. I'm trying to get a small sample to replicate the error and misteriously I can't. I will try to update Alevin and run it in the big samples, then if it fails, try to send you the a sample to work with.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-506225248
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-506225248:126,Deployability,update,update,126,"Hi!. Sorry for not answering. I'm trying to get a small sample to replicate the error and misteriously I can't. I will try to update Alevin and run it in the big samples, then if it fails, try to send you the a sample to work with.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-506225248
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508753067:198,Availability,error,error,198,"The main issue of alevin failing at a low % of analyzed cells is solved, after personal communication with Avi. It turns out that index contained some duplicated transcripts, which were causing the error. I have removed the duplicated transcripts from the .fa file and the transcript_to_gene.tsv matrix, and the analysis completed without errors in a sample run. If the error happens in the whole set of samples I will reopen the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508753067
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508753067:339,Availability,error,errors,339,"The main issue of alevin failing at a low % of analyzed cells is solved, after personal communication with Avi. It turns out that index contained some duplicated transcripts, which were causing the error. I have removed the duplicated transcripts from the .fa file and the transcript_to_gene.tsv matrix, and the analysis completed without errors in a sample run. If the error happens in the whole set of samples I will reopen the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508753067
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508753067:370,Availability,error,error,370,"The main issue of alevin failing at a low % of analyzed cells is solved, after personal communication with Avi. It turns out that index contained some duplicated transcripts, which were causing the error. I have removed the duplicated transcripts from the .fa file and the transcript_to_gene.tsv matrix, and the analysis completed without errors in a sample run. If the error happens in the whole set of samples I will reopen the issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508753067
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:1876,Availability,error,error,1876,"on for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it through the modified pipeline, alevin finished normally and I am attaching the quants generated; by alevin. >Thanks again for forwarding the data.; Best,; —Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:2078,Deployability,pipeline,pipeline,2078,"on for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it through the modified pipeline, alevin finished normally and I am attaching the quants generated; by alevin. >Thanks again for forwarding the data.; Best,; —Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:558,Safety,predict,predicting,558,"Hi @alexmascension ,. Thanks for confirming. I'll paste my response, I sent you earlier, here too. In case it's helpful to some other user. > Hi Alex,. >Thanks again for forwarding the data. I think I have the solution for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was ove",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:380,Testability,log,log,380,"Hi @alexmascension ,. Thanks for confirming. I'll paste my response, I sent you earlier, here too. In case it's helpful to some other user. > Hi Alex,. >Thanks again for forwarding the data. I think I have the solution for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was ove",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:1086,Testability,log,logs,1086," response, I sent you earlier, here too. In case it's helpful to some other user. > Hi Alex,. >Thanks again for forwarding the data. I think I have the solution for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:1936,Testability,log,logs,1936,"on for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it through the modified pipeline, alevin finished normally and I am attaching the quants generated; by alevin. >Thanks again for forwarding the data.; Best,; —Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845:2022,Usability,progress bar,progress bar,2022,"on for the problems you are facing in alevin. >Your first question was related to alevin quantifying very less number of reads. To answer that,; if you look at the log, at the first few lines, alevin warns about ~91% of the reads being thrown away because; of the noisy CBs. The problem is alevin’s first “knee"" estimation is overshooting in predicting the first boundary. You will find https://github.com/COMBINE-lab/salmon/issues/362 issue to be; very useful in understanding that. As a summary if you look at the plot I attached it has bi-modalities,; which is generally not the case and alevin is greedily finding the threshold at the first ~100 cells. If this; happens the general direction is to help alevin by proving a upper bound, in case of your data; would be ~14000 cells. You can tell alevin with `—expectCells 14000` and alevin start to work; normally and logs ~12% of the data is noisy. >You second question was a little complicated to answer. Seemingly, your salmon index has transcript with; same exact name `ENST00000399966.9`, occurring twice with different sequences. Just by looking at the index,; I am unsure it’s actually present in the reference or its salmon indexing messing up. If I Assume it was actually; present two times in the reference, alevin should report it instead of exiting abruptly in the middle of quantification.; Although, alevin does warns:; ```; [2019-07-04 14:12:32.519] [alevinLog] [warning] Found 1 transcripts with duplicate names; ```; >However, the bug i.e. not being able to distinguish duplicate names of the transcript, has been ; fixed and pushed in the develop branch of salmon. Alevin was reporting the error at the stage of quantification too, ; if you dump the logs in a file, but it was invisible in the console as it was over written my complex progress bar. . >Once I process it through the modified pipeline, alevin finished normally and I am attaching the quants generated; by alevin. >Thanks again for forwarding the data.; Best,; —Avi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/386#issuecomment-508754845
https://github.com/COMBINE-lab/salmon/issues/387#issuecomment-505965479:62,Availability,error,error,62,"Hey @BisonKok , just as a sanity can you check if you get any error/warning when you `samtools view ` the bam file ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/387#issuecomment-505965479
https://github.com/COMBINE-lab/salmon/issues/387#issuecomment-506399769:6,Availability,error,error,6,So no error/warning I guess ?; can you share a minimal bam from which we can replicate and work on solving the issue ?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/387#issuecomment-506399769
https://github.com/COMBINE-lab/salmon/issues/387#issuecomment-506585956:46,Testability,test,testing,46,I have attached a small subset of the bam for testing. ; [in.bam.zip](https://github.com/COMBINE-lab/salmon/files/3337430/in.bam.zip),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/387#issuecomment-506585956
https://github.com/COMBINE-lab/salmon/issues/387#issuecomment-506853630:334,Deployability,update,update,334,"Hi @BisonKok ,. If you looks at the first line of the sam file:; ```; @HD VN:1.0 SO:coordinate ; ```; There seem to be extra whitespaces after `SO:coordinate`, if you can edit sam and remove the tab after the first line in the header then salmon works fine. Not sure why the C++ parser is failing to remove extra whitespaces, we will update you once we have a fix.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/387#issuecomment-506853630
https://github.com/COMBINE-lab/salmon/issues/388#issuecomment-505859344:87,Modifiability,variab,variables,87,"Hi @TizianaS92,. One problem is that CMake is rather annoying with caching environment variables and versions. Could you see what happens if you `rm -fr` your `build` directory and then try again (this will obliterate `CMakeCache.txt` and the `CMakeFiles` subdirectory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/388#issuecomment-505859344
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-506494653:163,Testability,log,logs,163,"Hi @sudeep71 ,. Can you try `--decoys` I think there is one `-` is missing from the command line argument or just use `-d`. If this doesn't work can you share the logs ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-506494653
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-506496725:26,Deployability,update,update,26,@k3yavi . Works!! I would update the documentation file. Thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-506496725
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:237,Availability,error,error,237,"Hi,. I thought I would extend this issue rather than creating a new issue. Happy to create new if that would be better. I'm using the docker version of Salmon (latest version). I have also mounted a local directory. I'm getting the same error. (Note I have tried several different versions of the command, including the --no-version-check command). salmon index --gencode ; -t /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/gentrome.fa ; -d /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/decoys.txt ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index. Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; [2019-07-01 12:32:42.622] [jLog] [info] building index; [2019-07-01 12:32:42.628] [jointLog] [info] [Step 1 of 4] : counting k-mers. <Several warnings about transcripts that are disliked>. [2019-07-01 12:33:02.020] [jointLog] [info] Replaced 3801867 non-ATCG nucleotides; [2019-07-01 12:33:02.020] [jointLog] [info] Clipped poly-A tails from 1630 transcripts; [2019-07-01 12:33:02.041] [jointLog] [info] Building rank-select dictionary and saving to disk; [2019-07-01 12:33:02.248] [jointLog] [info] done; Elapsed time: 0.20793s; [2019-07-01 12:33:02.252] [jointLog] [info] Writing sequence data to file . . . ; [2019-07-01 12:33:04.501] [jointLog] [info] done; Elapsed time: 2.24861s; [2019-07-01 12:33:04.572] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 469043886); [2019-07-01 12:33:08.681] [jointLog] [info] Building suffix array . . . ; success; saving to disk . . . done; Elapsed time: 61.4932s; done; Elapsed time: 171.743s; processed 12000000 positionsKilled. I can send log files if required. The problem I have is that I cannot seem to run quant without the quant function. salmon quant --validateMappings ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:3781,Availability,Error,Error,3781,"e/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-01 12:51:42.856] [jointLog] [info] parsing read library format; [2019-07-01 12:51:42.856] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly. Any ideas what I'm doing wrong please?. Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:626,Deployability,upgrade,upgrade,626,"Hi,. I thought I would extend this issue rather than creating a new issue. Happy to create new if that would be better. I'm using the docker version of Salmon (latest version). I have also mounted a local directory. I'm getting the same error. (Note I have tried several different versions of the command, including the --no-version-check command). salmon index --gencode ; -t /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/gentrome.fa ; -d /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/decoys.txt ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index. Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; [2019-07-01 12:32:42.622] [jLog] [info] building index; [2019-07-01 12:32:42.628] [jointLog] [info] [Step 1 of 4] : counting k-mers. <Several warnings about transcripts that are disliked>. [2019-07-01 12:33:02.020] [jointLog] [info] Replaced 3801867 non-ATCG nucleotides; [2019-07-01 12:33:02.020] [jointLog] [info] Clipped poly-A tails from 1630 transcripts; [2019-07-01 12:33:02.041] [jointLog] [info] Building rank-select dictionary and saving to disk; [2019-07-01 12:33:02.248] [jointLog] [info] done; Elapsed time: 0.20793s; [2019-07-01 12:33:02.252] [jointLog] [info] Writing sequence data to file . . . ; [2019-07-01 12:33:04.501] [jointLog] [info] done; Elapsed time: 2.24861s; [2019-07-01 12:33:04.572] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 469043886); [2019-07-01 12:33:08.681] [jointLog] [info] Building suffix array . . . ; success; saving to disk . . . done; Elapsed time: 61.4932s; done; Elapsed time: 171.743s; processed 12000000 positionsKilled. I can send log files if required. The problem I have is that I cannot seem to run quant without the quant function. salmon quant --validateMappings ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:678,Deployability,upgrade,upgrades,678,"Hi,. I thought I would extend this issue rather than creating a new issue. Happy to create new if that would be better. I'm using the docker version of Salmon (latest version). I have also mounted a local directory. I'm getting the same error. (Note I have tried several different versions of the command, including the --no-version-check command). salmon index --gencode ; -t /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/gentrome.fa ; -d /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/decoys.txt ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index. Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; [2019-07-01 12:32:42.622] [jLog] [info] building index; [2019-07-01 12:32:42.628] [jointLog] [info] [Step 1 of 4] : counting k-mers. <Several warnings about transcripts that are disliked>. [2019-07-01 12:33:02.020] [jointLog] [info] Replaced 3801867 non-ATCG nucleotides; [2019-07-01 12:33:02.020] [jointLog] [info] Clipped poly-A tails from 1630 transcripts; [2019-07-01 12:33:02.041] [jointLog] [info] Building rank-select dictionary and saving to disk; [2019-07-01 12:33:02.248] [jointLog] [info] done; Elapsed time: 0.20793s; [2019-07-01 12:33:02.252] [jointLog] [info] Writing sequence data to file . . . ; [2019-07-01 12:33:04.501] [jointLog] [info] done; Elapsed time: 2.24861s; [2019-07-01 12:33:04.572] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 469043886); [2019-07-01 12:33:08.681] [jointLog] [info] Building suffix array . . . ; success; saving to disk . . . done; Elapsed time: 61.4932s; done; Elapsed time: 171.743s; processed 12000000 positionsKilled. I can send log files if required. The problem I have is that I cannot seem to run quant without the quant function. salmon quant --validateMappings ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:2269,Deployability,upgrade,upgrade,2269,sed time: 0.20793s; [2019-07-01 12:33:02.252] [jointLog] [info] Writing sequence data to file . . . ; [2019-07-01 12:33:04.501] [jointLog] [info] done; Elapsed time: 2.24861s; [2019-07-01 12:33:04.572] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 469043886); [2019-07-01 12:33:08.681] [jointLog] [info] Building suffix array . . . ; success; saving to disk . . . done; Elapsed time: 61.4932s; done; Elapsed time: 171.743s; processed 12000000 positionsKilled. I can send log files if required. The problem I have is that I cannot seem to run quant without the quant function. salmon quant --validateMappings ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFra,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:2321,Deployability,upgrade,upgrades,2321," file . . . ; [2019-07-01 12:33:04.501] [jointLog] [info] done; Elapsed time: 2.24861s; [2019-07-01 12:33:04.572] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 469043886); [2019-07-01 12:33:08.681] [jointLog] [info] Building suffix array . . . ; success; saving to disk . . . done; Elapsed time: 61.4932s; done; Elapsed time: 171.743s; processed 12000000 positionsKilled. I can send log files if required. The problem I have is that I cannot seem to run quant without the quant function. salmon quant --validateMappings ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-01 12:51:42.856",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:23,Modifiability,extend,extend,23,"Hi,. I thought I would extend this issue rather than creating a new issue. Happy to create new if that would be better. I'm using the docker version of Salmon (latest version). I have also mounted a local directory. I'm getting the same error. (Note I have tried several different versions of the command, including the --no-version-check command). salmon index --gencode ; -t /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/gentrome.fa ; -d /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/decoys.txt ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index. Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; [2019-07-01 12:32:42.622] [jLog] [info] building index; [2019-07-01 12:32:42.628] [jointLog] [info] [Step 1 of 4] : counting k-mers. <Several warnings about transcripts that are disliked>. [2019-07-01 12:33:02.020] [jointLog] [info] Replaced 3801867 non-ATCG nucleotides; [2019-07-01 12:33:02.020] [jointLog] [info] Clipped poly-A tails from 1630 transcripts; [2019-07-01 12:33:02.041] [jointLog] [info] Building rank-select dictionary and saving to disk; [2019-07-01 12:33:02.248] [jointLog] [info] done; Elapsed time: 0.20793s; [2019-07-01 12:33:02.252] [jointLog] [info] Writing sequence data to file . . . ; [2019-07-01 12:33:04.501] [jointLog] [info] done; Elapsed time: 2.24861s; [2019-07-01 12:33:04.572] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 469043886); [2019-07-01 12:33:08.681] [jointLog] [info] Building suffix array . . . ; success; saving to disk . . . done; Elapsed time: 61.4932s; done; Elapsed time: 171.743s; processed 12000000 positionsKilled. I can send log files if required. The problem I have is that I cannot seem to run quant without the quant function. salmon quant --validateMappings ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:1870,Security,validat,validateMappings,1870,019-07-01 12:33:02.020] [jointLog] [info] Replaced 3801867 non-ATCG nucleotides; [2019-07-01 12:33:02.020] [jointLog] [info] Clipped poly-A tails from 1630 transcripts; [2019-07-01 12:33:02.041] [jointLog] [info] Building rank-select dictionary and saving to disk; [2019-07-01 12:33:02.248] [jointLog] [info] done; Elapsed time: 0.20793s; [2019-07-01 12:33:02.252] [jointLog] [info] Writing sequence data to file . . . ; [2019-07-01 12:33:04.501] [jointLog] [info] done; Elapsed time: 2.24861s; [2019-07-01 12:33:04.572] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 469043886); [2019-07-01 12:33:08.681] [jointLog] [info] Building suffix array . . . ; success; saving to disk . . . done; Elapsed time: 61.4932s; done; Elapsed time: 171.743s; processed 12000000 positionsKilled. I can send log files if required. The problem I have is that I cannot seem to run quant without the quant function. salmon quant --validateMappings ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_fi,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:2853,Security,validat,validateMappings,2853,"NCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-01 12:51:42.856] [jointLog] [info] parsing read library format; [2019-07-01 12:51:42.856] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index/versionInfo.json doesn't seem to exist. Please try re-buil",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:3205,Security,validat,validateMappings,3205,"e/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-01 12:51:42.856] [jointLog] [info] parsing read library format; [2019-07-01 12:51:42.856] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly. Any ideas what I'm doing wrong please?. Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:3367,Security,validat,validateMappings,3367,"e/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-01 12:51:42.856] [jointLog] [info] parsing read library format; [2019-07-01 12:51:42.856] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly. Any ideas what I'm doing wrong please?. Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:3539,Security,validat,validateMappings,3539,"e/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-01 12:51:42.856] [jointLog] [info] parsing read library format; [2019-07-01 12:51:42.856] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly. Any ideas what I'm doing wrong please?. Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:1750,Testability,log,log,1750,42.622] [jLog] [info] building index; [2019-07-01 12:32:42.628] [jointLog] [info] [Step 1 of 4] : counting k-mers. <Several warnings about transcripts that are disliked>. [2019-07-01 12:33:02.020] [jointLog] [info] Replaced 3801867 non-ATCG nucleotides; [2019-07-01 12:33:02.020] [jointLog] [info] Clipped poly-A tails from 1630 transcripts; [2019-07-01 12:33:02.041] [jointLog] [info] Building rank-select dictionary and saving to disk; [2019-07-01 12:33:02.248] [jointLog] [info] done; Elapsed time: 0.20793s; [2019-07-01 12:33:02.252] [jointLog] [info] Writing sequence data to file . . . ; [2019-07-01 12:33:04.501] [jointLog] [info] done; Elapsed time: 2.24861s; [2019-07-01 12:33:04.572] [jointLog] [info] Building 32-bit suffix array (length of generalized text is 469043886); [2019-07-01 12:33:08.681] [jointLog] [info] Building suffix array . . . ; success; saving to disk . . . done; Elapsed time: 61.4932s; done; Elapsed time: 171.743s; processed 12000000 positionsKilled. I can send log files if required. The problem I have is that I cannot seem to run quant without the quant function. salmon quant --validateMappings ; -i /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fas,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:2945,Testability,Log,Logs,2945,"NCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-01 12:51:42.856] [jointLog] [info] parsing read library format; [2019-07-01 12:51:42.856] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index/versionInfo.json doesn't seem to exist. Please try re-buil",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562:3011,Testability,log,logs,3011,"NCODEv29/combined_index -l IU ; -1 /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz ; -2 /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz ; -o /home/RnaSeq/salmon_output_files/out/DM4h; Version Info: Could not resolve upgrade information in the alotted time.; Check for upgrades manually at https://combine-lab.github.io/salmon; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index }; ### [ libType ] => { IU }; ### [ mates1 ] => { /home/RnaSeq/fastq/DM_4a_H_1.fq.gz /home/RnaSeq/fastq/DM_4b_H_1.fq.gz /home/RnaSeq/fastq/DM_4c_H_1.fq.gz }; ### [ mates2 ] => { /home/RnaSeq/fastq/DM_4a_H_2.fq.gz /home/RnaSeq/fastq/DM_4b_H_2.fq.gz /home/RnaSeq/fastq/DM_4c_H_2.fq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { /home/RnaSeq/salmon_output_files/out/DM4h }; Logs will be written to /home/RnaSeq/salmon_output_files/out/DM4h/logs; [2019-07-01 12:51:42.856] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-01 12:51:42.856] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-01 12:51:42.856] [jointLog] [info] parsing read library format; [2019-07-01 12:51:42.856] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file /home/RnaSeq/transcriptome_gencode_v29/human_GENCODEv29/combined_index/versionInfo.json doesn't seem to exist. Please try re-buil",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/389#issuecomment-507253562
https://github.com/COMBINE-lab/salmon/issues/390#issuecomment-506744431:27,Energy Efficiency,adapt,adapter,27,"Hi @kvittingseerup,. Basic adapter and quality trimming should be done. There's some [nice work by Matt MacManes](https://www.frontiersin.org/articles/10.3389/fgene.2014.00013/full) showing that you should be careful about aggressive quality trimming, but light quality trimming is usually beneficial. This is particularly important if the underlying aligner isn't doing local alignment (e.g. STAR will likely just softclip bad bases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/390#issuecomment-506744431
https://github.com/COMBINE-lab/salmon/issues/390#issuecomment-506744431:27,Integrability,adapter,adapter,27,"Hi @kvittingseerup,. Basic adapter and quality trimming should be done. There's some [nice work by Matt MacManes](https://www.frontiersin.org/articles/10.3389/fgene.2014.00013/full) showing that you should be careful about aggressive quality trimming, but light quality trimming is usually beneficial. This is particularly important if the underlying aligner isn't doing local alignment (e.g. STAR will likely just softclip bad bases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/390#issuecomment-506744431
https://github.com/COMBINE-lab/salmon/issues/390#issuecomment-506744431:27,Modifiability,adapt,adapter,27,"Hi @kvittingseerup,. Basic adapter and quality trimming should be done. There's some [nice work by Matt MacManes](https://www.frontiersin.org/articles/10.3389/fgene.2014.00013/full) showing that you should be careful about aggressive quality trimming, but light quality trimming is usually beneficial. This is particularly important if the underlying aligner isn't doing local alignment (e.g. STAR will likely just softclip bad bases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/390#issuecomment-506744431
https://github.com/COMBINE-lab/salmon/pull/391#issuecomment-508132261:41,Deployability,update,update,41,"Thanks @jdrnevich for the heads up, I'll update the docs too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/391#issuecomment-508132261
https://github.com/COMBINE-lab/salmon/issues/395#issuecomment-510121247:38,Availability,avail,available,38,I've updated the doc and it should me available with the next release or the docs from the develop branch. Thanks again.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/395#issuecomment-510121247
https://github.com/COMBINE-lab/salmon/issues/395#issuecomment-510121247:5,Deployability,update,updated,5,I've updated the doc and it should me available with the next release or the docs from the develop branch. Thanks again.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/395#issuecomment-510121247
https://github.com/COMBINE-lab/salmon/issues/395#issuecomment-510121247:62,Deployability,release,release,62,I've updated the doc and it should me available with the next release or the docs from the develop branch. Thanks again.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/395#issuecomment-510121247
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540:923,Deployability,pipeline,pipeline,923,"Hi @ChelseaCHENX ,. Thanks for raising the issue.; I think if you can share the alevin log (say for 1192 cells ?) we can comment much more about the details. However, if you ask me to guess then I believe the initial whitelisting of alevin seems to be predicting a lot less cells, if you check the alevin log, it would say what % of CB are thrown due to noisy cellular barcodes. If the number is `>20%`, then the chances are indeed ""knee"" estimates are shooting up. The way to get better estimates from there would be to help alevin with a ballpark number of cells (as you are giving to cellranger with --expect-cell 8000, you can provide alevin with --expectCells 8000). Even after that if you get a lot of noisy CB prediction then you can force alevin to use certain number of cells with `--forceCells` option. https://github.com/COMBINE-lab/salmon/issues/362 this issue might help you understand more the details of the pipeline.; Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540:252,Safety,predict,predicting,252,"Hi @ChelseaCHENX ,. Thanks for raising the issue.; I think if you can share the alevin log (say for 1192 cells ?) we can comment much more about the details. However, if you ask me to guess then I believe the initial whitelisting of alevin seems to be predicting a lot less cells, if you check the alevin log, it would say what % of CB are thrown due to noisy cellular barcodes. If the number is `>20%`, then the chances are indeed ""knee"" estimates are shooting up. The way to get better estimates from there would be to help alevin with a ballpark number of cells (as you are giving to cellranger with --expect-cell 8000, you can provide alevin with --expectCells 8000). Even after that if you get a lot of noisy CB prediction then you can force alevin to use certain number of cells with `--forceCells` option. https://github.com/COMBINE-lab/salmon/issues/362 this issue might help you understand more the details of the pipeline.; Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540:717,Safety,predict,prediction,717,"Hi @ChelseaCHENX ,. Thanks for raising the issue.; I think if you can share the alevin log (say for 1192 cells ?) we can comment much more about the details. However, if you ask me to guess then I believe the initial whitelisting of alevin seems to be predicting a lot less cells, if you check the alevin log, it would say what % of CB are thrown due to noisy cellular barcodes. If the number is `>20%`, then the chances are indeed ""knee"" estimates are shooting up. The way to get better estimates from there would be to help alevin with a ballpark number of cells (as you are giving to cellranger with --expect-cell 8000, you can provide alevin with --expectCells 8000). Even after that if you get a lot of noisy CB prediction then you can force alevin to use certain number of cells with `--forceCells` option. https://github.com/COMBINE-lab/salmon/issues/362 this issue might help you understand more the details of the pipeline.; Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540:87,Testability,log,log,87,"Hi @ChelseaCHENX ,. Thanks for raising the issue.; I think if you can share the alevin log (say for 1192 cells ?) we can comment much more about the details. However, if you ask me to guess then I believe the initial whitelisting of alevin seems to be predicting a lot less cells, if you check the alevin log, it would say what % of CB are thrown due to noisy cellular barcodes. If the number is `>20%`, then the chances are indeed ""knee"" estimates are shooting up. The way to get better estimates from there would be to help alevin with a ballpark number of cells (as you are giving to cellranger with --expect-cell 8000, you can provide alevin with --expectCells 8000). Even after that if you get a lot of noisy CB prediction then you can force alevin to use certain number of cells with `--forceCells` option. https://github.com/COMBINE-lab/salmon/issues/362 this issue might help you understand more the details of the pipeline.; Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540:305,Testability,log,log,305,"Hi @ChelseaCHENX ,. Thanks for raising the issue.; I think if you can share the alevin log (say for 1192 cells ?) we can comment much more about the details. However, if you ask me to guess then I believe the initial whitelisting of alevin seems to be predicting a lot less cells, if you check the alevin log, it would say what % of CB are thrown due to noisy cellular barcodes. If the number is `>20%`, then the chances are indeed ""knee"" estimates are shooting up. The way to get better estimates from there would be to help alevin with a ballpark number of cells (as you are giving to cellranger with --expect-cell 8000, you can provide alevin with --expectCells 8000). Even after that if you get a lot of noisy CB prediction then you can force alevin to use certain number of cells with `--forceCells` option. https://github.com/COMBINE-lab/salmon/issues/362 this issue might help you understand more the details of the pipeline.; Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510540540
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611:878,Performance,Load,Loading,878,"Hey Avi, thanks for the quick reply!; Here is the salmon_quant_log file:; ```; [2019-07-09 09:07:39.153] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-07-09 09:07:39.153] [jointLog] [info] Using default value of 0.8 for minScoreFraction in Alevin; [2019-07-09 09:17:08.128] [jointLog] [info] There is 1 library.; [2019-07-09 09:17:08.180] [jointLog] [info] Loading Quasi index; [2019-07-09 09:17:08.180] [jointLog] [info] Loading 32-bit quasi index; [2019-07-09 09:17:14.970] [jointLog] [info] done; [2019-07-09 09:17:14.970] [jointLog] [info] Index contained 197,787 targets; [2019-07-09 10:02:20.484] [jointLog] [info] Computed 251,090 rich equivalence classes for further processing; [2019-07-09 10:02:20.484] [jointLog] [info] Counted 348,673,166 total reads in the equivalence classes ; [2019-07-09 10:02:20.485] [jointLog] [warning] Found 1893 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is too large; [2019-07-09 10:02:20.485] [jointLog] [info] Mapping rate = 39.7151%. [2019-07-09 10:02:20.485] [jointLog] [info] finished quantifyLibrary(); ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611:943,Performance,Load,Loading,943,"Hey Avi, thanks for the quick reply!; Here is the salmon_quant_log file:; ```; [2019-07-09 09:07:39.153] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-07-09 09:07:39.153] [jointLog] [info] Using default value of 0.8 for minScoreFraction in Alevin; [2019-07-09 09:17:08.128] [jointLog] [info] There is 1 library.; [2019-07-09 09:17:08.180] [jointLog] [info] Loading Quasi index; [2019-07-09 09:17:08.180] [jointLog] [info] Loading 32-bit quasi index; [2019-07-09 09:17:14.970] [jointLog] [info] done; [2019-07-09 09:17:14.970] [jointLog] [info] Index contained 197,787 targets; [2019-07-09 10:02:20.484] [jointLog] [info] Computed 251,090 rich equivalence classes for further processing; [2019-07-09 10:02:20.484] [jointLog] [info] Counted 348,673,166 total reads in the equivalence classes ; [2019-07-09 10:02:20.485] [jointLog] [warning] Found 1893 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is too large; [2019-07-09 10:02:20.485] [jointLog] [info] Mapping rate = 39.7151%. [2019-07-09 10:02:20.485] [jointLog] [info] finished quantifyLibrary(); ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611:267,Security,validat,validateMappings,267,"Hey Avi, thanks for the quick reply!; Here is the salmon_quant_log file:; ```; [2019-07-09 09:07:39.153] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-07-09 09:07:39.153] [jointLog] [info] Using default value of 0.8 for minScoreFraction in Alevin; [2019-07-09 09:17:08.128] [jointLog] [info] There is 1 library.; [2019-07-09 09:17:08.180] [jointLog] [info] Loading Quasi index; [2019-07-09 09:17:08.180] [jointLog] [info] Loading 32-bit quasi index; [2019-07-09 09:17:14.970] [jointLog] [info] done; [2019-07-09 09:17:14.970] [jointLog] [info] Index contained 197,787 targets; [2019-07-09 10:02:20.484] [jointLog] [info] Computed 251,090 rich equivalence classes for further processing; [2019-07-09 10:02:20.484] [jointLog] [info] Counted 348,673,166 total reads in the equivalence classes ; [2019-07-09 10:02:20.485] [jointLog] [warning] Found 1893 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is too large; [2019-07-09 10:02:20.485] [jointLog] [info] Mapping rate = 39.7151%. [2019-07-09 10:02:20.485] [jointLog] [info] finished quantifyLibrary(); ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611:429,Security,validat,validateMappings,429,"Hey Avi, thanks for the quick reply!; Here is the salmon_quant_log file:; ```; [2019-07-09 09:07:39.153] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-07-09 09:07:39.153] [jointLog] [info] Using default value of 0.8 for minScoreFraction in Alevin; [2019-07-09 09:17:08.128] [jointLog] [info] There is 1 library.; [2019-07-09 09:17:08.180] [jointLog] [info] Loading Quasi index; [2019-07-09 09:17:08.180] [jointLog] [info] Loading 32-bit quasi index; [2019-07-09 09:17:14.970] [jointLog] [info] done; [2019-07-09 09:17:14.970] [jointLog] [info] Index contained 197,787 targets; [2019-07-09 10:02:20.484] [jointLog] [info] Computed 251,090 rich equivalence classes for further processing; [2019-07-09 10:02:20.484] [jointLog] [info] Counted 348,673,166 total reads in the equivalence classes ; [2019-07-09 10:02:20.485] [jointLog] [warning] Found 1893 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is too large; [2019-07-09 10:02:20.485] [jointLog] [info] Mapping rate = 39.7151%. [2019-07-09 10:02:20.485] [jointLog] [info] finished quantifyLibrary(); ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611:579,Security,validat,validateMappings,579,"Hey Avi, thanks for the quick reply!; Here is the salmon_quant_log file:; ```; [2019-07-09 09:07:39.153] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-07-09 09:07:39.153] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 1. Setting consensusSlack to 1.; [2019-07-09 09:07:39.153] [jointLog] [info] Using default value of 0.8 for minScoreFraction in Alevin; [2019-07-09 09:17:08.128] [jointLog] [info] There is 1 library.; [2019-07-09 09:17:08.180] [jointLog] [info] Loading Quasi index; [2019-07-09 09:17:08.180] [jointLog] [info] Loading 32-bit quasi index; [2019-07-09 09:17:14.970] [jointLog] [info] done; [2019-07-09 09:17:14.970] [jointLog] [info] Index contained 197,787 targets; [2019-07-09 10:02:20.484] [jointLog] [info] Computed 251,090 rich equivalence classes for further processing; [2019-07-09 10:02:20.484] [jointLog] [info] Counted 348,673,166 total reads in the equivalence classes ; [2019-07-09 10:02:20.485] [jointLog] [warning] Found 1893 reads with `N` in the UMI sequence and ignored the reads.; Please report on github if this number is too large; [2019-07-09 10:02:20.485] [jointLog] [info] Mapping rate = 39.7151%. [2019-07-09 10:02:20.485] [jointLog] [info] finished quantifyLibrary(); ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510544611
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510545906:53,Testability,log,log,53,"Sorry for the confusion, what I meant was the alevin log, it should be inside the alevin folder of your output subdirectory with the name `alevin.log`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510545906
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510545906:146,Testability,log,log,146,"Sorry for the confusion, what I meant was the alevin log, it should be inside the alevin folder of your output subdirectory with the name `alevin.log`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510545906
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510547693:1402,Performance,optimiz,optimizer,1402,"2019-07-09 09:17:07.572] [alevinLog] [info] Gauss Corrected Boundary at [32m 795 [0m; [2019-07-09 09:17:07.572] [alevinLog] [info] Learned InvCov: 173.265 normfactor: 1097.45; [2019-07-09 09:17:07.597] [alevinLog] [info] Total 41.2673% reads will be thrown away because of noisy Cellular barcodes.; [2019-07-09 09:17:07.597] [alevinLog] [info] Total [32m1192[0m(has [32m397[0m low confidence) barcodes; [2019-07-09 09:17:07.765] [alevinLog] [info] Done True Barcode Sampling; [2019-07-09 09:17:08.039] [alevinLog] [info] Done populating Z matrix; [2019-07-09 09:17:08.067] [alevinLog] [info] Done indexing Barcodes; [2019-07-09 09:17:08.067] [alevinLog] [info] Total Unique barcodes found: 7881525; [2019-07-09 09:17:08.067] [alevinLog] [info] Used Barcodes except Whitelist: 84951; [2019-07-09 09:17:08.128] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-07-09 09:17:08.128] [alevinLog] [info] parsing read library format; [2019-07-09 10:02:26.992] [alevinLog] [info] Starting optimizer. [2019-07-09 10:13:56.661] [alevinLog] [info] Total 99488568.00 UMI after deduplicating.; [2019-07-09 10:13:56.701] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-07-09 10:14:11.020] [alevinLog] [info] Starting Import of the gene count matrix of size 1192x60053.; [2019-07-09 10:14:11.286] [alevinLog] [info] Done initializing the empty matrix.; [2019-07-09 10:14:13.421] [alevinLog] [info] Done Importing gene count matrix for dimension 1192x60053; [2019-07-09 10:14:13.622] [alevinLog] [info] Starting white listing; [2019-07-09 10:14:13.627] [alevinLog] [info] Done importing order of barcodes ""quants_mat_rows.txt"" file.; [2019-07-09 10:14:13.627] [alevinLog] [info] Total 1192 barcodes found; [2019-07-09 10:14:13.627] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2019-07-09 10:14:13.627] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-07-09 10:14:13.627] [alevinLog",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510547693
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510547693:2703,Performance,optimiz,optimizer,2703," Done indexing Barcodes; [2019-07-09 09:17:08.067] [alevinLog] [info] Total Unique barcodes found: 7881525; [2019-07-09 09:17:08.067] [alevinLog] [info] Used Barcodes except Whitelist: 84951; [2019-07-09 09:17:08.128] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-07-09 09:17:08.128] [alevinLog] [info] parsing read library format; [2019-07-09 10:02:26.992] [alevinLog] [info] Starting optimizer. [2019-07-09 10:13:56.661] [alevinLog] [info] Total 99488568.00 UMI after deduplicating.; [2019-07-09 10:13:56.701] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-07-09 10:14:11.020] [alevinLog] [info] Starting Import of the gene count matrix of size 1192x60053.; [2019-07-09 10:14:11.286] [alevinLog] [info] Done initializing the empty matrix.; [2019-07-09 10:14:13.421] [alevinLog] [info] Done Importing gene count matrix for dimension 1192x60053; [2019-07-09 10:14:13.622] [alevinLog] [info] Starting white listing; [2019-07-09 10:14:13.627] [alevinLog] [info] Done importing order of barcodes ""quants_mat_rows.txt"" file.; [2019-07-09 10:14:13.627] [alevinLog] [info] Total 1192 barcodes found; [2019-07-09 10:14:13.627] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2019-07-09 10:14:13.627] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-07-09 10:14:13.627] [alevinLog] [info] Starting to make feature Matrix; [2019-07-09 10:14:13.885] [alevinLog] [info] Done making regular featues; [2019-07-09 10:14:13.885] [alevinLog] [info] Done making feature Matrix; [2019-07-09 10:14:13.891] [alevinLog] [info] Finished white listing; [2019-07-09 10:14:13.909] [alevinLog] [info] Finished optimizer; ```. Indeed the fractions of BC thrown away is huge. I might need to try your `--expectCells/--forceCells` option - but not sure how this parameter influences final output? i.e. how to select a feasible number - 8000 might not be optimal strictly speaking. Thanks!; Chelsea",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510547693
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510547693:523,Usability,Learn,Learned,523,Get it - here it is:; ```; [2019-07-09 09:07:39.162] [alevinLog] [info] Processing barcodes files (if Present) . ; [2019-07-09 09:16:59.454] [alevinLog] [info] Done barcode density calculation.; [2019-07-09 09:16:59.454] [alevinLog] [info] # Barcodes Used: [32m877102495[0m / [31m877935734[0m.; [2019-07-09 09:17:06.234] [alevinLog] [info] Knee found left boundary at [32m 4375 [0m; [2019-07-09 09:17:07.572] [alevinLog] [info] Gauss Corrected Boundary at [32m 795 [0m; [2019-07-09 09:17:07.572] [alevinLog] [info] Learned InvCov: 173.265 normfactor: 1097.45; [2019-07-09 09:17:07.597] [alevinLog] [info] Total 41.2673% reads will be thrown away because of noisy Cellular barcodes.; [2019-07-09 09:17:07.597] [alevinLog] [info] Total [32m1192[0m(has [32m397[0m low confidence) barcodes; [2019-07-09 09:17:07.765] [alevinLog] [info] Done True Barcode Sampling; [2019-07-09 09:17:08.039] [alevinLog] [info] Done populating Z matrix; [2019-07-09 09:17:08.067] [alevinLog] [info] Done indexing Barcodes; [2019-07-09 09:17:08.067] [alevinLog] [info] Total Unique barcodes found: 7881525; [2019-07-09 09:17:08.067] [alevinLog] [info] Used Barcodes except Whitelist: 84951; [2019-07-09 09:17:08.128] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-07-09 09:17:08.128] [alevinLog] [info] parsing read library format; [2019-07-09 10:02:26.992] [alevinLog] [info] Starting optimizer. [2019-07-09 10:13:56.661] [alevinLog] [info] Total 99488568.00 UMI after deduplicating.; [2019-07-09 10:13:56.701] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-07-09 10:14:11.020] [alevinLog] [info] Starting Import of the gene count matrix of size 1192x60053.; [2019-07-09 10:14:11.286] [alevinLog] [info] Done initializing the empty matrix.; [2019-07-09 10:14:13.421] [alevinLog] [info] Done Importing gene count matrix for dimension 1192x60053; [2019-07-09 10:14:13.622] [alevinLog] [info] Starting white listing; [2019-07-09 10:14:13.627] [alevinLog] [info] Done imp,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510547693
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510547693:1547,Usability,Clear,Clearing,1547,"ctor: 1097.45; [2019-07-09 09:17:07.597] [alevinLog] [info] Total 41.2673% reads will be thrown away because of noisy Cellular barcodes.; [2019-07-09 09:17:07.597] [alevinLog] [info] Total [32m1192[0m(has [32m397[0m low confidence) barcodes; [2019-07-09 09:17:07.765] [alevinLog] [info] Done True Barcode Sampling; [2019-07-09 09:17:08.039] [alevinLog] [info] Done populating Z matrix; [2019-07-09 09:17:08.067] [alevinLog] [info] Done indexing Barcodes; [2019-07-09 09:17:08.067] [alevinLog] [info] Total Unique barcodes found: 7881525; [2019-07-09 09:17:08.067] [alevinLog] [info] Used Barcodes except Whitelist: 84951; [2019-07-09 09:17:08.128] [alevinLog] [info] Done with Barcode Processing; Moving to Quantify. [2019-07-09 09:17:08.128] [alevinLog] [info] parsing read library format; [2019-07-09 10:02:26.992] [alevinLog] [info] Starting optimizer. [2019-07-09 10:13:56.661] [alevinLog] [info] Total 99488568.00 UMI after deduplicating.; [2019-07-09 10:13:56.701] [alevinLog] [info] Clearing EqMap; Might take some time.; [2019-07-09 10:14:11.020] [alevinLog] [info] Starting Import of the gene count matrix of size 1192x60053.; [2019-07-09 10:14:11.286] [alevinLog] [info] Done initializing the empty matrix.; [2019-07-09 10:14:13.421] [alevinLog] [info] Done Importing gene count matrix for dimension 1192x60053; [2019-07-09 10:14:13.622] [alevinLog] [info] Starting white listing; [2019-07-09 10:14:13.627] [alevinLog] [info] Done importing order of barcodes ""quants_mat_rows.txt"" file.; [2019-07-09 10:14:13.627] [alevinLog] [info] Total 1192 barcodes found; [2019-07-09 10:14:13.627] [alevinLog] [warning] mrna file not provided; using is 1 less feature for whitelisting; [2019-07-09 10:14:13.627] [alevinLog] [warning] rrna file not provided; using is 1 less feature for whitelisting; [2019-07-09 10:14:13.627] [alevinLog] [info] Starting to make feature Matrix; [2019-07-09 10:14:13.885] [alevinLog] [info] Done making regular featues; [2019-07-09 10:14:13.885] [alevinLog] [info] Do",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510547693
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510549172:334,Availability,down,down,334,"Yep, check this line in the log:; > Total 41.2673% reads will be thrown away because of noisy Cellular barcodes. I think the CB frequency is most probably a bimodal distribution, I'd suggest you to try `--expectCells 8000` command line flag along with the regular command you are using above and check if the numbers in the log comes down to `~15%`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510549172
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510549172:28,Testability,log,log,28,"Yep, check this line in the log:; > Total 41.2673% reads will be thrown away because of noisy Cellular barcodes. I think the CB frequency is most probably a bimodal distribution, I'd suggest you to try `--expectCells 8000` command line flag along with the regular command you are using above and check if the numbers in the log comes down to `~15%`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510549172
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510549172:324,Testability,log,log,324,"Yep, check this line in the log:; > Total 41.2673% reads will be thrown away because of noisy Cellular barcodes. I think the CB frequency is most probably a bimodal distribution, I'd suggest you to try `--expectCells 8000` command line flag along with the regular command you are using above and check if the numbers in the log comes down to `~15%`.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510549172
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510603746:534,Availability,down,downstream,534,"Hey Avi,. I have ran alevin with addition of `--expectCells 8000` flag, the new output of cells detected: ; `3655, 5604, 4374` w/ `13%, 30%, 15.7%` reads thrown away. It is better than the first trial `1192, 4947, 3414` but nevertheless fewer than the cell ranger output `5150, 7618, 6404`. . Wonder ; 1. if I should set higher `--expectCells`, but which would result in more unconfident calls?; 2. From 1, if I just try to get more cells subjectively, will the expression matrix (and further analysis) be inaccurate/affected? (given downstream filtering of cells of low quality based on # of feature detected etc. would be performed anyway. ) ; 3. what could be the reason that these two algorithms output such different total cell numbers (precision in calling?) . Thanks!; Chelsea",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510603746
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510603746:624,Performance,perform,performed,624,"Hey Avi,. I have ran alevin with addition of `--expectCells 8000` flag, the new output of cells detected: ; `3655, 5604, 4374` w/ `13%, 30%, 15.7%` reads thrown away. It is better than the first trial `1192, 4947, 3414` but nevertheless fewer than the cell ranger output `5150, 7618, 6404`. . Wonder ; 1. if I should set higher `--expectCells`, but which would result in more unconfident calls?; 2. From 1, if I just try to get more cells subjectively, will the expression matrix (and further analysis) be inaccurate/affected? (given downstream filtering of cells of low quality based on # of feature detected etc. would be performed anyway. ) ; 3. what could be the reason that these two algorithms output such different total cell numbers (precision in calling?) . Thanks!; Chelsea",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510603746
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510603746:96,Safety,detect,detected,96,"Hey Avi,. I have ran alevin with addition of `--expectCells 8000` flag, the new output of cells detected: ; `3655, 5604, 4374` w/ `13%, 30%, 15.7%` reads thrown away. It is better than the first trial `1192, 4947, 3414` but nevertheless fewer than the cell ranger output `5150, 7618, 6404`. . Wonder ; 1. if I should set higher `--expectCells`, but which would result in more unconfident calls?; 2. From 1, if I just try to get more cells subjectively, will the expression matrix (and further analysis) be inaccurate/affected? (given downstream filtering of cells of low quality based on # of feature detected etc. would be performed anyway. ) ; 3. what could be the reason that these two algorithms output such different total cell numbers (precision in calling?) . Thanks!; Chelsea",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510603746
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510603746:601,Safety,detect,detected,601,"Hey Avi,. I have ran alevin with addition of `--expectCells 8000` flag, the new output of cells detected: ; `3655, 5604, 4374` w/ `13%, 30%, 15.7%` reads thrown away. It is better than the first trial `1192, 4947, 3414` but nevertheless fewer than the cell ranger output `5150, 7618, 6404`. . Wonder ; 1. if I should set higher `--expectCells`, but which would result in more unconfident calls?; 2. From 1, if I just try to get more cells subjectively, will the expression matrix (and further analysis) be inaccurate/affected? (given downstream filtering of cells of low quality based on # of feature detected etc. would be performed anyway. ) ; 3. what could be the reason that these two algorithms output such different total cell numbers (precision in calling?) . Thanks!; Chelsea",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510603746
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771:70,Safety,predict,predicted,70,"Hi @ChelseaCHENX ,. Thanks for confirming the counts of the number of predicted cells.; As much as I'd love to give you an exact answers, but in realty w/ current settings it requires a little more exploratory analysis. Whitelisting traditionally is done by making some greedy choices and generally can results in different number of predicted cells, and having an exact answer is difficult to have. For example, if you run alevin with `--dumpFeatures` and plot the frequency of CB, as dumped in the `raw_cb_frequency.txt`, you will observe a monotonically non-increasing function. Different tools try to get the ""knee"" in the distribution, so as alevin, as the first round of whitelisting. For cellranger, at least in my understanding, they try to take the top X% (I think it's 10) of the value suggested through `expectCells` command as high confidence and use all the CB which has the frequency greater than the lowest frequency of the high confidence barcodes for quantification. To counter the greediness of the CB calling, we in our suggested method for alevin, proposed a naive bayes based approach by learning features from not only CB frequency but various other features. There had been other methods like ""emptyDrops"" which you can try for more fine-grained whitelisting post quantification using alevin quants. Having said that, if you use expectCells with bigger value, alevin will start to include more and more cells. However as the frequency of the new CB which gets included as high confidence with each new iteration drops exponentially, and even though the new CB gets merged to a high confidence barcode its chance of affecting the quantification also drops. In summary, if you are sure about your experiment to have more cells then it's ideal to increase the value otherwise, I think, with the increased expectCells value the quants can potentially be effected but most probably not by a lot. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771:334,Safety,predict,predicted,334,"Hi @ChelseaCHENX ,. Thanks for confirming the counts of the number of predicted cells.; As much as I'd love to give you an exact answers, but in realty w/ current settings it requires a little more exploratory analysis. Whitelisting traditionally is done by making some greedy choices and generally can results in different number of predicted cells, and having an exact answer is difficult to have. For example, if you run alevin with `--dumpFeatures` and plot the frequency of CB, as dumped in the `raw_cb_frequency.txt`, you will observe a monotonically non-increasing function. Different tools try to get the ""knee"" in the distribution, so as alevin, as the first round of whitelisting. For cellranger, at least in my understanding, they try to take the top X% (I think it's 10) of the value suggested through `expectCells` command as high confidence and use all the CB which has the frequency greater than the lowest frequency of the high confidence barcodes for quantification. To counter the greediness of the CB calling, we in our suggested method for alevin, proposed a naive bayes based approach by learning features from not only CB frequency but various other features. There had been other methods like ""emptyDrops"" which you can try for more fine-grained whitelisting post quantification using alevin quants. Having said that, if you use expectCells with bigger value, alevin will start to include more and more cells. However as the frequency of the new CB which gets included as high confidence with each new iteration drops exponentially, and even though the new CB gets merged to a high confidence barcode its chance of affecting the quantification also drops. In summary, if you are sure about your experiment to have more cells then it's ideal to increase the value otherwise, I think, with the increased expectCells value the quants can potentially be effected but most probably not by a lot. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771:1109,Usability,learn,learning,1109,"Hi @ChelseaCHENX ,. Thanks for confirming the counts of the number of predicted cells.; As much as I'd love to give you an exact answers, but in realty w/ current settings it requires a little more exploratory analysis. Whitelisting traditionally is done by making some greedy choices and generally can results in different number of predicted cells, and having an exact answer is difficult to have. For example, if you run alevin with `--dumpFeatures` and plot the frequency of CB, as dumped in the `raw_cb_frequency.txt`, you will observe a monotonically non-increasing function. Different tools try to get the ""knee"" in the distribution, so as alevin, as the first round of whitelisting. For cellranger, at least in my understanding, they try to take the top X% (I think it's 10) of the value suggested through `expectCells` command as high confidence and use all the CB which has the frequency greater than the lowest frequency of the high confidence barcodes for quantification. To counter the greediness of the CB calling, we in our suggested method for alevin, proposed a naive bayes based approach by learning features from not only CB frequency but various other features. There had been other methods like ""emptyDrops"" which you can try for more fine-grained whitelisting post quantification using alevin quants. Having said that, if you use expectCells with bigger value, alevin will start to include more and more cells. However as the frequency of the new CB which gets included as high confidence with each new iteration drops exponentially, and even though the new CB gets merged to a high confidence barcode its chance of affecting the quantification also drops. In summary, if you are sure about your experiment to have more cells then it's ideal to increase the value otherwise, I think, with the increased expectCells value the quants can potentially be effected but most probably not by a lot. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510628771
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510639440:870,Availability,down,downstream,870,"Hi Avi, thanks for your detailed explanation!. From my understanding: a pre-selection of high-quality cells based on 1) CB frequency - finding the knee point (in the initial whitelisting) and 2) other features (in finalized/intelligent) whitelisting is performed in alevin, while [cell ranger count](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/algorithms/overview#cell_calling) does step 1) related to the `--expectCells` number and used an alternative method w/o knee point estimation. . Based on above, the newly included cells w/ increased number of `--expectCells` are also more likely to be filtered out in later steps using criteria such as `min of number of features/reads` detected per sample. But such filtering may not be expected if interests are also on cells with small transcriptomes such as TILs. I will try some downstream filtering to see how many good cells I can get. . Yeah it helps - thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510639440
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510639440:369,Deployability,pipeline,pipelines,369,"Hi Avi, thanks for your detailed explanation!. From my understanding: a pre-selection of high-quality cells based on 1) CB frequency - finding the knee point (in the initial whitelisting) and 2) other features (in finalized/intelligent) whitelisting is performed in alevin, while [cell ranger count](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/algorithms/overview#cell_calling) does step 1) related to the `--expectCells` number and used an alternative method w/o knee point estimation. . Based on above, the newly included cells w/ increased number of `--expectCells` are also more likely to be filtered out in later steps using criteria such as `min of number of features/reads` detected per sample. But such filtering may not be expected if interests are also on cells with small transcriptomes such as TILs. I will try some downstream filtering to see how many good cells I can get. . Yeah it helps - thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510639440
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510639440:253,Performance,perform,performed,253,"Hi Avi, thanks for your detailed explanation!. From my understanding: a pre-selection of high-quality cells based on 1) CB frequency - finding the knee point (in the initial whitelisting) and 2) other features (in finalized/intelligent) whitelisting is performed in alevin, while [cell ranger count](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/algorithms/overview#cell_calling) does step 1) related to the `--expectCells` number and used an alternative method w/o knee point estimation. . Based on above, the newly included cells w/ increased number of `--expectCells` are also more likely to be filtered out in later steps using criteria such as `min of number of features/reads` detected per sample. But such filtering may not be expected if interests are also on cells with small transcriptomes such as TILs. I will try some downstream filtering to see how many good cells I can get. . Yeah it helps - thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510639440
https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510639440:723,Safety,detect,detected,723,"Hi Avi, thanks for your detailed explanation!. From my understanding: a pre-selection of high-quality cells based on 1) CB frequency - finding the knee point (in the initial whitelisting) and 2) other features (in finalized/intelligent) whitelisting is performed in alevin, while [cell ranger count](https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/algorithms/overview#cell_calling) does step 1) related to the `--expectCells` number and used an alternative method w/o knee point estimation. . Based on above, the newly included cells w/ increased number of `--expectCells` are also more likely to be filtered out in later steps using criteria such as `min of number of features/reads` detected per sample. But such filtering may not be expected if interests are also on cells with small transcriptomes such as TILs. I will try some downstream filtering to see how many good cells I can get. . Yeah it helps - thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/396#issuecomment-510639440
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:41,Energy Efficiency,adapt,adapter,41,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:131,Energy Efficiency,adapt,adapter,131,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:179,Energy Efficiency,Adapt,Adapter,179,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:41,Integrability,adapter,adapter,41,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:131,Integrability,adapter,adapter,131,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:179,Integrability,Adapter,Adapter,179,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:41,Modifiability,adapt,adapter,41,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:131,Modifiability,adapt,adapter,131,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337:179,Modifiability,Adapt,Adapter,179,"Hi @diyang1354,. It is recommended to do adapter trimming prior to mapping and quantification (standard practices actually involve adapter and _light_ quality trimming of reads). Adapter contamination could affect the mapping rate, especially if selective-alignment, which is recommended, is being used.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/398#issuecomment-511428337
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:428,Security,validat,validateMappings,428,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:828,Security,validat,validateMappings,828,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:1205,Security,validat,validateMappings,1205,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:1425,Security,validat,validateMappings,1425,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:554,Testability,test,tested,554,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:58,Usability,clear,clear,58,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297:1189,Usability,simpl,simply,1189,"Hi @jckhearn,. The documentation could definitely be more clear, so let me try and clarify here and make a note to clean up the documentation more as well. I'll answer in reverse order:. > Given the above command should I go back to a non-decoy aware transcriptome?. No. What the statement in the documentation means to convey is that if you are using the basic quasi-mapping algorithm (not selective-alignment as enabled by `--validateMappings`, `--mimicBT2` or `--mimicStrictBT2`), then you should not be using a decoy-aware transcriptome. We have not tested the effect of decoys on the basic quasi-mapping approach, and though that may be supported in the future, it is not right now. However, if you are using any flavor of selective-alignment, then please _do_ use the decoy-aware transcriptome. . Regarding ""combining"" `--validateMappings`, `--mimicBT2` and `--mimicStrictBT2`, this is not possible. That is, you should view `--mimicBT2` and `--mimicStrictBT2` as ""meta-flags"" that enable selective-alignment and also set a few other options that are meant to mimic the BT2 behavior more closely. We generally do _not_ recommend `--mimicStrictBT2`, and so the main choice is between simply using `--validateMappings` vs. `--mimicBT2`. The main differences here are that `--mimicBT2` sets slightly more sensitive parameters to find alignments, but is also stricter in what it reports. The biggest differences is that `--validateMappings` will allow orphaned mappings (where one end of a paired-end fragment aligns but the mate doesn't), while `--mimicBT2` will not allow such mappings. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/399#issuecomment-511884297
https://github.com/COMBINE-lab/salmon/issues/401#issuecomment-512905804:773,Availability,down,down,773,"Hello,. I've actually been thinking of a different method that would require very stringent mapping. By providing transcripts of only exon 1 & 2, exon 2 & 3, and exon 1 & 3 I could get a better idea of the number of reads that skip exon 2 all together. Also, by averaging the read counts that map to the junctions of exon 1 & 2 and exon 2 & 3, I can help eliminate polyA tail bias that is heavily positioned towards exon 1 and would also allow me to get a more accurate prediction of the two gene versions since 1 read mapped to exon 1 & 2 and 1 read mapped to exon 2 & 3 would essentially tell me twice that the gene is there while a read mapped to exon 1 & 3 would only tell me once that the gene is there. However, doing so would force me to bring ```AuxSampleNumber``` down to very low numbers such as 10 - 100 as using stringent coverage parameters drastically reduces my reads mapped. . I do wonder though how these low AUX numbers might affect your model development and algorithm. Any input into the aspect of low AUX numbers?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/401#issuecomment-512905804
https://github.com/COMBINE-lab/salmon/issues/401#issuecomment-512905804:866,Energy Efficiency,reduce,reduces,866,"Hello,. I've actually been thinking of a different method that would require very stringent mapping. By providing transcripts of only exon 1 & 2, exon 2 & 3, and exon 1 & 3 I could get a better idea of the number of reads that skip exon 2 all together. Also, by averaging the read counts that map to the junctions of exon 1 & 2 and exon 2 & 3, I can help eliminate polyA tail bias that is heavily positioned towards exon 1 and would also allow me to get a more accurate prediction of the two gene versions since 1 read mapped to exon 1 & 2 and 1 read mapped to exon 2 & 3 would essentially tell me twice that the gene is there while a read mapped to exon 1 & 3 would only tell me once that the gene is there. However, doing so would force me to bring ```AuxSampleNumber``` down to very low numbers such as 10 - 100 as using stringent coverage parameters drastically reduces my reads mapped. . I do wonder though how these low AUX numbers might affect your model development and algorithm. Any input into the aspect of low AUX numbers?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/401#issuecomment-512905804
https://github.com/COMBINE-lab/salmon/issues/401#issuecomment-512905804:470,Safety,predict,prediction,470,"Hello,. I've actually been thinking of a different method that would require very stringent mapping. By providing transcripts of only exon 1 & 2, exon 2 & 3, and exon 1 & 3 I could get a better idea of the number of reads that skip exon 2 all together. Also, by averaging the read counts that map to the junctions of exon 1 & 2 and exon 2 & 3, I can help eliminate polyA tail bias that is heavily positioned towards exon 1 and would also allow me to get a more accurate prediction of the two gene versions since 1 read mapped to exon 1 & 2 and 1 read mapped to exon 2 & 3 would essentially tell me twice that the gene is there while a read mapped to exon 1 & 3 would only tell me once that the gene is there. However, doing so would force me to bring ```AuxSampleNumber``` down to very low numbers such as 10 - 100 as using stringent coverage parameters drastically reduces my reads mapped. . I do wonder though how these low AUX numbers might affect your model development and algorithm. Any input into the aspect of low AUX numbers?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/401#issuecomment-512905804
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-515138596:4,Deployability,update,updates,4,"Any updates on this?; The doc does not mention anything of the sort; ```The first entry in this line is the number of transcripts in the label of this equivalence class (the number of different transcripts to which fragments in this class map — call this k). The line then contains the k transcript IDs. Finally, the line contains the count of fragments in this equivalence class (how many fragments mapped to these transcripts). The values in each such line are tab separated.```; Thanks",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-515138596
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:2324,Availability,down,down-weight,2324,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:346,Deployability,release,release,346,"Hi @leagleag,. Thank you for your question. This is because with selective alignment (`--validateMappings`) salmon is making use of [range-factorized equivalence classes](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977), which do not directly correspond to the ""standard"" notion of equivalence classes. In fact, as of the next release, it will _always_ make use of these equivalence classes by default. This leads to potentially confusing results when used in conjunction with `--dumpEq`. Specifically, the range-factorized equivalence relation group fragments not only by the transcripts to which they map, but also with respect to the conditional probabilities of having generated that fragment & alignment score given each transcript. Practically, what happens is that the space of conditional probabilities is quantized, and an equivalence relation is defined based on both the transcript set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different co",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:1462,Deployability,release,release,1462,"rized equivalence relation group fragments not only by the transcripts to which they map, but also with respect to the conditional probabilities of having generated that fragment & alignment score given each transcript. Practically, what happens is that the space of conditional probabilities is quantized, and an equivalence relation is defined based on both the transcript set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:2794,Deployability,update,update,2794,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:2852,Deployability,release,release,2852,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:89,Security,validat,validateMappings,89,"Hi @leagleag,. Thank you for your question. This is because with selective alignment (`--validateMappings`) salmon is making use of [range-factorized equivalence classes](https://academic.oup.com/bioinformatics/article/33/14/i142/3953977), which do not directly correspond to the ""standard"" notion of equivalence classes. In fact, as of the next release, it will _always_ make use of these equivalence classes by default. This leads to potentially confusing results when used in conjunction with `--dumpEq`. Specifically, the range-factorized equivalence relation group fragments not only by the transcripts to which they map, but also with respect to the conditional probabilities of having generated that fragment & alignment score given each transcript. Practically, what happens is that the space of conditional probabilities is quantized, and an equivalence relation is defined based on both the transcript set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different co",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:1906,Usability,simpl,simple,1906,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654:2899,Usability,clear,clear,2899,"script set and the vector of conditional probability bins into which the mapping falls with respect to each transcript in the equivalence class. This means that range-factorized equivalence classes can have multiple classes of fragments that map to the same set of transcripts, but with different conditional probabilities. Additionally, for each bin, the average conditional probability of fragments arising from that bin is maintained. What you are seeing printed out are the transcript sets, followed by the conditional bin indexes. Starting in the next release (and currently in the develop branch), we've cleaned up the interaction of the range-factorized equivalence classes with the `--dumpEq` and `--dumpEqWeights` flags. If you run with the `--dumpEqWeights` flags, salmon will dump the transcript sets, followed by the conditional probability vector, followed by the fragment count. If you run with the `--dumpEq` flag, it will collapse all of the range-factorized equivalence classes into ""simple"" equivalence classes by combining classes with the same transcript set (but different conditional probability vectors) and summing the corresponding fragment counts. This, of course, is a lossy transformation, and the equivalence classes will no longer represent the relevant conditional probabilities used during inference. Also, since the range-factorized equivalence classes allow for (but probabilistically down-weight) non-optimal mappings of fragments to transcripts, these collapsed equivalence classes will tend to have bigger labels (i.e. more transcripts) which might be difficult to properly interpret without the relevant conditional probabilities. The `--hardFilter` flag will filter out transcripts that have non-best alignment scores (a big component of the conditional fragment probability), but that can have a negative effect on the modeling and inference. We'll update the documentation accordingly when we cut the next release to make all of these interactions more clear.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/402#issuecomment-517041654
https://github.com/COMBINE-lab/salmon/issues/404#issuecomment-513495991:95,Safety,predict,predicted,95,"Yes, that's a wrong file. You have to run alevin either w/o the whitelist or provide the CB as predicted by Cellranger in its output folder, usually inside `filtered_bc_matrix` folder. Please check the `--whitelist` section [here](https://salmon.readthedocs.io/en/latest/alevin.html) for more info.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/404#issuecomment-513495991
https://github.com/COMBINE-lab/salmon/issues/406#issuecomment-820027601:4,Deployability,update,updates,4,Any updates or recommendation on this front?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/406#issuecomment-820027601
https://github.com/COMBINE-lab/salmon/issues/407#issuecomment-514929400:32,Testability,log,logs,32,"Hi, is it possible to share the logs and the minimal version of the data on which we can replicate the bug?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/407#issuecomment-514929400
https://github.com/COMBINE-lab/salmon/issues/407#issuecomment-514935938:201,Testability,log,log,201,"Hi, this got rectified by using ENSEMBL gene ids rather than gene symbol/gene names.; Other issues are : none of my mitochondrial and ribosomal gene lists are able to find in the reference index.; The log file attached:; [alevin.log](https://github.com/COMBINE-lab/salmon/files/3430327/alevin.log); The index reference genome is from gencodeV31.pc.transcripts.fa.gz from gencode human; The txptogene contains list of ENST* -- > ENSG*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/407#issuecomment-514935938
https://github.com/COMBINE-lab/salmon/issues/407#issuecomment-514935938:229,Testability,log,log,229,"Hi, this got rectified by using ENSEMBL gene ids rather than gene symbol/gene names.; Other issues are : none of my mitochondrial and ribosomal gene lists are able to find in the reference index.; The log file attached:; [alevin.log](https://github.com/COMBINE-lab/salmon/files/3430327/alevin.log); The index reference genome is from gencodeV31.pc.transcripts.fa.gz from gencode human; The txptogene contains list of ENST* -- > ENSG*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/407#issuecomment-514935938
https://github.com/COMBINE-lab/salmon/issues/407#issuecomment-514935938:293,Testability,log,log,293,"Hi, this got rectified by using ENSEMBL gene ids rather than gene symbol/gene names.; Other issues are : none of my mitochondrial and ribosomal gene lists are able to find in the reference index.; The log file attached:; [alevin.log](https://github.com/COMBINE-lab/salmon/files/3430327/alevin.log); The index reference genome is from gencodeV31.pc.transcripts.fa.gz from gencode human; The txptogene contains list of ENST* -- > ENSG*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/407#issuecomment-514935938
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-514816761:74,Availability,error,error,74,"Hi @seboles ,. My guess is that the issue is related to this (non-salmon) error appearing before each salmon output:. ```; basename: extra operand ‘lightreceptor-1_S114_L005_R1_001.qc.fq.gz’; Try 'basename --help' for more information.; ```. it looks like there is an error in the way the paths to the files are being provided for the jobs, which is resulting in an incorrect command line being provided to salmon (and the index not being properly located). It's also of note that this is on the command line:. ```; -1 *R1_001.qc.fq.gz -2 R2_001.qc.fq.gz; ```. which is trying to expand a wildcard before read 1 but not read 2; is that intended?. Any idea what the output is if you run:. ```; #!/bin/bash -l ; #SBATCH -J male_salmon_map; #SBATCH -t 700:00:00; #SBATCH -p high; #SBATCH --cpus-per-task=24; source ~/.bashrc; source activate salmon; cd /home/seboles/abaloneraw/salmon_quantification/salmon_male/abalone_orfs/; ls -la salmon_index; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-514816761
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-514816761:268,Availability,error,error,268,"Hi @seboles ,. My guess is that the issue is related to this (non-salmon) error appearing before each salmon output:. ```; basename: extra operand ‘lightreceptor-1_S114_L005_R1_001.qc.fq.gz’; Try 'basename --help' for more information.; ```. it looks like there is an error in the way the paths to the files are being provided for the jobs, which is resulting in an incorrect command line being provided to salmon (and the index not being properly located). It's also of note that this is on the command line:. ```; -1 *R1_001.qc.fq.gz -2 R2_001.qc.fq.gz; ```. which is trying to expand a wildcard before read 1 but not read 2; is that intended?. Any idea what the output is if you run:. ```; #!/bin/bash -l ; #SBATCH -J male_salmon_map; #SBATCH -t 700:00:00; #SBATCH -p high; #SBATCH --cpus-per-task=24; source ~/.bashrc; source activate salmon; cd /home/seboles/abaloneraw/salmon_quantification/salmon_male/abalone_orfs/; ls -la salmon_index; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-514816761
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395:672,Availability,error,error,672,"Hi Rob,. I did some follow up using your suggestions, and I had indexed my; transcriptome incorrectly, but now it appears that I am having a separate; issue and was hoping that you might be able to point me in the right; direction? Here is my command:. ```#!/bin/bash -l; #SBATCH -J male_salmon_map; #SBATCH -t 150:00:00; #SBATCH -p high; #SBATCH --cpus-per-task=24; source ~/.bashrc; source activate salmon; cd /home/seboles/abaloneraw/salmon_quantification/SALMON_MALE/; for i in *.qc.fq.gz; do; salmon quant -i maleredabalone_index --libType IU -1 *R1_001.qc.fq.gz -2; *R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias --gcBias --validateMappings; done```. And here is the error message that I receive:. ```[2019-07-29 14:31:12.352] [jointLog] [error] You passed paired-end files; to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; You must pass the same number of files to both flags; Name : male_salmon_map; User : seboles; Partition : high; Nodes : c11-71; Cores : 24; GPUs : 0; State : FAILED; Submit : 2019-07-29T14:31:01; Start : 2019-07-29T14:31:02; End : 2019-07-29T14:31:13; Reserved walltime : 6-06:00:00; Used walltime : 00:00:11; Used CPU time : 00:00:09; % User (Computation): 54.66%; % System (I/O) : 45.33%; Mem reserved : 2000M/core; Max Mem used : 0.00 (c11-71); Max Disk Write : 0.00 (c11-71); Max Disk Read : 0.00 (c11-71)```. I have gone back and checked the directory containing the PE reads, and; they are all accounted for, so I am a little stumped at the moment. I; appreciate any advice you may have. Happy Monday,. Sara. On Wed, Jul 24, 2019 at 3:04 PM Rob Patro <notifications@github.com> wrote:. > Hi @seboles <https://github.com/seboles> ,; >; > My guess is that the issue is related to this (non-salmon) error appearing; > before each salmon output:; >; > basename: extra operand ‘lightreceptor-1_S114_L005_R1_001.qc.fq.gz’; >; > Try 'basename --help' for more information.; >; >; > it looks like there is an error in the way the paths to the files ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395:744,Availability,error,error,744,"Hi Rob,. I did some follow up using your suggestions, and I had indexed my; transcriptome incorrectly, but now it appears that I am having a separate; issue and was hoping that you might be able to point me in the right; direction? Here is my command:. ```#!/bin/bash -l; #SBATCH -J male_salmon_map; #SBATCH -t 150:00:00; #SBATCH -p high; #SBATCH --cpus-per-task=24; source ~/.bashrc; source activate salmon; cd /home/seboles/abaloneraw/salmon_quantification/SALMON_MALE/; for i in *.qc.fq.gz; do; salmon quant -i maleredabalone_index --libType IU -1 *R1_001.qc.fq.gz -2; *R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias --gcBias --validateMappings; done```. And here is the error message that I receive:. ```[2019-07-29 14:31:12.352] [jointLog] [error] You passed paired-end files; to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; You must pass the same number of files to both flags; Name : male_salmon_map; User : seboles; Partition : high; Nodes : c11-71; Cores : 24; GPUs : 0; State : FAILED; Submit : 2019-07-29T14:31:01; Start : 2019-07-29T14:31:02; End : 2019-07-29T14:31:13; Reserved walltime : 6-06:00:00; Used walltime : 00:00:11; Used CPU time : 00:00:09; % User (Computation): 54.66%; % System (I/O) : 45.33%; Mem reserved : 2000M/core; Max Mem used : 0.00 (c11-71); Max Disk Write : 0.00 (c11-71); Max Disk Read : 0.00 (c11-71)```. I have gone back and checked the directory containing the PE reads, and; they are all accounted for, so I am a little stumped at the moment. I; appreciate any advice you may have. Happy Monday,. Sara. On Wed, Jul 24, 2019 at 3:04 PM Rob Patro <notifications@github.com> wrote:. > Hi @seboles <https://github.com/seboles> ,; >; > My guess is that the issue is related to this (non-salmon) error appearing; > before each salmon output:; >; > basename: extra operand ‘lightreceptor-1_S114_L005_R1_001.qc.fq.gz’; >; > Try 'basename --help' for more information.; >; >; > it looks like there is an error in the way the paths to the files ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395:1756,Availability,error,error,1756,"d files; to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; You must pass the same number of files to both flags; Name : male_salmon_map; User : seboles; Partition : high; Nodes : c11-71; Cores : 24; GPUs : 0; State : FAILED; Submit : 2019-07-29T14:31:01; Start : 2019-07-29T14:31:02; End : 2019-07-29T14:31:13; Reserved walltime : 6-06:00:00; Used walltime : 00:00:11; Used CPU time : 00:00:09; % User (Computation): 54.66%; % System (I/O) : 45.33%; Mem reserved : 2000M/core; Max Mem used : 0.00 (c11-71); Max Disk Write : 0.00 (c11-71); Max Disk Read : 0.00 (c11-71)```. I have gone back and checked the directory containing the PE reads, and; they are all accounted for, so I am a little stumped at the moment. I; appreciate any advice you may have. Happy Monday,. Sara. On Wed, Jul 24, 2019 at 3:04 PM Rob Patro <notifications@github.com> wrote:. > Hi @seboles <https://github.com/seboles> ,; >; > My guess is that the issue is related to this (non-salmon) error appearing; > before each salmon output:; >; > basename: extra operand ‘lightreceptor-1_S114_L005_R1_001.qc.fq.gz’; >; > Try 'basename --help' for more information.; >; >; > it looks like there is an error in the way the paths to the files are; > being provided for the jobs, which is resulting in an incorrect command; > line being provided to salmon (and the index not being properly located).; > It's also of note that this is on the command line:; >; > -1 *R1_001.qc.fq.gz -2 R2_001.qc.fq.gz; >; >; > which is trying to expand a wildcard before read 1 but not read 2; is that; > intended?; >; > Any idea what the output is if you run:; >; > #!/bin/bash -l; >; > #SBATCH -J male_salmon_map; >; > #SBATCH -t 700:00:00; >; > #SBATCH -p high; >; > #SBATCH --cpus-per-task=24; >; > source ~/.bashrc; >; > source activate salmon; >; > cd /home/seboles/abaloneraw/salmon_quantification/salmon_male/abalone_orfs/; >; > ls -la salmon_index; >; >; > —; > You are receiving this because you were mentioned.; > Reply t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395:1961,Availability,error,error,1961,"14:31:01; Start : 2019-07-29T14:31:02; End : 2019-07-29T14:31:13; Reserved walltime : 6-06:00:00; Used walltime : 00:00:11; Used CPU time : 00:00:09; % User (Computation): 54.66%; % System (I/O) : 45.33%; Mem reserved : 2000M/core; Max Mem used : 0.00 (c11-71); Max Disk Write : 0.00 (c11-71); Max Disk Read : 0.00 (c11-71)```. I have gone back and checked the directory containing the PE reads, and; they are all accounted for, so I am a little stumped at the moment. I; appreciate any advice you may have. Happy Monday,. Sara. On Wed, Jul 24, 2019 at 3:04 PM Rob Patro <notifications@github.com> wrote:. > Hi @seboles <https://github.com/seboles> ,; >; > My guess is that the issue is related to this (non-salmon) error appearing; > before each salmon output:; >; > basename: extra operand ‘lightreceptor-1_S114_L005_R1_001.qc.fq.gz’; >; > Try 'basename --help' for more information.; >; >; > it looks like there is an error in the way the paths to the files are; > being provided for the jobs, which is resulting in an incorrect command; > line being provided to salmon (and the index not being properly located).; > It's also of note that this is on the command line:; >; > -1 *R1_001.qc.fq.gz -2 R2_001.qc.fq.gz; >; >; > which is trying to expand a wildcard before read 1 but not read 2; is that; > intended?; >; > Any idea what the output is if you run:; >; > #!/bin/bash -l; >; > #SBATCH -J male_salmon_map; >; > #SBATCH -t 700:00:00; >; > #SBATCH -p high; >; > #SBATCH --cpus-per-task=24; >; > source ~/.bashrc; >; > source activate salmon; >; > cd /home/seboles/abaloneraw/salmon_quantification/salmon_male/abalone_orfs/; >; > ls -la salmon_index; >; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXACFB45AVOUW4O3P5ODQBDGVZA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD2XXV6I#issuecomment-514816761>,;",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395:678,Integrability,message,message,678,"Hi Rob,. I did some follow up using your suggestions, and I had indexed my; transcriptome incorrectly, but now it appears that I am having a separate; issue and was hoping that you might be able to point me in the right; direction? Here is my command:. ```#!/bin/bash -l; #SBATCH -J male_salmon_map; #SBATCH -t 150:00:00; #SBATCH -p high; #SBATCH --cpus-per-task=24; source ~/.bashrc; source activate salmon; cd /home/seboles/abaloneraw/salmon_quantification/SALMON_MALE/; for i in *.qc.fq.gz; do; salmon quant -i maleredabalone_index --libType IU -1 *R1_001.qc.fq.gz -2; *R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias --gcBias --validateMappings; done```. And here is the error message that I receive:. ```[2019-07-29 14:31:12.352] [jointLog] [error] You passed paired-end files; to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; You must pass the same number of files to both flags; Name : male_salmon_map; User : seboles; Partition : high; Nodes : c11-71; Cores : 24; GPUs : 0; State : FAILED; Submit : 2019-07-29T14:31:01; Start : 2019-07-29T14:31:02; End : 2019-07-29T14:31:13; Reserved walltime : 6-06:00:00; Used walltime : 00:00:11; Used CPU time : 00:00:09; % User (Computation): 54.66%; % System (I/O) : 45.33%; Mem reserved : 2000M/core; Max Mem used : 0.00 (c11-71); Max Disk Write : 0.00 (c11-71); Max Disk Read : 0.00 (c11-71)```. I have gone back and checked the directory containing the PE reads, and; they are all accounted for, so I am a little stumped at the moment. I; appreciate any advice you may have. Happy Monday,. Sara. On Wed, Jul 24, 2019 at 3:04 PM Rob Patro <notifications@github.com> wrote:. > Hi @seboles <https://github.com/seboles> ,; >; > My guess is that the issue is related to this (non-salmon) error appearing; > before each salmon output:; >; > basename: extra operand ‘lightreceptor-1_S114_L005_R1_001.qc.fq.gz’; >; > Try 'basename --help' for more information.; >; >; > it looks like there is an error in the way the paths to the files ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395:629,Security,validat,validateMappings,629,"Hi Rob,. I did some follow up using your suggestions, and I had indexed my; transcriptome incorrectly, but now it appears that I am having a separate; issue and was hoping that you might be able to point me in the right; direction? Here is my command:. ```#!/bin/bash -l; #SBATCH -J male_salmon_map; #SBATCH -t 150:00:00; #SBATCH -p high; #SBATCH --cpus-per-task=24; source ~/.bashrc; source activate salmon; cd /home/seboles/abaloneraw/salmon_quantification/SALMON_MALE/; for i in *.qc.fq.gz; do; salmon quant -i maleredabalone_index --libType IU -1 *R1_001.qc.fq.gz -2; *R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias --gcBias --validateMappings; done```. And here is the error message that I receive:. ```[2019-07-29 14:31:12.352] [jointLog] [error] You passed paired-end files; to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; You must pass the same number of files to both flags; Name : male_salmon_map; User : seboles; Partition : high; Nodes : c11-71; Cores : 24; GPUs : 0; State : FAILED; Submit : 2019-07-29T14:31:01; Start : 2019-07-29T14:31:02; End : 2019-07-29T14:31:13; Reserved walltime : 6-06:00:00; Used walltime : 00:00:11; Used CPU time : 00:00:09; % User (Computation): 54.66%; % System (I/O) : 45.33%; Mem reserved : 2000M/core; Max Mem used : 0.00 (c11-71); Max Disk Write : 0.00 (c11-71); Max Disk Read : 0.00 (c11-71)```. I have gone back and checked the directory containing the PE reads, and; they are all accounted for, so I am a little stumped at the moment. I; appreciate any advice you may have. Happy Monday,. Sara. On Wed, Jul 24, 2019 at 3:04 PM Rob Patro <notifications@github.com> wrote:. > Hi @seboles <https://github.com/seboles> ,; >; > My guess is that the issue is related to this (non-salmon) error appearing; > before each salmon output:; >; > basename: extra operand ‘lightreceptor-1_S114_L005_R1_001.qc.fq.gz’; >; > Try 'basename --help' for more information.; >; >; > it looks like there is an error in the way the paths to the files ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516173395
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516185511:200,Testability,log,log,200,"```; You passed paired-end files; to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; You must pass the same number of files to both flags; ```. Is this true ? Can you share the log ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516185511
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201:816,Availability,error,error,816,"Hello Avi,. Here is my out put log. Thank you in advance for an help you can provide. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. Best,. Sara. On Mon, Jul 29, 2019 at 3:25 PM Avi Srivastava <notifications@github.com>; wrote:. > You passed paired-end files; > to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; > You must pass the same number of files to both flags; >; > Is this true ? Can you share the log ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAH7HQIR4ZVWMTE2KXLQB5U5LA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CF3JY#issuecomment-516185511>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEHDXAG7WI3B7QBMJOSXTATQB5U5LANCNFSM4IGU4ZTA>; > .; >. -- ; Sara E. Boles, MS; PhD Candidate | Whitehead Lab; Pharmacology and Toxicology Graduate Group; Department of Environmental Toxicology; Univer",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201:275,Security,validat,validateMappings,275,"Hello Avi,. Here is my out put log. Thank you in advance for an help you can provide. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. Best,. Sara. On Mon, Jul 29, 2019 at 3:25 PM Avi Srivastava <notifications@github.com>; wrote:. > You passed paired-end files; > to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; > You must pass the same number of files to both flags; >; > Is this true ? Can you share the log ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAH7HQIR4ZVWMTE2KXLQB5U5LA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CF3JY#issuecomment-516185511>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEHDXAG7WI3B7QBMJOSXTATQB5U5LANCNFSM4IGU4ZTA>; > .; >. -- ; Sara E. Boles, MS; PhD Candidate | Whitehead Lab; Pharmacology and Toxicology Graduate Group; Department of Environmental Toxicology; Univer",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201:439,Security,validat,validateMappings,439,"Hello Avi,. Here is my out put log. Thank you in advance for an help you can provide. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. Best,. Sara. On Mon, Jul 29, 2019 at 3:25 PM Avi Srivastava <notifications@github.com>; wrote:. > You passed paired-end files; > to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; > You must pass the same number of files to both flags; >; > Is this true ? Can you share the log ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAH7HQIR4ZVWMTE2KXLQB5U5LA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CF3JY#issuecomment-516185511>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEHDXAG7WI3B7QBMJOSXTATQB5U5LANCNFSM4IGU4ZTA>; > .; >. -- ; Sara E. Boles, MS; PhD Candidate | Whitehead Lab; Pharmacology and Toxicology Graduate Group; Department of Environmental Toxicology; Univer",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201:613,Security,validat,validateMappings,613,"Hello Avi,. Here is my out put log. Thank you in advance for an help you can provide. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. Best,. Sara. On Mon, Jul 29, 2019 at 3:25 PM Avi Srivastava <notifications@github.com>; wrote:. > You passed paired-end files; > to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; > You must pass the same number of files to both flags; >; > Is this true ? Can you share the log ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAH7HQIR4ZVWMTE2KXLQB5U5LA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CF3JY#issuecomment-516185511>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEHDXAG7WI3B7QBMJOSXTATQB5U5LANCNFSM4IGU4ZTA>; > .; >. -- ; Sara E. Boles, MS; PhD Candidate | Whitehead Lab; Pharmacology and Toxicology Graduate Group; Department of Environmental Toxicology; Univer",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201:31,Testability,log,log,31,"Hello Avi,. Here is my out put log. Thank you in advance for an help you can provide. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. Best,. Sara. On Mon, Jul 29, 2019 at 3:25 PM Avi Srivastava <notifications@github.com>; wrote:. > You passed paired-end files; > to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; > You must pass the same number of files to both flags; >; > Is this true ? Can you share the log ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAH7HQIR4ZVWMTE2KXLQB5U5LA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CF3JY#issuecomment-516185511>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEHDXAG7WI3B7QBMJOSXTATQB5U5LANCNFSM4IGU4ZTA>; > .; >. -- ; Sara E. Boles, MS; PhD Candidate | Whitehead Lab; Pharmacology and Toxicology Graduate Group; Department of Environmental Toxicology; Univer",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201:1377,Testability,log,log,1377,"ragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. Best,. Sara. On Mon, Jul 29, 2019 at 3:25 PM Avi Srivastava <notifications@github.com>; wrote:. > You passed paired-end files; > to salmon, but you passed 12 files to --mates1 and 13 files to --mates2.; > You must pass the same number of files to both flags; >; > Is this true ? Can you share the log ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAH7HQIR4ZVWMTE2KXLQB5U5LA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CF3JY#issuecomment-516185511>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEHDXAG7WI3B7QBMJOSXTATQB5U5LANCNFSM4IGU4ZTA>; > .; >. -- ; Sara E. Boles, MS; PhD Candidate | Whitehead Lab; Pharmacology and Toxicology Graduate Group; Department of Environmental Toxicology; University of California, Davis, CA 95616; http://whiteheadresearch.wordpress.com/; https://sites.google.com/a/ucdavis.edu/sara-e-boles/",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516194201
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516195181:166,Safety,detect,detecting,166,Oh Sorry about that what I meant was the salmon.log file or the the meta-info.json file created by salmon in the output directory. You can check what files salmon is detecting it seems there are 12 files in the mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file in that directory and their regex is same as you are using ? Can you also try putting the names of the file instead `*` as regex ?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516195181
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516195181:48,Testability,log,log,48,Oh Sorry about that what I meant was the salmon.log file or the the meta-info.json file created by salmon in the output directory. You can check what files salmon is detecting it seems there are 12 files in the mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file in that directory and their regex is same as you are using ? Can you also try putting the names of the file instead `*` as regex ?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516195181
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620:1006,Availability,error,error,1006,"g from one of my PE libraries. There are only 12; libraries for each in the directory, which is why I got confused when it; said 13. I will try putting in all of the file names and let you know how; it goes. Thank you for all of your help. Sara. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; wrote:. > Oh Sorry about that what I meant was the salmon.log file or the the; > meta-info.json file created by salmon in the output directory. You can; > check what files salmon is detecting it seems there are 12 files in the; > mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; > in that directory and their regex is same as you are using ? Can you also; > try putting the names of the file instead * as regex ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DHKAZKVCZY5N7ULQB5ZXXA5CNFSM4IG",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620:1527,Safety,detect,detecting,1527,"ly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; wrote:. > Oh Sorry about that what I meant was the salmon.log file or the the; > meta-info.json file created by salmon in the output directory. You can; > check what files salmon is detecting it seems there are 12 files in the; > mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; > in that directory and their regex is same as you are using ? Can you also; > try putting the names of the file instead * as regex ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DHKAZKVCZY5N7ULQB5ZXXA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CIG3I#issuecomment-516195181>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEHDXAHE56TJTIQFQDFDGMDQB5ZXXANCNFSM4IGU4ZTA>; > .; >. -- ; Sara E. Boles, MS; PhD Candidate | Whitehead Lab; Pharmacology and Toxicology Graduate Group; Department of Environmental Toxicology; University of California, Davis, CA 95616; http://whiteheadresearch.wordpress.com/; https://sites.google.com/a/ucdavis.edu/sara-e-boles/",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620:465,Security,validat,validateMappings,465,"Hi Avi,. Here is the salmon log from one of my PE libraries. There are only 12; libraries for each in the directory, which is why I got confused when it; said 13. I will try putting in all of the file names and let you know how; it goes. Thank you for all of your help. Sara. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; wrote:. > Oh Sorry about that what I meant was the salmon.log file or the the; > meta-info.json file created by salmon in the output directory. You can; > check what files salmon is detecting it seems there are 12 files in the; > mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; > in that directory and their regex is same as you are using ? Can you also; > try putting the names of the file instead * as regex ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DH",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620:629,Security,validat,validateMappings,629,"Hi Avi,. Here is the salmon log from one of my PE libraries. There are only 12; libraries for each in the directory, which is why I got confused when it; said 13. I will try putting in all of the file names and let you know how; it goes. Thank you for all of your help. Sara. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; wrote:. > Oh Sorry about that what I meant was the salmon.log file or the the; > meta-info.json file created by salmon in the output directory. You can; > check what files salmon is detecting it seems there are 12 files in the; > mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; > in that directory and their regex is same as you are using ? Can you also; > try putting the names of the file instead * as regex ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DH",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620:803,Security,validat,validateMappings,803,"Hi Avi,. Here is the salmon log from one of my PE libraries. There are only 12; libraries for each in the directory, which is why I got confused when it; said 13. I will try putting in all of the file names and let you know how; it goes. Thank you for all of your help. Sara. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; wrote:. > Oh Sorry about that what I meant was the salmon.log file or the the; > meta-info.json file created by salmon in the output directory. You can; > check what files salmon is detecting it seems there are 12 files in the; > mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; > in that directory and their regex is same as you are using ? Can you also; > try putting the names of the file instead * as regex ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DH",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620:28,Testability,log,log,28,"Hi Avi,. Here is the salmon log from one of my PE libraries. There are only 12; libraries for each in the directory, which is why I got confused when it; said 13. I will try putting in all of the file names and let you know how; it goes. Thank you for all of your help. Sara. [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; wrote:. > Oh Sorry about that what I meant was the salmon.log file or the the; > meta-info.json file created by salmon in the output directory. You can; > check what files salmon is detecting it seems there are 12 files in the; > mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; > in that directory and their regex is same as you are using ? Can you also; > try putting the names of the file instead * as regex ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DH",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620:1403,Testability,log,log,1403,"29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies use of minScoreFraction. Since not explicitly specified, it is; being set to 0.65; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; without --hardFilter implies use of range factorization.; rangeFactorizationBins is being set to 4; [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; any complete read libraries. Please make sure you provided arguments; properly to -1, -2 (for paired-end libraries) or -r (for single-end; libraries), and that the library format option (-l) *comes before* the read; libraries. On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; wrote:. > Oh Sorry about that what I meant was the salmon.log file or the the; > meta-info.json file created by salmon in the output directory. You can; > check what files salmon is detecting it seems there are 12 files in the; > mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; > in that directory and their regex is same as you are using ? Can you also; > try putting the names of the file instead * as regex ?; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DHKAZKVCZY5N7ULQB5ZXXA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CIG3I#issuecomment-516195181>,; > or mute the thread; > <https://github.com/notifications/unsubscribe-auth/AEHDXAHE56TJTIQFQDFDGMDQB5ZXXANCNFSM4IGU4ZTA>; > .; >. -- ; Sara E. Boles, MS; PhD Candidate | Whitehead Lab; Pharmacology and Toxicology Graduate Group; Department of Environmental Toxicology; University of Californ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516514620
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:54,Availability,error,error,54,"Hello Again,. I just ran the command and got the same error message as before. Here is my; command:. ```#!/bin/bash -l; #SBATCH -J male_salmon_map; #SBATCH -t 150:00:00; #SBATCH -p high; #SBATCH --cpus-per-task=24; source ~/.bashrc; source activate salmon; cd /home/seboles/abaloneraw/salmon_quantification/SALMON_MALE/; for i in *.qc.fq.gz; do; salmon quant -i maleredabalone_index --libType IU -1; mgonad-2_S121_L006_R1_001.qc.fq.gz; lightreceptor-1_S114_L006_R1_001.qc.fq.gz; mgonad-1_S120_L006_R1_001.qc.fq.gz; lightreceptor-2_S115_L006_R1_001.qc.fq.gz; mgonad-2_S121_L005_R1_001.qc.fq.gz mgonad-1_S120_L005_R1_001.qc.fq.gz; lightreceptor-2_S115_L005_R1_001.qc.fq.gz; lightreceptor-1_S114_L005_R1_001.qc.fq.gz; mgonad-2_S121_L004_R1_001.qc.fq.gz mgonad-1_S120_L004_R1_001.qc.fq.gz; lightreceptor-2_S115_L004_R1_001.qc.fq.gz; lightreceptor-1_S114_L004_R1_001.qc.fq.gz -2; mgonad-2_S121_L006_R2_001.qc.fq.gz; lightreceptor-1_S114_L006_R2_001.qc.fq.gz; mgonad-1_S120_L006_R2_001.qc.fq.gz; lightreceptor-2_S115_L006_R2_001.qc.fq.gz; mgonad-2_S121_L005_R2_001.qc.fq.gz mgonad-1_S120_L005_R2_001.qc.fq.gz; lightreceptor-2_S115_L005_R2_001.qc.fq.gz; lightreceptor-1_S114_L005_R2_001.qc.fq.gz; mgonad-2_S121_L004_R2_001.qc.fq.gz mgonad-1_S120_L004_R2_001.qc.fq.gz; lightreceptor-2_S115_L004_R2_001.qc.fq.gz; lightreceptor-1_S114_L004_R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias; --gcBias --validateMappings; done```. And here is my output from salmon.log. [2019-07-30 10:40:14.624] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-30 10:40:14.624] [jointLog] [error] You passed paired-end files to; salmon, but you passed 12 files to --mates1 and 13 files to --mates2. You; must pass the same number of files to both flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:1625,Availability,error,error,1625,"; lightreceptor-1_S114_L005_R1_001.qc.fq.gz; mgonad-2_S121_L004_R1_001.qc.fq.gz mgonad-1_S120_L004_R1_001.qc.fq.gz; lightreceptor-2_S115_L004_R1_001.qc.fq.gz; lightreceptor-1_S114_L004_R1_001.qc.fq.gz -2; mgonad-2_S121_L006_R2_001.qc.fq.gz; lightreceptor-1_S114_L006_R2_001.qc.fq.gz; mgonad-1_S120_L006_R2_001.qc.fq.gz; lightreceptor-2_S115_L006_R2_001.qc.fq.gz; mgonad-2_S121_L005_R2_001.qc.fq.gz mgonad-1_S120_L005_R2_001.qc.fq.gz; lightreceptor-2_S115_L005_R2_001.qc.fq.gz; lightreceptor-1_S114_L005_R2_001.qc.fq.gz; mgonad-2_S121_L004_R2_001.qc.fq.gz mgonad-1_S120_L004_R2_001.qc.fq.gz; lightreceptor-2_S115_L004_R2_001.qc.fq.gz; lightreceptor-1_S114_L004_R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias; --gcBias --validateMappings; done```. And here is my output from salmon.log. [2019-07-30 10:40:14.624] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-30 10:40:14.624] [jointLog] [error] You passed paired-end files to; salmon, but you passed 12 files to --mates1 and 13 files to --mates2. You; must pass the same number of files to both flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are only 12; > libraries for each in the directory, which is why I got confused when it; > said 13. I will try putting in all of the file names and let you know how; > it goes. Thank you for all of your help.; >; > Sara; >; > [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; > below threshold. Incompatible fragments will be ignored.; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies use of minScoreFraction. Since not explicitly specified, it is; > being set to 0.65; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; > without --hardFilter implies use of range factorization.; > ran",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:2976,Availability,error,error,2976," only 12; > libraries for each in the directory, which is why I got confused when it; > said 13. I will try putting in all of the file names and let you know how; > it goes. Thank you for all of your help.; >; > Sara; >; > [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; > below threshold. Incompatible fragments will be ignored.; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies use of minScoreFraction. Since not explicitly specified, it is; > being set to 0.65; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; > without --hardFilter implies use of range factorization.; > rangeFactorizationBins is being set to 4; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; > [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; > [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; > any complete read libraries. Please make sure you provided arguments; > properly to -1, -2 (for paired-end libraries) or -r (for single-end; > libraries), and that the library format option (-l) *comes before* the read; > libraries.; >; > On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; > wrote:; >; >> Oh Sorry about that what I meant was the salmon.log file or the the; >> meta-info.json file created by salmon in the output directory. You can; >> check what files salmon is detecting it seems there are 12 files in the; >> mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; >> in that directory and their regex is same as you are using ? Can you also; >> try putting the names of the file instead * as regex ?; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DH",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:60,Integrability,message,message,60,"Hello Again,. I just ran the command and got the same error message as before. Here is my; command:. ```#!/bin/bash -l; #SBATCH -J male_salmon_map; #SBATCH -t 150:00:00; #SBATCH -p high; #SBATCH --cpus-per-task=24; source ~/.bashrc; source activate salmon; cd /home/seboles/abaloneraw/salmon_quantification/SALMON_MALE/; for i in *.qc.fq.gz; do; salmon quant -i maleredabalone_index --libType IU -1; mgonad-2_S121_L006_R1_001.qc.fq.gz; lightreceptor-1_S114_L006_R1_001.qc.fq.gz; mgonad-1_S120_L006_R1_001.qc.fq.gz; lightreceptor-2_S115_L006_R1_001.qc.fq.gz; mgonad-2_S121_L005_R1_001.qc.fq.gz mgonad-1_S120_L005_R1_001.qc.fq.gz; lightreceptor-2_S115_L005_R1_001.qc.fq.gz; lightreceptor-1_S114_L005_R1_001.qc.fq.gz; mgonad-2_S121_L004_R1_001.qc.fq.gz mgonad-1_S120_L004_R1_001.qc.fq.gz; lightreceptor-2_S115_L004_R1_001.qc.fq.gz; lightreceptor-1_S114_L004_R1_001.qc.fq.gz -2; mgonad-2_S121_L006_R2_001.qc.fq.gz; lightreceptor-1_S114_L006_R2_001.qc.fq.gz; mgonad-1_S120_L006_R2_001.qc.fq.gz; lightreceptor-2_S115_L006_R2_001.qc.fq.gz; mgonad-2_S121_L005_R2_001.qc.fq.gz mgonad-1_S120_L005_R2_001.qc.fq.gz; lightreceptor-2_S115_L005_R2_001.qc.fq.gz; lightreceptor-1_S114_L005_R2_001.qc.fq.gz; mgonad-2_S121_L004_R2_001.qc.fq.gz mgonad-1_S120_L004_R2_001.qc.fq.gz; lightreceptor-2_S115_L004_R2_001.qc.fq.gz; lightreceptor-1_S114_L004_R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias; --gcBias --validateMappings; done```. And here is my output from salmon.log. [2019-07-30 10:40:14.624] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-30 10:40:14.624] [jointLog] [error] You passed paired-end files to; salmon, but you passed 12 files to --mates1 and 13 files to --mates2. You; must pass the same number of files to both flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:3519,Safety,detect,detecting,3519,"7-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; > without --hardFilter implies use of range factorization.; > rangeFactorizationBins is being set to 4; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; > [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; > [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; > any complete read libraries. Please make sure you provided arguments; > properly to -1, -2 (for paired-end libraries) or -r (for single-end; > libraries), and that the library format option (-l) *comes before* the read; > libraries.; >; > On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; > wrote:; >; >> Oh Sorry about that what I meant was the salmon.log file or the the; >> meta-info.json file created by salmon in the output directory. You can; >> check what files salmon is detecting it seems there are 12 files in the; >> mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; >> in that directory and their regex is same as you are using ? Can you also; >> try putting the names of the file instead * as regex ?; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DHKAZKVCZY5N7ULQB5ZXXA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CIG3I#issuecomment-516195181>,; >> or mute the thread; >> <https://github.com/notifications/unsubscribe-auth/AEHDXAHE56TJTIQFQDFDGMDQB5ZXXANCNFSM4IGU4ZTA>; >> .; >>; >; >; > --; > Sara E. Boles, MS; > PhD Candidate | Whitehead Lab; > Pharmacology and Toxicology Graduate Group; > Department of Environmental Toxicology; > University of California, Davis, CA 95616; > http://whiteheadresearch.wordpress.com/; > https://sites.google.com/a/ucdavi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:1387,Security,validat,validateMappings,1387,"ndex --libType IU -1; mgonad-2_S121_L006_R1_001.qc.fq.gz; lightreceptor-1_S114_L006_R1_001.qc.fq.gz; mgonad-1_S120_L006_R1_001.qc.fq.gz; lightreceptor-2_S115_L006_R1_001.qc.fq.gz; mgonad-2_S121_L005_R1_001.qc.fq.gz mgonad-1_S120_L005_R1_001.qc.fq.gz; lightreceptor-2_S115_L005_R1_001.qc.fq.gz; lightreceptor-1_S114_L005_R1_001.qc.fq.gz; mgonad-2_S121_L004_R1_001.qc.fq.gz mgonad-1_S120_L004_R1_001.qc.fq.gz; lightreceptor-2_S115_L004_R1_001.qc.fq.gz; lightreceptor-1_S114_L004_R1_001.qc.fq.gz -2; mgonad-2_S121_L006_R2_001.qc.fq.gz; lightreceptor-1_S114_L006_R2_001.qc.fq.gz; mgonad-1_S120_L006_R2_001.qc.fq.gz; lightreceptor-2_S115_L006_R2_001.qc.fq.gz; mgonad-2_S121_L005_R2_001.qc.fq.gz mgonad-1_S120_L005_R2_001.qc.fq.gz; lightreceptor-2_S115_L005_R2_001.qc.fq.gz; lightreceptor-1_S114_L005_R2_001.qc.fq.gz; mgonad-2_S121_L004_R2_001.qc.fq.gz mgonad-1_S120_L004_R2_001.qc.fq.gz; lightreceptor-2_S115_L004_R2_001.qc.fq.gz; lightreceptor-1_S114_L004_R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias; --gcBias --validateMappings; done```. And here is my output from salmon.log. [2019-07-30 10:40:14.624] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-30 10:40:14.624] [jointLog] [error] You passed paired-end files to; salmon, but you passed 12 files to --mates1 and 13 files to --mates2. You; must pass the same number of files to both flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are only 12; > libraries for each in the directory, which is why I got confused when it; > said 13. I will try putting in all of the file names and let you know how; > it goes. Thank you for all of your help.; >; > Sara; >; > [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; > below threshold. Incompatible fragments will be ignored.; > [2019-07-29 15:5",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:2417,Security,validat,validateMappings,2417,"my output from salmon.log. [2019-07-30 10:40:14.624] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-30 10:40:14.624] [jointLog] [error] You passed paired-end files to; salmon, but you passed 12 files to --mates1 and 13 files to --mates2. You; must pass the same number of files to both flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are only 12; > libraries for each in the directory, which is why I got confused when it; > said 13. I will try putting in all of the file names and let you know how; > it goes. Thank you for all of your help.; >; > Sara; >; > [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; > below threshold. Incompatible fragments will be ignored.; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies use of minScoreFraction. Since not explicitly specified, it is; > being set to 0.65; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; > without --hardFilter implies use of range factorization.; > rangeFactorizationBins is being set to 4; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; > [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; > [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; > any complete read libraries. Please make sure you provided arguments; > properly to -1, -2 (for paired-end libraries) or -r (for single-end; > libraries), and that the library format option (-l) *comes before* the read; > libraries.; >; > On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; > wrote:; >; >> Oh Sorry about that what I meant was the salmon.log file or the the; >> meta-info",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:2587,Security,validat,validateMappings,2587,"624] [jointLog] [error] You passed paired-end files to; salmon, but you passed 12 files to --mates1 and 13 files to --mates2. You; must pass the same number of files to both flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are only 12; > libraries for each in the directory, which is why I got confused when it; > said 13. I will try putting in all of the file names and let you know how; > it goes. Thank you for all of your help.; >; > Sara; >; > [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; > below threshold. Incompatible fragments will be ignored.; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies use of minScoreFraction. Since not explicitly specified, it is; > being set to 0.65; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; > without --hardFilter implies use of range factorization.; > rangeFactorizationBins is being set to 4; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; > [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; > [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; > any complete read libraries. Please make sure you provided arguments; > properly to -1, -2 (for paired-end libraries) or -r (for single-end; > libraries), and that the library format option (-l) *comes before* the read; > libraries.; >; > On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; > wrote:; >; >> Oh Sorry about that what I meant was the salmon.log file or the the; >> meta-info.json file created by salmon in the output directory. You can; >> check what files salmon is detecting it seems there are 12 files in the; >> mate1 and 13 files in the mate2. Can you ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:2767,Security,validat,validateMappings,2767,"th flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are only 12; > libraries for each in the directory, which is why I got confused when it; > said 13. I will try putting in all of the file names and let you know how; > it goes. Thank you for all of your help.; >; > Sara; >; > [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; > below threshold. Incompatible fragments will be ignored.; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies use of minScoreFraction. Since not explicitly specified, it is; > being set to 0.65; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; > without --hardFilter implies use of range factorization.; > rangeFactorizationBins is being set to 4; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; > [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; > [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; > any complete read libraries. Please make sure you provided arguments; > properly to -1, -2 (for paired-end libraries) or -r (for single-end; > libraries), and that the library format option (-l) *comes before* the read; > libraries.; >; > On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; > wrote:; >; >> Oh Sorry about that what I meant was the salmon.log file or the the; >> meta-info.json file created by salmon in the output directory. You can; >> check what files salmon is detecting it seems there are 12 files in the; >> mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; >> in that directory and their regex is same as you are using ? Can you also; >> try putting the names of the file instead * as regex ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:1448,Testability,log,log,1448,"1_S114_L006_R1_001.qc.fq.gz; mgonad-1_S120_L006_R1_001.qc.fq.gz; lightreceptor-2_S115_L006_R1_001.qc.fq.gz; mgonad-2_S121_L005_R1_001.qc.fq.gz mgonad-1_S120_L005_R1_001.qc.fq.gz; lightreceptor-2_S115_L005_R1_001.qc.fq.gz; lightreceptor-1_S114_L005_R1_001.qc.fq.gz; mgonad-2_S121_L004_R1_001.qc.fq.gz mgonad-1_S120_L004_R1_001.qc.fq.gz; lightreceptor-2_S115_L004_R1_001.qc.fq.gz; lightreceptor-1_S114_L004_R1_001.qc.fq.gz -2; mgonad-2_S121_L006_R2_001.qc.fq.gz; lightreceptor-1_S114_L006_R2_001.qc.fq.gz; mgonad-1_S120_L006_R2_001.qc.fq.gz; lightreceptor-2_S115_L006_R2_001.qc.fq.gz; mgonad-2_S121_L005_R2_001.qc.fq.gz mgonad-1_S120_L005_R2_001.qc.fq.gz; lightreceptor-2_S115_L005_R2_001.qc.fq.gz; lightreceptor-1_S114_L005_R2_001.qc.fq.gz; mgonad-2_S121_L004_R2_001.qc.fq.gz mgonad-1_S120_L004_R2_001.qc.fq.gz; lightreceptor-2_S115_L004_R2_001.qc.fq.gz; lightreceptor-1_S114_L004_R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias; --gcBias --validateMappings; done```. And here is my output from salmon.log. [2019-07-30 10:40:14.624] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-30 10:40:14.624] [jointLog] [error] You passed paired-end files to; salmon, but you passed 12 files to --mates1 and 13 files to --mates2. You; must pass the same number of files to both flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are only 12; > libraries for each in the directory, which is why I got confused when it; > said 13. I will try putting in all of the file names and let you know how; > it goes. Thank you for all of your help.; >; > Sara; >; > [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; > below threshold. Incompatible fragments will be ignored.; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies use o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:1959,Testability,log,log,1959,"nad-1_S120_L006_R2_001.qc.fq.gz; lightreceptor-2_S115_L006_R2_001.qc.fq.gz; mgonad-2_S121_L005_R2_001.qc.fq.gz mgonad-1_S120_L005_R2_001.qc.fq.gz; lightreceptor-2_S115_L005_R2_001.qc.fq.gz; lightreceptor-1_S114_L005_R2_001.qc.fq.gz; mgonad-2_S121_L004_R2_001.qc.fq.gz mgonad-1_S120_L004_R2_001.qc.fq.gz; lightreceptor-2_S115_L004_R2_001.qc.fq.gz; lightreceptor-1_S114_L004_R2_001.qc.fq.gz ${i} -o ${i}_quant --seqBias; --gcBias --validateMappings; done```. And here is my output from salmon.log. [2019-07-30 10:40:14.624] [jointLog] [info] Fragment incompatibility prior; below threshold. Incompatible fragments will be ignored.; [2019-07-30 10:40:14.624] [jointLog] [error] You passed paired-end files to; salmon, but you passed 12 files to --mates1 and 13 files to --mates2. You; must pass the same number of files to both flags. Thank you in advance for any tips you may have for me. Sara. On Tue, Jul 30, 2019 at 10:30 AM Sara Boles <seboles@ucdavis.edu> wrote:. > Hi Avi,; >; > Here is the salmon log from one of my PE libraries. There are only 12; > libraries for each in the directory, which is why I got confused when it; > said 13. I will try putting in all of the file names and let you know how; > it goes. Thank you for all of your help.; >; > Sara; >; > [2019-07-29 15:58:39.034] [jointLog] [info] Fragment incompatibility prior; > below threshold. Incompatible fragments will be ignored.; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies use of minScoreFraction. Since not explicitly specified, it is; > being set to 0.65; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; > without --hardFilter implies use of range factorization.; > rangeFactorizationBins is being set to 4; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; > [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; > [2019-07-29 15:58:3",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791:3393,Testability,log,log,3393,"e of --validateMappings; > implies use of minScoreFraction. Since not explicitly specified, it is; > being set to 0.65; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings,; > without --hardFilter implies use of range factorization.; > rangeFactorizationBins is being set to 4; > [2019-07-29 15:58:39.034] [jointLog] [info] Usage of --validateMappings; > implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; > [2019-07-29 15:58:39.034] [jointLog] [info] parsing read library format; > [2019-07-29 15:58:39.034] [jointLog] [error] Failed to successfully parse; > any complete read libraries. Please make sure you provided arguments; > properly to -1, -2 (for paired-end libraries) or -r (for single-end; > libraries), and that the library format option (-l) *comes before* the read; > libraries.; >; > On Mon, Jul 29, 2019 at 4:06 PM Avi Srivastava <notifications@github.com>; > wrote:; >; >> Oh Sorry about that what I meant was the salmon.log file or the the; >> meta-info.json file created by salmon in the output directory. You can; >> check what files salmon is detecting it seems there are 12 files in the; >> mate1 and 13 files in the mate2. Can you confirm there are 13 pairs of file; >> in that directory and their regex is same as you are using ? Can you also; >> try putting the names of the file instead * as regex ?; >>; >> —; >> You are receiving this because you were mentioned.; >> Reply to this email directly, view it on GitHub; >> <https://github.com/COMBINE-lab/salmon/issues/408?email_source=notifications&email_token=AEHDXAA5DHKAZKVCZY5N7ULQB5ZXXA5CNFSM4IGU4ZTKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOD3CIG3I#issuecomment-516195181>,; >> or mute the thread; >> <https://github.com/notifications/unsubscribe-auth/AEHDXAHE56TJTIQFQDFDGMDQB5ZXXANCNFSM4IGU4ZTA>; >> .; >>; >; >; > --; > Sara E. Boles, MS; > PhD Candidate | Whitehead Lab; > Pharmacology and Toxicology Graduate Group; > Department of Environmental Toxic",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/408#issuecomment-516521791
https://github.com/COMBINE-lab/salmon/issues/412#issuecomment-525503233:42,Deployability,Patch,Patching,42,Or it should honour `$MAKEFLAGS` somehow. Patching now in brew: https://github.com/brewsci/homebrew-bio/pull/747. `CMakelist.txt =~ s/-d0 -j2/\$MAKEFLAGS/`,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/412#issuecomment-525503233
https://github.com/COMBINE-lab/salmon/issues/412#issuecomment-549180097:79,Deployability,configurat,configuration,79,"Hi @tseemann, you can now use `-DBOOST_BUILD_THREADS=<desired threads>` during configuration to control this. Let me know if it works.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/412#issuecomment-549180097
https://github.com/COMBINE-lab/salmon/issues/412#issuecomment-549180097:79,Modifiability,config,configuration,79,"Hi @tseemann, you can now use `-DBOOST_BUILD_THREADS=<desired threads>` during configuration to control this. Let me know if it works.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/412#issuecomment-549180097
https://github.com/COMBINE-lab/salmon/issues/413#issuecomment-518443482:1946,Availability,avail,available,1946,"	CorrectedReads	MappedReads	DeduplicatedReads	MappingRate	DedupRate	MeanByMax	NumGenesExpressed	NumGenesOverMean	mRnaFraction	rRnaFraction	ArborescenceCount; ```; Unfortunately it seems the expected value from the last (12th) `ArborescenceCount` is getting dumped at 10th position in that case the header should be (but not currently is):; ```; CB	CorrectedReads	MappedReads	DeduplicatedReads	MappingRate	DedupRate	MeanByMax	NumGenesExpressed	NumGenesOverMean	ArborescenceCount	mRnaFraction	rRnaFraction; ```. 2.) if `--dumpUmiGraph` is provided (I am assuming you are) then instead we were experimenting with another feature which follows the convention `<Number of Reads in an Arborescence>:<Frequency of these Arborescences>` i.e. if in a cell X we observe 3 Arborescence each with 2,3,4 number of reads respectively in them then the entry of `ArborescenceCount` should look like `2:1	3:1	4:1`. Since just like the problem with 1) above there seems to be switch of the column 12 to 10 and you are observing weird looking file. . In short: Everything is fine till column 9th but there is a bug which makes column 12 to 10, 10 to 11 and 11 to 12. If you provide `--dumpUmiGraph` then last two columns represents `mRnaFraction	rRnaFraction` while everything in between column 9th and before last two represents `ArborescenceCount` with special frequency dump as defined above. My apologies for the trouble I just pushed a fix into the develop branch. However, we have significant other changes in develop branch too so recompilation might make your current index incompatible to `0.14.1` so if you don't wan't to reindex the reference then I'd not recompile from develop and the bugfix will available from the next version. Solution: The easiest solution is to remove `--dumpUMIgraph` if you don't need the flag and rerun then the only thing to handle is just change the header names. If rerunning alevin is too much work then I can forward you a python parser which can temporary handle the parsing.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/413#issuecomment-518443482
https://github.com/COMBINE-lab/salmon/issues/413#issuecomment-548614992:32,Deployability,release,release,32,"It's now fixed in latest stable release, closing the issue but feel free to reopen if you have any issues.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/413#issuecomment-548614992
https://github.com/COMBINE-lab/salmon/issues/414#issuecomment-521744430:134,Availability,error,error,134,"Hi @martynakgajos ,. It's very hard to say without looking at the data. If you can share a minimal data on which we can replicate the error, that'd be great.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/414#issuecomment-521744430
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521747003:332,Integrability,protocol,protocol,332,"Hi @dritoshi ,. Thanks for your request. I'd be happy to add the support for Quartz-seq2 into alevin but it'd be great if you can answer a few questions for us. Is it possible to share some reads/fastq file on which we can test alevin ? Also, please excuse my ignorance, what type of PCR amplification is performed in `Quartz-seq2` protocol, is it CelSeq type IVT (linear) amplification or Drop-Seq type template switching PCR amplification ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521747003
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521747003:305,Performance,perform,performed,305,"Hi @dritoshi ,. Thanks for your request. I'd be happy to add the support for Quartz-seq2 into alevin but it'd be great if you can answer a few questions for us. Is it possible to share some reads/fastq file on which we can test alevin ? Also, please excuse my ignorance, what type of PCR amplification is performed in `Quartz-seq2` protocol, is it CelSeq type IVT (linear) amplification or Drop-Seq type template switching PCR amplification ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521747003
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521747003:223,Testability,test,test,223,"Hi @dritoshi ,. Thanks for your request. I'd be happy to add the support for Quartz-seq2 into alevin but it'd be great if you can answer a few questions for us. Is it possible to share some reads/fastq file on which we can test alevin ? Also, please excuse my ignorance, what type of PCR amplification is performed in `Quartz-seq2` protocol, is it CelSeq type IVT (linear) amplification or Drop-Seq type template switching PCR amplification ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521747003
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521863053:187,Deployability,update,updated,187,Thanks @dritoshi for the data and info. Let me play a bit with the data over the weekend. It should be very straightforward to add but I might have to check some unit test. I'll keep you updated.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521863053
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521863053:167,Testability,test,test,167,Thanks @dritoshi for the data and info. Let me play a bit with the data over the weekend. It should be very straightforward to add but I might have to check some unit test. I'll keep you updated.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-521863053
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541:209,Availability,avail,available,209,"Hi @dritoshi ,. As per your request I've added the support for Quartz-seq2 data in the alevin framework with https://github.com/COMBINE-lab/salmon/commit/f6905b1d1dc6cf6bc4597927ad3be637ba615c9a and should be available with next salmon release. Currently the develop branch has to be compile from source to use the following command line argument.; ![image](https://user-images.githubusercontent.com/8772521/63282768-8df73600-c27d-11e9-832d-f4a1232f17f6.png). Currently I just have on flag i.e. quartzseq2 which assumes 15 length CB and 8 length UMI. Unfortunately adding multiple versioned is gonna be little complicated as I might have to discuss with the alevin team and that might take some time. As you can check through the new code through the commit (linked above) adding just the Rule of new protocol is not enough and we might have to add some helper code with each new protocol which increases the redundancy in the code. Currently we are in the process of figuring out a better way to handle new protocols.; Having said that it should not stop users from using alevin with previous version of quartzseq2, you can use the following command line triplet as `--end 5 --barcodeLength 14 --umiLength 8` along with you other alevin flag and it's gonna behave just like `QuartzSeq2v31` you specified above. If possible, It'd be great if you can share some of the results you get while comparing Quartz-seq2 pipeline with alevin. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541:236,Deployability,release,release,236,"Hi @dritoshi ,. As per your request I've added the support for Quartz-seq2 data in the alevin framework with https://github.com/COMBINE-lab/salmon/commit/f6905b1d1dc6cf6bc4597927ad3be637ba615c9a and should be available with next salmon release. Currently the develop branch has to be compile from source to use the following command line argument.; ![image](https://user-images.githubusercontent.com/8772521/63282768-8df73600-c27d-11e9-832d-f4a1232f17f6.png). Currently I just have on flag i.e. quartzseq2 which assumes 15 length CB and 8 length UMI. Unfortunately adding multiple versioned is gonna be little complicated as I might have to discuss with the alevin team and that might take some time. As you can check through the new code through the commit (linked above) adding just the Rule of new protocol is not enough and we might have to add some helper code with each new protocol which increases the redundancy in the code. Currently we are in the process of figuring out a better way to handle new protocols.; Having said that it should not stop users from using alevin with previous version of quartzseq2, you can use the following command line triplet as `--end 5 --barcodeLength 14 --umiLength 8` along with you other alevin flag and it's gonna behave just like `QuartzSeq2v31` you specified above. If possible, It'd be great if you can share some of the results you get while comparing Quartz-seq2 pipeline with alevin. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541:1412,Deployability,pipeline,pipeline,1412,"Hi @dritoshi ,. As per your request I've added the support for Quartz-seq2 data in the alevin framework with https://github.com/COMBINE-lab/salmon/commit/f6905b1d1dc6cf6bc4597927ad3be637ba615c9a and should be available with next salmon release. Currently the develop branch has to be compile from source to use the following command line argument.; ![image](https://user-images.githubusercontent.com/8772521/63282768-8df73600-c27d-11e9-832d-f4a1232f17f6.png). Currently I just have on flag i.e. quartzseq2 which assumes 15 length CB and 8 length UMI. Unfortunately adding multiple versioned is gonna be little complicated as I might have to discuss with the alevin team and that might take some time. As you can check through the new code through the commit (linked above) adding just the Rule of new protocol is not enough and we might have to add some helper code with each new protocol which increases the redundancy in the code. Currently we are in the process of figuring out a better way to handle new protocols.; Having said that it should not stop users from using alevin with previous version of quartzseq2, you can use the following command line triplet as `--end 5 --barcodeLength 14 --umiLength 8` along with you other alevin flag and it's gonna behave just like `QuartzSeq2v31` you specified above. If possible, It'd be great if you can share some of the results you get while comparing Quartz-seq2 pipeline with alevin. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541:801,Integrability,protocol,protocol,801,"Hi @dritoshi ,. As per your request I've added the support for Quartz-seq2 data in the alevin framework with https://github.com/COMBINE-lab/salmon/commit/f6905b1d1dc6cf6bc4597927ad3be637ba615c9a and should be available with next salmon release. Currently the develop branch has to be compile from source to use the following command line argument.; ![image](https://user-images.githubusercontent.com/8772521/63282768-8df73600-c27d-11e9-832d-f4a1232f17f6.png). Currently I just have on flag i.e. quartzseq2 which assumes 15 length CB and 8 length UMI. Unfortunately adding multiple versioned is gonna be little complicated as I might have to discuss with the alevin team and that might take some time. As you can check through the new code through the commit (linked above) adding just the Rule of new protocol is not enough and we might have to add some helper code with each new protocol which increases the redundancy in the code. Currently we are in the process of figuring out a better way to handle new protocols.; Having said that it should not stop users from using alevin with previous version of quartzseq2, you can use the following command line triplet as `--end 5 --barcodeLength 14 --umiLength 8` along with you other alevin flag and it's gonna behave just like `QuartzSeq2v31` you specified above. If possible, It'd be great if you can share some of the results you get while comparing Quartz-seq2 pipeline with alevin. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541:880,Integrability,protocol,protocol,880,"Hi @dritoshi ,. As per your request I've added the support for Quartz-seq2 data in the alevin framework with https://github.com/COMBINE-lab/salmon/commit/f6905b1d1dc6cf6bc4597927ad3be637ba615c9a and should be available with next salmon release. Currently the develop branch has to be compile from source to use the following command line argument.; ![image](https://user-images.githubusercontent.com/8772521/63282768-8df73600-c27d-11e9-832d-f4a1232f17f6.png). Currently I just have on flag i.e. quartzseq2 which assumes 15 length CB and 8 length UMI. Unfortunately adding multiple versioned is gonna be little complicated as I might have to discuss with the alevin team and that might take some time. As you can check through the new code through the commit (linked above) adding just the Rule of new protocol is not enough and we might have to add some helper code with each new protocol which increases the redundancy in the code. Currently we are in the process of figuring out a better way to handle new protocols.; Having said that it should not stop users from using alevin with previous version of quartzseq2, you can use the following command line triplet as `--end 5 --barcodeLength 14 --umiLength 8` along with you other alevin flag and it's gonna behave just like `QuartzSeq2v31` you specified above. If possible, It'd be great if you can share some of the results you get while comparing Quartz-seq2 pipeline with alevin. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541:1008,Integrability,protocol,protocols,1008,"Hi @dritoshi ,. As per your request I've added the support for Quartz-seq2 data in the alevin framework with https://github.com/COMBINE-lab/salmon/commit/f6905b1d1dc6cf6bc4597927ad3be637ba615c9a and should be available with next salmon release. Currently the develop branch has to be compile from source to use the following command line argument.; ![image](https://user-images.githubusercontent.com/8772521/63282768-8df73600-c27d-11e9-832d-f4a1232f17f6.png). Currently I just have on flag i.e. quartzseq2 which assumes 15 length CB and 8 length UMI. Unfortunately adding multiple versioned is gonna be little complicated as I might have to discuss with the alevin team and that might take some time. As you can check through the new code through the commit (linked above) adding just the Rule of new protocol is not enough and we might have to add some helper code with each new protocol which increases the redundancy in the code. Currently we are in the process of figuring out a better way to handle new protocols.; Having said that it should not stop users from using alevin with previous version of quartzseq2, you can use the following command line triplet as `--end 5 --barcodeLength 14 --umiLength 8` along with you other alevin flag and it's gonna behave just like `QuartzSeq2v31` you specified above. If possible, It'd be great if you can share some of the results you get while comparing Quartz-seq2 pipeline with alevin. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541:909,Safety,redund,redundancy,909,"Hi @dritoshi ,. As per your request I've added the support for Quartz-seq2 data in the alevin framework with https://github.com/COMBINE-lab/salmon/commit/f6905b1d1dc6cf6bc4597927ad3be637ba615c9a and should be available with next salmon release. Currently the develop branch has to be compile from source to use the following command line argument.; ![image](https://user-images.githubusercontent.com/8772521/63282768-8df73600-c27d-11e9-832d-f4a1232f17f6.png). Currently I just have on flag i.e. quartzseq2 which assumes 15 length CB and 8 length UMI. Unfortunately adding multiple versioned is gonna be little complicated as I might have to discuss with the alevin team and that might take some time. As you can check through the new code through the commit (linked above) adding just the Rule of new protocol is not enough and we might have to add some helper code with each new protocol which increases the redundancy in the code. Currently we are in the process of figuring out a better way to handle new protocols.; Having said that it should not stop users from using alevin with previous version of quartzseq2, you can use the following command line triplet as `--end 5 --barcodeLength 14 --umiLength 8` along with you other alevin flag and it's gonna behave just like `QuartzSeq2v31` you specified above. If possible, It'd be great if you can share some of the results you get while comparing Quartz-seq2 pipeline with alevin. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522658541
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522866012:114,Deployability,pipeline,pipeline,114,"Thank you so much! We are going to use the new version. Of course, we will share you some results of comparing Q2 pipeline with Alevin. We usually used the drop-seq pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522866012
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522866012:165,Deployability,pipeline,pipeline,165,"Thank you so much! We are going to use the new version. Of course, we will share you some results of comparing Q2 pipeline with Alevin. We usually used the drop-seq pipeline.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-522866012
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214:186,Availability,avail,available,186,"Sounds good, just wanted to give you the heads up, as we are working on some other part of the salmon pipeline, currently I can't give you an ETA when would the new version of salmon be available. If For the time being the choice are either you can compile the develop branch of salmon or I can forward you a linux usable salmon binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214:102,Deployability,pipeline,pipeline,102,"Sounds good, just wanted to give you the heads up, as we are working on some other part of the salmon pipeline, currently I can't give you an ETA when would the new version of salmon be available. If For the time being the choice are either you can compile the develop branch of salmon or I can forward you a linux usable salmon binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214:315,Usability,usab,usable,315,"Sounds good, just wanted to give you the heads up, as we are working on some other part of the salmon pipeline, currently I can't give you an ETA when would the new version of salmon be available. If For the time being the choice are either you can compile the develop branch of salmon or I can forward you a linux usable salmon binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-523090214
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-548614903:9,Availability,avail,available,9,"It's now available in latest stable release, closing the issue but feel free to reopen if you have any issues.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-548614903
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-548614903:36,Deployability,release,release,36,"It's now available in latest stable release, closing the issue but feel free to reopen if you have any issues.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-548614903
https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-550180608:59,Deployability,release,release,59,I'm sorry my late response. I will check the latest stable release. I will contact you if we find any problems. Thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/416#issuecomment-550180608
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522261180:22,Availability,error,error,22,"It may be a bad alloc error. Do you have a machine with a bit more ram to try it on? Also, you could try installing through bioconda to see if it may be an issue with the precompiled binary (e.g. librar compatibility).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522261180
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522261180:105,Deployability,install,installing,105,"It may be a bad alloc error. Do you have a machine with a bit more ram to try it on? Also, you could try installing through bioconda to see if it may be an issue with the precompiled binary (e.g. librar compatibility).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522261180
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:37,Availability,fault,fault,37,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:103,Availability,avail,avail,103,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:351,Availability,avail,available,351,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:600,Availability,error,errors,600,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:806,Availability,Error,Error,806,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:1000,Availability,Error,Error,1000,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:1236,Availability,Error,Error,1236,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:177,Deployability,install,installed,177,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:683,Deployability,install,install,683,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:748,Deployability,install,install,748,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:1565,Deployability,install,install,1565,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:782,Modifiability,Config,Configuring,782,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638:880,Testability,test,tests,880,"I increased ram to 24G. Segmentation fault happens even faster. I have fiddled with swap memory, to no avail, but I am not a good swap fiddler.; libs with malloc in their name, installed in directory /salmon-latest_linux_x86_64/ib adjacent to /salmon-latest_linux_x86_64/bin, are the same as elsewhere already on my system. conda and bioconda are not available for FreeBSD. What OS would work?; I have looked through the published papers and find no mention of which OS should work. My attempted command for compiling the sources from unzipped directory salmon-0.14.1 is: cmake -S src -B build; Many errors result, starting with:; TBB_LIBRARIES = ; Setting libdivsufsort = /external/install/lib/libdivsufsort.a; Setting libdivsufsort64 = /external/install/lib/libdivsufsort64.a; -- Configuring done; CMake Error at CMakeLists.txt:196 (add_executable):; Cannot find source file:. /tests/UnitTests.cpp. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. CMake Error at CMakeLists.txt:196 (add_executable):; Target ""unitTests"" links to target ""Threads::Threads"" but the target was; not found. Perhaps a find_package() call is missing for an IMPORTED; target, or an ALIAS target is missing?. CMake Error at CMakeLists.txt:162 (add_library):; Cannot find source file:. /src/jellyfish/mer_dna.cc. Tried extensions .c .C .c++ .cc .cpp .cxx .cu .m .M .mm .h .hh .h++ .hm; .hpp .hxx .in .txx. Apparently the so-called sources do not include many files ending in .cpp, for instance. Please, I repeat, what linux OS should be able to install salmon? ; And/Or what command could compile salmon?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522626638
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522628674:375,Availability,down,down,375,"We do no have access to BSD-based systems (apart from the extent to which OSX can be said to be BSD-based) on which to test during development. Bioconda works on many linux distributions; though I do not have a comprehensive list. For example, we regularly run on Ubuntu, CentOS, RedHat and Debian. If you have the facilities to use Docker on this machine, you can also pull down a docker image of the latest release from https://hub.docker.com/r/combinelab/salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522628674
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522628674:409,Deployability,release,release,409,"We do no have access to BSD-based systems (apart from the extent to which OSX can be said to be BSD-based) on which to test during development. Bioconda works on many linux distributions; though I do not have a comprehensive list. For example, we regularly run on Ubuntu, CentOS, RedHat and Debian. If you have the facilities to use Docker on this machine, you can also pull down a docker image of the latest release from https://hub.docker.com/r/combinelab/salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522628674
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522628674:14,Security,access,access,14,"We do no have access to BSD-based systems (apart from the extent to which OSX can be said to be BSD-based) on which to test during development. Bioconda works on many linux distributions; though I do not have a comprehensive list. For example, we regularly run on Ubuntu, CentOS, RedHat and Debian. If you have the facilities to use Docker on this machine, you can also pull down a docker image of the latest release from https://hub.docker.com/r/combinelab/salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522628674
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522628674:119,Testability,test,test,119,"We do no have access to BSD-based systems (apart from the extent to which OSX can be said to be BSD-based) on which to test during development. Bioconda works on many linux distributions; though I do not have a comprehensive list. For example, we regularly run on Ubuntu, CentOS, RedHat and Debian. If you have the facilities to use Docker on this machine, you can also pull down a docker image of the latest release from https://hub.docker.com/r/combinelab/salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-522628674
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524416706:213,Availability,Fault,Fault,213,"It seems FreeBSD prefers to use Docker under an emulator -- I did not try that.; I installed a ubuntu on VirtualBox. There I was able to run salmon index and salmon quant in a conda environment. Same Segmentation Fault crash.; I also get the same crashes if I use only the first 54K reads of the 23.5M in the files. So I do not ; think it is an issue of computer memory size.; Maybe virtual Ubuntu is not the same as hardware Ubuntu. Can you please try the fastq files that I tried, and see if they don't crash for you on Ubuntu 19.04?; If they do not crash for you, I will build a hardware Ubuntu box so I can use your salmon suite.; As you probably know, you can find these mouse brain RNA-seq data at; https://www.ncbi.nlm.nih.gov/sra/?term=SRR1818187. I just noticed that you have posted decoys, thank you.; I indexed one, with your recommended command, and I got many many many warnings; (I could not figure out how to count them) such as; -------------------------------------------------------------; [2019-08-21 13:19:31.122] [jointLog] [warning] Entry with header [ENSMUST00000103739.3], had length less than the k-mer length of 31 (perhaps after poly-A clipping); [2019-08-21 13:19:31.178] [jointLog] [warning] Entry with header [ENSMUST00000200713.1], had length less than the k-mer length of 31 (perhaps after poly-A clipping); [2019-08-21 13:19:31.201] [jointLog] [warning] Entry with header [ENSMUST00000191703.1], had length less than the k-mer length of 31 (perhaps after poly-A clipping); [2019-08-21 13:19:31.344] [jointLog] [warning] Entry with header [ENSMUST00000192089.1], had length less than the k-mer length of 31 (perhaps after poly-A clipping); -------------------------------------- ; Is that expected for the gentome.fa ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524416706
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524416706:83,Deployability,install,installed,83,"It seems FreeBSD prefers to use Docker under an emulator -- I did not try that.; I installed a ubuntu on VirtualBox. There I was able to run salmon index and salmon quant in a conda environment. Same Segmentation Fault crash.; I also get the same crashes if I use only the first 54K reads of the 23.5M in the files. So I do not ; think it is an issue of computer memory size.; Maybe virtual Ubuntu is not the same as hardware Ubuntu. Can you please try the fastq files that I tried, and see if they don't crash for you on Ubuntu 19.04?; If they do not crash for you, I will build a hardware Ubuntu box so I can use your salmon suite.; As you probably know, you can find these mouse brain RNA-seq data at; https://www.ncbi.nlm.nih.gov/sra/?term=SRR1818187. I just noticed that you have posted decoys, thank you.; I indexed one, with your recommended command, and I got many many many warnings; (I could not figure out how to count them) such as; -------------------------------------------------------------; [2019-08-21 13:19:31.122] [jointLog] [warning] Entry with header [ENSMUST00000103739.3], had length less than the k-mer length of 31 (perhaps after poly-A clipping); [2019-08-21 13:19:31.178] [jointLog] [warning] Entry with header [ENSMUST00000200713.1], had length less than the k-mer length of 31 (perhaps after poly-A clipping); [2019-08-21 13:19:31.201] [jointLog] [warning] Entry with header [ENSMUST00000191703.1], had length less than the k-mer length of 31 (perhaps after poly-A clipping); [2019-08-21 13:19:31.344] [jointLog] [warning] Entry with header [ENSMUST00000192089.1], had length less than the k-mer length of 31 (perhaps after poly-A clipping); -------------------------------------- ; Is that expected for the gentome.fa ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524416706
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:1956,Availability,Error,Error,1956," ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting.; (salmon) wayne@Ubuntu19:~/rnaseq$ ls -R *.json; ls: cannot access '*.json': No such file or directory. Try 2.; Instead of referring to my directory decoys/ , I moved to the directory decoys/ ; and ran salmon index again, using your command exactly:; salmon index -t gentrome.fa -d decoys.txt -i combined_index. This time a few .json files were produced in the directory combined_index/ [your name this time]; [contents of decoys= combined_index gentrome.fa mus_musculus.tar.gz Salmontranscripts_quant; decoys.txt links.txt salmonQuantDecoy22.sh]. then [sh salmonQuantDecoy22.sh]; salmon quant -p 3 -i combined_index -l A -1 ../SRR1818187_2.fastq.gz -2 ../SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant. Now no Segmentation Fault crash. ; The program finishes with; [2019-08",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:2931,Availability,Fault,Fault,2931,"Fraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting.; (salmon) wayne@Ubuntu19:~/rnaseq$ ls -R *.json; ls: cannot access '*.json': No such file or directory. Try 2.; Instead of referring to my directory decoys/ , I moved to the directory decoys/ ; and ran salmon index again, using your command exactly:; salmon index -t gentrome.fa -d decoys.txt -i combined_index. This time a few .json files were produced in the directory combined_index/ [your name this time]; [contents of decoys= combined_index gentrome.fa mus_musculus.tar.gz Salmontranscripts_quant; decoys.txt links.txt salmonQuantDecoy22.sh]. then [sh salmonQuantDecoy22.sh]; salmon quant -p 3 -i combined_index -l A -1 ../SRR1818187_2.fastq.gz -2 ../SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant. Now no Segmentation Fault crash. ; The program finishes with; [2019-08-25 12:37:39.056] [jointLog] [info] Finished optimizer; [2019-08-25 12:37:39.056] [jointLog] [info] writing output . Now I am going to look for the mRNA counts. I think a major secret is to have mus_musculus.tar.gz in the same directory.; If my description is accurate [I did not repeat everything] you should have -mRNA [path to transcriptome.gz] on your command line, or instruct users to have the transcriptome.gz in the same directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:102,Deployability,install,install,102,"Okay, now I am on a physical Ubuntu 19.04. I am leaving out most travails. [Don't allow ubuntu gui to install conda.]. Salmon (in the conda evironment) is going differently ! Skip to Try 2. below for success; Try 1.; Index seemed to go the same as before, using the command [from a script]; salmon index -t decoys/gentrome.fa -d decoys/decoys.txt -i salmonIndexDecoyMouse; but then command; salmon quant -p 3 -i salmonIndexDecoyMouse -l A -1 SRR1818187_2.fastq.gz -2 SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant; Fails as follows, saying it cannot find a .json file(s); ---------------------------------------------------------------------; (salmon) wayne@Ubuntu19:~/rnaseq$ sh salmonQuantDecoy.sh ; Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ threads ] => { 3 }; ### [ index ] => { salmonIndexDecoyMouse }; ### [ libType ] => { A }; ### [ mates1 ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDeco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:3026,Performance,optimiz,optimizer,3026,"Fraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting.; (salmon) wayne@Ubuntu19:~/rnaseq$ ls -R *.json; ls: cannot access '*.json': No such file or directory. Try 2.; Instead of referring to my directory decoys/ , I moved to the directory decoys/ ; and ran salmon index again, using your command exactly:; salmon index -t gentrome.fa -d decoys.txt -i combined_index. This time a few .json files were produced in the directory combined_index/ [your name this time]; [contents of decoys= combined_index gentrome.fa mus_musculus.tar.gz Salmontranscripts_quant; decoys.txt links.txt salmonQuantDecoy22.sh]. then [sh salmonQuantDecoy22.sh]; salmon quant -p 3 -i combined_index -l A -1 ../SRR1818187_2.fastq.gz -2 ../SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant. Now no Segmentation Fault crash. ; The program finishes with; [2019-08-25 12:37:39.056] [jointLog] [info] Finished optimizer; [2019-08-25 12:37:39.056] [jointLog] [info] writing output . Now I am going to look for the mRNA counts. I think a major secret is to have mus_musculus.tar.gz in the same directory.; If my description is accurate [I did not repeat everything] you should have -mRNA [path to transcriptome.gz] on your command line, or instruct users to have the transcriptome.gz in the same directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:491,Security,validat,validateMappings,491,"Okay, now I am on a physical Ubuntu 19.04. I am leaving out most travails. [Don't allow ubuntu gui to install conda.]. Salmon (in the conda evironment) is going differently ! Skip to Try 2. below for success; Try 1.; Index seemed to go the same as before, using the command [from a script]; salmon index -t decoys/gentrome.fa -d decoys/decoys.txt -i salmonIndexDecoyMouse; but then command; salmon quant -p 3 -i salmonIndexDecoyMouse -l A -1 SRR1818187_2.fastq.gz -2 SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant; Fails as follows, saying it cannot find a .json file(s); ---------------------------------------------------------------------; (salmon) wayne@Ubuntu19:~/rnaseq$ sh salmonQuantDecoy.sh ; Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ threads ] => { 3 }; ### [ index ] => { salmonIndexDecoyMouse }; ### [ libType ] => { A }; ### [ mates1 ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDeco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:1064,Security,validat,validateMappings,1064,"e conda evironment) is going differently ! Skip to Try 2. below for success; Try 1.; Index seemed to go the same as before, using the command [from a script]; salmon index -t decoys/gentrome.fa -d decoys/decoys.txt -i salmonIndexDecoyMouse; but then command; salmon quant -p 3 -i salmonIndexDecoyMouse -l A -1 SRR1818187_2.fastq.gz -2 SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant; Fails as follows, saying it cannot find a .json file(s); ---------------------------------------------------------------------; (salmon) wayne@Ubuntu19:~/rnaseq$ sh salmonQuantDecoy.sh ; Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ threads ] => { 3 }; ### [ index ] => { salmonIndexDecoyMouse }; ### [ libType ] => { A }; ### [ mates1 ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:1380,Security,validat,validateMappings,1380,"nd; salmon quant -p 3 -i salmonIndexDecoyMouse -l A -1 SRR1818187_2.fastq.gz -2 SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant; Fails as follows, saying it cannot find a .json file(s); ---------------------------------------------------------------------; (salmon) wayne@Ubuntu19:~/rnaseq$ sh salmonQuantDecoy.sh ; Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ threads ] => { 3 }; ### [ index ] => { salmonIndexDecoyMouse }; ### [ libType ] => { A }; ### [ mates1 ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting.; (salmon) wayne@Ubuntu19:~/rnaseq$ ls -R *.json; ls: cannot access '*.json': No such file or directory. Try 2.; Instead of referring to my directory decoys/ , I moved to the directory decoys/ ; and ran ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:1542,Security,validat,validateMappings,1542,"t cannot find a .json file(s); ---------------------------------------------------------------------; (salmon) wayne@Ubuntu19:~/rnaseq$ sh salmonQuantDecoy.sh ; Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ threads ] => { 3 }; ### [ index ] => { salmonIndexDecoyMouse }; ### [ libType ] => { A }; ### [ mates1 ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting.; (salmon) wayne@Ubuntu19:~/rnaseq$ ls -R *.json; ls: cannot access '*.json': No such file or directory. Try 2.; Instead of referring to my directory decoys/ , I moved to the directory decoys/ ; and ran salmon index again, using your command exactly:; salmon index -t gentrome.fa -d decoys.txt -i combined_index. This time a few .json files were produced in the directory combi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:1714,Security,validat,validateMappings,1714,"rsion Info: This is the most recent version of salmon.; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ threads ] => { 3 }; ### [ index ] => { salmonIndexDecoyMouse }; ### [ libType ] => { A }; ### [ mates1 ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting.; (salmon) wayne@Ubuntu19:~/rnaseq$ ls -R *.json; ls: cannot access '*.json': No such file or directory. Try 2.; Instead of referring to my directory decoys/ , I moved to the directory decoys/ ; and ran salmon index again, using your command exactly:; salmon index -t gentrome.fa -d decoys.txt -i combined_index. This time a few .json files were produced in the directory combined_index/ [your name this time]; [contents of decoys= combined_index gentrome.fa mus_musculus.tar.gz Salmontranscripts_quant; decoys.txt links.txt salmonQuantDeco",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:2246,Security,access,access,2246,"nt incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting.; (salmon) wayne@Ubuntu19:~/rnaseq$ ls -R *.json; ls: cannot access '*.json': No such file or directory. Try 2.; Instead of referring to my directory decoys/ , I moved to the directory decoys/ ; and ran salmon index again, using your command exactly:; salmon index -t gentrome.fa -d decoys.txt -i combined_index. This time a few .json files were produced in the directory combined_index/ [your name this time]; [contents of decoys= combined_index gentrome.fa mus_musculus.tar.gz Salmontranscripts_quant; decoys.txt links.txt salmonQuantDecoy22.sh]. then [sh salmonQuantDecoy22.sh]; salmon quant -p 3 -i combined_index -l A -1 ../SRR1818187_2.fastq.gz -2 ../SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant. Now no Segmentation Fault crash. ; The program finishes with; [2019-08-25 12:37:39.056] [jointLog] [info] Finished optimizer; [2019-08-25 12:37:39.056] [jointLog] [info] writing output . Now I am going to look for the mRNA counts. I think a major secret is to have mus_musculus.tar.gz in the same directory.; If my description is ac",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:2866,Security,validat,validateMappings,2866,"Fraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting.; (salmon) wayne@Ubuntu19:~/rnaseq$ ls -R *.json; ls: cannot access '*.json': No such file or directory. Try 2.; Instead of referring to my directory decoys/ , I moved to the directory decoys/ ; and ran salmon index again, using your command exactly:; salmon index -t gentrome.fa -d decoys.txt -i combined_index. This time a few .json files were produced in the directory combined_index/ [your name this time]; [contents of decoys= combined_index gentrome.fa mus_musculus.tar.gz Salmontranscripts_quant; decoys.txt links.txt salmonQuantDecoy22.sh]. then [sh salmonQuantDecoy22.sh]; salmon quant -p 3 -i combined_index -l A -1 ../SRR1818187_2.fastq.gz -2 ../SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant. Now no Segmentation Fault crash. ; The program finishes with; [2019-08-25 12:37:39.056] [jointLog] [info] Finished optimizer; [2019-08-25 12:37:39.056] [jointLog] [info] writing output . Now I am going to look for the mRNA counts. I think a major secret is to have mus_musculus.tar.gz in the same directory.; If my description is accurate [I did not repeat everything] you should have -mRNA [path to transcriptome.gz] on your command line, or instruct users to have the transcriptome.gz in the same directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:1138,Testability,Log,Logs,1138,"e conda evironment) is going differently ! Skip to Try 2. below for success; Try 1.; Index seemed to go the same as before, using the command [from a script]; salmon index -t decoys/gentrome.fa -d decoys/decoys.txt -i salmonIndexDecoyMouse; but then command; salmon quant -p 3 -i salmonIndexDecoyMouse -l A -1 SRR1818187_2.fastq.gz -2 SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant; Fails as follows, saying it cannot find a .json file(s); ---------------------------------------------------------------------; (salmon) wayne@Ubuntu19:~/rnaseq$ sh salmonQuantDecoy.sh ; Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ threads ] => { 3 }; ### [ index ] => { salmonIndexDecoyMouse }; ### [ libType ] => { A }; ### [ mates1 ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435:1186,Testability,log,logs,1186,"e conda evironment) is going differently ! Skip to Try 2. below for success; Try 1.; Index seemed to go the same as before, using the command [from a script]; salmon index -t decoys/gentrome.fa -d decoys/decoys.txt -i salmonIndexDecoyMouse; but then command; salmon quant -p 3 -i salmonIndexDecoyMouse -l A -1 SRR1818187_2.fastq.gz -2 SRR1818187_1.fastq.gz --validateMappings -o Salmontranscripts_quant; Fails as follows, saying it cannot find a .json file(s); ---------------------------------------------------------------------; (salmon) wayne@Ubuntu19:~/rnaseq$ sh salmonQuantDecoy.sh ; Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v0.14.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ threads ] => { 3 }; ### [ index ] => { salmonIndexDecoyMouse }; ### [ libType ] => { A }; ### [ mates1 ] => { SRR1818187_2.fastq.gz }; ### [ mates2 ] => { SRR1818187_1.fastq.gz }; ### [ validateMappings ] => { }; ### [ output ] => { Salmontranscripts_quant }; Logs will be written to Salmontranscripts_quant/logs; [2019-08-25 11:40:44.518] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings, without --hardFilter implies use of range factorization. rangeFactorizationBins is being set to 4; [2019-08-25 11:40:44.518] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.2.; [2019-08-25 11:40:44.518] [jointLog] [info] parsing read library format; [2019-08-25 11:40:44.518] [jointLog] [info] There is 1 library.; Exception : [Error: The index version file salmonIndexDecoyMouse/versionInfo.json doesn't seem to exist. Please try re-building the salmon index.]; salmon quant was invoked improperly.; For ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-524651435
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:132,Deployability,install,install,132,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:167,Deployability,install,installer,167,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:183,Deployability,install,install-,183,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:244,Deployability,install,installer,244,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:409,Deployability,install,install,409,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:473,Deployability,install,install,473,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:988,Deployability,install,installed,988,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:1018,Deployability,install,install,1018,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:64,Modifiability,config,configure,64,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:274,Modifiability,config,config,274,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:315,Modifiability,config,config,315,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:667,Performance,perform,performance,667,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051:576,Usability,simpl,simply,576,"FYI, miniconda works fine on FreeBSD. It's not too difficult to configure manually, but to make it even easier:. As root:. ```; pkg install auto-admin linux-miniconda-installer; auto-install-linux_base; ```; As a non-root user:. ```; miniconda-installer; conda-shell; conda config --add channels conda-forge; conda config --add channels bioconda; conda create -n salmon salmon; ```; Note: Just running `conda install salmon` instead of `conda create -n salmon salmon` will install a very old version rather than the latest. This utilizes the Linux compatibility module, which simply adds Linux system calls to the FreeBSD kernel. Unlike a virtual machine, there's no performance penalty and memory overhead is trivial. In fact, Linux binaries sometimes run slightly faster on FreeBSD than they do on Linux. Average speed is about the same. I'd only use conda as a stop-gap, though. There's a large and growing selection of bioinformatics software in FreeBSD ports that can be more easily installed and used, e.g. 'pkg install samtools bwa'. Also I'm working on a native FreeBSD port for salmon:. https://github.com/COMBINE-lab/salmon/issues/162. Best,. Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-917648051
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989319618:198,Availability,fault,fault,198,"FreeBSD port of 1.6.0 is building now:; [https://github.com/outpaddling/freebsd-ports-wip/tree/master/salmon](https://github.com/outpaddling/freebsd-ports-wip/tree/master/salmon); I'm getting a seg fault trying to run it, though. ; https://github.com/COMBINE-lab/salmon/issues/725",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989319618
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989865038:213,Deployability,install,installed,213,The Arabidopsis example in the getting started guide ([https://combine-lab.github.io/salmon/getting_started/](https://combine-lab.github.io/salmon/getting_started/)) seems to work fine on FreeBSD 13.0 with salmon installed via miniconda per instructions above.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989865038
https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989865038:47,Usability,guid,guide,47,The Arabidopsis example in the getting started guide ([https://combine-lab.github.io/salmon/getting_started/](https://combine-lab.github.io/salmon/getting_started/)) seems to work fine on FreeBSD 13.0 with salmon installed via miniconda per instructions above.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/419#issuecomment-989865038
https://github.com/COMBINE-lab/salmon/issues/420#issuecomment-524756680:34,Availability,fault,fault,34,"Hi,; I found the point, it was my fault... I've replaced salmon by a script calling salmon after changing the working directory, so salmon can both be in the PATH and find its libraries, forgetting that relative paths will hence not work.; I guess playing with LD_LIBRARY_PATH would be a better way to achieve that.; Sorry for the disturbance",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/420#issuecomment-524756680
https://github.com/COMBINE-lab/salmon/pull/421#issuecomment-524436646:203,Deployability,update,updated,203,"Hi @jdrnevich ,. Thanks for the PR.; I think I've already merged your previous PR https://github.com/COMBINE-lab/salmon/pull/391 about the same issue. It was merged with the develop branch and should be updated with the new salmon release, which is suppose to happen soon. My apologies for the confusion.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/421#issuecomment-524436646
https://github.com/COMBINE-lab/salmon/pull/421#issuecomment-524436646:231,Deployability,release,release,231,"Hi @jdrnevich ,. Thanks for the PR.; I think I've already merged your previous PR https://github.com/COMBINE-lab/salmon/pull/391 about the same issue. It was merged with the develop branch and should be updated with the new salmon release, which is suppose to happen soon. My apologies for the confusion.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/pull/421#issuecomment-524436646
https://github.com/COMBINE-lab/salmon/issues/422#issuecomment-524643119:508,Integrability,protocol,protocol,508,"Hi @thu1911,. The strand bias here is actually quite moderate. The calibration of `strand_bias` is such that a value of 0.5 means there is no bias (i.e. half of the fragments start with read 1 on the forward strand and half start with read 1 on the reverse complement strand). Your value is `""strand_mapping_bias"": 0.5258466052704426`, so you are pretty close to the ideal value of 0.5, though there is a _slight_ bias in the data. I wouldn't be concerned about this assuming you were assuming an unstranded protocol. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/422#issuecomment-524643119
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-526339339:89,Deployability,install,installed,89,All staden and libstaden aswells as libcurl4-openssl-dev and other libcurl packages were installed.; I fixed it by uninstalling curl and staden library packages and letting the CMAKE deal with compiling the required Staden IOLib itself. Could it have been that some required flags were not passed correctly to the linker?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-526339339
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-612356265:21,Availability,error,error,21,Encountered the same error,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-612356265
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-962540229:47,Deployability,install,installing,47,"Solved with ; `-DFETCH_STADEN=TRUE`. to recap, installing on an Ubuntu 20.04: ; ```; git clone --depth=1 https://github.com/COMBINE-lab/salmon.git; cd salmon; git checkout tags/v1.5.2. apt-get build-dep -y salmon; cmake -DFETCH_BOOST=FALSE --log-level=VERBOSE -DCMAKE_INSTALL_PREFIX=/directory_to_place/salmon/1.5.2 -DFETCH_STADEN=TRUE -DNO_IPO=TRUE && make && make install; ```. Please note you can't mkdir build and cd build as the cmake files are bundled under the git's root dir. You'll need to move the files (but i'm not familiar with cmake so i just run it from the git root). Compilation is required as the distributed binaries use the older libc (GLIBC_2.29)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-962540229
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-962540229:366,Deployability,install,install,366,"Solved with ; `-DFETCH_STADEN=TRUE`. to recap, installing on an Ubuntu 20.04: ; ```; git clone --depth=1 https://github.com/COMBINE-lab/salmon.git; cd salmon; git checkout tags/v1.5.2. apt-get build-dep -y salmon; cmake -DFETCH_BOOST=FALSE --log-level=VERBOSE -DCMAKE_INSTALL_PREFIX=/directory_to_place/salmon/1.5.2 -DFETCH_STADEN=TRUE -DNO_IPO=TRUE && make && make install; ```. Please note you can't mkdir build and cd build as the cmake files are bundled under the git's root dir. You'll need to move the files (but i'm not familiar with cmake so i just run it from the git root). Compilation is required as the distributed binaries use the older libc (GLIBC_2.29)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-962540229
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-962540229:242,Testability,log,log-level,242,"Solved with ; `-DFETCH_STADEN=TRUE`. to recap, installing on an Ubuntu 20.04: ; ```; git clone --depth=1 https://github.com/COMBINE-lab/salmon.git; cd salmon; git checkout tags/v1.5.2. apt-get build-dep -y salmon; cmake -DFETCH_BOOST=FALSE --log-level=VERBOSE -DCMAKE_INSTALL_PREFIX=/directory_to_place/salmon/1.5.2 -DFETCH_STADEN=TRUE -DNO_IPO=TRUE && make && make install; ```. Please note you can't mkdir build and cd build as the cmake files are bundled under the git's root dir. You'll need to move the files (but i'm not familiar with cmake so i just run it from the git root). Compilation is required as the distributed binaries use the older libc (GLIBC_2.29)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-962540229
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:306,Availability,error,errors,306,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:545,Availability,error,errors,545,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:429,Deployability,install,installed,429,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:632,Deployability,install,installed,632,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:781,Deployability,install,install,781,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:1069,Deployability,install,install,1069,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:1097,Deployability,install,installation,1097,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:1278,Deployability,install,installation,1278,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922:1110,Integrability,message,message,1110,"thank you @alpapan, your post in this open issue had the information that helped me build the latest version of salmon (1.10.0 at this time) on Ubuntu 20 and 22. The documentation at https://salmon.readthedocs.io/en/latest/building.html#requirements-for-building-from-source was not helping with the build errors reported here, which is what I encountered too. . In my case the problem was that I had a custom build of libstaden installed (that I did not want to remove) that cmake was picking up, but which triggered those many libcurl linking errors (misleadingly I would say, since it seems to be related to the way libstaden is installed, not directly libcurl related, which is fine on my system). Here it is the build recipe that worked for me on Ubuntu 20/22:; ```; sudo apt install libboost-iostreams-dev libboost-chrono-dev libboost-filesystem-dev \; libboost-timer-dev libboost-program-options-dev ; PREFIX=$HOME/sw # or wherever you want it; mkdir build && cd build; cmake -DNO_IPO=TRUE -DFETCH_STADEN=TRUE -DCMAKE_INSTALL_PREFIX=${PREFIX} ..; make -j6; make install; ```; Note that the installation message states:; `Please add $PREFIX/lib to your LD_LIBRARY_PATH` ; .. but that does not seem to be needed, the linker seems to resolve those libraries properly in the installation directory.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/425#issuecomment-1445139922
https://github.com/COMBINE-lab/salmon/issues/428#issuecomment-530430735:142,Availability,down,downstream,142,"Hi @annajbott ,. Thanks for your question.; It's an expected behavior. The idea is to dump some low confidence CB as well for certain kind of downstream processing. You'd see a file `whitelist.txt` as well in the output alevin folder which should contain whitelisted CB names (4340 in your case). You might have to filter those matrix out after loading the full matrix to get cells only passes the whitelisting filter. Please checkout [tximport](https://github.com/mikelove/tximport) to import the matrix in R, it's very efficient to load. In case you need some stats regarding the resource usage check [EDS](https://github.com/COMBINE-lab/EDS).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/428#issuecomment-530430735
https://github.com/COMBINE-lab/salmon/issues/428#issuecomment-530430735:521,Energy Efficiency,efficient,efficient,521,"Hi @annajbott ,. Thanks for your question.; It's an expected behavior. The idea is to dump some low confidence CB as well for certain kind of downstream processing. You'd see a file `whitelist.txt` as well in the output alevin folder which should contain whitelisted CB names (4340 in your case). You might have to filter those matrix out after loading the full matrix to get cells only passes the whitelisting filter. Please checkout [tximport](https://github.com/mikelove/tximport) to import the matrix in R, it's very efficient to load. In case you need some stats regarding the resource usage check [EDS](https://github.com/COMBINE-lab/EDS).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/428#issuecomment-530430735
https://github.com/COMBINE-lab/salmon/issues/428#issuecomment-530430735:345,Performance,load,loading,345,"Hi @annajbott ,. Thanks for your question.; It's an expected behavior. The idea is to dump some low confidence CB as well for certain kind of downstream processing. You'd see a file `whitelist.txt` as well in the output alevin folder which should contain whitelisted CB names (4340 in your case). You might have to filter those matrix out after loading the full matrix to get cells only passes the whitelisting filter. Please checkout [tximport](https://github.com/mikelove/tximport) to import the matrix in R, it's very efficient to load. In case you need some stats regarding the resource usage check [EDS](https://github.com/COMBINE-lab/EDS).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/428#issuecomment-530430735
https://github.com/COMBINE-lab/salmon/issues/428#issuecomment-530430735:534,Performance,load,load,534,"Hi @annajbott ,. Thanks for your question.; It's an expected behavior. The idea is to dump some low confidence CB as well for certain kind of downstream processing. You'd see a file `whitelist.txt` as well in the output alevin folder which should contain whitelisted CB names (4340 in your case). You might have to filter those matrix out after loading the full matrix to get cells only passes the whitelisting filter. Please checkout [tximport](https://github.com/mikelove/tximport) to import the matrix in R, it's very efficient to load. In case you need some stats regarding the resource usage check [EDS](https://github.com/COMBINE-lab/EDS).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/428#issuecomment-530430735
https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-534653897:112,Deployability,release,released,112,"Hi @knokknok ,. Thanks for your question.; Unfortunately, we don't have a command line option to do that in the released version.; However, you can potentially change [this](https://github.com/COMBINE-lab/salmon/blob/master/src/Graph.cpp#L52) from `oneHamming` to `kHamming` with your required distance, and compile salmon from source, alevin can potentially use higher distance UMIs. One thing to note here would be, since we are checking for more distant UMIs the overall running time can increase. Hope it helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-534653897
https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-535015422:104,Testability,test,test,104,"Hi @knokknok,. Nice catch ! K1 bad k2 are just two bit encoding of the UMI sequences, it passed my unit test because I was testing it with 10x data but you are right it should be dynamic based on the umi length. I'll make the changes to reflect that in the develop branch. Thanks !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-535015422
https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-535015422:123,Testability,test,testing,123,"Hi @knokknok,. Nice catch ! K1 bad k2 are just two bit encoding of the UMI sequences, it passed my unit test because I was testing it with 10x data but you are right it should be dynamic based on the umi length. I'll make the changes to reflect that in the develop branch. Thanks !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-535015422
https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-536052532:396,Availability,ping,ping,396,"Hi @knokknok ,. The latest commit on develop branch https://github.com/COMBINE-lab/salmon/commit/94eab5dae04da4ca02b8da8b630e0750bc15e010 , should fix it. However, there are other major changes in develop branch which can change things from last stable release. I'd recommend, if you can, then please wait for some time before we make the official release of `1.0`. If it's urgent, then try grep(ping) for the changes related to `umiLength` from the above commit and making them into your local salmon code. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-536052532
https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-536052532:253,Deployability,release,release,253,"Hi @knokknok ,. The latest commit on develop branch https://github.com/COMBINE-lab/salmon/commit/94eab5dae04da4ca02b8da8b630e0750bc15e010 , should fix it. However, there are other major changes in develop branch which can change things from last stable release. I'd recommend, if you can, then please wait for some time before we make the official release of `1.0`. If it's urgent, then try grep(ping) for the changes related to `umiLength` from the above commit and making them into your local salmon code. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-536052532
https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-536052532:348,Deployability,release,release,348,"Hi @knokknok ,. The latest commit on develop branch https://github.com/COMBINE-lab/salmon/commit/94eab5dae04da4ca02b8da8b630e0750bc15e010 , should fix it. However, there are other major changes in develop branch which can change things from last stable release. I'd recommend, if you can, then please wait for some time before we make the official release of `1.0`. If it's urgent, then try grep(ping) for the changes related to `umiLength` from the above commit and making them into your local salmon code. Hope this helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-536052532
https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-548614854:9,Availability,avail,available,9,"It's now available in latest stable release, closing the issue but feel free to reopen if you have any issues.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-548614854
https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-548614854:36,Deployability,release,release,36,"It's now available in latest stable release, closing the issue but feel free to reopen if you have any issues.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/430#issuecomment-548614854
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537068509:56,Deployability,release,release,56,Already fixed @k3yavi ? Is there any chance of a 0.14.2 release containing the fix?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537068509
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:145,Deployability,release,release,145,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:396,Deployability,pipeline,pipeline,396,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:426,Deployability,release,release,426,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:482,Deployability,release,releases,482,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:566,Deployability,release,release,566,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:599,Deployability,update,updates,599,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:716,Deployability,pipeline,pipeline,716,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:812,Deployability,install,install,812,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:822,Deployability,release,release,822,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:888,Deployability,install,install,888,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:935,Deployability,install,install,935,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:974,Deployability,install,install,974,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:1090,Deployability,release,release,1090,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929:691,Safety,avoid,avoid,691,"HI @pinin4fjords ,. Thanks for reporting the this. There was bug associated with binary format to mtx format conversion we fixed in the upcoming release. The problem was associated with the last index of the matrix which can be off by max 8 indices because we were using `uint_8` for storing the bit vectors. ; My apologies for the trouble, there are two ways to solve this issue:; 1.) Rerun the pipeline with the latest beta release of [0.99](https://github.com/COMBINE-lab/salmon/releases/tag/v0.99.0-beta2), unfortunately this is not in conda yet, but we plan to release it soon with awesome new updates like consuming both genome and transcriptome for read mapping.; 2.) If you wan't to avoid rerunning the full pipeline, try this. Try cloning [this](https://github.com/COMBINE-lab/EDS) repo and do a `cargo install --release` for the code in `src-rs` folder. Note: You might have to install [Rust](https://www.rust-lang.org/tools/install) for this, it's just one liner install. Once compiled the EDS code, you can just do the following to generate the correct mtx file.; ```; ./target/release/eds convert -i <Path to output/alevin/quants_mat.gz> --mtx -c <num_cells> -f <num_genes>; ```. Please let me know if it works out for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537069929
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537071575:203,Deployability,release,release,203,"Thanks @k3yavi - do I need to assume that all of our existing matrices could be corrupted? . We've built Alevin into our production processes, so I'm loath to hack my way to a solution. When is the 0.99 release due?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537071575
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537076262:29,Deployability,release,released,29,A back port of this fix to a released 0.14.2 would be the most awesome solution ;-),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537076262
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512:24,Availability,error,error,24,"Hi @pinin4fjords ,; The error is very subtle, It happens when the total number of genes are exactly a multiple of 8, which in your case it is. It'd be great that before moving forward, if you can quickly verify if 0.99 actually fixes the issue or is it something else. Unfortunately, there is a chance that the `mtx` format output is corrupted for the last 8 genes (in the order they present in the quants_mat_cols.txt), however, the baseline output of alevin `quants_mat.gz` would still be correct and already be in the output folder of your pipeline. The EDS repo I forwarded you just convert the binary output into mtx format correctly. Due to these subtle issues making into production environment, we changed the release cycle to be like a beta to stable type. Currently 0.99 is a beta release, which has been out for some time. If I have to guess, we plan to release 1.0 (the stable release) in approximately 2 weeks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512:543,Deployability,pipeline,pipeline,543,"Hi @pinin4fjords ,; The error is very subtle, It happens when the total number of genes are exactly a multiple of 8, which in your case it is. It'd be great that before moving forward, if you can quickly verify if 0.99 actually fixes the issue or is it something else. Unfortunately, there is a chance that the `mtx` format output is corrupted for the last 8 genes (in the order they present in the quants_mat_cols.txt), however, the baseline output of alevin `quants_mat.gz` would still be correct and already be in the output folder of your pipeline. The EDS repo I forwarded you just convert the binary output into mtx format correctly. Due to these subtle issues making into production environment, we changed the release cycle to be like a beta to stable type. Currently 0.99 is a beta release, which has been out for some time. If I have to guess, we plan to release 1.0 (the stable release) in approximately 2 weeks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512:718,Deployability,release,release,718,"Hi @pinin4fjords ,; The error is very subtle, It happens when the total number of genes are exactly a multiple of 8, which in your case it is. It'd be great that before moving forward, if you can quickly verify if 0.99 actually fixes the issue or is it something else. Unfortunately, there is a chance that the `mtx` format output is corrupted for the last 8 genes (in the order they present in the quants_mat_cols.txt), however, the baseline output of alevin `quants_mat.gz` would still be correct and already be in the output folder of your pipeline. The EDS repo I forwarded you just convert the binary output into mtx format correctly. Due to these subtle issues making into production environment, we changed the release cycle to be like a beta to stable type. Currently 0.99 is a beta release, which has been out for some time. If I have to guess, we plan to release 1.0 (the stable release) in approximately 2 weeks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512:791,Deployability,release,release,791,"Hi @pinin4fjords ,; The error is very subtle, It happens when the total number of genes are exactly a multiple of 8, which in your case it is. It'd be great that before moving forward, if you can quickly verify if 0.99 actually fixes the issue or is it something else. Unfortunately, there is a chance that the `mtx` format output is corrupted for the last 8 genes (in the order they present in the quants_mat_cols.txt), however, the baseline output of alevin `quants_mat.gz` would still be correct and already be in the output folder of your pipeline. The EDS repo I forwarded you just convert the binary output into mtx format correctly. Due to these subtle issues making into production environment, we changed the release cycle to be like a beta to stable type. Currently 0.99 is a beta release, which has been out for some time. If I have to guess, we plan to release 1.0 (the stable release) in approximately 2 weeks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512:865,Deployability,release,release,865,"Hi @pinin4fjords ,; The error is very subtle, It happens when the total number of genes are exactly a multiple of 8, which in your case it is. It'd be great that before moving forward, if you can quickly verify if 0.99 actually fixes the issue or is it something else. Unfortunately, there is a chance that the `mtx` format output is corrupted for the last 8 genes (in the order they present in the quants_mat_cols.txt), however, the baseline output of alevin `quants_mat.gz` would still be correct and already be in the output folder of your pipeline. The EDS repo I forwarded you just convert the binary output into mtx format correctly. Due to these subtle issues making into production environment, we changed the release cycle to be like a beta to stable type. Currently 0.99 is a beta release, which has been out for some time. If I have to guess, we plan to release 1.0 (the stable release) in approximately 2 weeks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512:889,Deployability,release,release,889,"Hi @pinin4fjords ,; The error is very subtle, It happens when the total number of genes are exactly a multiple of 8, which in your case it is. It'd be great that before moving forward, if you can quickly verify if 0.99 actually fixes the issue or is it something else. Unfortunately, there is a chance that the `mtx` format output is corrupted for the last 8 genes (in the order they present in the quants_mat_cols.txt), however, the baseline output of alevin `quants_mat.gz` would still be correct and already be in the output folder of your pipeline. The EDS repo I forwarded you just convert the binary output into mtx format correctly. Due to these subtle issues making into production environment, we changed the release cycle to be like a beta to stable type. Currently 0.99 is a beta release, which has been out for some time. If I have to guess, we plan to release 1.0 (the stable release) in approximately 2 weeks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537077512
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537079202:31,Deployability,release,released,31,> A back port of this fix to a released 0.14.2 would be the most awesome solution ;-). Let me take this to the salmon team and get back to you. My apologies again for the stupid bug.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537079202
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537081810:20,Testability,test,test,20,... but will try to test the 0.99 too,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-537081810
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538345350:115,Availability,avail,available,115,Could you let me know if a minor release is on the cards (or not) @k3yavi ? Would be nice if at least one Bioconda-available release had the fix.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538345350
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538345350:33,Deployability,release,release,33,Could you let me know if a minor release is on the cards (or not) @k3yavi ? Would be nice if at least one Bioconda-available release had the fix.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538345350
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538345350:125,Deployability,release,release,125,Could you let me know if a minor release is on the cards (or not) @k3yavi ? Would be nice if at least one Bioconda-available release had the fix.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538345350
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954:461,Availability,avail,available,461,"Hi @pinin4fjords ,. I am gonna make the hot-fix release for the `0.14.2` now.; We plan to release `v1.0.0` mid October, so with the `v0.14.2` I can make just the bioconda and github release from the master branch to fix the `mtx` issue. Just wanted to give you heads up, I am making 14.2 from master, so the only difference from 14.1 would be the hotfix i.e. there would be no Genome+txome capability as that's planned with 1.0. I'll keep you updated once it's available through bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954:48,Deployability,release,release,48,"Hi @pinin4fjords ,. I am gonna make the hot-fix release for the `0.14.2` now.; We plan to release `v1.0.0` mid October, so with the `v0.14.2` I can make just the bioconda and github release from the master branch to fix the `mtx` issue. Just wanted to give you heads up, I am making 14.2 from master, so the only difference from 14.1 would be the hotfix i.e. there would be no Genome+txome capability as that's planned with 1.0. I'll keep you updated once it's available through bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954:90,Deployability,release,release,90,"Hi @pinin4fjords ,. I am gonna make the hot-fix release for the `0.14.2` now.; We plan to release `v1.0.0` mid October, so with the `v0.14.2` I can make just the bioconda and github release from the master branch to fix the `mtx` issue. Just wanted to give you heads up, I am making 14.2 from master, so the only difference from 14.1 would be the hotfix i.e. there would be no Genome+txome capability as that's planned with 1.0. I'll keep you updated once it's available through bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954:182,Deployability,release,release,182,"Hi @pinin4fjords ,. I am gonna make the hot-fix release for the `0.14.2` now.; We plan to release `v1.0.0` mid October, so with the `v0.14.2` I can make just the bioconda and github release from the master branch to fix the `mtx` issue. Just wanted to give you heads up, I am making 14.2 from master, so the only difference from 14.1 would be the hotfix i.e. there would be no Genome+txome capability as that's planned with 1.0. I'll keep you updated once it's available through bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954:347,Deployability,hotfix,hotfix,347,"Hi @pinin4fjords ,. I am gonna make the hot-fix release for the `0.14.2` now.; We plan to release `v1.0.0` mid October, so with the `v0.14.2` I can make just the bioconda and github release from the master branch to fix the `mtx` issue. Just wanted to give you heads up, I am making 14.2 from master, so the only difference from 14.1 would be the hotfix i.e. there would be no Genome+txome capability as that's planned with 1.0. I'll keep you updated once it's available through bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954:443,Deployability,update,updated,443,"Hi @pinin4fjords ,. I am gonna make the hot-fix release for the `0.14.2` now.; We plan to release `v1.0.0` mid October, so with the `v0.14.2` I can make just the bioconda and github release from the master branch to fix the `mtx` issue. Just wanted to give you heads up, I am making 14.2 from master, so the only difference from 14.1 would be the hotfix i.e. there would be no Genome+txome capability as that's planned with 1.0. I'll keep you updated once it's available through bioconda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538507954
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395:123,Availability,avail,available,123,"Ok, pushed to [bioconda](https://github.com/bioconda/bioconda-recipes/pull/17922/checks?check_run_id=248588035), should be available in a couple of hours. Once it's available I'll make the official release too on the github. It'd be great if you can quickly test the new release for the bug once it's available. Thanks again !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395:165,Availability,avail,available,165,"Ok, pushed to [bioconda](https://github.com/bioconda/bioconda-recipes/pull/17922/checks?check_run_id=248588035), should be available in a couple of hours. Once it's available I'll make the official release too on the github. It'd be great if you can quickly test the new release for the bug once it's available. Thanks again !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395:301,Availability,avail,available,301,"Ok, pushed to [bioconda](https://github.com/bioconda/bioconda-recipes/pull/17922/checks?check_run_id=248588035), should be available in a couple of hours. Once it's available I'll make the official release too on the github. It'd be great if you can quickly test the new release for the bug once it's available. Thanks again !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395:198,Deployability,release,release,198,"Ok, pushed to [bioconda](https://github.com/bioconda/bioconda-recipes/pull/17922/checks?check_run_id=248588035), should be available in a couple of hours. Once it's available I'll make the official release too on the github. It'd be great if you can quickly test the new release for the bug once it's available. Thanks again !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395:271,Deployability,release,release,271,"Ok, pushed to [bioconda](https://github.com/bioconda/bioconda-recipes/pull/17922/checks?check_run_id=248588035), should be available in a couple of hours. Once it's available I'll make the official release too on the github. It'd be great if you can quickly test the new release for the bug once it's available. Thanks again !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395
https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395:258,Testability,test,test,258,"Ok, pushed to [bioconda](https://github.com/bioconda/bioconda-recipes/pull/17922/checks?check_run_id=248588035), should be available in a couple of hours. Once it's available I'll make the official release too on the github. It'd be great if you can quickly test the new release for the bug once it's available. Thanks again !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/431#issuecomment-538581395
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108:1238,Security,Hash,Hash,1238,"ng] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2019-10-04 10:37:22.318] [ff::console] [info] Replaced 1,775,734,603 non-ATCG nucleotides; [2019-10-04 10:37:22.318] [ff::console] [info] Clipped poly-A tails from 422 transcripts; wrote 593292 cleaned references; seqHash 256 : bd425816a78195ed31cf17ce9df99c2bf56bff98f0df5ace1e958b263d805390; seqHash 512 : 845b625de6f8f018796e464f7c49f6596d2b31b28a58771d56ece24b3d9cad98b8189572ff43d6a3eb8ef24b5d3bc5ac0f89845a57e3682498a56a1bc920e7b7; nameHash 256 : 3bd11eac1e6b05e93689676ca056c165e7c26723c4b137fd284bb8b40ef5df62; nameHash 512 : e68449cfd99f5968182735275b00779b8a396e413a3629beef933e51bd18902c821c26e2a5461c687d023ef85168e58d76bacd5fe1f0a3111bfccc34af9c4035; [2019-10-04 10:37:37.931] [console] [info] Filter size not provided; estimating from number of distinct k-mers; [2019-10-04 10:38:33.012] [console] [info] ntHll estimated 2765935300 distinct k-mers, setting filter size to 2^36; Threads = 8; Vertex length = 31; Hash functions = 5; Filter size = 68719476736; Capacity = 2; Files:; test_pufferfish_index/ref_k31_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:68719476736; Pass Filling Filtering; 1 385 3124; 2 1258 2; True junctions count = 5437144; False junctions count = 4410615; Hash table size = 9847759; Candidate marks count = 26276463; --------------------------------------------------------------------------------; Reallocating bifurcations time: 6; True marks count: 20290262; Edges construction time: 5004; --------------------------------------------------------------------------------; Distinct junctions = 5437144. approximateContigTotalLength: 1543877663; counters:; 49076 936 921 40; Exception : [std::bad_alloc]; ./testing/src/novartis-pisces/pisces/redist/salmon/bin/salmon index was invoked improperly.; For usage information, try ./testing/src/novartis-pisces/pisces/redist/salmon/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108:1564,Security,Hash,Hash,1564,"ng] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2019-10-04 10:37:22.318] [ff::console] [info] Replaced 1,775,734,603 non-ATCG nucleotides; [2019-10-04 10:37:22.318] [ff::console] [info] Clipped poly-A tails from 422 transcripts; wrote 593292 cleaned references; seqHash 256 : bd425816a78195ed31cf17ce9df99c2bf56bff98f0df5ace1e958b263d805390; seqHash 512 : 845b625de6f8f018796e464f7c49f6596d2b31b28a58771d56ece24b3d9cad98b8189572ff43d6a3eb8ef24b5d3bc5ac0f89845a57e3682498a56a1bc920e7b7; nameHash 256 : 3bd11eac1e6b05e93689676ca056c165e7c26723c4b137fd284bb8b40ef5df62; nameHash 512 : e68449cfd99f5968182735275b00779b8a396e413a3629beef933e51bd18902c821c26e2a5461c687d023ef85168e58d76bacd5fe1f0a3111bfccc34af9c4035; [2019-10-04 10:37:37.931] [console] [info] Filter size not provided; estimating from number of distinct k-mers; [2019-10-04 10:38:33.012] [console] [info] ntHll estimated 2765935300 distinct k-mers, setting filter size to 2^36; Threads = 8; Vertex length = 31; Hash functions = 5; Filter size = 68719476736; Capacity = 2; Files:; test_pufferfish_index/ref_k31_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:68719476736; Pass Filling Filtering; 1 385 3124; 2 1258 2; True junctions count = 5437144; False junctions count = 4410615; Hash table size = 9847759; Candidate marks count = 26276463; --------------------------------------------------------------------------------; Reallocating bifurcations time: 6; True marks count: 20290262; Edges construction time: 5004; --------------------------------------------------------------------------------; Distinct junctions = 5437144. approximateContigTotalLength: 1543877663; counters:; 49076 936 921 40; Exception : [std::bad_alloc]; ./testing/src/novartis-pisces/pisces/redist/salmon/bin/salmon index was invoked improperly.; For usage information, try ./testing/src/novartis-pisces/pisces/redist/salmon/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108:26,Testability,log,log,26,"The relevant parts of the log are here:; ```; [2019-10-04 10:37:22.243] [ff::console] [warning] Removed 89618 transcripts that were sequence duplicates of indexed transcripts.; [2019-10-04 10:37:22.243] [ff::console] [warning] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2019-10-04 10:37:22.318] [ff::console] [info] Replaced 1,775,734,603 non-ATCG nucleotides; [2019-10-04 10:37:22.318] [ff::console] [info] Clipped poly-A tails from 422 transcripts; wrote 593292 cleaned references; seqHash 256 : bd425816a78195ed31cf17ce9df99c2bf56bff98f0df5ace1e958b263d805390; seqHash 512 : 845b625de6f8f018796e464f7c49f6596d2b31b28a58771d56ece24b3d9cad98b8189572ff43d6a3eb8ef24b5d3bc5ac0f89845a57e3682498a56a1bc920e7b7; nameHash 256 : 3bd11eac1e6b05e93689676ca056c165e7c26723c4b137fd284bb8b40ef5df62; nameHash 512 : e68449cfd99f5968182735275b00779b8a396e413a3629beef933e51bd18902c821c26e2a5461c687d023ef85168e58d76bacd5fe1f0a3111bfccc34af9c4035; [2019-10-04 10:37:37.931] [console] [info] Filter size not provided; estimating from number of distinct k-mers; [2019-10-04 10:38:33.012] [console] [info] ntHll estimated 2765935300 distinct k-mers, setting filter size to 2^36; Threads = 8; Vertex length = 31; Hash functions = 5; Filter size = 68719476736; Capacity = 2; Files:; test_pufferfish_index/ref_k31_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:68719476736; Pass Filling Filtering; 1 385 3124; 2 1258 2; True junctions count = 5437144; False junctions count = 4410615; Hash table size = 9847759; Candidate marks count = 26276463; --------------------------------------------------------------------------------; Reallocating bifurcations time: 6; True marks count: 20290262; Edges construction time: 5004; --------------------------------------------------------------------------------; Distinct junctions = 5437144. approximateContigTotalLength: 1543877663; counters:; 49076 936 921 40; Exception : [std:",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108:2016,Testability,test,testing,2016,"ng] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2019-10-04 10:37:22.318] [ff::console] [info] Replaced 1,775,734,603 non-ATCG nucleotides; [2019-10-04 10:37:22.318] [ff::console] [info] Clipped poly-A tails from 422 transcripts; wrote 593292 cleaned references; seqHash 256 : bd425816a78195ed31cf17ce9df99c2bf56bff98f0df5ace1e958b263d805390; seqHash 512 : 845b625de6f8f018796e464f7c49f6596d2b31b28a58771d56ece24b3d9cad98b8189572ff43d6a3eb8ef24b5d3bc5ac0f89845a57e3682498a56a1bc920e7b7; nameHash 256 : 3bd11eac1e6b05e93689676ca056c165e7c26723c4b137fd284bb8b40ef5df62; nameHash 512 : e68449cfd99f5968182735275b00779b8a396e413a3629beef933e51bd18902c821c26e2a5461c687d023ef85168e58d76bacd5fe1f0a3111bfccc34af9c4035; [2019-10-04 10:37:37.931] [console] [info] Filter size not provided; estimating from number of distinct k-mers; [2019-10-04 10:38:33.012] [console] [info] ntHll estimated 2765935300 distinct k-mers, setting filter size to 2^36; Threads = 8; Vertex length = 31; Hash functions = 5; Filter size = 68719476736; Capacity = 2; Files:; test_pufferfish_index/ref_k31_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:68719476736; Pass Filling Filtering; 1 385 3124; 2 1258 2; True junctions count = 5437144; False junctions count = 4410615; Hash table size = 9847759; Candidate marks count = 26276463; --------------------------------------------------------------------------------; Reallocating bifurcations time: 6; True marks count: 20290262; Edges construction time: 5004; --------------------------------------------------------------------------------; Distinct junctions = 5437144. approximateContigTotalLength: 1543877663; counters:; 49076 936 921 40; Exception : [std::bad_alloc]; ./testing/src/novartis-pisces/pisces/redist/salmon/bin/salmon index was invoked improperly.; For usage information, try ./testing/src/novartis-pisces/pisces/redist/salmon/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108:2136,Testability,test,testing,2136,"ng] If you wish to retain duplicate transcripts, please use the `--keepDuplicates` flag; [2019-10-04 10:37:22.318] [ff::console] [info] Replaced 1,775,734,603 non-ATCG nucleotides; [2019-10-04 10:37:22.318] [ff::console] [info] Clipped poly-A tails from 422 transcripts; wrote 593292 cleaned references; seqHash 256 : bd425816a78195ed31cf17ce9df99c2bf56bff98f0df5ace1e958b263d805390; seqHash 512 : 845b625de6f8f018796e464f7c49f6596d2b31b28a58771d56ece24b3d9cad98b8189572ff43d6a3eb8ef24b5d3bc5ac0f89845a57e3682498a56a1bc920e7b7; nameHash 256 : 3bd11eac1e6b05e93689676ca056c165e7c26723c4b137fd284bb8b40ef5df62; nameHash 512 : e68449cfd99f5968182735275b00779b8a396e413a3629beef933e51bd18902c821c26e2a5461c687d023ef85168e58d76bacd5fe1f0a3111bfccc34af9c4035; [2019-10-04 10:37:37.931] [console] [info] Filter size not provided; estimating from number of distinct k-mers; [2019-10-04 10:38:33.012] [console] [info] ntHll estimated 2765935300 distinct k-mers, setting filter size to 2^36; Threads = 8; Vertex length = 31; Hash functions = 5; Filter size = 68719476736; Capacity = 2; Files:; test_pufferfish_index/ref_k31_fixed.fa; --------------------------------------------------------------------------------; Round 0, 0:68719476736; Pass Filling Filtering; 1 385 3124; 2 1258 2; True junctions count = 5437144; False junctions count = 4410615; Hash table size = 9847759; Candidate marks count = 26276463; --------------------------------------------------------------------------------; Reallocating bifurcations time: 6; True marks count: 20290262; Edges construction time: 5004; --------------------------------------------------------------------------------; Distinct junctions = 5437144. approximateContigTotalLength: 1543877663; counters:; 49076 936 921 40; Exception : [std::bad_alloc]; ./testing/src/novartis-pisces/pisces/redist/salmon/bin/salmon index was invoked improperly.; For usage information, try ./testing/src/novartis-pisces/pisces/redist/salmon/bin/salmon index --help; Exiting.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538547108
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538548418:177,Availability,avail,available,177,"Hi @mdshw5,. Thanks for the detailed report (and trying out the new version). Could you provide some details about the system you're running on? For example, how much memory is available? Also, how easy would it be to provide the actual fasta you are trying to index (for us to attempt to reproduce this locally)?. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538548418
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538602738:665,Energy Efficiency,allocate,allocated,665,"Hi @mdshw5,. Thanks for sharing the file. I was able to build the transcriptome. The final index on this is ~15G. Here are the stats from the run on our machine:. ```; ~/salmon/build/issue432$ du -h big_idx/; 15G big_idx/; ~/salmon/build/issue432$ cat indexing_time.txt; 9737.34user 339.95system 21:11.51elapsed 792%CPU (0avgtext+0avgdata 14894700maxresident)k; 1232inputs+150622584outputs (0major+99352946minor)pagefaults 0swaps; ```. so the peak construction memory was about the same and the build took 21m with 16 threads. If you have a place, I can share the built index. What were the stats of the machine on which you were building? Was there sufficient RAM allocated?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538602738
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538606398:43,Deployability,update,update,43,"Wow. Thanks for getting back so fast. I’ll update more info about the machine. It’s got 16GB of RAM and only 3GB of swap, so I do think it was memory pressure. In fact, I just looked through the system kernel messages and found the OOM routine killed my process:. ```Out of memory: Kill process 12997 (R) score 846 or sacrifice child │10-03 22:39 INFO Encountered FastxParser destructor while parser was still marked active (or while parsing threads were ; Killed process 12997, UID 1506502601, (R) total-vm:17105100kB, anon-rss:15306012kB, file-rss:12kB ; ```. Sorry I didn’t check this earlier!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538606398
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538606398:209,Integrability,message,messages,209,"Wow. Thanks for getting back so fast. I’ll update more info about the machine. It’s got 16GB of RAM and only 3GB of swap, so I do think it was memory pressure. In fact, I just looked through the system kernel messages and found the OOM routine killed my process:. ```Out of memory: Kill process 12997 (R) score 846 or sacrifice child │10-03 22:39 INFO Encountered FastxParser destructor while parser was still marked active (or while parsing threads were ; Killed process 12997, UID 1506502601, (R) total-vm:17105100kB, anon-rss:15306012kB, file-rss:12kB ; ```. Sorry I didn’t check this earlier!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538606398
https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538606398:236,Integrability,rout,routine,236,"Wow. Thanks for getting back so fast. I’ll update more info about the machine. It’s got 16GB of RAM and only 3GB of swap, so I do think it was memory pressure. In fact, I just looked through the system kernel messages and found the OOM routine killed my process:. ```Out of memory: Kill process 12997 (R) score 846 or sacrifice child │10-03 22:39 INFO Encountered FastxParser destructor while parser was still marked active (or while parsing threads were ; Killed process 12997, UID 1506502601, (R) total-vm:17105100kB, anon-rss:15306012kB, file-rss:12kB ; ```. Sorry I didn’t check this earlier!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/432#issuecomment-538606398
https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932:327,Integrability,depend,depend,327,"Hi @pinin4fjords ,; Thanks for raising an important question and running alevin for the training.; I think there is a confusion regarding the `quantmerge` command. That command works only with bulk RNA-seq quants not with alevin output. To answer your question of running multiple alevin instance for multiple file pair, might depend on what are the separate files from, are they from separate lanes or are they separated based on cellular barcode ? The basic intuition is after initial barcode assignment, alevin works on each cell disjointly meaning as long as you are confident that each file pair is cell disjoint then at the end you can just cat the output of the alevin quants. Also, depending on what's the training about you can think of multiple workarounds like you can use very small 100 cell (7 million reads) datasets from 10x and combine it all together in one file if size and multiple files is a problem.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932
https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932:690,Integrability,depend,depending,690,"Hi @pinin4fjords ,; Thanks for raising an important question and running alevin for the training.; I think there is a confusion regarding the `quantmerge` command. That command works only with bulk RNA-seq quants not with alevin output. To answer your question of running multiple alevin instance for multiple file pair, might depend on what are the separate files from, are they from separate lanes or are they separated based on cellular barcode ? The basic intuition is after initial barcode assignment, alevin works on each cell disjointly meaning as long as you are confident that each file pair is cell disjoint then at the end you can just cat the output of the alevin quants. Also, depending on what's the training about you can think of multiple workarounds like you can use very small 100 cell (7 million reads) datasets from 10x and combine it all together in one file if size and multiple files is a problem.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932
https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932:460,Usability,intuit,intuition,460,"Hi @pinin4fjords ,; Thanks for raising an important question and running alevin for the training.; I think there is a confusion regarding the `quantmerge` command. That command works only with bulk RNA-seq quants not with alevin output. To answer your question of running multiple alevin instance for multiple file pair, might depend on what are the separate files from, are they from separate lanes or are they separated based on cellular barcode ? The basic intuition is after initial barcode assignment, alevin works on each cell disjointly meaning as long as you are confident that each file pair is cell disjoint then at the end you can just cat the output of the alevin quants. Also, depending on what's the training about you can think of multiple workarounds like you can use very small 100 cell (7 million reads) datasets from 10x and combine it all together in one file if size and multiple files is a problem.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540033932
https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540400660:200,Usability,clear,clear,200,"Thanks @k3yavi for the clarification. In my example case the files are not cell disjoint, being multiple lanes run from the same library. Obviously I can use just one lane for the training, but to be clear: in the real world in this situation all files for a library need to be run together, right?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/434#issuecomment-540400660
https://github.com/COMBINE-lab/salmon/issues/435#issuecomment-544178376:121,Availability,down,downloaded,121,"If you put it in e.g. a google drive or dropbox link and share privately with me by email, i will let you know once it's downloaded. Otherwise, i might be able to set a google drive link where you can upload, but I've had mixed success with that in the past.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/435#issuecomment-544178376
https://github.com/COMBINE-lab/salmon/issues/436#issuecomment-821498452:83,Integrability,wrap,wrappers,83,This should be fixed with the [seurat_wrapper](https://github.com/satijalab/seurat-wrappers) repo.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/436#issuecomment-821498452
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-545108970:44,Deployability,release,release,44,"Hi Hamdi,. http://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html has a really great description of how to use the output from Salmon with DESeq2.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-545108970
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-545201602:447,Usability,simpl,simple,447,"Hi @roryk ; I know about tximport but is there any way to generate the input data for DESeq2 without using R? I am processing the data on one platform and then transfer to another platform for R/DESeq2 analysis. I would like to be able to generate the output of the first part (salmon) without using an R library. . If it is not possible and I have run R to get the count matrix for DEseq2, I can figure out a way to do it. DESeq2 input file is a simple matrix of counts and ""salmon quantmerge"" already generates this, can you please explain to me why an external library is required ? Is there something I am missing that tximport package is doing to the data? Does tximport takes into account gene lengths or library size to generate the output? . Thanks; Best Regards; Hamdi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-545201602
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-549185551:34,Testability,log,logic,34,Hi Hamdi. Bit confused about your logic here - why would you not want to use tximport in R when your next step (DESeq2) is still going to be R? . I am curious to know your reasoning . > I am processing the data on one platform and then transfer to another platform for R/DESeq2 analysis. I would like to be able to generate the output of the first part (salmon) without using an R library.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-549185551
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751189286:59,Deployability,update,update,59,"I also wonder why salmon not output original reads counts. update:. Because it is more accurate. DeSeq2 should accept it. As for now, maybe we could simply round to the nearest integer. > NumReads — This is salmon’s estimate of the number of reads mapping to each transcript that was quantified. It is an “estimate” insofar as it is the expected number of reads that have originated from each transcript given the structure of the uniquely mapping and multi-mapping reads and the relative abundance estimates for each transcript.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751189286
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751189286:149,Usability,simpl,simply,149,"I also wonder why salmon not output original reads counts. update:. Because it is more accurate. DeSeq2 should accept it. As for now, maybe we could simply round to the nearest integer. > NumReads — This is salmon’s estimate of the number of reads mapping to each transcript that was quantified. It is an “estimate” insofar as it is the expected number of reads that have originated from each transcript given the structure of the uniquely mapping and multi-mapping reads and the relative abundance estimates for each transcript.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751189286
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751190682:295,Deployability,release,release,295,"If you can already use DESeq2, then using tximport should not make it any harder at all. Given the tximport data, getting it into DESeq2 is as easy as. ```; dds <- DESeqDataSetFromTximport(txi, sampleTable, ~condition); ```. as shown in the [tximport vignette](https://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html#Introduction). . Regarding outputting ""original read counts""; salmon *does* output the estimates for the number of reads deriving from each transcript. If the question is, why is this number not an integer, that's because the best estimate (the maximum likelihood estimate) is often not integral. Tools that simply count reads (e.g. HTSeq) produce integer counts, but these are in no way ""original read counts"" for the corresponding genes, and are usually less accurate (farther from the true number of fragments deriving from a transcript / gene) than the estimates produced by salmon. The fact that the best estimate is often not an integer is a direct result of the fact one is considering a statistical model and taking expectations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751190682
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751190682:658,Usability,simpl,simply,658,"If you can already use DESeq2, then using tximport should not make it any harder at all. Given the tximport data, getting it into DESeq2 is as easy as. ```; dds <- DESeqDataSetFromTximport(txi, sampleTable, ~condition); ```. as shown in the [tximport vignette](https://bioconductor.org/packages/release/bioc/vignettes/tximport/inst/doc/tximport.html#Introduction). . Regarding outputting ""original read counts""; salmon *does* output the estimates for the number of reads deriving from each transcript. If the question is, why is this number not an integer, that's because the best estimate (the maximum likelihood estimate) is often not integral. Tools that simply count reads (e.g. HTSeq) produce integer counts, but these are in no way ""original read counts"" for the corresponding genes, and are usually less accurate (farther from the true number of fragments deriving from a transcript / gene) than the estimates produced by salmon. The fact that the best estimate is often not an integer is a direct result of the fact one is considering a statistical model and taking expectations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751190682
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751195284:143,Availability,reliab,reliable,143,"You mean like cloud services to perform the DE analysis? It’s always possible to round the non-integer counts to the nearest integer. However, reliable abundance estimation tools (e.g. RSEM) have been around long enough now that it’s worth pushing any cloud service you might be using to properly deal with these types of inputs. We do differential analysis quite commonly with DESeq2, and salmon -> tximport -> DESeq2 is a quite low-friction solution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751195284
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751195284:32,Performance,perform,perform,32,"You mean like cloud services to perform the DE analysis? It’s always possible to round the non-integer counts to the nearest integer. However, reliable abundance estimation tools (e.g. RSEM) have been around long enough now that it’s worth pushing any cloud service you might be using to properly deal with these types of inputs. We do differential analysis quite commonly with DESeq2, and salmon -> tximport -> DESeq2 is a quite low-friction solution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-751195284
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-782990393:340,Deployability,pipeline,pipeline,340,"To authentic-zz:; You mean you have get the counts into a data frame. I don't know how to transfer the salmon data frame into DESeq2，and I fail to handle data frame from salmon, too. So I have these recommendations to you: ; 1. Get the salmon raw results, the sf files, as the tximport input files.; 2. Run the salmon again or choose other pipeline like HISAT2-Stringtie/featurecounts/HTseq.; 3. Try to generate the salmon sf file. The sf file's row name is ""Name Length EffectiveLength TPM NumReads"". You have the gene ID and counts, so fill the length, effect length, and TPM by tab or something. But I do not sure if this handle is correct. https://salmon.readthedocs.io/en/latest/file_formats.html#fileformats",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-782990393
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-1535046180:145,Availability,reliab,reliable,145,"> You mean like cloud services to perform the DE analysis? It’s always possible to round the non-integer counts to the nearest integer. However, reliable abundance estimation tools (e.g. RSEM) have been around long enough now that it’s worth pushing any cloud service you might be using to properly deal with these types of inputs. We do differential analysis quite commonly with DESeq2, and salmon -> tximport -> DESeq2 is a quite low-friction solution. I noticed that now salmon can export the quant.gene.sf file if I add the parameters""-g xx.gtf"". What's difference between this file and the result of tximport? Can I use the result to replace tximport?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-1535046180
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-1535046180:34,Performance,perform,perform,34,"> You mean like cloud services to perform the DE analysis? It’s always possible to round the non-integer counts to the nearest integer. However, reliable abundance estimation tools (e.g. RSEM) have been around long enough now that it’s worth pushing any cloud service you might be using to properly deal with these types of inputs. We do differential analysis quite commonly with DESeq2, and salmon -> tximport -> DESeq2 is a quite low-friction solution. I noticed that now salmon can export the quant.gene.sf file if I add the parameters""-g xx.gtf"". What's difference between this file and the result of tximport? Can I use the result to replace tximport?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-1535046180
https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-1535065239:519,Integrability,interface,interface,519,"It is possible to output gene counts directly, but using `tximport` is the preferred and officially supported method. The reason for this is that the gene-level aggregation built into salmon is _per-sample_, that is each sample is aggregated to the gene level independently. On the other hand, `tximport` considers all samples in an experiment to determine e.g. the average expressed length of a gene over all samples. This leads to better aggregation. Likewise, `tximport` (specifically via `tximeta`) provides a nice interface to track and propagate reference annotation provenance, which ultimately leads to more reproducible analyses.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/437#issuecomment-1535065239
https://github.com/COMBINE-lab/salmon/issues/439#issuecomment-622019385:182,Deployability,pipeline,pipelines,182,"So which --chemistry flag in Cell Ranger does the change to -lISF correspond to? Is it `SC5P-R2` or `fiveprime`? https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/count. Also since salmon/alevin can detect the library type automatically, would detect the correct library in the case of 5'-tagged scRNAseq 10X Feature barcode?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/439#issuecomment-622019385
https://github.com/COMBINE-lab/salmon/issues/439#issuecomment-622019385:241,Safety,detect,detect,241,"So which --chemistry flag in Cell Ranger does the change to -lISF correspond to? Is it `SC5P-R2` or `fiveprime`? https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/count. Also since salmon/alevin can detect the library type automatically, would detect the correct library in the case of 5'-tagged scRNAseq 10X Feature barcode?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/439#issuecomment-622019385
https://github.com/COMBINE-lab/salmon/issues/439#issuecomment-622019385:286,Safety,detect,detect,286,"So which --chemistry flag in Cell Ranger does the change to -lISF correspond to? Is it `SC5P-R2` or `fiveprime`? https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/using/count. Also since salmon/alevin can detect the library type automatically, would detect the correct library in the case of 5'-tagged scRNAseq 10X Feature barcode?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/439#issuecomment-622019385
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549146224:945,Deployability,update,update,945,"Following are some weird thing I am noticing in your log:. ```; 2019-11-02T16:23:27.745502492Z [2019-11-02 16:23:27.745] [puff::index::jointLog] [warning] The decoy name JH584303.1 was encountered more than once --- please be sure all decoy names and sequences are unique.; 2019-11-02T16:23:27.745504753Z [2019-11-02 16:23:27.745] [puff::index::jointLog] [warning] The decoy name JH584304.1 was encountered more than once --- please be sure all decoy names and sequences are unique.; 2019-11-02T16:24:33.408457659Z [2019-11-02 16:24:33.408] [puff::index::jointLog] [warning] The decoy file contained the names of 88 decoy sequences, but 66 were matched by sequences in the reference file provided.; ```; Where we expect only 66 decoys (genomic targets) to start with. I think it's the issue with the gencode reference names having blank space as a delimiter in its target name with repeated names. The ipython notebook was right but I missed to update the static website, in the prepare metadata section I have updated the decoy name extracting step to:. ```; grep ""^>"" <(zcat GRCm38.primary_assembly.genome.fa.gz) | cut -d "" "" -f 1 > decoys.txt; ```. That is I splitting the genomic target names by space and taking just the first part as the target name. I working on checking what's happening if I follow the step of using the full gencode names and would update you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549146224
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549146224:1011,Deployability,update,updated,1011,"Following are some weird thing I am noticing in your log:. ```; 2019-11-02T16:23:27.745502492Z [2019-11-02 16:23:27.745] [puff::index::jointLog] [warning] The decoy name JH584303.1 was encountered more than once --- please be sure all decoy names and sequences are unique.; 2019-11-02T16:23:27.745504753Z [2019-11-02 16:23:27.745] [puff::index::jointLog] [warning] The decoy name JH584304.1 was encountered more than once --- please be sure all decoy names and sequences are unique.; 2019-11-02T16:24:33.408457659Z [2019-11-02 16:24:33.408] [puff::index::jointLog] [warning] The decoy file contained the names of 88 decoy sequences, but 66 were matched by sequences in the reference file provided.; ```; Where we expect only 66 decoys (genomic targets) to start with. I think it's the issue with the gencode reference names having blank space as a delimiter in its target name with repeated names. The ipython notebook was right but I missed to update the static website, in the prepare metadata section I have updated the decoy name extracting step to:. ```; grep ""^>"" <(zcat GRCm38.primary_assembly.genome.fa.gz) | cut -d "" "" -f 1 > decoys.txt; ```. That is I splitting the genomic target names by space and taking just the first part as the target name. I working on checking what's happening if I follow the step of using the full gencode names and would update you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549146224
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549146224:1359,Deployability,update,update,1359,"Following are some weird thing I am noticing in your log:. ```; 2019-11-02T16:23:27.745502492Z [2019-11-02 16:23:27.745] [puff::index::jointLog] [warning] The decoy name JH584303.1 was encountered more than once --- please be sure all decoy names and sequences are unique.; 2019-11-02T16:23:27.745504753Z [2019-11-02 16:23:27.745] [puff::index::jointLog] [warning] The decoy name JH584304.1 was encountered more than once --- please be sure all decoy names and sequences are unique.; 2019-11-02T16:24:33.408457659Z [2019-11-02 16:24:33.408] [puff::index::jointLog] [warning] The decoy file contained the names of 88 decoy sequences, but 66 were matched by sequences in the reference file provided.; ```; Where we expect only 66 decoys (genomic targets) to start with. I think it's the issue with the gencode reference names having blank space as a delimiter in its target name with repeated names. The ipython notebook was right but I missed to update the static website, in the prepare metadata section I have updated the decoy name extracting step to:. ```; grep ""^>"" <(zcat GRCm38.primary_assembly.genome.fa.gz) | cut -d "" "" -f 1 > decoys.txt; ```. That is I splitting the genomic target names by space and taking just the first part as the target name. I working on checking what's happening if I follow the step of using the full gencode names and would update you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549146224
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549146224:53,Testability,log,log,53,"Following are some weird thing I am noticing in your log:. ```; 2019-11-02T16:23:27.745502492Z [2019-11-02 16:23:27.745] [puff::index::jointLog] [warning] The decoy name JH584303.1 was encountered more than once --- please be sure all decoy names and sequences are unique.; 2019-11-02T16:23:27.745504753Z [2019-11-02 16:23:27.745] [puff::index::jointLog] [warning] The decoy name JH584304.1 was encountered more than once --- please be sure all decoy names and sequences are unique.; 2019-11-02T16:24:33.408457659Z [2019-11-02 16:24:33.408] [puff::index::jointLog] [warning] The decoy file contained the names of 88 decoy sequences, but 66 were matched by sequences in the reference file provided.; ```; Where we expect only 66 decoys (genomic targets) to start with. I think it's the issue with the gencode reference names having blank space as a delimiter in its target name with repeated names. The ipython notebook was right but I missed to update the static website, in the prepare metadata section I have updated the decoy name extracting step to:. ```; grep ""^>"" <(zcat GRCm38.primary_assembly.genome.fa.gz) | cut -d "" "" -f 1 > decoys.txt; ```. That is I splitting the genomic target names by space and taking just the first part as the target name. I working on checking what's happening if I follow the step of using the full gencode names and would update you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549146224
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549155511:72,Availability,error,error,72,"I can confirm that the problem in the log is not creating the exception error raised by this issue and the indexing procedure took care of the potential duplicates due to repeated names. There is something else going on. I used the binary from github and indexed, it seems to work on our end. @rob-p thoughts ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549155511
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549155511:38,Testability,log,log,38,"I can confirm that the problem in the log is not creating the exception error raised by this issue and the indexing procedure took care of the potential duplicates due to repeated names. There is something else going on. I used the binary from github and indexed, it seems to work on our end. @rob-p thoughts ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549155511
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549171596:228,Availability,down,downloaded,228,"Here is the part of the log I've left out previously; <img width=""978"" alt=""Screen Shot 2019-11-03 at 8 41 57 PM"" src=""https://user-images.githubusercontent.com/17168657/68090974-860d6200-fe7a-11e9-972f-d529453bbea8.png"">. I've downloaded Linux executables on 11/02/ from the following link: https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz. Decoys and gentrome files seem to be ok since they are working properly with bioconda version of salmon. I am sharing a Dockerfile in case you would like to reproduce the entire environment I was using. [Dockerfile](https://github.com/COMBINE-lab/salmon/files/3802055/Dockerfile)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549171596
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549171596:339,Availability,down,download,339,"Here is the part of the log I've left out previously; <img width=""978"" alt=""Screen Shot 2019-11-03 at 8 41 57 PM"" src=""https://user-images.githubusercontent.com/17168657/68090974-860d6200-fe7a-11e9-972f-d529453bbea8.png"">. I've downloaded Linux executables on 11/02/ from the following link: https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz. Decoys and gentrome files seem to be ok since they are working properly with bioconda version of salmon. I am sharing a Dockerfile in case you would like to reproduce the entire environment I was using. [Dockerfile](https://github.com/COMBINE-lab/salmon/files/3802055/Dockerfile)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549171596
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549171596:330,Deployability,release,releases,330,"Here is the part of the log I've left out previously; <img width=""978"" alt=""Screen Shot 2019-11-03 at 8 41 57 PM"" src=""https://user-images.githubusercontent.com/17168657/68090974-860d6200-fe7a-11e9-972f-d529453bbea8.png"">. I've downloaded Linux executables on 11/02/ from the following link: https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz. Decoys and gentrome files seem to be ok since they are working properly with bioconda version of salmon. I am sharing a Dockerfile in case you would like to reproduce the entire environment I was using. [Dockerfile](https://github.com/COMBINE-lab/salmon/files/3802055/Dockerfile)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549171596
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549171596:24,Testability,log,log,24,"Here is the part of the log I've left out previously; <img width=""978"" alt=""Screen Shot 2019-11-03 at 8 41 57 PM"" src=""https://user-images.githubusercontent.com/17168657/68090974-860d6200-fe7a-11e9-972f-d529453bbea8.png"">. I've downloaded Linux executables on 11/02/ from the following link: https://github.com/COMBINE-lab/salmon/releases/download/v1.0.0/salmon-1.0.0_linux_x86_64.tar.gz. Decoys and gentrome files seem to be ok since they are working properly with bioconda version of salmon. I am sharing a Dockerfile in case you would like to reproduce the entire environment I was using. [Dockerfile](https://github.com/COMBINE-lab/salmon/files/3802055/Dockerfile)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-549171596
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:313,Energy Efficiency,schedul,scheduling,313,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:539,Integrability,message,message,539,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:547,Performance,queue,queues,547,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:1013,Testability,log,logs,1013,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:1031,Testability,log,log,1031,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:1167,Testability,log,log,1167,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:1237,Testability,log,log,1237,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:1648,Testability,log,log,1648,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589:1706,Testability,log,log,1706,"hello! have you by any chance figured it out? I have quite similiar problem. . I am running salmon v.1.1.0 on my ubuntu machine with 128GB of RAM. I set the limit for vitrual memory at ~75GB to not overload the system:. ```bash; ○ → ulimit -a; core file size (blocks, -c) 0; data seg size (kbytes, -d) unlimited; scheduling priority (-e) 0; file size (blocks, -f) unlimited; pending signals (-i) 514510; max locked memory (kbytes, -l) 65536; max memory size (kbytes, -m) unlimited; open files (-n) 1024; pipe size (512 bytes, -p) 8; POSIX message queues (bytes, -q) 819200; real-time priority (-r) 0; stack size (kbytes, -s) 8192; cpu time (seconds, -t) unlimited; max user processes (-u) 514510; virtual memory (kbytes, -v) 75331648; file locks (-x) unlimited; ```. I am building the index with the following command:. ```bash; salmon index \; -t /mnt/rescomp/ref/hg38/gentrome.fa.gz \; -i /mnt/rescomp/ref/hg38/salmon_index -k 31 \; --decoys /mnt/rescomp/ref/hg38/decoys.txt \; --threads 16 \; --gencode |& tee logs/salmon_index.log; ```. gentrome is created based on the gencode transcriptome (v33) and genome primary algnment sequence (GRCh38.p13). [salmon_index.log](https://github.com/COMBINE-lab/salmon/files/4392725/salmon_index.log). The output directory:; ```; ○ → ll /mnt/rescomp/ref/hg38/salmon_index; total 7.9G; drwxr-sr-x 1 37304 723 4.0K Mar 27 01:36 ./; drwxr-sr-x 1 37304 723 4.0K Mar 26 22:13 ../; -rw-r--r-- 1 37304 723 888K Mar 27 00:32 complete_ref_lens.bin; -rw-r--r-- 1 37304 723 31K Mar 27 00:27 duplicate_clusters.tsv; -rw-r--r-- 1 37304 723 674M Mar 27 01:46 path.bin; -rw-r--r-- 1 37304 723 55 Mar 27 01:46 pre_indexing.log; -rw-r--r-- 1 37304 723 40K Mar 27 01:46 ref_indexing.log; -rw-r--r-- 1 37304 723 3.3G Mar 27 00:32 ref_k31_fixed.fa; -rw-r--r-- 1 37304 723 703 Mar 27 00:32 ref_sigs.json; -rw-r--r-- 1 37304 723 4.1G Mar 27 01:36 tmp_dbg.bin; ```; I know for a fact that the memory usage did not go over 16GB. Any hints how to proceed?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-604919589
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-605106040:347,Deployability,release,release,347,"Hi @kzkedzierska,. I'm not sure why the virtual memory usage here is so high, and am also not aware of a great way to predict it. One thing I might ask is if you could test this executable on your system ( [salmon-1.2.0-beta](https://drive.google.com/open?id=1QHYCT3Vs9bRD7UmJY6JJKjlzmmUE4wRl)). This is the near-final beta version of 1.2.0 whose release is imminent. One of the big changes in this version is a considerably more memory-efficient construction. We have been measuring this in terms of resident memory, but it may also apply to virtual memory. Would you mind giving it a try if you have a chance?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-605106040
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-605106040:437,Energy Efficiency,efficient,efficient,437,"Hi @kzkedzierska,. I'm not sure why the virtual memory usage here is so high, and am also not aware of a great way to predict it. One thing I might ask is if you could test this executable on your system ( [salmon-1.2.0-beta](https://drive.google.com/open?id=1QHYCT3Vs9bRD7UmJY6JJKjlzmmUE4wRl)). This is the near-final beta version of 1.2.0 whose release is imminent. One of the big changes in this version is a considerably more memory-efficient construction. We have been measuring this in terms of resident memory, but it may also apply to virtual memory. Would you mind giving it a try if you have a chance?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-605106040
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-605106040:118,Safety,predict,predict,118,"Hi @kzkedzierska,. I'm not sure why the virtual memory usage here is so high, and am also not aware of a great way to predict it. One thing I might ask is if you could test this executable on your system ( [salmon-1.2.0-beta](https://drive.google.com/open?id=1QHYCT3Vs9bRD7UmJY6JJKjlzmmUE4wRl)). This is the near-final beta version of 1.2.0 whose release is imminent. One of the big changes in this version is a considerably more memory-efficient construction. We have been measuring this in terms of resident memory, but it may also apply to virtual memory. Would you mind giving it a try if you have a chance?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-605106040
https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-605106040:168,Testability,test,test,168,"Hi @kzkedzierska,. I'm not sure why the virtual memory usage here is so high, and am also not aware of a great way to predict it. One thing I might ask is if you could test this executable on your system ( [salmon-1.2.0-beta](https://drive.google.com/open?id=1QHYCT3Vs9bRD7UmJY6JJKjlzmmUE4wRl)). This is the near-final beta version of 1.2.0 whose release is imminent. One of the big changes in this version is a considerably more memory-efficient construction. We have been measuring this in terms of resident memory, but it may also apply to virtual memory. Would you mind giving it a try if you have a chance?. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/441#issuecomment-605106040
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:220,Deployability,pipeline,pipeline,220,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:333,Deployability,pipeline,pipeline,333,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:367,Deployability,pipeline,pipeline,367,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:486,Deployability,pipeline,pipeline,486,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:652,Deployability,pipeline,pipeline,652,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:1337,Deployability,release,released,1337,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:1451,Deployability,pipeline,pipeline,1451,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:1742,Performance,perform,performance,1742,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:1781,Performance,optimiz,optimization,1781,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035:1078,Security,validat,validateMapping,1078,"Hi @tamuanand ,. I think these are very important question and thanks for raising the issue.; As you mention, In the preprint we put out two different modes of Selective Alignment:; A) SA: The mashmap and bedtools based pipeline which follows old [SalmonTools](https://github.com/COMBINE-lab/SalmonTools/blob/master/README.md) based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows [this](https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/) pipeline. The distinction between the two comes from how the decoy sequence are actually generated. To answer your question point wise.; 1.) That's correct SAF based pipeline follows the tutorial as mentioned in B above and uses the full genome as decoys.; 2.) That's correct, if a user wan't to run SA method, then they should follow mashmap based tutorial A. This might be useful for situation where the index is too big to fit into the machine's memory.; 3.) That's also correct, yes if you don't provide decoys `-d` you can still run salmon on the transcriptome. We have just enabled the validateMapping option by default, which is also used in transcriptome only mode, currently there is no option to _disable_ it.; 4) That's also correct, we have dropped the quasi-mapping based support from the latest version, If you need to run quasi we have released `0.15` just as a last version into the archive.; 5 & 6) Very good question, short answer is your default pipeline of VBEM is the recommended way. We have to use additional flags `--mimicBT2 and --useEM` while comparing the methods in the preprint. RSEM can only do EM and as we were comparing against Bowtie2 we have to mimic it with more stricter requirements for fair comparison. We expect the performance to be better with VB based optimization and not using `mimcBT2` . @rob-p Feel free to add if I missed something.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549187035
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549189494:628,Availability,avail,available,628,"Hi @k3yavi . Can you elaborate on 3 and 4 with command line usage examples? I feel you misunderstood my Question 3. My question 3 was ""how to do `salmon index` if I do not have a genome"" with salmon/v1.0. If my understanding is right (based on your response 4 - about dropping quasi mapping support with salmon/v1.0), I believe **_you cannot use salmon/v1.0 to do something like this below_** for my Question 3 (salmon index in absence of genome) ;; `salmon index -t txome_fasta -i txome_index`. Other questions:. 1. I don't believe bioconda has salmon/v1.0 - I checked on 01-Nov-2019 (around 7am Eastern); 2. Is salmon v0.15.0 available via bioconda - when I tried (same time as above) updating salmon via bioconda channel on my conda env it still pointed me to 0.14.1 . Thanks in advance to both @k3yavi and @rob-p",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549189494
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:125,Availability,avail,available,125,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:299,Deployability,update,update,299,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:312,Deployability,update,update,312,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:938,Deployability,pipeline,pipeline,938,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:983,Deployability,pipeline,pipeline,983,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:1017,Deployability,pipeline,pipeline,1017,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:1061,Deployability,pipeline,pipeline,1061,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:1174,Deployability,Release,Release,1174,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321:1428,Performance,perform,performed,1428,"Right, in short `salmon index -t txome_fasta -i txome_index` should work and both the versions of salmon (v0.15 and v1.0) is available on bioconda, check [here](https://bioconda.github.io/recipes/salmon/README.html), you may wanna try [force](https://docs.conda.io/projects/conda/en/latest/commands/update.html) update of conda. I think the confusion is you are thinking of the concept of Selective Alignment as the same as aligning to transcriptome w/ decoys (can be genome or mashmap based). Although they are related methods but the concept of Selective Alignment predates the idea of decoy based alignment, checkout [this](https://dl.acm.org/citation.cfm?id=3233589) paper from our lab where we discuss how Selectively Aligning difficult reads to just the transcriptome itself can result in improved quantification estimates compared to quasi or pseduo alignment. To summarize: ; In version 1.0; A) SA: The mashmap and bedtools based pipeline which follows old SalmonTools based pipeline.; B) SAF: Inbuilt salmon pipeline to consume genome and follows this pipeline.; C) If you don't provide any decoys, salmon will do Selective Alignment just on the transcriptome. The Release notes you quoted just means you cannot disable this feature i.e. you cannot fall back to quasi-mapping (in quasi mapping there is no alignment of the reads at all). In version 0.15.0; You cannot provide decoys and the transcriptome based mapping performed in this version would be quasi-mapping i.e. no Alignment of reads. Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195321
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:790,Deployability,release,release,790,"Hi @tamuanand,. Thank you for the detailed questions! Let me elaborate a bit on a few of @k3yavi's answers. 1&2) Yes; if you want to use SAF, you no longer need mashmap, as what you are essentially doing is treating the entire genome as a ""decoy"". As @k3yavi alludes, SA is still useful when you need to run in a very memory-constrained environment. After adopting the new [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index, the size of the transcriptome plush mashmap 2 decoys becomes considerably smaller than the previous size of the transcriptome in earlier versions of salmon (<= 0.15.0). However, depending on the organism, indexing the entire genome as decoy, even though it yields the best accuracy, does require a bit more memory, as specified in the release notes for the 0.99 betas and 1.0.0. 3) Yes; it is still possible to use `salmon index` without any decoy sequence. In this case, one can expect results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:1916,Deployability,update,update,1916,"t results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alignment against the full genome on the same machine, you will need both versions, as quasi-mapping is supported only in the [RapMap](https://github.com/COMBINE-lab/RapMap/tree/develop-salmon), while indexing something on the scale of the genome when not using the [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index has tremendous memory requirements (as is not recommended ). 5 & 6) To re-iterate @k3yavi's answer --- the extra flags used in the pre-print were only for the purpose of holding as many variables fixed as possible when comparing different approaches. It continues to be recommended to use the VBEM over the EM; it seems to perform better with respect to the ways in which we can measure and such improvements have also been documented in [other work](https://www.ncbi.nlm.nih.gov/pubmed/23821651). The _main_ effect of `--mi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:633,Integrability,depend,depending,633,"Hi @tamuanand,. Thank you for the detailed questions! Let me elaborate a bit on a few of @k3yavi's answers. 1&2) Yes; if you want to use SAF, you no longer need mashmap, as what you are essentially doing is treating the entire genome as a ""decoy"". As @k3yavi alludes, SA is still useful when you need to run in a very memory-constrained environment. After adopting the new [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index, the size of the transcriptome plush mashmap 2 decoys becomes considerably smaller than the previous size of the transcriptome in earlier versions of salmon (<= 0.15.0). However, depending on the organism, indexing the entire genome as decoy, even though it yields the best accuracy, does require a bit more memory, as specified in the release notes for the 0.99 betas and 1.0.0. 3) Yes; it is still possible to use `salmon index` without any decoy sequence. In this case, one can expect results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:2602,Modifiability,variab,variables,2602," decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alignment against the full genome on the same machine, you will need both versions, as quasi-mapping is supported only in the [RapMap](https://github.com/COMBINE-lab/RapMap/tree/develop-salmon), while indexing something on the scale of the genome when not using the [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index has tremendous memory requirements (as is not recommended ). 5 & 6) To re-iterate @k3yavi's answer --- the extra flags used in the pre-print were only for the purpose of holding as many variables fixed as possible when comparing different approaches. It continues to be recommended to use the VBEM over the EM; it seems to perform better with respect to the ways in which we can measure and such improvements have also been documented in [other work](https://www.ncbi.nlm.nih.gov/pubmed/23821651). The _main_ effect of `--mimicBT2` is to discard orphan alignments for the purposes of quantification. This is a more strict requirement than the default behavior of allowing orphans if there is no satisfactory alignment of both ends of a fragment. However, there is no obvious reason why it is better behavior than accounting for these orphan fragments (when appropriately adjusting the conditional probability given their distance from the transcript boundaries, as salmon does).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:1041,Performance,perform,perform,1041,"bit on a few of @k3yavi's answers. 1&2) Yes; if you want to use SAF, you no longer need mashmap, as what you are essentially doing is treating the entire genome as a ""decoy"". As @k3yavi alludes, SA is still useful when you need to run in a very memory-constrained environment. After adopting the new [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index, the size of the transcriptome plush mashmap 2 decoys becomes considerably smaller than the previous size of the transcriptome in earlier versions of salmon (<= 0.15.0). However, depending on the organism, indexing the entire genome as decoy, even though it yields the best accuracy, does require a bit more memory, as specified in the release notes for the 0.99 betas and 1.0.0. 3) Yes; it is still possible to use `salmon index` without any decoy sequence. In this case, one can expect results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alig",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:2739,Performance,perform,perform,2739," decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alignment against the full genome on the same machine, you will need both versions, as quasi-mapping is supported only in the [RapMap](https://github.com/COMBINE-lab/RapMap/tree/develop-salmon), while indexing something on the scale of the genome when not using the [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index has tremendous memory requirements (as is not recommended ). 5 & 6) To re-iterate @k3yavi's answer --- the extra flags used in the pre-print were only for the purpose of holding as many variables fixed as possible when comparing different approaches. It continues to be recommended to use the VBEM over the EM; it seems to perform better with respect to the ways in which we can measure and such improvements have also been documented in [other work](https://www.ncbi.nlm.nih.gov/pubmed/23821651). The _main_ effect of `--mimicBT2` is to discard orphan alignments for the purposes of quantification. This is a more strict requirement than the default behavior of allowing orphans if there is no satisfactory alignment of both ends of a fragment. However, there is no obvious reason why it is better behavior than accounting for these orphan fragments (when appropriately adjusting the conditional probability given their distance from the transcript boundaries, as salmon does).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390:1061,Usability,simpl,simply,1061,"bit on a few of @k3yavi's answers. 1&2) Yes; if you want to use SAF, you no longer need mashmap, as what you are essentially doing is treating the entire genome as a ""decoy"". As @k3yavi alludes, SA is still useful when you need to run in a very memory-constrained environment. After adopting the new [pufferfish-based](https://github.com/COMBINE-lab/pufferfish/tree/develop) index, the size of the transcriptome plush mashmap 2 decoys becomes considerably smaller than the previous size of the transcriptome in earlier versions of salmon (<= 0.15.0). However, depending on the organism, indexing the entire genome as decoy, even though it yields the best accuracy, does require a bit more memory, as specified in the release notes for the 0.99 betas and 1.0.0. 3) Yes; it is still possible to use `salmon index` without any decoy sequence. In this case, one can expect results similar to if you had aligned to the target transcriptome using Bowtie2. In this case, you perform indexing by simply not providing any `--decoy` flag to the `index` command. In that case, all of the records in the target fasta will be treated as valid and quantifiable targets. Of course, for reasons detailed in the pre-print --- the high _sensitivity_ of both Bowtie2 and selective-alignment --- we recommend including either mashmap-derived decoys or the organism's genome as a decoy whenever possible. . 4) Related to @k3yavi's response and my elaboration above: we have dropped quasi-mapping from 1.0.0 (though something akin to it may return in the future if there is sufficient demand and if the shortcomings described in the manuscript can be overcome). However, as I mention in part 3 above, this doesn't mean it's not possible to use v1.0.0 without an explicit decoy sequence. The `--decoy` flag of the indexing command is optional, not required. We will update this in the documentation making it more explicit. However, as @k3yavi points out, it is true that if you wish to use quasi-mapping and selective-alig",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549195390
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549200108:185,Deployability,update,updated,185,@rob-p and @k3yavi . Thanks for your answers. **_Suggestion_**: It will be great if the [getting started with Salmon document](https://combine-lab.github.io/salmon/getting_started/) is updated to reflect all possible scenarios you list out above. It might also be better to pull off that document till it gets updated OR if it is redirected to something more pertinent.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549200108
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549200108:310,Deployability,update,updated,310,@rob-p and @k3yavi . Thanks for your answers. **_Suggestion_**: It will be great if the [getting started with Salmon document](https://combine-lab.github.io/salmon/getting_started/) is updated to reflect all possible scenarios you list out above. It might also be better to pull off that document till it gets updated OR if it is redirected to something more pertinent.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549200108
https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549220121:22,Deployability,release,release,22,"@rob-p @k3yavi . With release of salmon/v1.0, do you have any recommendations for `salmon quant command line for QuantSeq`. Many users (me included) would [like to hear from you to the question posted here](https://github.com/COMBINE-lab/salmon/issues/365)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/442#issuecomment-549220121
https://github.com/COMBINE-lab/salmon/issues/444#issuecomment-550387405:408,Availability,down,downloadable,408,"Hi @tamuanand ,. I don't understand the question. I think there is some mis understanding, genome is the decoys you have to just cat it with the transcriptome to create the gentrome, no extra processing is required. Unless, your suggestion is to maintain the concatenated transcriptome + genome in a tar ball ? If it is then I don't really see the use case as both the genome and transcriptome can be easily downloadable from either ensembl or gencode.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/444#issuecomment-550387405
https://github.com/COMBINE-lab/salmon/issues/444#issuecomment-550389858:52,Availability,avail,available,52,@rob-p . Yes - you are correct. Would you be making available pre-computed SAF gentrome indices as tarball on github or dropbox.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/444#issuecomment-550389858
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-550391177:186,Performance,perform,performs,186,"Hi @kaukrise ,. Thanks for the very interesting question. I don't think there is any theoretical limit wrt the alevin's method, however, it would be interesting to check how does alevin performs when we increase the CB length wrt the running time. The 20 length bound was just for sanity checking and can be increased, like you already did.; I'd be very interested, if possible, in hearing back about your experience with alevin using longer length CB both wrt running time and gene expression estimates generated. Also if I may ask what's the reason behind using this long CB ? Are you expecting tons of real cells, if there is we can think about improving alevin even more, in my experience, we have generally seen individual 10x experiment with ~20k cells max. Even the 1.3M dataset is 164 separate experiments.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-550391177
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-550391177:281,Safety,sanity check,sanity checking,281,"Hi @kaukrise ,. Thanks for the very interesting question. I don't think there is any theoretical limit wrt the alevin's method, however, it would be interesting to check how does alevin performs when we increase the CB length wrt the running time. The 20 length bound was just for sanity checking and can be increased, like you already did.; I'd be very interested, if possible, in hearing back about your experience with alevin using longer length CB both wrt running time and gene expression estimates generated. Also if I may ask what's the reason behind using this long CB ? Are you expecting tons of real cells, if there is we can think about improving alevin even more, in my experience, we have generally seen individual 10x experiment with ~20k cells max. Even the 1.3M dataset is 164 separate experiments.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-550391177
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:1090,Performance,perform,performance,1090,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:463,Safety,sanity check,sanity check,463,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:784,Safety,detect,detection,784,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:317,Usability,guid,guides,317,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490:329,Usability,guid,guides,329,"Thank you for the swift answer!. We are working with [BD Rhapsody](https://www.bdbiosciences.com/en-us/instruments/research-instruments/single-cell-multiomics/single-cell-analysis-system), which uses a complex barcode structure (you can read about this in their [bioinformatics handbook](https://www.bd.com/documents/guides/user-guides/GMX_BD-Rhapsody-genomics-informatics_UG_EN.pdf) on page 14). The extracted, combined CB is 27bp long, which is why the default sanity check was too low for our purposes. In terms of cell numbers, BD Rhapsody appears to generate a lot of ""false-positive cells"", actually (we are seeing up to 90% of false positives). This is expected, and also mentioned in their bioinformatics handbook (pages 23-25), but appears to be an issue for the alevin cell detection: with standard settings this is approximately two orders of magnitude lower than expected, `--expectCells` improves matters drastically, however. We have opted for removing the false positives in post-processing ourselves - the low count depth population is very easily identifiable. In terms of performance, a complete alevin run on 150M reads (25k expected cells) takes around 1.5 hours using 10 threads, which is perfectly reasonable for us.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-551083490
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-565298381:100,Testability,test,testing,100,"My apologies for the late reply, somehow I missed the reply.; I am glad to hear that and thanks for testing alevin with BD Rhapsody.; Let us know if you need help with anything else, we'd be happy to help. Closing this issue but feel free to reopen.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-565298381
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777836526:1336,Availability,error,error,1336,"Could you please increase the maximum barcode length, as per this issue [i.e. [here](https://github.com/COMBINE-lab/salmon/blob/0813a0a287c2bd80071511830befe5d786a59ad1/src/AlevinUtils.cpp#L926)]? Salmon is still complaining when I use a converted file with the linker sequences spliced out, having a concatenated cell barcode length of 27, and there doesn't seem to be any user-definable option to modify or ignore the limit:. ```; gringer@elegans:/mnt/ufds/jmayer$ salmon alevin -l ISR -1 normalised_H2GYLDRXY_1_210203_FD09251586_Other_CGAGGCTG_R_210203_DAVGAL_INDEXLIBNOVASEQ_M001_R1.fastq.gz normalised_H2GYLDRXY_2_210203_FD09251586_Other_CGAGGCTG_R_210203_DAVGAL_INDEXLIBNOVASEQ_M001_R1.fastq.gz -2 H2GYLDRXY_1_210203_FD09251586_Other_CGAGGCTG_R_210203_DAVGAL_INDEXLIBNOVASEQ_M001_R2.fastq.gz -2 H2GYLDRXY_2_210203_FD09251586_Other_CGAGGCTG_R_210203_DAVGAL_INDEXLIBNOVASEQ_M001_R2.fastq.gz -i /mnt/ufds/salmon/gencode_M23/salmon_1.4.0_decoy_M23 -p 10 -o salmon_1.4_5_27_8_JM_2021-02-12 --tgMap txp2gene.txt --end 5 --barcodeLength 27 --umiLength 8; [2021-02-12 11:01:37.654] [alevinLog] [warning] Note: the use of --end, --barcodeLength and --umiLength to describe the barcode and umi geometry is deprecated. Please start using the `--barcode-geometry` and `--umi-geometry` options instead.; [2021-02-12 11:01:37.655] [alevinLog] [error] Barcode length (27) was not in the required length range [1, 20].; Exiting now.; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777836526
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777881915:116,Safety,avoid,avoid,116,"As an alternative, documenting the *geometry* format for specifying custom barcodes would be helpful. This seems to avoid the barcode length issue. From what I can tell, the format is `<readNum>[start-end]`, i.e. for my case:. --umi-geometry '1[28-35]' --bc-geometry '1[1-27]' --read-geometry '2[1-end]'",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777881915
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:1587,Availability,avail,available,1587,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:1761,Availability,robust,robust,1761,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:188,Deployability,release,release,188,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:148,Modifiability,flexible,flexible,148,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:1783,Modifiability,flexible,flexible,1783,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:358,Usability,learn,learned,358,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:644,Usability,intuit,intuitive,644,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823:1221,Usability,simpl,simply,1221,"Hi @gringer,. Yes, we can add a section for this in the docs. It will replace the old way for specifying geometry soon, as its just easier and more flexible. We talk about it in the 1.4.0 release notes. I copy the relevant info below (@k3yavi pulled for the 1-based indexing and won out ... this time):. generic barcode / umi / read geometry syntax : Alevin learned to support a generic syntax to specify the read sequence that should be used for barcodes, UMIs and the read sequence. The syntax allows one to specify how the pattern corresponding to the barcode, UMI, and read sequence should be pieced together, and the syntax is meant to be intuitive and general. For example, one can specify the 10Xv2 geometry in the following manner using the generic syntax:. --read-geometry 2[1-end] --bc-geometry 1[1-16] --umi-geometry 1[17-26]. This specifies that the ""sequence"" read (the biological sequence to be aligned) comes from read 2, and it spans from the first index 1 (this syntax used 1-based indexing) until the end of the read. Likewise, the barcode derives from read 1 and occupies positions 1-16, and the UMI comes from read 1 and occupies positions 17-26. The syntax can specify multiple ranges, and they will simply be concatenated together to produce the string. For example, one could specify --bc-geometry 1[1-8,16-23] to designate that the barcode should be taken from the substring in positions 1-8 of read 1 followed by the substring in positions 16-23 of read 1. It is even possible to have the string pieced together across both reads, but that functionality is only available if you are running with --rad or --sketch and preparing a RAD file for alevin-fry. If you are running classic alevin, the barcode must reside on a single read. The robust parsing of the flexible geometry syntax is made possible by the cpp-peglib project.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/445#issuecomment-777884823
https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-552987872:267,Security,validat,validateMappings,267,"Hi @CloXD . The library is in the container in `/usr/local/lib`, but isn't being found by default for some reason. Can you try the following?. ```; singularity exec -e ""LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH"" docker://combinelab/salmon:latest salmon quant --validateMappings -l A -p ${threads} -o ./salmon_map -i ${salmon_index} -1 ${file_1} -2 ${file_2}; ```. and let me know if that works for you. I'll have to figure out how to make sure it's in the path by default.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-552987872
https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722:55,Availability,error,error,55,"Hi,; it goes literally in panic: . ```; panic: runtime error: slice bounds out of range. goroutine 1 [running]:; github.com/sylabs/singularity/internal/pkg/util/uri.Split(0x7ffd422b2b65, 0x1f, 0xc00003c195, 0xc0004852f0, 0xc0004e5c78, 0x929217); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/internal/pkg/util/uri/uri.go:104 +0x13e; github.com/sylabs/singularity/cmd/singularity/cli.replaceURIWithImage(0x19d2a60, 0xc0000b8900, 0x11, 0x12); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/actions.go:189 +0x5d; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).execute(0x19d2a60, 0xc000030160, 0x12, 0x12, 0x19d2a60, 0xc000030160); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:755 +0x4ed; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x19d65c0, 0x0, 0xf6, 0xfc0b01); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:852 +0x2fd; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).Execute(0x19d65c0, 0x4, 0x1133611); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:800 +0x2b; github.com/sylabs/singularity/cmd/singularity/cli.ExecuteSingularity(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/singularity.go:114 +0x110; main.main(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli.go:16 +0x20; ```; with -e you clean environment before running container.; I haven't found how to add the environmental variable, but logging in as shell and exporting the variable, it works ( or at least, I discover that I have to rebuild the index since it was built with the old verision in RapMap). I'll see if there is another way to import the variable or I'll build an image with the env; Thanks ; Claudio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722
https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722:1705,Modifiability,variab,variable,1705,"Hi,; it goes literally in panic: . ```; panic: runtime error: slice bounds out of range. goroutine 1 [running]:; github.com/sylabs/singularity/internal/pkg/util/uri.Split(0x7ffd422b2b65, 0x1f, 0xc00003c195, 0xc0004852f0, 0xc0004e5c78, 0x929217); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/internal/pkg/util/uri/uri.go:104 +0x13e; github.com/sylabs/singularity/cmd/singularity/cli.replaceURIWithImage(0x19d2a60, 0xc0000b8900, 0x11, 0x12); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/actions.go:189 +0x5d; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).execute(0x19d2a60, 0xc000030160, 0x12, 0x12, 0x19d2a60, 0xc000030160); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:755 +0x4ed; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x19d65c0, 0x0, 0xf6, 0xfc0b01); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:852 +0x2fd; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).Execute(0x19d65c0, 0x4, 0x1133611); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:800 +0x2b; github.com/sylabs/singularity/cmd/singularity/cli.ExecuteSingularity(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/singularity.go:114 +0x110; main.main(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli.go:16 +0x20; ```; with -e you clean environment before running container.; I haven't found how to add the environmental variable, but logging in as shell and exporting the variable, it works ( or at least, I discover that I have to rebuild the index since it was built with the old verision in RapMap). I'll see if there is another way to import the variable or I'll build an image with the env; Thanks ; Claudio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722
https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722:1757,Modifiability,variab,variable,1757,"Hi,; it goes literally in panic: . ```; panic: runtime error: slice bounds out of range. goroutine 1 [running]:; github.com/sylabs/singularity/internal/pkg/util/uri.Split(0x7ffd422b2b65, 0x1f, 0xc00003c195, 0xc0004852f0, 0xc0004e5c78, 0x929217); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/internal/pkg/util/uri/uri.go:104 +0x13e; github.com/sylabs/singularity/cmd/singularity/cli.replaceURIWithImage(0x19d2a60, 0xc0000b8900, 0x11, 0x12); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/actions.go:189 +0x5d; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).execute(0x19d2a60, 0xc000030160, 0x12, 0x12, 0x19d2a60, 0xc000030160); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:755 +0x4ed; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x19d65c0, 0x0, 0xf6, 0xfc0b01); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:852 +0x2fd; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).Execute(0x19d65c0, 0x4, 0x1133611); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:800 +0x2b; github.com/sylabs/singularity/cmd/singularity/cli.ExecuteSingularity(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/singularity.go:114 +0x110; main.main(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli.go:16 +0x20; ```; with -e you clean environment before running container.; I haven't found how to add the environmental variable, but logging in as shell and exporting the variable, it works ( or at least, I discover that I have to rebuild the index since it was built with the old verision in RapMap). I'll see if there is another way to import the variable or I'll build an image with the env; Thanks ; Claudio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722
https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722:1935,Modifiability,variab,variable,1935,"Hi,; it goes literally in panic: . ```; panic: runtime error: slice bounds out of range. goroutine 1 [running]:; github.com/sylabs/singularity/internal/pkg/util/uri.Split(0x7ffd422b2b65, 0x1f, 0xc00003c195, 0xc0004852f0, 0xc0004e5c78, 0x929217); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/internal/pkg/util/uri/uri.go:104 +0x13e; github.com/sylabs/singularity/cmd/singularity/cli.replaceURIWithImage(0x19d2a60, 0xc0000b8900, 0x11, 0x12); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/actions.go:189 +0x5d; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).execute(0x19d2a60, 0xc000030160, 0x12, 0x12, 0x19d2a60, 0xc000030160); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:755 +0x4ed; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x19d65c0, 0x0, 0xf6, 0xfc0b01); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:852 +0x2fd; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).Execute(0x19d65c0, 0x4, 0x1133611); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:800 +0x2b; github.com/sylabs/singularity/cmd/singularity/cli.ExecuteSingularity(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/singularity.go:114 +0x110; main.main(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli.go:16 +0x20; ```; with -e you clean environment before running container.; I haven't found how to add the environmental variable, but logging in as shell and exporting the variable, it works ( or at least, I discover that I have to rebuild the index since it was built with the old verision in RapMap). I'll see if there is another way to import the variable or I'll build an image with the env; Thanks ; Claudio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722
https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722:1719,Testability,log,logging,1719,"Hi,; it goes literally in panic: . ```; panic: runtime error: slice bounds out of range. goroutine 1 [running]:; github.com/sylabs/singularity/internal/pkg/util/uri.Split(0x7ffd422b2b65, 0x1f, 0xc00003c195, 0xc0004852f0, 0xc0004e5c78, 0x929217); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/internal/pkg/util/uri/uri.go:104 +0x13e; github.com/sylabs/singularity/cmd/singularity/cli.replaceURIWithImage(0x19d2a60, 0xc0000b8900, 0x11, 0x12); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/actions.go:189 +0x5d; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).execute(0x19d2a60, 0xc000030160, 0x12, 0x12, 0x19d2a60, 0xc000030160); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:755 +0x4ed; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).ExecuteC(0x19d65c0, 0x0, 0xf6, 0xfc0b01); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:852 +0x2fd; github.com/sylabs/singularity/vendor/github.com/spf13/cobra.(*Command).Execute(0x19d65c0, 0x4, 0x1133611); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/vendor/github.com/spf13/cobra/command.go:800 +0x2b; github.com/sylabs/singularity/cmd/singularity/cli.ExecuteSingularity(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli/singularity.go:114 +0x110; main.main(); 	/tools/others_tools/Go/go-1.11.2/src/github.com/sylabs/singularity/cmd/singularity/cli.go:16 +0x20; ```; with -e you clean environment before running container.; I haven't found how to add the environmental variable, but logging in as shell and exporting the variable, it works ( or at least, I discover that I have to rebuild the index since it was built with the old verision in RapMap). I'll see if there is another way to import the variable or I'll build an image with the env; Thanks ; Claudio",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553005722
https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553008756:66,Deployability,update,updated,66,Interesting! That looks docker / singularity related. I pushed an updated latest tag with the proper ld library path. Perhaps pulling thay again will resolve it?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553008756
https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553016971:16,Availability,error,error,16,"The singularity error must be related to the -e argument... I'll try tomorrow, but I think that will solve the problem, thank you very much!; ###; It works, thanks again.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/447#issuecomment-553016971
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:962,Deployability,Pipeline,Pipeline,962,"@rob-p Could it be that I am not using the correct command line `salmon quant` options for Lexogen/QuantSeq _(this will be referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1016,Energy Efficiency,Adapt,Adapter,1016,"e correct command line `salmon quant` options for Lexogen/QuantSeq _(this will be referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximpo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1101,Energy Efficiency,Adapt,Adapter,1101,"referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1177,Energy Efficiency,Adapt,Adapter,1177," --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transcripts and the decoys - this lead me to post my original question on this thread.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1273,Energy Efficiency,Adapt,Adapter,1273,"--noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transcripts and the decoys - this lead me to post my original question on this thread.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:160,Integrability,message,message,160,"@rob-p Could it be that I am not using the correct command line `salmon quant` options for Lexogen/QuantSeq _(this will be referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:434,Integrability,message,message,434,"@rob-p Could it be that I am not using the correct command line `salmon quant` options for Lexogen/QuantSeq _(this will be referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1016,Integrability,Adapter,Adapter,1016,"e correct command line `salmon quant` options for Lexogen/QuantSeq _(this will be referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximpo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1101,Integrability,Adapter,Adapter,1101,"referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1177,Integrability,Adapter,Adapter,1177," --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transcripts and the decoys - this lead me to post my original question on this thread.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1273,Integrability,Adapter,Adapter,1273,"--noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transcripts and the decoys - this lead me to post my original question on this thread.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1016,Modifiability,Adapt,Adapter,1016,"e correct command line `salmon quant` options for Lexogen/QuantSeq _(this will be referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximpo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1101,Modifiability,Adapt,Adapter,1101,"referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1177,Modifiability,Adapt,Adapter,1177," --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transcripts and the decoys - this lead me to post my original question on this thread.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:1273,Modifiability,Adapt,Adapter,1273,"--noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in my final summarized table (after tximport). I got a colSum for all my samples and then checked the numbers for the transcripts and the decoys - this lead me to post my original question on this thread.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195:226,Security,validat,validateMappings,226,"@rob-p Could it be that I am not using the correct command line `salmon quant` options for Lexogen/QuantSeq _(this will be referred to as QS in the rest of the message(s))_ ?. `salmon quant --threads 16 --noLengthCorrection --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`. I chose the above command line options (`especially --noLengthCorrection`) based on [Rob's message here](https://groups.google.com/d/msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ) and a [thread here](https://github.com/COMBINE-lab/salmon/issues/108). Let me elaborate the big picture of my analyses and give more details about how I came up with the mapping numbers in my original post. Big Picture - DEG identification for samples sequenced by ILMN (whole transcript method) and QS (3' method) - [something similar to this paper](https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-018-5393-3). Bioinformatics Pipeline(s) for both ILMN and QS :. 1. HISAT Method : Adapter/Quality Trimming, Hisat2-HTSEQ, Get_Count_Table, DESeq; 2. STAR_RSEM Method: Adapter/Quality Trimming, STAR_RSEM, Get_Count_Table, DESeq; 3. SAF Method: Adapter/Quality Trimming, SAF_SALMON, Get_Count_Table, DESeq; 4. Quasi-Mapping or TXOME Method: Adapter/Quality Trimming, TXOME_SALMON, Get_Count_Table, DESeq. I used UpSetR plots for comparisons of sets of DEGs from each method just [as you have shown in your recent preprint](https://www.biorxiv.org/content/10.1101/657874v1.full). In the ILMN analyses, there is great concordance between the SAF method and HISAT/STAR_RSEM method. However, in the QS analyses, there is very limited concordance between SAF and the HISAT/STAR_RSEM method. For QS analyses, the TXOME method shows great concordance with HISAT/STAR_RSEM. This finding made me wonder if this has to be something with my salmon quant command line options for QS. Therefore, I wanted to check how the QS expected counts for SAF method show up for all samples in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554768195
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554822134:510,Security,validat,validateMappings,510,"@rob-p More info - if you find it useful . The mapping stats as I calculated above for ILMN with SAF method - 91% to the transcripts and 9% to the decoys.. . To reiterate, this I what I did:. > expected counts for SAF method -- convert to final summarized table (after tximport). take colSum for all my samples and then checked the numbers for the transcripts and the decoys. For QS, I also used incompatPrior - the results are the same . ` salmon quant --threads 16 --noLengthCorrection --incompatPrior 0.0 --validateMappings --numBootstraps 100 -l SF -i <path_to_SAF_Gentrome_Index> -r <SE_READ_1.fq> -o <salmon_SE_READ_1>`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-554822134
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-564848220:459,Integrability,protocol,protocol,459,"Hi @k3yavi . Thanks for pointing me to the paper. . Some findings/questions from that paper:. 1. The authors do not mention which version of salmon they are using and whether they are using the TXOME or SA or SAF method. I am assuming they are using salmon v.0.12 or a prior version since they submitted their paper in May 2019 and hence, they are probably using the TXOME method; ; 2. The authors do not mention whether they use QuantSeq FWD or QuantSeq REV protocol. I am assuming they are using QuantSeq REV as they have the` salmon quant libtype to be SR`. 3. I think the authors should have used `-noLengthCorrection` with QuantSeq Data. @rob-p and @k3yavi Isn't this flag/option a salmon unique feature that [Rob introduced exclusively for QuantSeq](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ). . Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-564848220
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565245678:637,Integrability,protocol,protocols,637,"> Hi @tamuanand ,; > ; > I am not well versed in Lexongen Quantseq but the following paper is worth checking.; > https://www.nature.com/articles/s41598-019-55434-x , let us know if you have any thoughts. @k3yavi You mention that you are not well-versed in Lexogen Quantseq. Just to clarify, Lexogen is the company, Quantseq is the technology. . It is the same Quantseq that is mentioned in the salmon quant help text . ```; salmon quant --help-reads; ................................; --noLengthCorrection [experimental] : Entirely disables; length correction when estimating the; abundance of transcripts. This option; can be used with protocols where one; expects that fragments derive from; their underlying targets without regard; to that target's length (e.g. QuantSeq); ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565245678
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:2085,Integrability,protocol,protocol,2085,"odel for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how much difference it can create in generating the estimates. . I Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:1990,Safety,avoid,avoid,1990,"approach right. I believe, It takes time and understanding to develop a good model for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:2013,Testability,log,logic,2013,"odel for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how much difference it can create in generating the estimates. . I Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:2595,Testability,test,test,2595,"odel for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how much difference it can create in generating the estimates. . I Hope it helps .",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:1206,Usability,learn,learn,1206,"ort as many use cases as possible but since it involves multiples developers it's hard to know the intrinsic details about every use-case by all the developers. Having said that, salmon is primarily designed for bulk RNA-seq quantification and as the help page you shared says -- `noLengthCorrection` is experimental. . I agree with all three comments above regarding the paper I shared. In fact, that was my reading as well, like I said I am not sure about the intricacies involved with QuantSeq ""technology"" and that's why I forwarded to you for confirming. I understand the difference between Lexogen and Quantseq but what I meant was more data specific knowledge as I _personally_ don't find ""one model fits all"" kind of approach right. I believe, It takes time and understanding to develop a good model for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512:1813,Usability,intuit,intuition,1813,"approach right. I believe, It takes time and understanding to develop a good model for data generated from any technology and that's what we have been trying to do with salmon for years, piece by piece. Having said that, we don't mean to discourage people from trying salmon, that's one of the way we learn how can we improve the model even further. Now, coming back to your original question about using QuantSeq with salmon and how the paper above approach to solve it. I have a couple of thoughts:; 1.) Like you said, from the reading of their command line argument they didn't use the `nolengthcorrection` and I am surprised about the results myself. Since you have experience with the technology, you are best person to explore the difference in using and not using the length correction with salmon, that's why I shared.; 2.) Salmon models the transcript lengths in its quantification model. The basic intuition being longer length transcripts have higher probability of a read being sampled from them and has to be corrected for when using relative count metrices (like TPM) to avoid length bias. The logic behind `noLengthCorrection` is to _not_ correct for length for 3' protocol since we expect all the reads from one end of the transcript and if we do length correction, I hypothesize, we might end up biasing the estimates on the opposite direction; however the effect size of this hypothesis is still an open question and seemingly from the results from the paper it has minor effect. On the flip side may be it does have effect but their baseline estimates were not great and any improvement is good, for that again since you have experience with the data it's good to know / test what's going on.; 3.) A little experimental thought, although `noLengthCorrection` flag can generate decent estimates, it's actually fully disabling the length effect, which in my opinion we can do better as you look at Figure 1B of the paper it shows some length based affect but again we don't know how ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565256512
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:243,Integrability,protocol,protocol,243,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:441,Integrability,protocol,protocol,441,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:469,Integrability,protocol,protocol,469,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:1009,Integrability,depend,dependent,1009,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:481,Performance,perform,performing,481,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:616,Security,validat,validation,616,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:704,Testability,test,tested,704,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:1400,Testability,test,test,1400,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:368,Usability,simpl,simply,368,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:1090,Usability,learn,learning,1090,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540:1462,Usability,feedback,feedback,1462,"Hi @tamuanand,. Thanks again for your detailed questions and thoughts on this issue. Just to follow-up / expand a bit on what @k3yavi has said (and to answer your other question): Yes, one would imagine that, given the details of the QuantSeq protocol, turning off length correction would make the most sense. The main reason this flag is listed as _experimental_, is simply that it was designed based on the expected characteristics of the protocol. Conceptually, the protocol is performing tagged-end sequencing, and so there should be little-to-no length effect. However, since we haven't done extensive internal validation on QuantSeq data, we have left this flag as experimental until it is further tested by ourselves or others. > Also @rob-p , weren't you referring to the RSEM caveat with QuantSeq data analysis wherein one cannot ask RSEM to disable lengthCorrection and hence the count statistics might be misleading?. Correct; as far as we are aware, there is no way to disable the built-in length-dependent assumptions of RSEM. One could use the `--estimate-rspd` flag to allow learning of a non-uniform read distribution (the equivalent of `--posBias` in salmon), though it's unclear / unlikely if this would be as effective as fully disabling the length correction for this type of tagged-end data. If you have any good empirical assessment mechanism for QuantSeq data, and a chance to test out these different salmon options, we'd be happy to get feedback and discuss details further!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565285540
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565314277:427,Availability,avail,available,427,Hi; I reply as an author of the Scientific Reports paper referred to above. The salmon analysis was done in May 2018 using salmon v 0.9.1. I should have noted the version in the paper as I did for other software used - my slip up there.; It did take longer than expected to get the work written up and published. In the meantime there have been changes to Salmon. When I used it I am not sure the --noLengthCorrection flag was available ?? If it was then it wasn't apparent to me in the documentation.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565314277
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:513,Integrability,protocol,protocol,513,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:1028,Integrability,protocol,protocol,1028,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:685,Performance,optimiz,optimization,685,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:956,Performance,optimiz,optimize,956,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:292,Security,access,access,292,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:363,Testability,test,testing,363,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:722,Testability,test,testing,722,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848:1074,Usability,feedback,feedback,1074,"Hi @s1corley,. Congratulations on your publication! The `--noLengthCorrection` flag has been around for a long time (e.g. where it is suggested in the post to which @tamuanand [links](https://groups.google.com/forum/#!msg/sailfish-users/VIfqBwgF6xQ/fw-rgC_kAwAJ)). However, given our limited access to QuantSeq and our limited (student) bandwidth to do extensive testing on alternative tech, we have kept this flag marked as experimental. As I mention above, it was introduced since, _conceptually_, the QuantSeq protocol should not exhibit a length effect and so the one may not wish to account for the length when determining assignment probabilities during the variational Bayesian optimization. However, the empirical testing of this has been limited. Now that your paper is published, and contains what look to be some _very through_ assessment methodologies, we may be able to look into this and determine if there is anything we can do to, perhaps, optimize salmon even more for accurate quantification from the QuantSeq protocol. We would welcome any suggestions or feedback you may have. Congratulations again on the paper!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565474848
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150:1528,Availability,echo,echo,1528,"Hi @s1corley . As @rob-p mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper; >The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the [Lexogen Website data analysis pipeline for QuantSeq FWD](https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf) recommends using the below htseq command line. ```; htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt; ```; > QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; > stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the` libType argument from salmon quant should have been SF` . One way I checked these with my datasets was to run the salmon quant command 3 times - once with `libType A`, once with` libType SF` and once with `libType SR` -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p says - Congratulations once again on the paper.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150:619,Deployability,pipeline,pipeline,619,"Hi @s1corley . As @rob-p mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper; >The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the [Lexogen Website data analysis pipeline for QuantSeq FWD](https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf) recommends using the below htseq command line. ```; htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt; ```; > QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; > stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the` libType argument from salmon quant should have been SF` . One way I checked these with my datasets was to run the salmon quant command 3 times - once with `libType A`, once with` libType SF` and once with `libType SR` -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p says - Congratulations once again on the paper.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150:993,Deployability,pipeline,pipeline,993,"Hi @s1corley . As @rob-p mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper; >The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the [Lexogen Website data analysis pipeline for QuantSeq FWD](https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf) recommends using the below htseq command line. ```; htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt; ```; > QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; > stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the` libType argument from salmon quant should have been SF` . One way I checked these with my datasets was to run the salmon quant command 3 times - once with `libType A`, once with` libType SF` and once with `libType SR` -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p says - Congratulations once again on the paper.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150:1091,Deployability,pipeline,pipeline,1091,"Hi @s1corley . As @rob-p mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper; >The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the [Lexogen Website data analysis pipeline for QuantSeq FWD](https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf) recommends using the below htseq command line. ```; htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt; ```; > QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; > stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the` libType argument from salmon quant should have been SF` . One way I checked these with my datasets was to run the salmon quant command 3 times - once with `libType A`, once with` libType SF` and once with `libType SR` -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p says - Congratulations once again on the paper.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150:962,Integrability,protocol,protocol,962,"Hi @s1corley . As @rob-p mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper; >The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the [Lexogen Website data analysis pipeline for QuantSeq FWD](https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf) recommends using the below htseq command line. ```; htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt; ```; > QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; > stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the` libType argument from salmon quant should have been SF` . One way I checked these with my datasets was to run the salmon quant command 3 times - once with `libType A`, once with` libType SF` and once with `libType SR` -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p says - Congratulations once again on the paper.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150:121,Performance,optimiz,optimize,121,"Hi @s1corley . As @rob-p mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper; >The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the [Lexogen Website data analysis pipeline for QuantSeq FWD](https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf) recommends using the below htseq command line. ```; htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt; ```; > QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; > stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the` libType argument from salmon quant should have been SF` . One way I checked these with my datasets was to run the salmon quant command 3 times - once with `libType A`, once with` libType SF` and once with `libType SR` -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p says - Congratulations once again on the paper.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565653150
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552:2505,Availability,echo,echo,2505,"cify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the Lexogen Website data analysis pipeline for QuantSeq FWD<https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf> recommends using the below htseq command line. htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt. QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the libType argument from salmon quant should have been SF. One way I checked these with my datasets was to run the salmon quant command 3 times - once with libType A, once with libType SF and once with libType SR -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p<https://github.com/rob-p> says - Congratulations once again on the paper. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/449?email_source=notifications&email_token=AC4A5AGWBAOLTI4MOFLAJNDQYQN7FA5CNFSM4JOIEHZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEG3S5HQ#issuecomment-565653150>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AC4A5AFB7EMLYHVLVSHVLBDQYQN7FANCNFSM4JOIEHZQ>. Sample S1. meta_info.json. ""salmon_version"": ""0.9.1"",; ""samp_type"": ""none"",; ""quant_errors"": [],; ""num_libraries"": 1,; ""library_types"": [; ""SR""; ],; ""frag_dist_length"": 1001,; ""seq_bias_correct"": true,; ""gc_bias_correct"": true,; ""num_bias_bins"": 4096,; ""mapping_type"": ""mapping"",; ""num_targets"": 202863,; ""serialized_eq_classes"": false,; ""eq_class_properties"": [],; ""length_classes"": [; 513,; 656,; 1013,; 2240,; 104301; ],; ""index_seq_hash"": ""5734f6acf3be3aa5103c5302b5a0807dd46d217d",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552:1620,Deployability,pipeline,pipeline,1620,"__________; From: tamuanand <notifications@github.com>; Sent: Saturday, 14 December 2019 10:53 AM; To: COMBINE-lab/salmon <salmon@noreply.github.com>; Cc: Susan Corley <s.corley@unsw.edu.au>; Mention <mention@noreply.github.com>; Subject: Re: [COMBINE-lab/salmon] Salmon SAF method - Read mapping issue with Lexogen/QuantSeq data?? (#449). Hi @s1corley<https://github.com/s1corley>. As @rob-p<https://github.com/rob-p> mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper. The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the Lexogen Website data analysis pipeline for QuantSeq FWD<https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf> recommends using the below htseq command line. htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt. QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the libType argument from salmon quant should have been SF. One way I checked these with my datasets was to run the salmon quant command 3 times - once with libType A, once with libType SF and once with libType SR -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p<https://github.com/rob-p> says - Congratulations once again on the paper. —; You are rec",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552:1981,Deployability,pipeline,pipeline,1981,"://github.com/rob-p> mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper. The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the Lexogen Website data analysis pipeline for QuantSeq FWD<https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf> recommends using the below htseq command line. htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt. QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the libType argument from salmon quant should have been SF. One way I checked these with my datasets was to run the salmon quant command 3 times - once with libType A, once with libType SF and once with libType SR -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p<https://github.com/rob-p> says - Congratulations once again on the paper. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/449?email_source=notifications&email_token=AC4A5AGWBAOLTI4MOFLAJNDQYQN7FA5CNFSM4JOIEHZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEG3S5HQ#issuecomment-565653150>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AC4A5AFB7EMLYHVLVS",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552:2077,Deployability,pipeline,pipeline,2077,"hodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper. The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the Lexogen Website data analysis pipeline for QuantSeq FWD<https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf> recommends using the below htseq command line. htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt. QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the libType argument from salmon quant should have been SF. One way I checked these with my datasets was to run the salmon quant command 3 times - once with libType A, once with libType SF and once with libType SR -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p<https://github.com/rob-p> says - Congratulations once again on the paper. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/449?email_source=notifications&email_token=AC4A5AGWBAOLTI4MOFLAJNDQYQN7FA5CNFSM4JOIEHZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEG3S5HQ#issuecomment-565653150>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AC4A5AFB7EMLYHVLVSHVLBDQYQN7FANCNFSM4JOIEHZQ>. Sample S1. meta_info.json. ""salmon_version"":",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552:1950,Integrability,protocol,protocol,1950,"#449). Hi @s1corley<https://github.com/s1corley>. As @rob-p<https://github.com/rob-p> mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper. The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the Lexogen Website data analysis pipeline for QuantSeq FWD<https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf> recommends using the below htseq command line. htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt. QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; stranded in the sense orientation. For the QuantSeq REV pipeline -s reverse is used. Similar to the above htseq command line arguments, I think if you are using QuantSeq FWD, the libType argument from salmon quant should have been SF. One way I checked these with my datasets was to run the salmon quant command 3 times - once with libType A, once with libType SF and once with libType SR -- with QuantSeq FWD the estimated counts will be almost same with libType A and libType SF. I echo what @rob-p<https://github.com/rob-p> says - Congratulations once again on the paper. —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/449?email_source=notifications&email_token=AC4A5AGWBAOLTI4MOFLAJNDQYQN7FA5CNFSM4JOIEHZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEG3S5HQ#issuecomment-565653150>, or unsubscribe<http",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552:1124,Performance,optimiz,optimize,1124,"R library. flag As you can see the percentage of mapped reads is high and not consistent with incorrect strand mapping in my view. I did the salmon quantification both with and without using seq_bias and gc_bias corrections and got the same result. Info for both included. I was advised that a FWD library was used - this is a bit confusing given the success of running the SR option. I suggest you continue exploring with your own data in the current environment, which does differ from May 2018. ________________________________; From: tamuanand <notifications@github.com>; Sent: Saturday, 14 December 2019 10:53 AM; To: COMBINE-lab/salmon <salmon@noreply.github.com>; Cc: Susan Corley <s.corley@unsw.edu.au>; Mention <mention@noreply.github.com>; Subject: Re: [COMBINE-lab/salmon] Salmon SAF method - Read mapping issue with Lexogen/QuantSeq data?? (#449). Hi @s1corley<https://github.com/s1corley>. As @rob-p<https://github.com/rob-p> mentions, your paper could help assess different methodologies for quantification and also help optimize salmon further for QuantSeq. I would still like you to check if you have used salmon quant command line correctly for QuantSeq data analysis. Your paper briefly alludes to QuantSeq Forward in the Introduction section of the paper. The QuantSeq Forward kit has an oligo (dT) primer which contains the Illumina-specific Read 2 linker ... but the Methods section of your paper does not specify if you have used QuantSeq FWD or REV. Page 14 of the PDF from the Lexogen Website data analysis pipeline for QuantSeq FWD<https://www.bluebee.com/wp-content/uploads/2018/11/015UG108V0201-QuantSeq-Data-Analysis-Pipeline_2018-10-18.pdf> recommends using the below htseq command line. htseq-count -m intersection-nonempty -s yes -f bam -r pos $bam; $resource_dir/annotation.gtf > $bam_dir/read_counts.txt. QuantSeq is a stranded protocol. For the QuantSeq FWD pipeline the argument -s yes indicates; stranded in the sense orientation. For the QuantSeq REV pipeline -s ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565684552
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565686395:343,Integrability,protocol,protocol,343,Hi @s1corley . Thanks for your inputs and thanks for taking the time to respond here. You mention you attached the Salmon meta_info output - I guess the attachment did Not come through. @k3yavi @rob-p - any ideas why the attachment did not make it . Yes - I am surprised with the results using the SR salmon quant option with the QuantSeq FWD protocol.; >I was advised that a FWD library was used - this is a bit confusing given the success of running the SR option. @rob-p What should be the libType option one should set with the QuantSeq FWD protocol - I have explained above why the SF option would be appropriate one (based on what Lexogen recommends for use with htseq-count for QuantSeq FWD),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565686395
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565686395:545,Integrability,protocol,protocol,545,Hi @s1corley . Thanks for your inputs and thanks for taking the time to respond here. You mention you attached the Salmon meta_info output - I guess the attachment did Not come through. @k3yavi @rob-p - any ideas why the attachment did not make it . Yes - I am surprised with the results using the SR salmon quant option with the QuantSeq FWD protocol.; >I was advised that a FWD library was used - this is a bit confusing given the success of running the SR option. @rob-p What should be the libType option one should set with the QuantSeq FWD protocol - I have explained above why the SF option would be appropriate one (based on what Lexogen recommends for use with htseq-count for QuantSeq FWD),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565686395
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565692581:784,Integrability,protocol,protocol,784,"________________________________; From: tamuanand <notifications@github.com>; Sent: Saturday, 14 December 2019 4:42 PM; To: COMBINE-lab/salmon <salmon@noreply.github.com>; Cc: Susan Corley <s.corley@unsw.edu.au>; Mention <mention@noreply.github.com>; Subject: Re: [COMBINE-lab/salmon] Salmon SAF method - Read mapping issue with Lexogen/QuantSeq data?? (#449). Hi @s1corley<https://github.com/s1corley>. Thanks for your inputs and thanks for taking the time to respond here. You mention you attached the Salmon meta_info output - I guess the attachment did Not come through. @k3yavi<https://github.com/k3yavi> @rob-p<https://github.com/rob-p> - any ideas why the attachment did not make it. Yes - I am surprised with the results using the SR salmon quant option with the QuantSeq FWD protocol. I was advised that a FWD library was used - this is a bit confusing given the success of running the SR option. @rob-p<https://github.com/rob-p> What should be the libType option one should set with the QuantSeq FWD protocol - I have explained above why the SF option would be appropriate one (based on what Lexogen recommends for use with htseq-count for QuantSeq FWD). —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/449?email_source=notifications&email_token=AC4A5AHH5I66447AJKAIA6LQYRW4BA5CNFSM4JOIEHZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEG33A6Y#issuecomment-565686395>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AC4A5AEBKTGGPZ22F7IVIA3QYRW4BANCNFSM4JOIEHZQ>. Sample S1. meta_info.json. ""salmon_version"": ""0.9.1"",; ""samp_type"": ""none"",; ""quant_errors"": [],; ""num_libraries"": 1,; ""library_types"": [; ""SR""; ],; ""frag_dist_length"": 1001,; ""seq_bias_correct"": true,; ""gc_bias_correct"": true,; ""num_bias_bins"": 4096,; ""mapping_type"": ""mapping"",; ""num_targets"": 202863,; ""serialized_eq_classes"": false,; ""eq_class_properties"": [],; ""length_classes"": [; 513,; 656",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565692581
https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565692581:1010,Integrability,protocol,protocol,1010,"and <notifications@github.com>; Sent: Saturday, 14 December 2019 4:42 PM; To: COMBINE-lab/salmon <salmon@noreply.github.com>; Cc: Susan Corley <s.corley@unsw.edu.au>; Mention <mention@noreply.github.com>; Subject: Re: [COMBINE-lab/salmon] Salmon SAF method - Read mapping issue with Lexogen/QuantSeq data?? (#449). Hi @s1corley<https://github.com/s1corley>. Thanks for your inputs and thanks for taking the time to respond here. You mention you attached the Salmon meta_info output - I guess the attachment did Not come through. @k3yavi<https://github.com/k3yavi> @rob-p<https://github.com/rob-p> - any ideas why the attachment did not make it. Yes - I am surprised with the results using the SR salmon quant option with the QuantSeq FWD protocol. I was advised that a FWD library was used - this is a bit confusing given the success of running the SR option. @rob-p<https://github.com/rob-p> What should be the libType option one should set with the QuantSeq FWD protocol - I have explained above why the SF option would be appropriate one (based on what Lexogen recommends for use with htseq-count for QuantSeq FWD). —; You are receiving this because you were mentioned.; Reply to this email directly, view it on GitHub<https://github.com/COMBINE-lab/salmon/issues/449?email_source=notifications&email_token=AC4A5AHH5I66447AJKAIA6LQYRW4BA5CNFSM4JOIEHZ2YY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEG33A6Y#issuecomment-565686395>, or unsubscribe<https://github.com/notifications/unsubscribe-auth/AC4A5AEBKTGGPZ22F7IVIA3QYRW4BANCNFSM4JOIEHZQ>. Sample S1. meta_info.json. ""salmon_version"": ""0.9.1"",; ""samp_type"": ""none"",; ""quant_errors"": [],; ""num_libraries"": 1,; ""library_types"": [; ""SR""; ],; ""frag_dist_length"": 1001,; ""seq_bias_correct"": true,; ""gc_bias_correct"": true,; ""num_bias_bins"": 4096,; ""mapping_type"": ""mapping"",; ""num_targets"": 202863,; ""serialized_eq_classes"": false,; ""eq_class_properties"": [],; ""length_classes"": [; 513,; 656,; 1013,; 2240,; 104301; ],; ""index_seq_hash""",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/449#issuecomment-565692581
https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555136444:112,Availability,down,downstream,112,"Hi @vertesy ,. Thanks for asking the very interesting question.; I'd say the answer might depend on what's your downstream use case. Traditionally, no quantification pipeline, in my knowledge, has used the pre-mRNA counts alone to bump up the gene counts, however, recent method of estimating RNA-velocity does utilizes the intronic counts for extracting the ratio of spliced/unspliced counts. If you are interested in disjoint signals (gene count matrix) for spliced and unspliced molecules you can use the recent scheme of decoy indexing from our latest [preprint](https://www.biorxiv.org/content/10.1101/657874v2). We (mostly @csoneson) have been testing alevin with following scheme for generating spliced and unspliced counts. 1.) Spliced Counts: Index transcriptome w/ pre-mRNA sequence as the decoys.; 2) Unspliced Counts: Index pre-mRNA sequence w/ transcriptome as the decoys. The third case is a little tricky because if you index both pre-mRNA and transcriptome, due to relatively longer length of pre-mRNA sequence compared to transcripts it might end-up biasing the UMI deduplication algorithm towards unspliced counts. To summarize, the best way to have an additive spliced and unspliced counts is still an open area of research.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555136444
https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555136444:166,Deployability,pipeline,pipeline,166,"Hi @vertesy ,. Thanks for asking the very interesting question.; I'd say the answer might depend on what's your downstream use case. Traditionally, no quantification pipeline, in my knowledge, has used the pre-mRNA counts alone to bump up the gene counts, however, recent method of estimating RNA-velocity does utilizes the intronic counts for extracting the ratio of spliced/unspliced counts. If you are interested in disjoint signals (gene count matrix) for spliced and unspliced molecules you can use the recent scheme of decoy indexing from our latest [preprint](https://www.biorxiv.org/content/10.1101/657874v2). We (mostly @csoneson) have been testing alevin with following scheme for generating spliced and unspliced counts. 1.) Spliced Counts: Index transcriptome w/ pre-mRNA sequence as the decoys.; 2) Unspliced Counts: Index pre-mRNA sequence w/ transcriptome as the decoys. The third case is a little tricky because if you index both pre-mRNA and transcriptome, due to relatively longer length of pre-mRNA sequence compared to transcripts it might end-up biasing the UMI deduplication algorithm towards unspliced counts. To summarize, the best way to have an additive spliced and unspliced counts is still an open area of research.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555136444
https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555136444:90,Integrability,depend,depend,90,"Hi @vertesy ,. Thanks for asking the very interesting question.; I'd say the answer might depend on what's your downstream use case. Traditionally, no quantification pipeline, in my knowledge, has used the pre-mRNA counts alone to bump up the gene counts, however, recent method of estimating RNA-velocity does utilizes the intronic counts for extracting the ratio of spliced/unspliced counts. If you are interested in disjoint signals (gene count matrix) for spliced and unspliced molecules you can use the recent scheme of decoy indexing from our latest [preprint](https://www.biorxiv.org/content/10.1101/657874v2). We (mostly @csoneson) have been testing alevin with following scheme for generating spliced and unspliced counts. 1.) Spliced Counts: Index transcriptome w/ pre-mRNA sequence as the decoys.; 2) Unspliced Counts: Index pre-mRNA sequence w/ transcriptome as the decoys. The third case is a little tricky because if you index both pre-mRNA and transcriptome, due to relatively longer length of pre-mRNA sequence compared to transcripts it might end-up biasing the UMI deduplication algorithm towards unspliced counts. To summarize, the best way to have an additive spliced and unspliced counts is still an open area of research.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555136444
https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555136444:650,Testability,test,testing,650,"Hi @vertesy ,. Thanks for asking the very interesting question.; I'd say the answer might depend on what's your downstream use case. Traditionally, no quantification pipeline, in my knowledge, has used the pre-mRNA counts alone to bump up the gene counts, however, recent method of estimating RNA-velocity does utilizes the intronic counts for extracting the ratio of spliced/unspliced counts. If you are interested in disjoint signals (gene count matrix) for spliced and unspliced molecules you can use the recent scheme of decoy indexing from our latest [preprint](https://www.biorxiv.org/content/10.1101/657874v2). We (mostly @csoneson) have been testing alevin with following scheme for generating spliced and unspliced counts. 1.) Spliced Counts: Index transcriptome w/ pre-mRNA sequence as the decoys.; 2) Unspliced Counts: Index pre-mRNA sequence w/ transcriptome as the decoys. The third case is a little tricky because if you index both pre-mRNA and transcriptome, due to relatively longer length of pre-mRNA sequence compared to transcripts it might end-up biasing the UMI deduplication algorithm towards unspliced counts. To summarize, the best way to have an additive spliced and unspliced counts is still an open area of research.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555136444
https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555288890:194,Energy Efficiency,efficient,efficient,194,"@k3yavi It seems like indexing both mature and un-spliced transcripts in one index, quantifying them jointly, and then post processing (re-normalizing) the two feature types could be a bit more efficient. Is there any obvious advantage to explicitly specifying which features are decoys vs. mapping and quantifying everything in one go?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/450#issuecomment-555288890
https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-555998732:221,Availability,ping,ping,221,"Thanks for the report, @chilampoon. I think this is due to an arbitrary (but fixable) limitation on the length of identifier names in the input. Can you look into this, @fataltes, and fix in upstream indexing code. We'll ping back here once we have a fix. I believe version 0.15.0 should not have this issue if you are in immediate need.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-555998732
https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-558448799:15,Availability,error,error,15,I get the same error in buliding 3' UTR index and expect the next version.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-558448799
https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-558449963:86,Deployability,release,release,86,"Right; so this has been fixed upstream and the limitation will be removed in the next release. As @k3yavi says, one option is to modify the reference input names to be of length <255. The other option is to make use of the 0.15.0 release, which does not have this limitation, until the next release that fixes this under the pufferfish-based index.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-558449963
https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-558449963:230,Deployability,release,release,230,"Right; so this has been fixed upstream and the limitation will be removed in the next release. As @k3yavi says, one option is to modify the reference input names to be of length <255. The other option is to make use of the 0.15.0 release, which does not have this limitation, until the next release that fixes this under the pufferfish-based index.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-558449963
https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-558449963:291,Deployability,release,release,291,"Right; so this has been fixed upstream and the limitation will be removed in the next release. As @k3yavi says, one option is to modify the reference input names to be of length <255. The other option is to make use of the 0.15.0 release, which does not have this limitation, until the next release that fixes this under the pufferfish-based index.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/451#issuecomment-558449963
https://github.com/COMBINE-lab/salmon/issues/452#issuecomment-565297678:12,Deployability,install,installing,12,"Can you try installing trinity through bioconda ? It seems like an environment related issue. Since it's outside the scope of salmon tool and seems to be an issue with trinity, closing this issue but feel free to reopen if the bioconda solution didn't work.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/452#issuecomment-565297678
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581:383,Availability,avail,available,383,"Hi @shanmugavadivelps,. This is because, to properly find and link libiconv, the build requires a version of CMake that ships with FindIConv.cmake. So, to build salmon from source, you should have at least CMake version 3.12. Internally and on our continuous integration servers, we use version 3.15. . Also, I'll mention that it may not be essential to build from source. Salmon is available via Bioconda, and a docker image is available via DockerHub. Also, we have a pre-compiled binary that should work on many linux distributions available under our [releases](https://github.com/COMBINE-lab/salmon/releases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581:429,Availability,avail,available,429,"Hi @shanmugavadivelps,. This is because, to properly find and link libiconv, the build requires a version of CMake that ships with FindIConv.cmake. So, to build salmon from source, you should have at least CMake version 3.12. Internally and on our continuous integration servers, we use version 3.15. . Also, I'll mention that it may not be essential to build from source. Salmon is available via Bioconda, and a docker image is available via DockerHub. Also, we have a pre-compiled binary that should work on many linux distributions available under our [releases](https://github.com/COMBINE-lab/salmon/releases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581:535,Availability,avail,available,535,"Hi @shanmugavadivelps,. This is because, to properly find and link libiconv, the build requires a version of CMake that ships with FindIConv.cmake. So, to build salmon from source, you should have at least CMake version 3.12. Internally and on our continuous integration servers, we use version 3.15. . Also, I'll mention that it may not be essential to build from source. Salmon is available via Bioconda, and a docker image is available via DockerHub. Also, we have a pre-compiled binary that should work on many linux distributions available under our [releases](https://github.com/COMBINE-lab/salmon/releases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581:248,Deployability,continuous,continuous,248,"Hi @shanmugavadivelps,. This is because, to properly find and link libiconv, the build requires a version of CMake that ships with FindIConv.cmake. So, to build salmon from source, you should have at least CMake version 3.12. Internally and on our continuous integration servers, we use version 3.15. . Also, I'll mention that it may not be essential to build from source. Salmon is available via Bioconda, and a docker image is available via DockerHub. Also, we have a pre-compiled binary that should work on many linux distributions available under our [releases](https://github.com/COMBINE-lab/salmon/releases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581:259,Deployability,integrat,integration,259,"Hi @shanmugavadivelps,. This is because, to properly find and link libiconv, the build requires a version of CMake that ships with FindIConv.cmake. So, to build salmon from source, you should have at least CMake version 3.12. Internally and on our continuous integration servers, we use version 3.15. . Also, I'll mention that it may not be essential to build from source. Salmon is available via Bioconda, and a docker image is available via DockerHub. Also, we have a pre-compiled binary that should work on many linux distributions available under our [releases](https://github.com/COMBINE-lab/salmon/releases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581:556,Deployability,release,releases,556,"Hi @shanmugavadivelps,. This is because, to properly find and link libiconv, the build requires a version of CMake that ships with FindIConv.cmake. So, to build salmon from source, you should have at least CMake version 3.12. Internally and on our continuous integration servers, we use version 3.15. . Also, I'll mention that it may not be essential to build from source. Salmon is available via Bioconda, and a docker image is available via DockerHub. Also, we have a pre-compiled binary that should work on many linux distributions available under our [releases](https://github.com/COMBINE-lab/salmon/releases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581:604,Deployability,release,releases,604,"Hi @shanmugavadivelps,. This is because, to properly find and link libiconv, the build requires a version of CMake that ships with FindIConv.cmake. So, to build salmon from source, you should have at least CMake version 3.12. Internally and on our continuous integration servers, we use version 3.15. . Also, I'll mention that it may not be essential to build from source. Salmon is available via Bioconda, and a docker image is available via DockerHub. Also, we have a pre-compiled binary that should work on many linux distributions available under our [releases](https://github.com/COMBINE-lab/salmon/releases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581:259,Integrability,integrat,integration,259,"Hi @shanmugavadivelps,. This is because, to properly find and link libiconv, the build requires a version of CMake that ships with FindIConv.cmake. So, to build salmon from source, you should have at least CMake version 3.12. Internally and on our continuous integration servers, we use version 3.15. . Also, I'll mention that it may not be essential to build from source. Salmon is available via Bioconda, and a docker image is available via DockerHub. Also, we have a pre-compiled binary that should work on many linux distributions available under our [releases](https://github.com/COMBINE-lab/salmon/releases).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557149581
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557680455:216,Availability,error,error,216,"If Salmon requires CMake 3.12 (or later), it would probably be worth bumping the `cmake_minimum_required` to this version in CMakeLists.txt. It's currently set to 3.9. This would at least point users to the ""proper"" error. At the moment, it's unclear if Salmon meant to provide its own FindIconv.cmake (like it does for several other packages) or not.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557680455
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557703985:165,Testability,test,testing,165,@cljacobs : yes; great point. This has been [fixed in develop](https://github.com/COMBINE-lab/salmon/blob/develop/CMakeLists.txt#L1). It was an oversight due to our testing infrastructure already having a newer version of CMake that didn't run into this problem.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557703985
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774500:309,Testability,test,testing,309,"Thanks for your input sir. On Sat, Nov 23, 2019 at 3:07 AM Rob Patro <notifications@github.com> wrote:. > @cljacobs <https://github.com/cljacobs> : yes; great point. This has been fixed; > in develop; > <https://github.com/COMBINE-lab/salmon/blob/develop/CMakeLists.txt#L1>.; > It was an oversight due to our testing infrastructure already having a; > newer version of CMake that didn't run into this problem.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HXRZ7T5IT4HH7XJX2DQVBGLJA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE66GMI#issuecomment-557703985>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTSAKLGW4THGPC2QSDQVBGLJANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774500
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774552:329,Availability,error,error,329,"Thanks for the help sir. On Sat, Nov 23, 2019 at 1:52 AM cljacobs <notifications@github.com> wrote:. > If Salmon requires CMake 3.12 (or later), it would probably be worth; > bumping the cmake_minimum_required to this version in CMakeLists.txt.; > It's currently set to 3.9.; >; > This would at least point users to the ""proper"" error. At the moment, it's; > unclear if Salmon meant to provide its own FindIconv.cmake (like it does; > for several other packages) or not.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HUD74JWFOQATIRUGM3QVA5QXA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE6YORY#issuecomment-557680455>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HT2IHF63T56PP3EECTQVA5QXANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774552
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568:534,Availability,avail,available,534,"Thanks sir. On Thu, Nov 21, 2019 at 9:27 PM Rob Patro <notifications@github.com> wrote:. > Hi @shanmugavadivelps <https://github.com/shanmugavadivelps>,; >; > This is because, to properly find and link libiconv, the build requires a; > version of CMake that ships with FindIConv.cmake. So, to build salmon from; > source, you should have at least CMake version 3.12. Internally and on our; > continuous integration servers, we use version 3.15.; >; > Also, I'll mention that it may not be essential to build from source.; > Salmon is available via Bioconda, and a docker image is available via; > DockerHub. Also, we have a pre-compiled binary that should work on many; > linux distributions available under our releases; > <https://github.com/COMBINE-lab/salmon/releases>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HW3GLUZR52T4BJKOFLQU2VYHA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2W3DI#issuecomment-557149581>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTVJB3TCKRKDY6YKI3QU2VYHANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568:580,Availability,avail,available,580,"Thanks sir. On Thu, Nov 21, 2019 at 9:27 PM Rob Patro <notifications@github.com> wrote:. > Hi @shanmugavadivelps <https://github.com/shanmugavadivelps>,; >; > This is because, to properly find and link libiconv, the build requires a; > version of CMake that ships with FindIConv.cmake. So, to build salmon from; > source, you should have at least CMake version 3.12. Internally and on our; > continuous integration servers, we use version 3.15.; >; > Also, I'll mention that it may not be essential to build from source.; > Salmon is available via Bioconda, and a docker image is available via; > DockerHub. Also, we have a pre-compiled binary that should work on many; > linux distributions available under our releases; > <https://github.com/COMBINE-lab/salmon/releases>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HW3GLUZR52T4BJKOFLQU2VYHA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2W3DI#issuecomment-557149581>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTVJB3TCKRKDY6YKI3QU2VYHANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568:692,Availability,avail,available,692,"Thanks sir. On Thu, Nov 21, 2019 at 9:27 PM Rob Patro <notifications@github.com> wrote:. > Hi @shanmugavadivelps <https://github.com/shanmugavadivelps>,; >; > This is because, to properly find and link libiconv, the build requires a; > version of CMake that ships with FindIConv.cmake. So, to build salmon from; > source, you should have at least CMake version 3.12. Internally and on our; > continuous integration servers, we use version 3.15.; >; > Also, I'll mention that it may not be essential to build from source.; > Salmon is available via Bioconda, and a docker image is available via; > DockerHub. Also, we have a pre-compiled binary that should work on many; > linux distributions available under our releases; > <https://github.com/COMBINE-lab/salmon/releases>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HW3GLUZR52T4BJKOFLQU2VYHA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2W3DI#issuecomment-557149581>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTVJB3TCKRKDY6YKI3QU2VYHANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568:392,Deployability,continuous,continuous,392,"Thanks sir. On Thu, Nov 21, 2019 at 9:27 PM Rob Patro <notifications@github.com> wrote:. > Hi @shanmugavadivelps <https://github.com/shanmugavadivelps>,; >; > This is because, to properly find and link libiconv, the build requires a; > version of CMake that ships with FindIConv.cmake. So, to build salmon from; > source, you should have at least CMake version 3.12. Internally and on our; > continuous integration servers, we use version 3.15.; >; > Also, I'll mention that it may not be essential to build from source.; > Salmon is available via Bioconda, and a docker image is available via; > DockerHub. Also, we have a pre-compiled binary that should work on many; > linux distributions available under our releases; > <https://github.com/COMBINE-lab/salmon/releases>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HW3GLUZR52T4BJKOFLQU2VYHA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2W3DI#issuecomment-557149581>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTVJB3TCKRKDY6YKI3QU2VYHANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568:403,Deployability,integrat,integration,403,"Thanks sir. On Thu, Nov 21, 2019 at 9:27 PM Rob Patro <notifications@github.com> wrote:. > Hi @shanmugavadivelps <https://github.com/shanmugavadivelps>,; >; > This is because, to properly find and link libiconv, the build requires a; > version of CMake that ships with FindIConv.cmake. So, to build salmon from; > source, you should have at least CMake version 3.12. Internally and on our; > continuous integration servers, we use version 3.15.; >; > Also, I'll mention that it may not be essential to build from source.; > Salmon is available via Bioconda, and a docker image is available via; > DockerHub. Also, we have a pre-compiled binary that should work on many; > linux distributions available under our releases; > <https://github.com/COMBINE-lab/salmon/releases>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HW3GLUZR52T4BJKOFLQU2VYHA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2W3DI#issuecomment-557149581>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTVJB3TCKRKDY6YKI3QU2VYHANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568:712,Deployability,release,releases,712,"Thanks sir. On Thu, Nov 21, 2019 at 9:27 PM Rob Patro <notifications@github.com> wrote:. > Hi @shanmugavadivelps <https://github.com/shanmugavadivelps>,; >; > This is because, to properly find and link libiconv, the build requires a; > version of CMake that ships with FindIConv.cmake. So, to build salmon from; > source, you should have at least CMake version 3.12. Internally and on our; > continuous integration servers, we use version 3.15.; >; > Also, I'll mention that it may not be essential to build from source.; > Salmon is available via Bioconda, and a docker image is available via; > DockerHub. Also, we have a pre-compiled binary that should work on many; > linux distributions available under our releases; > <https://github.com/COMBINE-lab/salmon/releases>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HW3GLUZR52T4BJKOFLQU2VYHA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2W3DI#issuecomment-557149581>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTVJB3TCKRKDY6YKI3QU2VYHANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568:763,Deployability,release,releases,763,"Thanks sir. On Thu, Nov 21, 2019 at 9:27 PM Rob Patro <notifications@github.com> wrote:. > Hi @shanmugavadivelps <https://github.com/shanmugavadivelps>,; >; > This is because, to properly find and link libiconv, the build requires a; > version of CMake that ships with FindIConv.cmake. So, to build salmon from; > source, you should have at least CMake version 3.12. Internally and on our; > continuous integration servers, we use version 3.15.; >; > Also, I'll mention that it may not be essential to build from source.; > Salmon is available via Bioconda, and a docker image is available via; > DockerHub. Also, we have a pre-compiled binary that should work on many; > linux distributions available under our releases; > <https://github.com/COMBINE-lab/salmon/releases>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HW3GLUZR52T4BJKOFLQU2VYHA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2W3DI#issuecomment-557149581>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTVJB3TCKRKDY6YKI3QU2VYHANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568:403,Integrability,integrat,integration,403,"Thanks sir. On Thu, Nov 21, 2019 at 9:27 PM Rob Patro <notifications@github.com> wrote:. > Hi @shanmugavadivelps <https://github.com/shanmugavadivelps>,; >; > This is because, to properly find and link libiconv, the build requires a; > version of CMake that ships with FindIConv.cmake. So, to build salmon from; > source, you should have at least CMake version 3.12. Internally and on our; > continuous integration servers, we use version 3.15.; >; > Also, I'll mention that it may not be essential to build from source.; > Salmon is available via Bioconda, and a docker image is available via; > DockerHub. Also, we have a pre-compiled binary that should work on many; > linux distributions available under our releases; > <https://github.com/COMBINE-lab/salmon/releases>.; >; > —; > You are receiving this because you were mentioned.; > Reply to this email directly, view it on GitHub; > <https://github.com/COMBINE-lab/salmon/issues/453?email_source=notifications&email_token=AN2V7HW3GLUZR52T4BJKOFLQU2VYHA5CNFSM4JP7NHKKYY3PNVWWK3TUL52HS4DFVREXG43VMVBW63LNMVXHJKTDN5WW2ZLOORPWSZGOEE2W3DI#issuecomment-557149581>,; > or unsubscribe; > <https://github.com/notifications/unsubscribe-auth/AN2V7HTVJB3TCKRKDY6YKI3QU2VYHANCNFSM4JP7NHKA>; > .; >. -- ; *Shanmugavadivel, P. S.*; *Scientist (Agricultural Biotechnology),*. *#216, Block A,*; *ICAR-Indian Institute of Pulses Research,*. *Min. of Agriculture & Farmers Welfare,*. *Govt. of India,Kanpur - 208 024.*; *email: shanmugavadivel.ps@icar.gov.in <shanmugavadivel.ps@icar.gov.in>*; *www.iipr.res.in <http://www.iipr.res.in>*",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-557774568
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:391,Availability,failure,failure,391,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1856,Availability,Error,Error,1856,".1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed corre",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2451,Availability,error,errors,2451,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2865,Availability,error,error,2865,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:54,Deployability,install,install,54,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:144,Deployability,install,installs,144,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:206,Deployability,install,installed,206,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:564,Deployability,release,releases,564,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2814,Deployability,install,installed,2814,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:250,Integrability,depend,dependency,250,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1994,Integrability,message,message,1994,"-p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.l",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2427,Modifiability,Config,Configuring,2427,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:804,Performance,load,load,804,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1768,Performance,Perform,Performing,1768,".1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed corre",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1806,Performance,Perform,Performing,1806,".1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed corre",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1474,Testability,test,tests,1474,"_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/sof",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1623,Testability,test,tests,1623," -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/so",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1779,Testability,Test,Test,1779,".1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed corre",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:1817,Testability,Test,Test,1817,".1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed corre",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2573,Testability,log,log,2573,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2683,Testability,log,log,2683,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2729,Testability,log,logs,2729,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:2985,Testability,log,log,2985,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:3053,Testability,log,log,3053,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:3072,Testability,log,log,3072,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:3141,Testability,log,log,3141,"r -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message):; Could NOT find Iconv (missing: Iconv_LIBRARY); Call Stack (most recent call first):; /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:378 (_FPHSA_FAILURE_MESSAGE); /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindIconv.cmake:120 (find_package_handle_standard_args); CMakeLists.txt:362 (find_package). -- Configuring incomplete, errors occurred!; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeOutput.log"".; See also ""/clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/CMakeFiles/CMakeError.log"".; ```; I'm also attaching the full CMake logs. This is right at the edge of my knowledge, so I'm not 100% sure I got libiconv installed correctly. Compilation completed without error, and I added the bin, include, and lib directories to PATH, CPATH, and LD_LIBRARY_PATH, respectively. [CMakeError.log](https://github.com/COMBINE-lab/salmon/files/6665942/CMakeError.log); [CMakeOutput.log](https://github.com/COMBINE-lab/salmon/files/6665943/CMakeOutput.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315:312,Usability,guid,guidance,312,"I've run into this (or a similar) issue attempting to install Salmon on the UC Berkeley HPC cluster. Iconv was present within one of our Python installs, but that didn't seem to have the header files, so I installed libiconv/1.16 thinking this was a dependency issue. Unfortunately this didn't seem to help. Any guidance would be greatly appreciated. Here is my build script to the point of failure:; ```sh; #!/bin/sh ; MODULE_HOME=/clusterfs/vector/home/groups/software/sl-7.x86_64; PACKAGE_NAME=salmon; GITHUB_URL=https://api.github.com/repos/COMBINE-lab/salmon/releases/latest; VERSION=$(curl -s $GITHUB_URL | \; grep '""tag_name"":' | \; cut -d : -f 2,3 | \; tr -d \"",v | \; xargs); LATEST_RELEASE=$(curl -s $GITHUB_URL | \; grep '""tarball_url""' | \; cut -d : -f 2,3 | \; tr -d \"", | \; xargs); module load gcc/7.4.0 cmake/3.15.1 boost/1.70.0-gcc libiconv/1.16; export CC=`which gcc`; export CXX=`which c++`. cd $MODULE_HOME; mkdir -p source/$PACKAGE_NAME/$VERSION; INSTALL_DIR=$MODULE_HOME/modules/$PACKAGE_NAME/$VERSION; mkdir -p $INSTALL_DIR; mkdir -p modfiles/$PACKAGE_NAME. cd source/$PACKAGE_NAME/$VERSION; wget $LATEST_RELEASE -O - | tar -xz --strip-components 1; cmake -DBOOST_ROOT=/global/software/sl-7.x86_64/modules/gcc/7.4.0/boost/1.70.0-gcc -DCMAKE_INSTALL_PREFIX=$INSTALL_DIR; make; ```; And the tail of the output from make:. ```; creating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/; inflating: /clusterfs/vector/home/groups/software/sl-7.x86_64/source/salmon/1.5.1/scripts/../external/pufferfish-salmon-v1.5.1/tests/compile_tests/int128_numeric_limits.cpp ; -- fetch PUFFERFISH exit code 0; -- Found ZLIB: /usr/lib64/libz.so (found version ""1.2.11"") ; -- Performing Test Iconv_IS_BUILT_IN; -- Performing Test Iconv_IS_BUILT_IN - Failed; CMake Error at /global/home/groups/consultsw/sl-7.x86_64/modules/cmake/3.15.1/share/cmake-3.15/Modules/FindPackageHandleStandardArgs.cmake:137 (message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/453#issuecomment-862737315
https://github.com/COMBINE-lab/salmon/issues/454#issuecomment-557558554:994,Deployability,pipeline,pipeline,994,"Hi @Zhuxitong, . You can easily change the k-mer length used for indexing by passing the desired value to the `-k` option of the `index` command. So, that part isn't technically a problem. The bigger issue is that Ribo-seq data doesn't follow the same basic model as RNA-seq data. That is, the coverage variation in RNA-seq is more often an issue to be corrected (e.g. evidence of bias during library prep / sequencing), whereas it is integral to the interpretation of Ribo-seq data (i.e. the peaks are primary features of interest). Therefore, it's not clear to me that using any RNA-seq abundance estimation software on Ribo-seq data ""off-the-shelf"" is conceptually the right thing to do, though you are welcome to experiment with it. However, there is some interesting work on combining transcript abundance profiles with Ribo-seq data to infer isoform-level information in the Ribo-seq data. For example, [this recent pre-print](https://www.biorxiv.org/content/10.1101/582031v3) provides a pipeline for this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/454#issuecomment-557558554
https://github.com/COMBINE-lab/salmon/issues/454#issuecomment-557558554:554,Usability,clear,clear,554,"Hi @Zhuxitong, . You can easily change the k-mer length used for indexing by passing the desired value to the `-k` option of the `index` command. So, that part isn't technically a problem. The bigger issue is that Ribo-seq data doesn't follow the same basic model as RNA-seq data. That is, the coverage variation in RNA-seq is more often an issue to be corrected (e.g. evidence of bias during library prep / sequencing), whereas it is integral to the interpretation of Ribo-seq data (i.e. the peaks are primary features of interest). Therefore, it's not clear to me that using any RNA-seq abundance estimation software on Ribo-seq data ""off-the-shelf"" is conceptually the right thing to do, though you are welcome to experiment with it. However, there is some interesting work on combining transcript abundance profiles with Ribo-seq data to infer isoform-level information in the Ribo-seq data. For example, [this recent pre-print](https://www.biorxiv.org/content/10.1101/582031v3) provides a pipeline for this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/454#issuecomment-557558554
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558565387:0,Availability,Error,Error,0,"Error confirmed. OS RHEL7, GCC 6.4.0, CMake 3.13.x",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558565387
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532:58,Availability,down,down,58,"I've run into this as well, but's proving tricky to track down. I'm by no mean a expert on build chains (or a C/C++ for that matter), but as far as I can figure, this issue seems specific to RedHat systems. I get exactly the same linking error on RH6 and RH7 using any GCC compiler I have access to on those systems (4.8.5, 5.2.0, 7.2.0). Compiling on an Arch system with GCC 9.2.0 (glibc 2.30) sees no issue. Unfortunately I don't have easy access to the same compiler versions on both systems. I'm compiling GCC 5.2.0 (with glibc2.28) on the Arch system to test this now, but it's going to be a little while before I even know if I have a working toolchain that can use it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532:238,Availability,error,error,238,"I've run into this as well, but's proving tricky to track down. I'm by no mean a expert on build chains (or a C/C++ for that matter), but as far as I can figure, this issue seems specific to RedHat systems. I get exactly the same linking error on RH6 and RH7 using any GCC compiler I have access to on those systems (4.8.5, 5.2.0, 7.2.0). Compiling on an Arch system with GCC 9.2.0 (glibc 2.30) sees no issue. Unfortunately I don't have easy access to the same compiler versions on both systems. I'm compiling GCC 5.2.0 (with glibc2.28) on the Arch system to test this now, but it's going to be a little while before I even know if I have a working toolchain that can use it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532:289,Security,access,access,289,"I've run into this as well, but's proving tricky to track down. I'm by no mean a expert on build chains (or a C/C++ for that matter), but as far as I can figure, this issue seems specific to RedHat systems. I get exactly the same linking error on RH6 and RH7 using any GCC compiler I have access to on those systems (4.8.5, 5.2.0, 7.2.0). Compiling on an Arch system with GCC 9.2.0 (glibc 2.30) sees no issue. Unfortunately I don't have easy access to the same compiler versions on both systems. I'm compiling GCC 5.2.0 (with glibc2.28) on the Arch system to test this now, but it's going to be a little while before I even know if I have a working toolchain that can use it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532:442,Security,access,access,442,"I've run into this as well, but's proving tricky to track down. I'm by no mean a expert on build chains (or a C/C++ for that matter), but as far as I can figure, this issue seems specific to RedHat systems. I get exactly the same linking error on RH6 and RH7 using any GCC compiler I have access to on those systems (4.8.5, 5.2.0, 7.2.0). Compiling on an Arch system with GCC 9.2.0 (glibc 2.30) sees no issue. Unfortunately I don't have easy access to the same compiler versions on both systems. I'm compiling GCC 5.2.0 (with glibc2.28) on the Arch system to test this now, but it's going to be a little while before I even know if I have a working toolchain that can use it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532:559,Testability,test,test,559,"I've run into this as well, but's proving tricky to track down. I'm by no mean a expert on build chains (or a C/C++ for that matter), but as far as I can figure, this issue seems specific to RedHat systems. I get exactly the same linking error on RH6 and RH7 using any GCC compiler I have access to on those systems (4.8.5, 5.2.0, 7.2.0). Compiling on an Arch system with GCC 9.2.0 (glibc 2.30) sees no issue. Unfortunately I don't have easy access to the same compiler versions on both systems. I'm compiling GCC 5.2.0 (with glibc2.28) on the Arch system to test this now, but it's going to be a little while before I even know if I have a working toolchain that can use it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558714532
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558716670:376,Modifiability,config,configure,376,"Thanks for the details @cljacobs. We'll see if we can get a Docker image up to reproduce this under RH7. Our development machine is ubuntu based, and our CI is CentOS. It also builds on the environment used by bioconda. So it looks like we'll need a RH image to reproduce this. Out of curiosity, does anything happen differently if you pass `-DNO_IPO=TRUE` during the `cmake` configure step? That disables interprocedural optimization (whole program link-time optimization).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558716670
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558716670:422,Performance,optimiz,optimization,422,"Thanks for the details @cljacobs. We'll see if we can get a Docker image up to reproduce this under RH7. Our development machine is ubuntu based, and our CI is CentOS. It also builds on the environment used by bioconda. So it looks like we'll need a RH image to reproduce this. Out of curiosity, does anything happen differently if you pass `-DNO_IPO=TRUE` during the `cmake` configure step? That disables interprocedural optimization (whole program link-time optimization).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558716670
https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558716670:460,Performance,optimiz,optimization,460,"Thanks for the details @cljacobs. We'll see if we can get a Docker image up to reproduce this under RH7. Our development machine is ubuntu based, and our CI is CentOS. It also builds on the environment used by bioconda. So it looks like we'll need a RH image to reproduce this. Out of curiosity, does anything happen differently if you pass `-DNO_IPO=TRUE` during the `cmake` configure step? That disables interprocedural optimization (whole program link-time optimization).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/455#issuecomment-558716670
https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-559801533:264,Deployability,release,release,264,"Hi Jamal,. Can you say something about the hardware (specifically, the CPU) you are running on? It looks like the compiles code contains a hardware instruction not supported by your CPU. Also, could you check if the precompiled linux binary from the tagged GitHub release has the same issur for you? Compiling from source would fix this for you, but thats probably more trouble than it's worth if there is another path.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-559801533
https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-559802958:94,Deployability,install,install,94,"Hi,; Thank you for your response!. I'm working from a server that has 48 CPU.; I'll try to re-install this version using the precompiled linux binary file from the release page, and let you know how it goes.; PS: I hadn't this issue with previous versions (0.14.1, 0.9..etc). Kind regards,; Jamal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-559802958
https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-559802958:164,Deployability,release,release,164,"Hi,; Thank you for your response!. I'm working from a server that has 48 CPU.; I'll try to re-install this version using the precompiled linux binary file from the release page, and let you know how it goes.; PS: I hadn't this issue with previous versions (0.14.1, 0.9..etc). Kind regards,; Jamal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-559802958
https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562051770:83,Deployability,install,installing,83,"Hi,; Unfortunately no. It did not work even using the binary file from github, nor installing using bioconda. I hope this will be fixed as soon as possible!; Best regards,; Jamal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562051770
https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562069760:537,Performance,cache,cache,537,"Hi,. I'm working on a GNU/Linux operating system ""4.19.10-200.fc28.x86_64"". Herewith the detailed information on CPU:; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 48; On-line CPU(s) list: 0-47; Thread(s) per core: 1; Core(s) per socket: 12; Socket(s): 4; NUMA node(s): 8; Vendor ID: AuthenticAMD; CPU family: 16; Model: 9; Model name: AMD Opteron(tm) Processor 6176; Stepping: 1; CPU MHz: 2300.000; CPU max MHz: 2300.0000; CPU min MHz: 800.0000; BogoMIPS: 4600.38; Virtualization: AMD-V; L1d cache: 64K; L1i cache: 64K; L2 cache: 512K; L3 cache: 5118K; NUMA node0 CPU(s): 0-5; NUMA node1 CPU(s): 6-11; NUMA node2 CPU(s): 12-17; NUMA node3 CPU(s): 18-23; NUMA node4 CPU(s): 24-29; NUMA node5 CPU(s): 30-35; NUMA node6 CPU(s): 36-41; NUMA node7 CPU(s): 42-47. Best regards,; Jamal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562069760
https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562069760:553,Performance,cache,cache,553,"Hi,. I'm working on a GNU/Linux operating system ""4.19.10-200.fc28.x86_64"". Herewith the detailed information on CPU:; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 48; On-line CPU(s) list: 0-47; Thread(s) per core: 1; Core(s) per socket: 12; Socket(s): 4; NUMA node(s): 8; Vendor ID: AuthenticAMD; CPU family: 16; Model: 9; Model name: AMD Opteron(tm) Processor 6176; Stepping: 1; CPU MHz: 2300.000; CPU max MHz: 2300.0000; CPU min MHz: 800.0000; BogoMIPS: 4600.38; Virtualization: AMD-V; L1d cache: 64K; L1i cache: 64K; L2 cache: 512K; L3 cache: 5118K; NUMA node0 CPU(s): 0-5; NUMA node1 CPU(s): 6-11; NUMA node2 CPU(s): 12-17; NUMA node3 CPU(s): 18-23; NUMA node4 CPU(s): 24-29; NUMA node5 CPU(s): 30-35; NUMA node6 CPU(s): 36-41; NUMA node7 CPU(s): 42-47. Best regards,; Jamal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562069760
https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562069760:568,Performance,cache,cache,568,"Hi,. I'm working on a GNU/Linux operating system ""4.19.10-200.fc28.x86_64"". Herewith the detailed information on CPU:; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 48; On-line CPU(s) list: 0-47; Thread(s) per core: 1; Core(s) per socket: 12; Socket(s): 4; NUMA node(s): 8; Vendor ID: AuthenticAMD; CPU family: 16; Model: 9; Model name: AMD Opteron(tm) Processor 6176; Stepping: 1; CPU MHz: 2300.000; CPU max MHz: 2300.0000; CPU min MHz: 800.0000; BogoMIPS: 4600.38; Virtualization: AMD-V; L1d cache: 64K; L1i cache: 64K; L2 cache: 512K; L3 cache: 5118K; NUMA node0 CPU(s): 0-5; NUMA node1 CPU(s): 6-11; NUMA node2 CPU(s): 12-17; NUMA node3 CPU(s): 18-23; NUMA node4 CPU(s): 24-29; NUMA node5 CPU(s): 30-35; NUMA node6 CPU(s): 36-41; NUMA node7 CPU(s): 42-47. Best regards,; Jamal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562069760
https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562069760:584,Performance,cache,cache,584,"Hi,. I'm working on a GNU/Linux operating system ""4.19.10-200.fc28.x86_64"". Herewith the detailed information on CPU:; Architecture: x86_64; CPU op-mode(s): 32-bit, 64-bit; Byte Order: Little Endian; CPU(s): 48; On-line CPU(s) list: 0-47; Thread(s) per core: 1; Core(s) per socket: 12; Socket(s): 4; NUMA node(s): 8; Vendor ID: AuthenticAMD; CPU family: 16; Model: 9; Model name: AMD Opteron(tm) Processor 6176; Stepping: 1; CPU MHz: 2300.000; CPU max MHz: 2300.0000; CPU min MHz: 800.0000; BogoMIPS: 4600.38; Virtualization: AMD-V; L1d cache: 64K; L1i cache: 64K; L2 cache: 512K; L3 cache: 5118K; NUMA node0 CPU(s): 0-5; NUMA node1 CPU(s): 6-11; NUMA node2 CPU(s): 12-17; NUMA node3 CPU(s): 18-23; NUMA node4 CPU(s): 24-29; NUMA node5 CPU(s): 30-35; NUMA node6 CPU(s): 36-41; NUMA node7 CPU(s): 42-47. Best regards,; Jamal.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/458#issuecomment-562069760
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:1871,Availability,echo,echo,1871,"=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/cmake/3.17.1-CentOS-vanilla/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/common/bin:/usr/common/modules/.local/bin:/usr/common/modules/bin; echo $LD_LIBRARY_PATH; /usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib64:/usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib:/usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/lib64:/usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/lib:/usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/lib; ls /usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/lib; libstaden-read.a libstaden-read.la libstaden-read.so libstaden-read.so.11 libstaden-read.so.11.1.0; ls /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/lib64; libgff.a. ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:2238,Availability,echo,echo,2238,"=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/cmake/3.17.1-CentOS-vanilla/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/usr/common/bin:/usr/common/modules/.local/bin:/usr/common/modules/bin; echo $LD_LIBRARY_PATH; /usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib64:/usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib:/usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/lib64:/usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/lib:/usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/lib; ls /usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/lib; libstaden-read.a libstaden-read.la libstaden-read.so libstaden-read.so.11 libstaden-read.so.11.1.0; ls /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/lib64; libgff.a. ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:495,Integrability,message,message,495,"I tried this again today with salmon 1.2.1 on CentOS 8 (with cmake 3.17.1). This time it could find libtbb but it still could not find Staden IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:546,Integrability,message,message,546,"I tried this again today with salmon 1.2.1 on CentOS 8 (with cmake 3.17.1). This time it could find libtbb but it still could not find Staden IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:598,Integrability,message,message,598,"I tried this again today with salmon 1.2.1 on CentOS 8 (with cmake 3.17.1). This time it could find libtbb but it still could not find Staden IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:662,Integrability,message,message,662,"I tried this again today with salmon 1.2.1 on CentOS 8 (with cmake 3.17.1). This time it could find libtbb but it still could not find Staden IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:758,Performance,load,load,758,"I tried this again today with salmon 1.2.1 on CentOS 8 (with cmake 3.17.1). This time it could find libtbb but it still could not find Staden IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:777,Performance,load,load,777,"I tried this again today with salmon 1.2.1 on CentOS 8 (with cmake 3.17.1). This time it could find libtbb but it still could not find Staden IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:797,Performance,load,load,797,"I tried this again today with salmon 1.2.1 on CentOS 8 (with cmake 3.17.1). This time it could find libtbb but it still could not find Staden IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:817,Performance,load,load,817,"I tried this again today with salmon 1.2.1 on CentOS 8 (with cmake 3.17.1). This time it could find libtbb but it still could not find Staden IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684:1045,Testability,log,log,1045," IO_LIB and libgff. In addition for it to use Boost169 it was necessary to modify the CmakeLists.txt file like so. ```; --- CMakeLists.txt.dist 2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt 2020-06-08 17:13:23.295499154 -0700; @@ -419,6 +419,8 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; ```. and to invoke cmake with:. ```; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; cmake \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_08.log; ```; Inkscape was built using cmake a couple of weeks ago on the same system and the -D flags for Boost in the cmake invocation were sufficient, there was no need to modify its CMakeLists.txt. Perhaps you might to compare that CMakeLIsts.txt with salmon's to see why theirs works and salmon's does not. I reiterate my plea for salmon's cmake file to accept some form of ROOT_LIBGFF, ROOT_LIBSTADEN, and ROOT_LIBTBB. Those modules ; were all defined but cmake could only figure out TBB this time, and for all I know it won't next time around (since it failed to do so for no apparent reason on CentOS 7). Salmon is a useful program but it has so far failed to build using existing libraries on this OS (unless extraordinary measures were applied) for CO 6, 7, and now 8! This is the information it had to work with:. ```; echo $PATH; /usr/common/modules/el8/x86_64/software/libgff/1.2-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/bin:/usr/common/modules/el8/x86_64/software/cmake/3.17.1-CentOS-vanilla/bin:/usr/lib64/ccache:/usr/local/bin:/usr/bin:/u",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:4031,Availability,error,errors,4031,"bb/2020.5-CentOS-vanilla/lib/libtbb.so -lgomp /usr/lib64/libjemalloc.so -lrt ../external/pufferfish/src/libksw2pp.a libalevin_core.a -ldl -pthread ; /tmp/cc91ASWS.ltrans1.ltrans.o: In function `boost::iostreams::basic_gzip_compressor<std::allocator<char> >::basic_gzip_compressor(boost::iostreams::gzip_params const&, long) [clone .constprop.783]':; <artificial>:(.text+0xdca4): undefined reference to `boost::iostreams::detail::zlib_base::zlib_base()'; <artificial>:(.text+0xdcbc): undefined reference to `boost::iostreams::detail::zlib_base::do_init(boost::iostreams::zlib_params const&, bool, void* (*)(void*, unsigned int, unsigned int), void (*)(void*, void*), void*)'; <artificial>:(.text+0xddc8): undefined reference to `boost::iostreams::zlib::best_compression'; <artificial>:(.text+0xddd4): undefined reference to `boost::iostreams::zlib::best_speed'; /tmp/cc91ASWS.ltrans1.ltrans.o: In function `GZipWriter::writeMtx(std::shared_ptr<spdlog::logger>&, boost::filesystem::path&, unsigned long, unsigned long, unsigned long) [clone .constprop.780]':; <artificial>:(.text+0xe056): undefined reference to `boost::iostreams::zlib::default_strategy'; <artificial>:(.text+0xe05d): undefined reference to `boost::iostreams::zlib::deflated'; <artificial>:(.text+0xe2d1): undefined reference to `boost::filesystem::detail::status(boost::filesystem::path const&, boost::system::error_code*)'; ...; ```. The boost related errors go on forever. There is nothing about ""boost"" in that command line, so apparently the relevant pieces never made it into the makefile. Running that very long command line with this added on the end:. ```; -L/usr/lib64/boost169 -lboost_filesystem -lboost_system -lboost_program_options -lboost_iostreams; ```; ; let Salmon link (with no other warnings). The resulting binary will do; ""salmon -h"" correctly but has so far not been tested further. So, in short CMakeLists.txt's handling of boost is still badly broken on CentOS, 8 this time, but it was terrible on 7 and 6 too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:72,Deployability,install,installed,72,"Some progress. Found a src rpm for cereal, rebuilt that into an RPM and installed. Then this (ROOT_* env variables come from the respective module load commands):. ```; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; ```; found everything. The ""make"" went along pretty well until here:; ```; [100%] Linking CXX executable salmon; cd /usr/common/src/salmon-1.2.1/build/src && /usr/common/src/cmake-3.17.1/bin/cmake -E cmake_link_script CMakeFiles/salmon.dir/link.txt --verbose=1; /usr/lib64/ccache/c++ -O3 -DNDEBUG -flto -fno-fat-lto-objects CMakeFiles/salmon.dir/EMUtils.cpp.o CMakeFiles/salmon.dir/CollapsedEMOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedCellOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedGibbsSampler.cpp.o CMakeFiles/salmon.dir/Salmon.cpp.o CMakeFiles/salmon.dir/BuildSalmonIndex.cpp.o CMakeFiles/salmon.dir/Graph.cpp.o CMakeFiles/salmon.dir/DedupUMI.cpp.o CMakeFiles/salmon.dir/Alevin.cpp.o CMakeFiles/salmon.dir/AlevinHash.cpp.o CMakeFiles/salmon.dir/SalmonAlevin.cpp.o CMakeFiles/salmon.dir/WhiteList.cpp.o CMakeFiles/salmon.dir/SalmonQuantify.cpp.o CMakeFiles/salmon.dir/FragmentLengthDistribution.cpp.o CMakeFiles/salmon.dir/FragmentStartPositionDistribution.cpp.o CMakeFiles/salmon.dir/GZipWriter.cpp.o CMakeFiles/salmon.dir/SalmonQuantMerge.cpp.o CMakeFiles/salmon.dir/ProgramOptionsGenerator.cpp.o CMakeFiles/salmon.dir/FASTAParser.cpp.o CMakeFiles/salmon.dir/AlignmentModel.cpp.o CMakeFiles/salmon.dir/SalmonQuantifyAlignments.cpp.o CMakeFiles/salmon.dir/BAMUtils.cpp.o -o salmon -L/usr/common/src/salmon-1.2.1/lib -L/usr/common/src/salmon-1.2.1/external/install/lib -Wl,-rpath,""\$ORIGIN/../lib:\$ORIGIN/../../lib:\$ORIGIN/:\$ORIGIN/../../external/install/lib"" ../external/pufferfish/src/libpuffer.a libs",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:1852,Deployability,install,install,1852,"ir/CollapsedEMOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedCellOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedGibbsSampler.cpp.o CMakeFiles/salmon.dir/Salmon.cpp.o CMakeFiles/salmon.dir/BuildSalmonIndex.cpp.o CMakeFiles/salmon.dir/Graph.cpp.o CMakeFiles/salmon.dir/DedupUMI.cpp.o CMakeFiles/salmon.dir/Alevin.cpp.o CMakeFiles/salmon.dir/AlevinHash.cpp.o CMakeFiles/salmon.dir/SalmonAlevin.cpp.o CMakeFiles/salmon.dir/WhiteList.cpp.o CMakeFiles/salmon.dir/SalmonQuantify.cpp.o CMakeFiles/salmon.dir/FragmentLengthDistribution.cpp.o CMakeFiles/salmon.dir/FragmentStartPositionDistribution.cpp.o CMakeFiles/salmon.dir/GZipWriter.cpp.o CMakeFiles/salmon.dir/SalmonQuantMerge.cpp.o CMakeFiles/salmon.dir/ProgramOptionsGenerator.cpp.o CMakeFiles/salmon.dir/FASTAParser.cpp.o CMakeFiles/salmon.dir/AlignmentModel.cpp.o CMakeFiles/salmon.dir/SalmonQuantifyAlignments.cpp.o CMakeFiles/salmon.dir/BAMUtils.cpp.o -o salmon -L/usr/common/src/salmon-1.2.1/lib -L/usr/common/src/salmon-1.2.1/external/install/lib -Wl,-rpath,""\$ORIGIN/../lib:\$ORIGIN/../../lib:\$ORIGIN/:\$ORIGIN/../../external/install/lib"" ../external/pufferfish/src/libpuffer.a libsalmon_core.a ../external/pufferfish/external/twopaco/graphconstructor/libtwopaco.a ../external/pufferfish/external/twopaco/graphdump/libgraphdump.a ../external/pufferfish/external/ntcard/libntcard.a -lgff /usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/lib/libstaden-read.a /usr/lib64/libcurl.so /usr/lib64/libz.so -lm /usr/lib64/liblzma.so /usr/lib64/libbz2.so /usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib/libtbbmalloc_proxy.so /usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib/libtbbmalloc.so /usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib/libtbb.so -lgomp /usr/lib64/libjemalloc.so -lrt ../external/pufferfish/src/libksw2pp.a libalevin_core.a -ldl -pthread ; /tmp/cc91ASWS.ltrans1.ltrans.o: In function `boost::iostreams::basic_gzip_compressor<std::allocator<cha",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:1945,Deployability,install,install,1945,"keFiles/salmon.dir/CollapsedGibbsSampler.cpp.o CMakeFiles/salmon.dir/Salmon.cpp.o CMakeFiles/salmon.dir/BuildSalmonIndex.cpp.o CMakeFiles/salmon.dir/Graph.cpp.o CMakeFiles/salmon.dir/DedupUMI.cpp.o CMakeFiles/salmon.dir/Alevin.cpp.o CMakeFiles/salmon.dir/AlevinHash.cpp.o CMakeFiles/salmon.dir/SalmonAlevin.cpp.o CMakeFiles/salmon.dir/WhiteList.cpp.o CMakeFiles/salmon.dir/SalmonQuantify.cpp.o CMakeFiles/salmon.dir/FragmentLengthDistribution.cpp.o CMakeFiles/salmon.dir/FragmentStartPositionDistribution.cpp.o CMakeFiles/salmon.dir/GZipWriter.cpp.o CMakeFiles/salmon.dir/SalmonQuantMerge.cpp.o CMakeFiles/salmon.dir/ProgramOptionsGenerator.cpp.o CMakeFiles/salmon.dir/FASTAParser.cpp.o CMakeFiles/salmon.dir/AlignmentModel.cpp.o CMakeFiles/salmon.dir/SalmonQuantifyAlignments.cpp.o CMakeFiles/salmon.dir/BAMUtils.cpp.o -o salmon -L/usr/common/src/salmon-1.2.1/lib -L/usr/common/src/salmon-1.2.1/external/install/lib -Wl,-rpath,""\$ORIGIN/../lib:\$ORIGIN/../../lib:\$ORIGIN/:\$ORIGIN/../../external/install/lib"" ../external/pufferfish/src/libpuffer.a libsalmon_core.a ../external/pufferfish/external/twopaco/graphconstructor/libtwopaco.a ../external/pufferfish/external/twopaco/graphdump/libgraphdump.a ../external/pufferfish/external/ntcard/libntcard.a -lgff /usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/lib/libstaden-read.a /usr/lib64/libcurl.so /usr/lib64/libz.so -lm /usr/lib64/liblzma.so /usr/lib64/libbz2.so /usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib/libtbbmalloc_proxy.so /usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib/libtbbmalloc.so /usr/common/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib/libtbb.so -lgomp /usr/lib64/libjemalloc.so -lrt ../external/pufferfish/src/libksw2pp.a libalevin_core.a -ldl -pthread ; /tmp/cc91ASWS.ltrans1.ltrans.o: In function `boost::iostreams::basic_gzip_compressor<std::allocator<char> >::basic_gzip_compressor(boost::iostreams::gzip_params const&, long) [clone .con",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:105,Modifiability,variab,variables,105,"Some progress. Found a src rpm for cereal, rebuilt that into an RPM and installed. Then this (ROOT_* env variables come from the respective module load commands):. ```; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; ```; found everything. The ""make"" went along pretty well until here:; ```; [100%] Linking CXX executable salmon; cd /usr/common/src/salmon-1.2.1/build/src && /usr/common/src/cmake-3.17.1/bin/cmake -E cmake_link_script CMakeFiles/salmon.dir/link.txt --verbose=1; /usr/lib64/ccache/c++ -O3 -DNDEBUG -flto -fno-fat-lto-objects CMakeFiles/salmon.dir/EMUtils.cpp.o CMakeFiles/salmon.dir/CollapsedEMOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedCellOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedGibbsSampler.cpp.o CMakeFiles/salmon.dir/Salmon.cpp.o CMakeFiles/salmon.dir/BuildSalmonIndex.cpp.o CMakeFiles/salmon.dir/Graph.cpp.o CMakeFiles/salmon.dir/DedupUMI.cpp.o CMakeFiles/salmon.dir/Alevin.cpp.o CMakeFiles/salmon.dir/AlevinHash.cpp.o CMakeFiles/salmon.dir/SalmonAlevin.cpp.o CMakeFiles/salmon.dir/WhiteList.cpp.o CMakeFiles/salmon.dir/SalmonQuantify.cpp.o CMakeFiles/salmon.dir/FragmentLengthDistribution.cpp.o CMakeFiles/salmon.dir/FragmentStartPositionDistribution.cpp.o CMakeFiles/salmon.dir/GZipWriter.cpp.o CMakeFiles/salmon.dir/SalmonQuantMerge.cpp.o CMakeFiles/salmon.dir/ProgramOptionsGenerator.cpp.o CMakeFiles/salmon.dir/FASTAParser.cpp.o CMakeFiles/salmon.dir/AlignmentModel.cpp.o CMakeFiles/salmon.dir/SalmonQuantifyAlignments.cpp.o CMakeFiles/salmon.dir/BAMUtils.cpp.o -o salmon -L/usr/common/src/salmon-1.2.1/lib -L/usr/common/src/salmon-1.2.1/external/install/lib -Wl,-rpath,""\$ORIGIN/../lib:\$ORIGIN/../../lib:\$ORIGIN/:\$ORIGIN/../../external/install/lib"" ../external/pufferfish/src/libpuffer.a libs",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:147,Performance,load,load,147,"Some progress. Found a src rpm for cereal, rebuilt that into an RPM and installed. Then this (ROOT_* env variables come from the respective module load commands):. ```; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; ```; found everything. The ""make"" went along pretty well until here:; ```; [100%] Linking CXX executable salmon; cd /usr/common/src/salmon-1.2.1/build/src && /usr/common/src/cmake-3.17.1/bin/cmake -E cmake_link_script CMakeFiles/salmon.dir/link.txt --verbose=1; /usr/lib64/ccache/c++ -O3 -DNDEBUG -flto -fno-fat-lto-objects CMakeFiles/salmon.dir/EMUtils.cpp.o CMakeFiles/salmon.dir/CollapsedEMOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedCellOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedGibbsSampler.cpp.o CMakeFiles/salmon.dir/Salmon.cpp.o CMakeFiles/salmon.dir/BuildSalmonIndex.cpp.o CMakeFiles/salmon.dir/Graph.cpp.o CMakeFiles/salmon.dir/DedupUMI.cpp.o CMakeFiles/salmon.dir/Alevin.cpp.o CMakeFiles/salmon.dir/AlevinHash.cpp.o CMakeFiles/salmon.dir/SalmonAlevin.cpp.o CMakeFiles/salmon.dir/WhiteList.cpp.o CMakeFiles/salmon.dir/SalmonQuantify.cpp.o CMakeFiles/salmon.dir/FragmentLengthDistribution.cpp.o CMakeFiles/salmon.dir/FragmentStartPositionDistribution.cpp.o CMakeFiles/salmon.dir/GZipWriter.cpp.o CMakeFiles/salmon.dir/SalmonQuantMerge.cpp.o CMakeFiles/salmon.dir/ProgramOptionsGenerator.cpp.o CMakeFiles/salmon.dir/FASTAParser.cpp.o CMakeFiles/salmon.dir/AlignmentModel.cpp.o CMakeFiles/salmon.dir/SalmonQuantifyAlignments.cpp.o CMakeFiles/salmon.dir/BAMUtils.cpp.o -o salmon -L/usr/common/src/salmon-1.2.1/lib -L/usr/common/src/salmon-1.2.1/external/install/lib -Wl,-rpath,""\$ORIGIN/../lib:\$ORIGIN/../../lib:\$ORIGIN/:\$ORIGIN/../../external/install/lib"" ../external/pufferfish/src/libpuffer.a libs",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:479,Testability,log,log,479,"Some progress. Found a src rpm for cereal, rebuilt that into an RPM and installed. Then this (ROOT_* env variables come from the respective module load commands):. ```; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; ```; found everything. The ""make"" went along pretty well until here:; ```; [100%] Linking CXX executable salmon; cd /usr/common/src/salmon-1.2.1/build/src && /usr/common/src/cmake-3.17.1/bin/cmake -E cmake_link_script CMakeFiles/salmon.dir/link.txt --verbose=1; /usr/lib64/ccache/c++ -O3 -DNDEBUG -flto -fno-fat-lto-objects CMakeFiles/salmon.dir/EMUtils.cpp.o CMakeFiles/salmon.dir/CollapsedEMOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedCellOptimizer.cpp.o CMakeFiles/salmon.dir/CollapsedGibbsSampler.cpp.o CMakeFiles/salmon.dir/Salmon.cpp.o CMakeFiles/salmon.dir/BuildSalmonIndex.cpp.o CMakeFiles/salmon.dir/Graph.cpp.o CMakeFiles/salmon.dir/DedupUMI.cpp.o CMakeFiles/salmon.dir/Alevin.cpp.o CMakeFiles/salmon.dir/AlevinHash.cpp.o CMakeFiles/salmon.dir/SalmonAlevin.cpp.o CMakeFiles/salmon.dir/WhiteList.cpp.o CMakeFiles/salmon.dir/SalmonQuantify.cpp.o CMakeFiles/salmon.dir/FragmentLengthDistribution.cpp.o CMakeFiles/salmon.dir/FragmentStartPositionDistribution.cpp.o CMakeFiles/salmon.dir/GZipWriter.cpp.o CMakeFiles/salmon.dir/SalmonQuantMerge.cpp.o CMakeFiles/salmon.dir/ProgramOptionsGenerator.cpp.o CMakeFiles/salmon.dir/FASTAParser.cpp.o CMakeFiles/salmon.dir/AlignmentModel.cpp.o CMakeFiles/salmon.dir/SalmonQuantifyAlignments.cpp.o CMakeFiles/salmon.dir/BAMUtils.cpp.o -o salmon -L/usr/common/src/salmon-1.2.1/lib -L/usr/common/src/salmon-1.2.1/external/install/lib -Wl,-rpath,""\$ORIGIN/../lib:\$ORIGIN/../../lib:\$ORIGIN/:\$ORIGIN/../../external/install/lib"" ../external/pufferfish/src/libpuffer.a libs",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:3563,Testability,log,logger,3563,"on/modules/el8/x86_64/software/libtbb/2020.5-CentOS-vanilla/lib/libtbb.so -lgomp /usr/lib64/libjemalloc.so -lrt ../external/pufferfish/src/libksw2pp.a libalevin_core.a -ldl -pthread ; /tmp/cc91ASWS.ltrans1.ltrans.o: In function `boost::iostreams::basic_gzip_compressor<std::allocator<char> >::basic_gzip_compressor(boost::iostreams::gzip_params const&, long) [clone .constprop.783]':; <artificial>:(.text+0xdca4): undefined reference to `boost::iostreams::detail::zlib_base::zlib_base()'; <artificial>:(.text+0xdcbc): undefined reference to `boost::iostreams::detail::zlib_base::do_init(boost::iostreams::zlib_params const&, bool, void* (*)(void*, unsigned int, unsigned int), void (*)(void*, void*), void*)'; <artificial>:(.text+0xddc8): undefined reference to `boost::iostreams::zlib::best_compression'; <artificial>:(.text+0xddd4): undefined reference to `boost::iostreams::zlib::best_speed'; /tmp/cc91ASWS.ltrans1.ltrans.o: In function `GZipWriter::writeMtx(std::shared_ptr<spdlog::logger>&, boost::filesystem::path&, unsigned long, unsigned long, unsigned long) [clone .constprop.780]':; <artificial>:(.text+0xe056): undefined reference to `boost::iostreams::zlib::default_strategy'; <artificial>:(.text+0xe05d): undefined reference to `boost::iostreams::zlib::deflated'; <artificial>:(.text+0xe2d1): undefined reference to `boost::filesystem::detail::status(boost::filesystem::path const&, boost::system::error_code*)'; ...; ```. The boost related errors go on forever. There is nothing about ""boost"" in that command line, so apparently the relevant pieces never made it into the makefile. Running that very long command line with this added on the end:. ```; -L/usr/lib64/boost169 -lboost_filesystem -lboost_system -lboost_program_options -lboost_iostreams; ```; ; let Salmon link (with no other warnings). The resulting binary will do; ""salmon -h"" correctly but has so far not been tested further. So, in short CMakeLists.txt's handling of boost is still badly broken on CentOS, 8 this time, ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162:4467,Testability,test,tested,4467,"bb/2020.5-CentOS-vanilla/lib/libtbb.so -lgomp /usr/lib64/libjemalloc.so -lrt ../external/pufferfish/src/libksw2pp.a libalevin_core.a -ldl -pthread ; /tmp/cc91ASWS.ltrans1.ltrans.o: In function `boost::iostreams::basic_gzip_compressor<std::allocator<char> >::basic_gzip_compressor(boost::iostreams::gzip_params const&, long) [clone .constprop.783]':; <artificial>:(.text+0xdca4): undefined reference to `boost::iostreams::detail::zlib_base::zlib_base()'; <artificial>:(.text+0xdcbc): undefined reference to `boost::iostreams::detail::zlib_base::do_init(boost::iostreams::zlib_params const&, bool, void* (*)(void*, unsigned int, unsigned int), void (*)(void*, void*), void*)'; <artificial>:(.text+0xddc8): undefined reference to `boost::iostreams::zlib::best_compression'; <artificial>:(.text+0xddd4): undefined reference to `boost::iostreams::zlib::best_speed'; /tmp/cc91ASWS.ltrans1.ltrans.o: In function `GZipWriter::writeMtx(std::shared_ptr<spdlog::logger>&, boost::filesystem::path&, unsigned long, unsigned long, unsigned long) [clone .constprop.780]':; <artificial>:(.text+0xe056): undefined reference to `boost::iostreams::zlib::default_strategy'; <artificial>:(.text+0xe05d): undefined reference to `boost::iostreams::zlib::deflated'; <artificial>:(.text+0xe2d1): undefined reference to `boost::filesystem::detail::status(boost::filesystem::path const&, boost::system::error_code*)'; ...; ```. The boost related errors go on forever. There is nothing about ""boost"" in that command line, so apparently the relevant pieces never made it into the makefile. Running that very long command line with this added on the end:. ```; -L/usr/lib64/boost169 -lboost_filesystem -lboost_system -lboost_program_options -lboost_iostreams; ```; ; let Salmon link (with no other warnings). The resulting binary will do; ""salmon -h"" correctly but has so far not been tested further. So, in short CMakeLists.txt's handling of boost is still badly broken on CentOS, 8 this time, but it was terrible on 7 and 6 too.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641531162
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641545954:223,Availability,fault,fault,223,"Ugh, salmon built, but it crashes. ```; module load salmon; cd /tmp; gunzip -c $ROOT_SALMON/sample_data.tgz | tar -xf -; cd sample_data; salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Segmentation fault (core dumped); ```; I wonder if this is related to the segfault seen when Pufferfish was built on the same platfrom from the git repository (today also.)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641545954
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641545954:47,Performance,load,load,47,"Ugh, salmon built, but it crashes. ```; module load salmon; cd /tmp; gunzip -c $ROOT_SALMON/sample_data.tgz | tar -xf -; cd sample_data; salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Segmentation fault (core dumped); ```; I wonder if this is related to the segfault seen when Pufferfish was built on the same platfrom from the git repository (today also.)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641545954
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641546215:304,Deployability,configurat,configuration,304,"Looking back to the [earlier post](https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684), I wonder if this stems from CMake not being able to properly find Boost on its own. Granted, the CMake / Boost infrastructure has never been great, partly due to the complexity of salmon's CMake configuration, and partly due to the strange way that CMake, itself, handles Boost versions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641546215
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641546215:304,Modifiability,config,configuration,304,"Looking back to the [earlier post](https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-640962684), I wonder if this stems from CMake not being able to properly find Boost on its own. Granted, the CMake / Boost infrastructure has never been great, partly due to the complexity of salmon's CMake configuration, and partly due to the strange way that CMake, itself, handles Boost versions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641546215
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:701,Availability,Error,Error,701,"cmake produced quite an odd link line with mixed -l/-L and explicit paths, and dynamic and static libraries all intermixed. Here is a small part (expanded for readability):. ```; ../external/pufferfish/external/ntcard/libntcard.a \; -lgff \; /usr/common/modules/el8/x86_64/software/io_lib/1.14.9-CentOS-vanilla/lib/libstaden-read.a \; /usr/lib64/libcurl.so \; /usr/lib64/libz.so \; -lm \; /usr/lib64/liblzma.so \; ```. The distributed binary won't do that command either, but at least it does not crash:; ```. /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Version Info: This is the most recent version of salmon.; Exception : [Error: FMD indexing is not supported in this version of salmon.]; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; #this worked OK; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type puff. ```; Here is what happens in gdb for the version I built:. ```; gdb --args salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; GNU gdb (GDB) Red Hat Enterprise Linux 8.2-6.el8; Copyright (C) 2018 Free Software Foundation, Inc.; License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4950,Availability,fault,fault,4950,": Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.6-26.el8.x86_64 cyrus-sasl-lib-2.1.27-1.el8.x86_64 jemalloc-5.2.1-2.el8.x86_64 keyutils-libs-1.5.10-6.el8.x86_64 krb5-libs-1.17-9.el8.x86_64 libcom_err-1.44.6-3.el8.x86_64 libcurl-7.61.1-11.el8.x86_64 libgcc-8.3.1-4.5.el8.x86_64 libgomp-8.3.1-4.5.el8.x86_64 libidn2-2.2.0-1.el8.x86_64 libnghttp2-1.33.0-1.el8_0.1.x86_64 libpsl-0.20.2-5.el8.x86_64 libselinux-2.9-2.1.el8.x86_64 libssh-0.9.0-4.el8.x86_64 libstdc++-8.3.1-4.5.el8.x86_64 libunistring-0.9.9-3.el8.x86_64 libxcrypt-4.1.1-4.el8.x86_64 openldap-2.4.46-11.el8_1.x86_64 pcre2-10.32-1.el8.x86_64 tbb-devel-2018.2-9.el8.x86_",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:1666,Deployability,configurat,configuration,1666,"almon.; Exception : [Error: FMD indexing is not supported in this version of salmon.]; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; #this worked OK; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type puff. ```; Here is what happens in gdb for the version I built:. ```; gdb --args salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; GNU gdb (GDB) Red Hat Enterprise Linux 8.2-6.el8; Copyright (C) 2018 Free Software Foundation, Inc.; License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable secti",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:1685,Deployability,configurat,configuration,1685,"almon.; Exception : [Error: FMD indexing is not supported in this version of salmon.]; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; #this worked OK; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type puff. ```; Here is what happens in gdb for the version I built:. ```; gdb --args salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; GNU gdb (GDB) Red Hat Enterprise Linux 8.2-6.el8; Copyright (C) 2018 Free Software Foundation, Inc.; License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable secti",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2259,Deployability,install,install,2259,"_fmd_index --type fmd; GNU gdb (GDB) Red Hat Enterprise Linux 8.2-6.el8; Copyright (C) 2018 Free Software Foundation, Inc.; License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of EL",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:5137,Deployability,install,install,5137,"ments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.6-26.el8.x86_64 cyrus-sasl-lib-2.1.27-1.el8.x86_64 jemalloc-5.2.1-2.el8.x86_64 keyutils-libs-1.5.10-6.el8.x86_64 krb5-libs-1.17-9.el8.x86_64 libcom_err-1.44.6-3.el8.x86_64 libcurl-7.61.1-11.el8.x86_64 libgcc-8.3.1-4.5.el8.x86_64 libgomp-8.3.1-4.5.el8.x86_64 libidn2-2.2.0-1.el8.x86_64 libnghttp2-1.33.0-1.el8_0.1.x86_64 libpsl-0.20.2-5.el8.x86_64 libselinux-2.9-2.1.el8.x86_64 libssh-0.9.0-4.el8.x86_64 libstdc++-8.3.1-4.5.el8.x86_64 libunistring-0.9.9-3.el8.x86_64 libxcrypt-4.1.1-4.el8.x86_64 openldap-2.4.46-11.el8_1.x86_64 pcre2-10.32-1.el8.x86_64 tbb-devel-2018.2-9.el8.x86_64 xz-libs-5.2.4-3.el8.x86_64 zlib-1.2.11-10.el8.x86_64; (gdb) bt; #0 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; #1 0x00007ffff68221ee in tcache_flush_cache () from",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:1613,Modifiability,config,configured,1613,"fmd; Version Info: This is the most recent version of salmon.; Exception : [Error: FMD indexing is not supported in this version of salmon.]; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; #this worked OK; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type puff. ```; Here is what happens in gdb for the version I built:. ```; gdb --args salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; GNU gdb (GDB) Red Hat Enterprise Linux 8.2-6.el8; Copyright (C) 2018 Free Software Foundation, Inc.; License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.prop",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:1666,Modifiability,config,configuration,1666,"almon.; Exception : [Error: FMD indexing is not supported in this version of salmon.]; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; #this worked OK; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type puff. ```; Here is what happens in gdb for the version I built:. ```; gdb --args salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; GNU gdb (GDB) Red Hat Enterprise Linux 8.2-6.el8; Copyright (C) 2018 Free Software Foundation, Inc.; License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable secti",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:1685,Modifiability,config,configuration,1685,"almon.; Exception : [Error: FMD indexing is not supported in this version of salmon.]; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index was invoked improperly.; For usage information, try /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index --help; Exiting.; #this worked OK; /usr/common/src/salmon-latest_linux_x86_64/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type puff. ```; Here is what happens in gdb for the version I built:. ```; gdb --args salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; GNU gdb (GDB) Red Hat Enterprise Linux 8.2-6.el8; Copyright (C) 2018 Free Software Foundation, Inc.; License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable secti",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2306,Performance,Load,Loadable,2306,"2018 Free Software Foundation, Inc.; License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [T",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2378,Performance,Load,Loadable,2378,"PL version 3 or later <http://gnu.org/licenses/gpl.html>; This is free software: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host lib",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2450,Performance,Load,Loadable,2450,"oftware: you are free to change and redistribute it.; There is NO WARRANTY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2522,Performance,Load,Loadable,2522,"TY, to the extent permitted by law.; Type ""show copying"" and ""show warranty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2594,Performance,Load,Loadable,2594,"nty"" for details.; This GDB was configured as ""x86_64-redhat-linux-gnu"".; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2666,Performance,Load,Loadable,2666,"; Type ""show configuration"" for configuration details.; For bug reporting instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2738,Performance,Load,Loadable,2738,"g instructions, please see:; <http://www.gnu.org/software/gdb/bugs/>.; Find the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2810,Performance,Load,Loadable,2810,"ind the GDB manual and other documentation resources online at:; <http://www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2882,Performance,Load,Loadable,2882,"/www.gnu.org/software/gdb/documentation/>. For help, type ""help"".; Type ""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:2954,Performance,Load,Loadable,2954,"""apropos word"" to search for commands related to ""word""...; Reading symbols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3026,Performance,Load,Loadable,3026,"ols from salmon...done.; (gdb) r; Starting program: /home/common/modules/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3098,Performance,Load,Loadable,3098,"/el8/x86_64/software/salmon/1.2.1-CentOS-vanilla/bin/salmon index -t transcripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3170,Performance,Load,Loadable,3170,"nscripts.fasta -i sample_salmon_fmd_index --type fmd; Missing separate debuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3242,Performance,Load,Loadable,3242,"ebuginfos, use: yum debuginfo-install glibc-2.28-72.el8_1.1.x86_64; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3422,Performance,Load,Loadable,3422," ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warni",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3494,Performance,Load,Loadable,3494,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3566,Performance,Load,Loadable,3566,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3638,Performance,Load,Loadable,3638,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3710,Performance,Load,Loadable,3710,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3782,Performance,Load,Loadable,3782,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3854,Performance,Load,Loadable,3854,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3926,Performance,Load,Loadable,3926,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" re",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:3998,Performance,Load,Loadable,3998,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4070,Performance,Load,Loadable,4070,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small ()",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4142,Performance,Load,Loadable,4142,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum deb",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4214,Performance,Load,Loadable,4214,"perty"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostream",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4286,Performance,Load,Loadable,4286,"perty"" outside of ELF segments; [Thread debugging using libthread_db enabled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4358,Performance,Load,Loadable,4358,"bled]; Using host libthread_db library ""/lib64/libthread_db.so.1"".; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4430,Performance,Load,Loadable,4430,"ing: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.6-26.el8.x86_64 cyrus-sasl-lib-2.1.27-1.el8.x86_64 jemalloc-5.2.1-2.el8.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4502,Performance,Load,Loadable,4502,"ing: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.6-26.el8.x86_64 cyrus-sasl-lib-2.1.27-1.el8.x86_64 jemalloc-5.2.1-2.el8.x86_64 keyutils-libs-1.5.10-6.el8.x86_64 krb5-libs-1.17-9.el8.x86_64 lib",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4574,Performance,Load,Loadable,4574,"ing: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.6-26.el8.x86_64 cyrus-sasl-lib-2.1.27-1.el8.x86_64 jemalloc-5.2.1-2.el8.x86_64 keyutils-libs-1.5.10-6.el8.x86_64 krb5-libs-1.17-9.el8.x86_64 libcom_err-1.44.6-3.el8.x86_64 libcurl-7.61.1-11.el8.x86_64 libgcc-8.3.1-4.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4646,Performance,Load,Loadable,4646,"ing: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.6-26.el8.x86_64 cyrus-sasl-lib-2.1.27-1.el8.x86_64 jemalloc-5.2.1-2.el8.x86_64 keyutils-libs-1.5.10-6.el8.x86_64 krb5-libs-1.17-9.el8.x86_64 libcom_err-1.44.6-3.el8.x86_64 libcurl-7.61.1-11.el8.x86_64 libgcc-8.3.1-4.5.el8.x86_64 libgomp-8.3.1-4.5.el8.x86_64 libidn2-2.2.0-1.el8.x86_64 lib",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4718,Performance,Load,Loadable,4718,"ing: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.6-26.el8.x86_64 cyrus-sasl-lib-2.1.27-1.el8.x86_64 jemalloc-5.2.1-2.el8.x86_64 keyutils-libs-1.5.10-6.el8.x86_64 krb5-libs-1.17-9.el8.x86_64 libcom_err-1.44.6-3.el8.x86_64 libcurl-7.61.1-11.el8.x86_64 libgcc-8.3.1-4.5.el8.x86_64 libgomp-8.3.1-4.5.el8.x86_64 libidn2-2.2.0-1.el8.x86_64 libnghttp2-1.33.0-1.el8_0.1.x86_64 libpsl-0.20.2-5.el8.x86_64 libselinux-2.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410:4790,Performance,Load,Loadable,4790,"ing: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; warning: Loadable section "".note.gnu.property"" outside of ELF segments; [New Thread 0x7ffff0987700 (LWP 17537)]. Thread 2 ""salmon"" received signal SIGSEGV, Segmentation fault.; [Switching to Thread 0x7ffff0987700 (LWP 17537)]; 0x00007ffff68202ab in je_tcache_bin_flush_small () from /lib64/libjemalloc.so.2; Missing separate debuginfos, use: yum debuginfo-install boost169-filesystem-1.69.0-4.el8.x86_64 boost169-iostreams-1.69.0-4.el8.x86_64 boost169-program-options-1.69.0-4.el8.x86_64 boost169-system-1.69.0-4.el8.x86_64 brotli-1.0.6-1.el8.x86_64 bzip2-libs-1.0.6-26.el8.x86_64 cyrus-sasl-lib-2.1.27-1.el8.x86_64 jemalloc-5.2.1-2.el8.x86_64 keyutils-libs-1.5.10-6.el8.x86_64 krb5-libs-1.17-9.el8.x86_64 libcom_err-1.44.6-3.el8.x86_64 libcurl-7.61.1-11.el8.x86_64 libgcc-8.3.1-4.5.el8.x86_64 libgomp-8.3.1-4.5.el8.x86_64 libidn2-2.2.0-1.el8.x86_64 libnghttp2-1.33.0-1.el8_0.1.x86_64 libpsl-0.20.2-5.el8.x86_64 libselinux-2.9-2.1.el8.x86_64 libssh-0.9.0-4.el8.x86_64 libstdc++-8.3.1-4.5.el8.x86_6",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641594410
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:1122,Deployability,patch,patch,1122,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:722,Integrability,message,message,722,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:773,Integrability,message,message,773,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:825,Integrability,message,message,825,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:1065,Integrability,message,message,1065,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:1149,Performance,load,load,1149,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:1168,Performance,load,load,1168,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:1188,Performance,load,load,1188,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:1208,Performance,load,load,1208,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:1604,Testability,log,log,1604,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831:1647,Testability,log,log,1647,"lt still segfaults. It still needed an edit of the CMakeLists.txt file. Still, for future reference:. ```; pversion=1.2.1; package=salmon; TOPDIR=/usr/common/modules/el8/x86_64/software/${package}/${pversion}-CentOS-vanilla; wget https://github.com/COMBINE-lab/salmon/archive/v1.2.1.tar.gz; gunzip -c v1.2.1.tar.gz | tar -xf -; /bin/rm v1.2.1.tar.gz; cd ${package}-${pversion}; mv CMakeLists.txt CMakeLists.txt.dist; cat >mypatch <<'EOD'; --- CMakeLists.txt.dist	2020-04-21 22:31:07.000000000 -0700; +++ CMakeLists.txt	2020-06-09 14:55:02.733885772 -0700; @@ -419,6 +419,10 @@; find_package(Boost 1.59.0 COMPONENTS iostreams filesystem system timer chrono program_options); message(""BOOST_INCLUDEDIR = ${BOOST_INCLUDEDIR}""); message(""BOOST_LIBRARYDIR = ${BOOST_LIBRARYDIR}""); +message(""Forcing Boost_FOUND to TRUE""); +set(Boost_FOUND TRUE); +set(Boost_LIBRARY_DIRS ""/usr/lib64/boost169""); +set(Boost_LIBRARIES -lboost_iostreams -lboost_filesystem -lboost_system -lboost_timer -lboost_chrono -lboost_program_options); message(""Boost_FOUND = ${Boost_FOUND}""); endif(); ; EOD; patch -p0 <mypatch; module load cmake; module load io_lib; module load libgff; module load libtbb; mkdir build; cd build; export CFLAGS=""-g -O0""; export CXXFLAGS=""-g -O0""; cmake \; -DCMAKE_INSTALL_PREFIX=$TOPDIR \; -DSTADEN_ROOT=$ROOT_IO_LIB \; -DGFF_ROOT=$ROOT_LIBGFF \; -DTBB_ROOT=$ROOT_LIBTBB \; -DCMAKE_VERBOSE_MAKEFILE:BOOL=ON \; -DBOOST_LIBRARYDIR=/usr/lib64/boost169 \; -DBOOST_INCLUDEDIR=/usr/include/boost169 \; -DBoost_NO_SYSTEM_PATHS=ON \; .. 2>&1 | tee cmake_2020_06_09.log; make -j 4 2>&1 | tee build_2020_06_09.log. ```. Since it was compiled ""-g -O0"" this time it was easier to step through it. Well, somewhat. In Salmon.cpp line 195 is the last place a break point works. If one is set for 197 it segfaults before reaching it. Line 195 is:. `	 po::store(parsed, vm);; `; I tried briefly to trace inward from there but couldn't make heads or tails of the path it was taking through an endless series of headers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641612831
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641652976:213,Availability,avail,available,213,"That does not seem very likely in this case. The default C++ compiler mode is C++14 and pretty much everything on the system is compiled that way. Still, I'm not sure how boost169 was compiled (the src.rpm is not available at the moment, waiting for email from the builder). It was used with other packages though, and if it had an incompatible ABI they should not have worked either. Checking my notes there are a couple of specific programs which had to be compiled with older C++ standards, but none of those are linked to Salmon. Also, if that was the problem, shouldn't these have shown up as unresolved references because of the ABI_TAG (""cxx11"" or ""_cxx11"", according to this:. https://developers.redhat.com/blog/2015/02/05/gcc5-and-the-c11-abi/. ) ?. ccache was installed and was active for the salmon build. It was removed and salmon rebuilt. No difference, it still segfaults.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641652976
https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641652976:770,Deployability,install,installed,770,"That does not seem very likely in this case. The default C++ compiler mode is C++14 and pretty much everything on the system is compiled that way. Still, I'm not sure how boost169 was compiled (the src.rpm is not available at the moment, waiting for email from the builder). It was used with other packages though, and if it had an incompatible ABI they should not have worked either. Checking my notes there are a couple of specific programs which had to be compiled with older C++ standards, but none of those are linked to Salmon. Also, if that was the problem, shouldn't these have shown up as unresolved references because of the ABI_TAG (""cxx11"" or ""_cxx11"", according to this:. https://developers.redhat.com/blog/2015/02/05/gcc5-and-the-c11-abi/. ) ?. ccache was installed and was active for the salmon build. It was removed and salmon rebuilt. No difference, it still segfaults.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/460#issuecomment-641652976
https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-565296541:88,Deployability,install,install,88,may be try creating a new environment and/or specify the version of salmon you wan't to install ? it can happen sometimes based on the dependency structure already installed in your environment. You can also the pre build binaries from https://github.com/COMBINE-lab/salmon/releases. It's more of an issue with conda than salmon itself. Closing this one but let us know if you still face any issue.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-565296541
https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-565296541:164,Deployability,install,installed,164,may be try creating a new environment and/or specify the version of salmon you wan't to install ? it can happen sometimes based on the dependency structure already installed in your environment. You can also the pre build binaries from https://github.com/COMBINE-lab/salmon/releases. It's more of an issue with conda than salmon itself. Closing this one but let us know if you still face any issue.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-565296541
https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-565296541:274,Deployability,release,releases,274,may be try creating a new environment and/or specify the version of salmon you wan't to install ? it can happen sometimes based on the dependency structure already installed in your environment. You can also the pre build binaries from https://github.com/COMBINE-lab/salmon/releases. It's more of an issue with conda than salmon itself. Closing this one but let us know if you still face any issue.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-565296541
https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-565296541:135,Integrability,depend,dependency,135,may be try creating a new environment and/or specify the version of salmon you wan't to install ? it can happen sometimes based on the dependency structure already installed in your environment. You can also the pre build binaries from https://github.com/COMBINE-lab/salmon/releases. It's more of an issue with conda than salmon itself. Closing this one but let us know if you still face any issue.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-565296541
https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-567461956:29,Deployability,update,update,29,@k3yavi . I cannot get it to update even on new env. My command is; ```; conda install -c bioconda salmon=1.0.0; ```; and it freezes everytime; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; ```,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-567461956
https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-567461956:79,Deployability,install,install,79,@k3yavi . I cannot get it to update even on new env. My command is; ```; conda install -c bioconda salmon=1.0.0; ```; and it freezes everytime; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; ```,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-567461956
https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-567461956:277,Modifiability,flexible,flexible,277,@k3yavi . I cannot get it to update even on new env. My command is; ```; conda install -c bioconda salmon=1.0.0; ```; and it freezes everytime; ```; Collecting package metadata (current_repodata.json): done; Solving environment: failed with initial frozen solve. Retrying with flexible solve.; ```,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/461#issuecomment-567461956
https://github.com/COMBINE-lab/salmon/issues/465#issuecomment-567973865:584,Deployability,release,release,584,"Hi @jcasc,. Thanks for raising the issue. The reason for this that when selectively aligning the reads, salmon writes in the SAM file the length of each transcript as retained in the index. This is the length the transcript has after the basic pre-processing, which includes clipping polyA tails. Thus, the length will not always precisely match the input fasta file. Interestingly, we _do_ also maintain the original length of each transcript prior to any processing in the index, and I agree that this makes more sense to put in the header, so I will tag this as a fix for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/465#issuecomment-567973865
https://github.com/COMBINE-lab/salmon/issues/465#issuecomment-567974966:75,Availability,down,down,75,"Also, it is worth noting that, with the `-z` option, salmon will not write down a CIGAR string for the alignments, but it _will_ write down the computed alignment score. As it is _highly recommended_ (and the default behavior) to use the error model when quantifying in alignment based mode, we should ensure that, if the input alignments are coming from salmon itself, the `AS` tag is used as a proxy for the missing CIGAR string to assign conditional fragment probabilities.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/465#issuecomment-567974966
https://github.com/COMBINE-lab/salmon/issues/465#issuecomment-567974966:135,Availability,down,down,135,"Also, it is worth noting that, with the `-z` option, salmon will not write down a CIGAR string for the alignments, but it _will_ write down the computed alignment score. As it is _highly recommended_ (and the default behavior) to use the error model when quantifying in alignment based mode, we should ensure that, if the input alignments are coming from salmon itself, the `AS` tag is used as a proxy for the missing CIGAR string to assign conditional fragment probabilities.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/465#issuecomment-567974966
https://github.com/COMBINE-lab/salmon/issues/465#issuecomment-567974966:238,Availability,error,error,238,"Also, it is worth noting that, with the `-z` option, salmon will not write down a CIGAR string for the alignments, but it _will_ write down the computed alignment score. As it is _highly recommended_ (and the default behavior) to use the error model when quantifying in alignment based mode, we should ensure that, if the input alignments are coming from salmon itself, the `AS` tag is used as a proxy for the missing CIGAR string to assign conditional fragment probabilities.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/465#issuecomment-567974966
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568768688:186,Testability,log,log,186,"Hi @PlantDr430,. Thanks for the detailed report. I'll provjde a detailed explanation of the bootstrap variance later when I'm at my computer. However, regarding the gibbs issue with the log file not being properly populated; you still get the appropriate gibbs samples as output, right? The issue is just with writing the log? We'll check into what might be causing this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568768688
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568768688:322,Testability,log,log,322,"Hi @PlantDr430,. Thanks for the detailed report. I'll provjde a detailed explanation of the bootstrap variance later when I'm at my computer. However, regarding the gibbs issue with the log file not being properly populated; you still get the appropriate gibbs samples as output, right? The issue is just with writing the log? We'll check into what might be causing this.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568768688
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568769945:162,Testability,log,log,162,"Thanks, for the response and I'll wait for the detail explanation. . As for the Gibbs, yes I get the appropriate gibbs sample outputs, it's just appears that the log file is not being written.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568769945
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:2154,Availability,down,downstream,2154,"xpect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this inf",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:3218,Availability,down,downstream,3218,"which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this information is to allow you to assess the posterior variance, and also to pass these samples to uncertainty-aware differential analysis tools, like [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), downstream of salmon. . Anyway, thanks again for the detailed report! We'll look into the logging issue, and please let me know if my description above answers your question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:303,Deployability,update,updates,303,"Hi Stephen,. So, the variation you see when you re-run salmon multiple times is _expected_ to be different (and _much_ smaller) than the variance you see when bootstrapping. Why is this? When you re-run salmon, the only variance you are seeing is due to small differences in the order of observations / updates from the streaming collapsed variational Bayes phase of the algorithm. This, in turn can have a _slight_ effect on the initialization conditions of the offline phase of the algorithm, and some of the parameters learned for the auxiliary parameters. However, in each run, you are observing _exactly_ the same set of reads and salmon is producing _exactly_ the same set of alignments; only the order and therefore some of the streaming updates change. So, we expect the final estimated abundances to be _very_ similar to each other. However, when salmon performs bootstrapping, it is actually resampling _with replacement_, from the counts of the range-factorized equivalence classes. Roughly, we expect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same under",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:745,Deployability,update,updates,745,"Hi Stephen,. So, the variation you see when you re-run salmon multiple times is _expected_ to be different (and _much_ smaller) than the variance you see when bootstrapping. Why is this? When you re-run salmon, the only variance you are seeing is due to small differences in the order of observations / updates from the streaming collapsed variational Bayes phase of the algorithm. This, in turn can have a _slight_ effect on the initialization conditions of the offline phase of the algorithm, and some of the parameters learned for the auxiliary parameters. However, in each run, you are observing _exactly_ the same set of reads and salmon is producing _exactly_ the same set of alignments; only the order and therefore some of the streaming updates change. So, we expect the final estimated abundances to be _very_ similar to each other. However, when salmon performs bootstrapping, it is actually resampling _with replacement_, from the counts of the range-factorized equivalence classes. Roughly, we expect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same under",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:863,Performance,perform,performs,863,"Hi Stephen,. So, the variation you see when you re-run salmon multiple times is _expected_ to be different (and _much_ smaller) than the variance you see when bootstrapping. Why is this? When you re-run salmon, the only variance you are seeing is due to small differences in the order of observations / updates from the streaming collapsed variational Bayes phase of the algorithm. This, in turn can have a _slight_ effect on the initialization conditions of the offline phase of the algorithm, and some of the parameters learned for the auxiliary parameters. However, in each run, you are observing _exactly_ the same set of reads and salmon is producing _exactly_ the same set of alignments; only the order and therefore some of the streaming updates change. So, we expect the final estimated abundances to be _very_ similar to each other. However, when salmon performs bootstrapping, it is actually resampling _with replacement_, from the counts of the range-factorized equivalence classes. Roughly, we expect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same under",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:1849,Performance,perform,performing,1849,"xpect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this inf",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:2301,Performance,optimiz,optimization,2301,"tstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this information is to allow you to assess the posterior variance, and also to pass these samples to uncertainty-aware differential analysis tools, like [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), downstream of salmon. . Anyway, thanks again f",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:2557,Performance,perform,performing,2557,"which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this information is to allow you to assess the posterior variance, and also to pass these samples to uncertainty-aware differential analysis tools, like [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), downstream of salmon. . Anyway, thanks again for the detailed report! We'll look into the logging issue, and please let me know if my description above answers your question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:3308,Testability,log,logging,3308,"which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same underlying ""population"" as the ones we have in our specific sample, but subject to the random sampling effect induced by sequencing and all of the subsequent downstream effects it has on our estimator (i.e. salmon's computational procedure for estimating transcript abundance via the variational Bayesian optimization algorithm). From the practical perspective, one would not necessarily expect taking the average of the bootstrap estimates to produce a more accurate point estimate than taking the normal point estimate produced by salmon. The main purpose of performing the Gibbs sampling or bootstrapping is to allow accurate assessment of the _posterior variance_ of the point estimates (to build things like credible intervals). The mean of the bootstrap estimates should be highly-correlated with the normal point estimates, but I wouldn't expect it to be identical. Also, you might try seeing what you get with a different summary statistic, like the median. However, the main point of producing this information is to allow you to assess the posterior variance, and also to pass these samples to uncertainty-aware differential analysis tools, like [swish](https://academic.oup.com/nar/article/47/18/e105/5542870), downstream of salmon. . Anyway, thanks again for the detailed report! We'll look into the logging issue, and please let me know if my description above answers your question.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362:522,Usability,learn,learned,522,"Hi Stephen,. So, the variation you see when you re-run salmon multiple times is _expected_ to be different (and _much_ smaller) than the variance you see when bootstrapping. Why is this? When you re-run salmon, the only variance you are seeing is due to small differences in the order of observations / updates from the streaming collapsed variational Bayes phase of the algorithm. This, in turn can have a _slight_ effect on the initialization conditions of the offline phase of the algorithm, and some of the parameters learned for the auxiliary parameters. However, in each run, you are observing _exactly_ the same set of reads and salmon is producing _exactly_ the same set of alignments; only the order and therefore some of the streaming updates change. So, we expect the final estimated abundances to be _very_ similar to each other. However, when salmon performs bootstrapping, it is actually resampling _with replacement_, from the counts of the range-factorized equivalence classes. Roughly, we expect this resampling to be similar to if we re-sampled _with replacement_ from the original set of input reads. That is, we are re-sampling from our population sample — the observed set of reads — to estimate the variance due to inference. So, for the bootstrap re-samplings, we expect significantly more variance than between subsequent runs of salmon, because the observations from which we are making the inference are actually changing. It is possible e.g. that some uniquely mapped reads may not be chosen in some bootstrap sample (since we are re-sampling the observed read count, but doing so _with replacement_), and so the estimates of sets of related isoforms will change in those samples. Thus, since the observations themselves are changing, we expect the estimates to display greater variance. In fact, this is the main goal of performing the bootstrapping (or Gibbs sampling) — to estimate the uncertainty due to inference if we had observed many reads coming from the same under",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/466#issuecomment-568828362
https://github.com/COMBINE-lab/salmon/issues/469#issuecomment-569966210:323,Security,hash,hash,323,"Writing the targets to JSON is probably a good idea. My only remaining concern would be how to make it as easy as possible to compare these files against each other:. - Does target order matter? If so, JSON parsers will almost always make an unordered object during deserialization. ; - Will a user just be able to diff or hash these files against each other? If so I’d say that’s the best solution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/469#issuecomment-569966210
https://github.com/COMBINE-lab/salmon/issues/474#issuecomment-615160502:52,Integrability,message,message,52,"In Salmon v1.2.0, `--posBias` is listed in the help message without any hints on being ""experimental"". At the same time, the read the docs manual still lists it as an experimental feature. Does someone know if it is now considered as a stable feature?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/474#issuecomment-615160502
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574392998:20,Deployability,update,update,20,"Hi @ialbert,. Small update. I've identified the issue. Internally, the position is being computed properly (hence the appropriate alignment score `AS` tag) for the read. However, there is a prefix offset we compute when the initial part of the read before the first MEM may contain indels under the optimal alignment. This should be added to the position, and that isn't currently happening. We'll fix this upstream. It would be trivial, except that I have to think about reverse complements ... sigh.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574392998
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574472965:517,Deployability,Update,Update,517,"@ialbert (and @mdshw5 who seems to be peeking in ;P),. Ok, actually, the adjustment code seems correct. The issue actually seems to be stemming from the use of sse in the call to ksw2. It's actually a bit unclear why this is happening, but interestingly, if we *don't* use the sse variant, we get the correct positions for these alignments. So, we know how to fix these case, but I'd like to understand *why* better before I push the fix upstream (and make sure it doesn't adversely affect other alignments). _edit_: Update to clarify the culprit is `extz2_sse` in combination with `KSW_EZ_RIGHT` and not just `KSW_EZ_RIGHT` by itself.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574472965
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:111,Testability,test,test,111,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:232,Testability,test,test,232,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:415,Testability,test,test,415,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:784,Testability,test,test,784,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993:677,Usability,simpl,simply,677,"@mohsenzakeri — if you have any insight here, I'd be interested to know your thoughts. Check out the following test from the ksw2 cli program (modified to output the contents of the `ksw_extz_t` structure:. *with extz*. ```; ./ksw2-test -s -t extz AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:16; ```. *with extz2sse*. ```; ./ksw2-test -s -t extz2_sse AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA TCGGGCATTACCGGATC; first second -48 max:0 max_t:-1 max_q:-1 mqe:-18 mte:-48 mqe_t:13 mte_q:3; ```. note, specifically, the differences in the `mte_q` field. Presumably, using the sse instructions should simply speed things up, not change the results! The strings here are taken from the actual strings for the test read. The first `AGCAGAAGCGGGTATTGAGGAGCGTAAATTGTAGTTA` is the buffer of the reference and the second `TCGGGCATTACCGGATC` is the bit of the read before the first MEM.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574491993
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940:16,Deployability,update,update,16,"Ok all; another update. The issue I raise above still exists (differences between calls to `ksw_extz` and `ksw_extz2sse`). *However*, I think that what is happening in this case is actually explained more simply. That is, the positions being reported by salmon are _correct_ given the optimal alignment. Specifically, salmon is performing an end-to-end alignment of the read, and the optimal alignment here includes an indel of length 3 in the initial portion of the read. If we were outputting the CIGAR string along with the position, then the bases would line up because the ""off by 3"" issue that happens above for the reads would be addressed when walking the CIGAR. However, we don't (currently) output the CIGAR — rather, we output a decoy CIGAR that does not represent the optimal alignment as computed by ksw2. So, if we assume all matches / mismatches (an indel-free prefix for this read), then we see the position shift noted in the initial bug report. I think the easiest solution, for the time being at least, is to report the position as if the prefix before the first MEM is indel free under the optimal alignment (even if it is not and the optimal score reflects that). However, if there are other suggestions for the best way to address this, I'm open to those as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940:328,Performance,perform,performing,328,"Ok all; another update. The issue I raise above still exists (differences between calls to `ksw_extz` and `ksw_extz2sse`). *However*, I think that what is happening in this case is actually explained more simply. That is, the positions being reported by salmon are _correct_ given the optimal alignment. Specifically, salmon is performing an end-to-end alignment of the read, and the optimal alignment here includes an indel of length 3 in the initial portion of the read. If we were outputting the CIGAR string along with the position, then the bases would line up because the ""off by 3"" issue that happens above for the reads would be addressed when walking the CIGAR. However, we don't (currently) output the CIGAR — rather, we output a decoy CIGAR that does not represent the optimal alignment as computed by ksw2. So, if we assume all matches / mismatches (an indel-free prefix for this read), then we see the position shift noted in the initial bug report. I think the easiest solution, for the time being at least, is to report the position as if the prefix before the first MEM is indel free under the optimal alignment (even if it is not and the optimal score reflects that). However, if there are other suggestions for the best way to address this, I'm open to those as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940:205,Usability,simpl,simply,205,"Ok all; another update. The issue I raise above still exists (differences between calls to `ksw_extz` and `ksw_extz2sse`). *However*, I think that what is happening in this case is actually explained more simply. That is, the positions being reported by salmon are _correct_ given the optimal alignment. Specifically, salmon is performing an end-to-end alignment of the read, and the optimal alignment here includes an indel of length 3 in the initial portion of the read. If we were outputting the CIGAR string along with the position, then the bases would line up because the ""off by 3"" issue that happens above for the reads would be addressed when walking the CIGAR. However, we don't (currently) output the CIGAR — rather, we output a decoy CIGAR that does not represent the optimal alignment as computed by ksw2. So, if we assume all matches / mismatches (an indel-free prefix for this read), then we see the position shift noted in the initial bug report. I think the easiest solution, for the time being at least, is to report the position as if the prefix before the first MEM is indel free under the optimal alignment (even if it is not and the optimal score reflects that). However, if there are other suggestions for the best way to address this, I'm open to those as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574719940
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574738566:538,Availability,down,down,538,"From my somewhat superficial understanding, I have feeling that it is the only the soft clipping (deletion at either end) that causes the problem (and not INDELs in general). I think it might be better if the CIGAR string contained the soft clipping operations (the POS would still need to be shifted) of course. Right now when one visualizes the BAM file various distracting artifacts manifest themselves with both salmon and kallisto even when the POS field is correct. See the image below:. ![Alignments](https://www.ialbert.me/static/down/pseudo_alignments/pseudo_aln.png). (Top Kallisto, second Salmon, bottom Hisat. ). The soft clipped sequences are not marked as such, therefore lead to ugly misalignment at the ends, that in turn dominate the visualization. . Ideally, the pseudo-bam should look a little more like Hisat, I don't know how feasible that is though, perhaps knowing that only the ends need to be fixed makes for a simpler solution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574738566
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574738566:936,Usability,simpl,simpler,936,"From my somewhat superficial understanding, I have feeling that it is the only the soft clipping (deletion at either end) that causes the problem (and not INDELs in general). I think it might be better if the CIGAR string contained the soft clipping operations (the POS would still need to be shifted) of course. Right now when one visualizes the BAM file various distracting artifacts manifest themselves with both salmon and kallisto even when the POS field is correct. See the image below:. ![Alignments](https://www.ialbert.me/static/down/pseudo_alignments/pseudo_aln.png). (Top Kallisto, second Salmon, bottom Hisat. ). The soft clipped sequences are not marked as such, therefore lead to ugly misalignment at the ends, that in turn dominate the visualization. . Ideally, the pseudo-bam should look a little more like Hisat, I don't know how feasible that is though, perhaps knowing that only the ends need to be fixed makes for a simpler solution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-574738566
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:867,Deployability,release,release,867,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:434,Testability,test,test,434,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:545,Testability,test,tested,545,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:773,Testability,test,testing,773,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053:605,Usability,feedback,feedback,605,"@ialbert,. Thanks again for the detailed bug report and reproducible example for this. We (@mohsenzakeri and I) have pushed experimental support for soft-clipping to the develop branch. You can enable this feature by passing `--softclip` flag to the `quant` command. We have also made a pre-compiled binary that includes this feature [here](https://drive.google.com/open?id=1Si1BqGXLievhol-e3RWjhxzajvVHY2mS). If you have a chance to test this on some of your data to see if the soft-clipping is working as expected in IGV on a larger scale (we tested on the data you provided), we'd be happy to have any feedback. In a future version, we will likely provide the ability to write the full CIGAR string (with mismatches, indels, etc.) out, but that requires the merging and testing of two branches of pufferfish upstream, and so will probably be reserved for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/475#issuecomment-596774053
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-575858246:87,Testability,benchmark,benchmark,87,"I assumed it was latter :) I can re-run it in a few days with more memory and actually benchmark (time, max memory, etc) it if that's helpful!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-575858246
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416:999,Availability,fault,faults,999,"I also have had to submit indexing jobs with much larger resources and times since the switch to the new indexing method with whole genome decoys. Prior to this I could built and index asking for only 16GB of ram in minutes. Now, I have to request ~256GB of memory and it runs for 7-10 hours. These are just ""standard"" mouse transcriptomes (GENCODE M23). I should note that using 17-mers as my kmer length dramatically increased these requirements. I re-ran using 31-mers, and the time reduces to a couple of hours and only used ~20GB of memory. I've attached two files that have summaries of the resources used in the jobs I ran in the above. Everything about these jobs is the same, except for the k-mer lengths. I requested the same amount of resources for each, but you can see that the one labeled 31mer has drastically less ""ru_maxrss"", which is the maximum amount of memory used by the process (it's in KB, although it's not labeled in the log). I also noted that there weren't any hard page faults for either of the jobs (""ru_majflt""). The longer job did have more soft page faults/page reclaims (""ru_minflt""). I don't know if that's useful information or not. [qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4172209/qacct-17mer.log); [qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4172210/qacct-31mer.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416:1083,Availability,fault,faults,1083,"I also have had to submit indexing jobs with much larger resources and times since the switch to the new indexing method with whole genome decoys. Prior to this I could built and index asking for only 16GB of ram in minutes. Now, I have to request ~256GB of memory and it runs for 7-10 hours. These are just ""standard"" mouse transcriptomes (GENCODE M23). I should note that using 17-mers as my kmer length dramatically increased these requirements. I re-ran using 31-mers, and the time reduces to a couple of hours and only used ~20GB of memory. I've attached two files that have summaries of the resources used in the jobs I ran in the above. Everything about these jobs is the same, except for the k-mer lengths. I requested the same amount of resources for each, but you can see that the one labeled 31mer has drastically less ""ru_maxrss"", which is the maximum amount of memory used by the process (it's in KB, although it's not labeled in the log). I also noted that there weren't any hard page faults for either of the jobs (""ru_majflt""). The longer job did have more soft page faults/page reclaims (""ru_minflt""). I don't know if that's useful information or not. [qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4172209/qacct-17mer.log); [qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4172210/qacct-31mer.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416:486,Energy Efficiency,reduce,reduces,486,"I also have had to submit indexing jobs with much larger resources and times since the switch to the new indexing method with whole genome decoys. Prior to this I could built and index asking for only 16GB of ram in minutes. Now, I have to request ~256GB of memory and it runs for 7-10 hours. These are just ""standard"" mouse transcriptomes (GENCODE M23). I should note that using 17-mers as my kmer length dramatically increased these requirements. I re-ran using 31-mers, and the time reduces to a couple of hours and only used ~20GB of memory. I've attached two files that have summaries of the resources used in the jobs I ran in the above. Everything about these jobs is the same, except for the k-mer lengths. I requested the same amount of resources for each, but you can see that the one labeled 31mer has drastically less ""ru_maxrss"", which is the maximum amount of memory used by the process (it's in KB, although it's not labeled in the log). I also noted that there weren't any hard page faults for either of the jobs (""ru_majflt""). The longer job did have more soft page faults/page reclaims (""ru_minflt""). I don't know if that's useful information or not. [qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4172209/qacct-17mer.log); [qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4172210/qacct-31mer.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416:947,Testability,log,log,947,"I also have had to submit indexing jobs with much larger resources and times since the switch to the new indexing method with whole genome decoys. Prior to this I could built and index asking for only 16GB of ram in minutes. Now, I have to request ~256GB of memory and it runs for 7-10 hours. These are just ""standard"" mouse transcriptomes (GENCODE M23). I should note that using 17-mers as my kmer length dramatically increased these requirements. I re-ran using 31-mers, and the time reduces to a couple of hours and only used ~20GB of memory. I've attached two files that have summaries of the resources used in the jobs I ran in the above. Everything about these jobs is the same, except for the k-mer lengths. I requested the same amount of resources for each, but you can see that the one labeled 31mer has drastically less ""ru_maxrss"", which is the maximum amount of memory used by the process (it's in KB, although it's not labeled in the log). I also noted that there weren't any hard page faults for either of the jobs (""ru_majflt""). The longer job did have more soft page faults/page reclaims (""ru_minflt""). I don't know if that's useful information or not. [qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4172209/qacct-17mer.log); [qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4172210/qacct-31mer.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416:1182,Testability,log,log,1182,"I also have had to submit indexing jobs with much larger resources and times since the switch to the new indexing method with whole genome decoys. Prior to this I could built and index asking for only 16GB of ram in minutes. Now, I have to request ~256GB of memory and it runs for 7-10 hours. These are just ""standard"" mouse transcriptomes (GENCODE M23). I should note that using 17-mers as my kmer length dramatically increased these requirements. I re-ran using 31-mers, and the time reduces to a couple of hours and only used ~20GB of memory. I've attached two files that have summaries of the resources used in the jobs I ran in the above. Everything about these jobs is the same, except for the k-mer lengths. I requested the same amount of resources for each, but you can see that the one labeled 31mer has drastically less ""ru_maxrss"", which is the maximum amount of memory used by the process (it's in KB, although it's not labeled in the log). I also noted that there weren't any hard page faults for either of the jobs (""ru_majflt""). The longer job did have more soft page faults/page reclaims (""ru_minflt""). I don't know if that's useful information or not. [qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4172209/qacct-17mer.log); [qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4172210/qacct-31mer.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416:1251,Testability,log,log,1251,"I also have had to submit indexing jobs with much larger resources and times since the switch to the new indexing method with whole genome decoys. Prior to this I could built and index asking for only 16GB of ram in minutes. Now, I have to request ~256GB of memory and it runs for 7-10 hours. These are just ""standard"" mouse transcriptomes (GENCODE M23). I should note that using 17-mers as my kmer length dramatically increased these requirements. I re-ran using 31-mers, and the time reduces to a couple of hours and only used ~20GB of memory. I've attached two files that have summaries of the resources used in the jobs I ran in the above. Everything about these jobs is the same, except for the k-mer lengths. I requested the same amount of resources for each, but you can see that the one labeled 31mer has drastically less ""ru_maxrss"", which is the maximum amount of memory used by the process (it's in KB, although it's not labeled in the log). I also noted that there weren't any hard page faults for either of the jobs (""ru_majflt""). The longer job did have more soft page faults/page reclaims (""ru_minflt""). I don't know if that's useful information or not. [qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4172209/qacct-17mer.log); [qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4172210/qacct-31mer.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416:1270,Testability,log,log,1270,"I also have had to submit indexing jobs with much larger resources and times since the switch to the new indexing method with whole genome decoys. Prior to this I could built and index asking for only 16GB of ram in minutes. Now, I have to request ~256GB of memory and it runs for 7-10 hours. These are just ""standard"" mouse transcriptomes (GENCODE M23). I should note that using 17-mers as my kmer length dramatically increased these requirements. I re-ran using 31-mers, and the time reduces to a couple of hours and only used ~20GB of memory. I've attached two files that have summaries of the resources used in the jobs I ran in the above. Everything about these jobs is the same, except for the k-mer lengths. I requested the same amount of resources for each, but you can see that the one labeled 31mer has drastically less ""ru_maxrss"", which is the maximum amount of memory used by the process (it's in KB, although it's not labeled in the log). I also noted that there weren't any hard page faults for either of the jobs (""ru_majflt""). The longer job did have more soft page faults/page reclaims (""ru_minflt""). I don't know if that's useful information or not. [qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4172209/qacct-17mer.log); [qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4172210/qacct-31mer.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416:1339,Testability,log,log,1339,"I also have had to submit indexing jobs with much larger resources and times since the switch to the new indexing method with whole genome decoys. Prior to this I could built and index asking for only 16GB of ram in minutes. Now, I have to request ~256GB of memory and it runs for 7-10 hours. These are just ""standard"" mouse transcriptomes (GENCODE M23). I should note that using 17-mers as my kmer length dramatically increased these requirements. I re-ran using 31-mers, and the time reduces to a couple of hours and only used ~20GB of memory. I've attached two files that have summaries of the resources used in the jobs I ran in the above. Everything about these jobs is the same, except for the k-mer lengths. I requested the same amount of resources for each, but you can see that the one labeled 31mer has drastically less ""ru_maxrss"", which is the maximum amount of memory used by the process (it's in KB, although it's not labeled in the log). I also noted that there weren't any hard page faults for either of the jobs (""ru_majflt""). The longer job did have more soft page faults/page reclaims (""ru_minflt""). I don't know if that's useful information or not. [qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4172209/qacct-17mer.log); [qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4172210/qacct-31mer.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583526416
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583546762:429,Security,access,access,429,"@cljacobs,. 256G and 7-10 hours; holy **expletive deleted**. This _certainly_ has to do with the creation of many small temporary files by TwoPaCo during the initial cdbg creation. Can you say something specific about the setup of the cluster on which you are running these jobs? When we build on the M23 transcriptome using the genome as decoys, it takes ~30 minutes and 16-18G of memory. What is the situation in terms of disk access on your cluster? Will the index be constructed on an written to a local disk, or to a networked file system? I imagine the latter could become _much_ (pathologically?) slower.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583546762
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:401,Energy Efficiency,schedul,scheduling,401,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:250,Modifiability,config,configured,250,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:599,Testability,log,log,599,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:737,Testability,log,logs,737,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:1243,Testability,log,log,1243,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:1339,Testability,log,log,1339,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362:1492,Usability,clear,clear,1492,"Hi @rob-p. I'm using a SGE-based cluster. The disk I'm writing to is a networked disk that is mounted via NFS on the machines the cluster runs on. I've attached the output of running `qconf -sconf`, which provides details on how the cluster has been configured (I've edited out some lines about the admin e-mails, etc.). I'm not sure how useful much of this information is. A lot of it has to do with scheduling of jobs -- how many jobs/resources users can attempt to claim, that kind of thing. Let me know if there's something else that would be more useful in this context. I've also attached the log that was generated by the indexing run itself (just for the 17mer index), just in case. I can say one thing from having inspected the logs of these things failing a number of times before I finally caved and started giving it insane amounts of memory: by far the longest time and (most likely) the biggest resource hog is between the first and second pass. Even with only 16GB, it manages to complete the first pass (it still takes quite a while, though):. ```; Pass	Filling	Filtering; 1	718	3236	; 2	1839	237; ```. [qconf-sconf.txt](https://github.com/COMBINE-lab/salmon/files/4172585/qconf-sconf.txt); [index_GRCm38_GENCODE_M23_PRI_17mer.log.txt](https://github.com/COMBINE-lab/salmon/files/4172594/index_GRCm38_GENCODE_M23_PRI_17mer.log.txt). EDIT: Oh, I should also probably say, that I'm only seeing this slowdown on index creation. I'm sure that was implied, but I just wanted to be clear that at the moment, I'm happy enough to let the index build for a few hours every once in a while. I'm still saving huge amounts of mapping time, relative to ""full"" aligners.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-583567362
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-587082126:197,Deployability,update,updated,197,"Hi @cljacobs,. There was at least one unnecessarily large allocation within our pufferfish code, and now Ilia has also massively optimized the intermediate disk space usage behavior of TwoPaCo. An updated binary that incorporates these changes can be obtained [here](https://drive.google.com/open?id=1QHYCT3Vs9bRD7UmJY6JJKjlzmmUE4wRl). If you have a chance, it would be fantastic if you could test this out and see how the resource requirements change for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-587082126
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-587082126:129,Performance,optimiz,optimized,129,"Hi @cljacobs,. There was at least one unnecessarily large allocation within our pufferfish code, and now Ilia has also massively optimized the intermediate disk space usage behavior of TwoPaCo. An updated binary that incorporates these changes can be obtained [here](https://drive.google.com/open?id=1QHYCT3Vs9bRD7UmJY6JJKjlzmmUE4wRl). If you have a chance, it would be fantastic if you could test this out and see how the resource requirements change for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-587082126
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-587082126:393,Testability,test,test,393,"Hi @cljacobs,. There was at least one unnecessarily large allocation within our pufferfish code, and now Ilia has also massively optimized the intermediate disk space usage behavior of TwoPaCo. An updated binary that incorporates these changes can be obtained [here](https://drive.google.com/open?id=1QHYCT3Vs9bRD7UmJY6JJKjlzmmUE4wRl). If you have a chance, it would be fantastic if you could test this out and see how the resource requirements change for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-587082126
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:992,Availability,avail,available,992,"Hi @rob-p,. I was finally able to grab some time to try running the beta version you linked (see attached logs). This certainly helped, although I'm still nowhere near a time-frame of ~30min. Here are my results:. The 31-mer running took a bit over an hour and consumed ~17GB of memory. This is about half the running time as the previous version, but approx. the same amount of memory requirement (more on that below). The 17-mer running, took 4.5hrs to complete and consumed ~64GB of memory. This particular running is again, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, accordi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:1279,Availability,fault,faults,1279,"ing took a bit over an hour and consumed ~17GB of memory. This is about half the running time as the previous version, but approx. the same amount of memory requirement (more on that below). The 17-mer running, took 4.5hrs to complete and consumed ~64GB of memory. This particular running is again, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:1476,Availability,fault,faults,1476,"n, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2626,Availability,fault,faulted,2626,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:894,Deployability,update,update,894,"Hi @rob-p,. I was finally able to grab some time to try running the beta version you linked (see attached logs). This certainly helped, although I'm still nowhere near a time-frame of ~30min. Here are my results:. The 31-mer running took a bit over an hour and consumed ~17GB of memory. This is about half the running time as the previous version, but approx. the same amount of memory requirement (more on that below). The 17-mer running, took 4.5hrs to complete and consumed ~64GB of memory. This particular running is again, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, accordi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:1796,Deployability,UPDATE,UPDATE,1796,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:1098,Energy Efficiency,power,powers,1098," you linked (see attached logs). This certainly helped, although I'm still nowhere near a time-frame of ~30min. Here are my results:. The 31-mer running took a bit over an hour and consumed ~17GB of memory. This is about half the running time as the previous version, but approx. the same amount of memory requirement (more on that below). The 17-mer running, took 4.5hrs to complete and consumed ~64GB of memory. This particular running is again, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossib",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2225,Energy Efficiency,allocate,allocated,2225,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2246,Energy Efficiency,schedul,scheduler,2246,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:574,Integrability,depend,depends,574,"Hi @rob-p,. I was finally able to grab some time to try running the beta version you linked (see attached logs). This certainly helped, although I'm still nowhere near a time-frame of ~30min. Here are my results:. The 31-mer running took a bit over an hour and consumed ~17GB of memory. This is about half the running time as the previous version, but approx. the same amount of memory requirement (more on that below). The 17-mer running, took 4.5hrs to complete and consumed ~64GB of memory. This particular running is again, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, accordi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2130,Performance,race condition,race condition,2130,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:106,Testability,log,logs,106,"Hi @rob-p,. I was finally able to grab some time to try running the beta version you linked (see attached logs). This certainly helped, although I'm still nowhere near a time-frame of ~30min. Here are my results:. The 31-mer running took a bit over an hour and consumed ~17GB of memory. This is about half the running time as the previous version, but approx. the same amount of memory requirement (more on that below). The 17-mer running, took 4.5hrs to complete and consumed ~64GB of memory. This particular running is again, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, accordi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:723,Testability,test,tested,723,"Hi @rob-p,. I was finally able to grab some time to try running the beta version you linked (see attached logs). This certainly helped, although I'm still nowhere near a time-frame of ~30min. Here are my results:. The 31-mer running took a bit over an hour and consumed ~17GB of memory. This is about half the running time as the previous version, but approx. the same amount of memory requirement (more on that below). The 17-mer running, took 4.5hrs to complete and consumed ~64GB of memory. This particular running is again, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, accordi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:941,Testability,log,logs,941,"Hi @rob-p,. I was finally able to grab some time to try running the beta version you linked (see attached logs). This certainly helped, although I'm still nowhere near a time-frame of ~30min. Here are my results:. The 31-mer running took a bit over an hour and consumed ~17GB of memory. This is about half the running time as the previous version, but approx. the same amount of memory requirement (more on that below). The 17-mer running, took 4.5hrs to complete and consumed ~64GB of memory. This particular running is again, about twice as fast, although the time really depends on the memory limitations I gave it. Since it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, accordi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:1613,Testability,log,log,1613,"e it appears that this version no longer crashes when given less than about 250GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:1688,Testability,log,log,1688,"50GB of memory, I also tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [ind",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:1713,Testability,log,log,1713,"tested with 32G and 16GB of memory, just to see what impact this would have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:1788,Testability,log,log,1788,"uld have on the times. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2022,Testability,log,log,2022,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2412,Testability,log,logs,2412,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2555,Testability,test,tested,2555,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2723,Testability,log,log,2723,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702:2805,Testability,log,log,2805,"es. Those jobs are still running (it's only been about 4hrs as of this writing, I'll update my post if/when they complete). Current logs are showing that they quickly consume all the available memory, but have not yet crashed. I've also got versions with 128-512GB of memory requested (by powers of 2) for comparison. Some random notes: both the 31-mer index experienced about twice as many soft page reclaims with the new/faster version and experienced a few hard page faults (the previous version saw none of the latter). The 17-mer version experienced fewer page reclaims than any of the 31-mer indices and far fewer than with the prior version. Again, a few page faults crept in, but relatively few by percentage and likely not contributing any significant amount of time overall. [index-qacct-17mer.log](https://github.com/COMBINE-lab/salmon/files/4246516/index-qacct-17mer.log); [index-qacct-31mer.log](https://github.com/COMBINE-lab/salmon/files/4246517/index-qacct-31mer.log). **UPDATE**; The 16GB version finished running. It actually only took a little over 4 hours to run, as well. The troubling thing about this job seems to be that, despite having successfully completed, according to the accounting log it used over 20GB of memory... which should be impossible to do. Our resident experts suspect there's a race condition occurring at the tail end of the job and that all of that extra memory is being allocated before the scheduler can kill it for exceeding the limit. Whatever the case, though, this throws into question some of those numbers that I've been grabbing from the accounting logs --- it's either being misreported, or the memory gobbling is happening so rapidly that it may not, in fact, be being properly recorded. I tested the index anyway. It *appears* to be working just fine. Nothing faulted or crashed when I attempted to quantify some reads against it. [index-qacct-17mer-16gigs.log](https://github.com/COMBINE-lab/salmon/files/4247214/index-qacct-17mer-16gigs.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590516702
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590565684:791,Performance,perform,performance,791,"Hi @cljacobs,. Thank you again for the detailed info! Just to verify, what you are indexing here is the transcriptome ([this](ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M24/gencode.vM24.transcripts.fa.gz) file), using the genome ([this](ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M24/GRCm38.primary_assembly.genome.fa.gz) file) as decoy? Both the memory requirement and _definitely_ the time requirement are something that I've not been able to reproduce. I wonder if you could say something about the disk where the index is being written and where the program is being run. If this is all being done on NFS partitions, would it be possible to write the index to a local scratch on the node to see if disk access times have anything to do with the performance? I am scratching my head a bit about the memory though, because I don't have a good explanation for the discrepancy on those numbers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590565684
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590565684:749,Security,access,access,749,"Hi @cljacobs,. Thank you again for the detailed info! Just to verify, what you are indexing here is the transcriptome ([this](ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M24/gencode.vM24.transcripts.fa.gz) file), using the genome ([this](ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M24/GRCm38.primary_assembly.genome.fa.gz) file) as decoy? Both the memory requirement and _definitely_ the time requirement are something that I've not been able to reproduce. I wonder if you could say something about the disk where the index is being written and where the program is being run. If this is all being done on NFS partitions, would it be possible to write the index to a local scratch on the node to see if disk access times have anything to do with the performance? I am scratching my head a bit about the memory though, because I don't have a good explanation for the discrepancy on those numbers.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590565684
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590575743:927,Modifiability,variab,variable,927,"Hi @rob-p,. I'm not seeing any links in your post (I assume that the references to ""this file"" were meant to be pointers to GENCODE's FTP files). Just to make sure, here are links to the files I've been working with:. - Decoy FASTA: [GRCm38.p6](http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M24/GRCm38.p6.genome.fa.gz); - Transcriptome: [GENCODE vM24](http://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M24/gencode.vM24.primary_assembly.annotation.gtf.gz). In order to create the requisite FASTA of the transcripts from this GTF, I used [gffread](https://github.com/gpertea/gffread) (version 0.11.6). I'll look into trying to write the index to a local scratch directory. The temporary directory for these nodes is similarly mounted via NFS and we are actively discouraged from writing anything to `/tmp` itself (`$TMPDIR` points to to the mounted drive for programs that obey this environment variable).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590575743
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590592852:35,Testability,test,test,35,"I edited in the results of my 16GB test to the post above. There's something a bit weird going on with that job in particular. However, I can say that despite the (relatively) severe memory limitation, it appears that everything worked out just fine in ~4hrs. Not the fastest time overall, but factoring the times that I was stuck waiting for resources on those huge jobs, this is definitely an improvement.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-590592852
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-603455186:741,Deployability,update,updated,741,I just tried to index the GRCh38 transcriptome with decoys on a local machine (macOS Catalina 16GB) and it seemed to be doing ok for a while but after ~3hrs it got stuck (see log attached). I waited a good long while before killing it. Any suggestions?; These are the files I'm using:; ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/gencode.v33.transcripts.fa.gz; ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/GRCh38.primary_assembly.genome.fa.gz. [ref_indexing.log](https://github.com/COMBINE-lab/salmon/files/4377206/ref_indexing.log). EDIT: works fine without using decoys.; EDIT2: do you recommend using the indexes you link to here: http://bit.ly/30yn3FJ ? Or is there someway you could link to updated versions for the latest Salmon? ; thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-603455186
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-603455186:175,Testability,log,log,175,I just tried to index the GRCh38 transcriptome with decoys on a local machine (macOS Catalina 16GB) and it seemed to be doing ok for a while but after ~3hrs it got stuck (see log attached). I waited a good long while before killing it. Any suggestions?; These are the files I'm using:; ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/gencode.v33.transcripts.fa.gz; ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/GRCh38.primary_assembly.genome.fa.gz. [ref_indexing.log](https://github.com/COMBINE-lab/salmon/files/4377206/ref_indexing.log). EDIT: works fine without using decoys.; EDIT2: do you recommend using the indexes you link to here: http://bit.ly/30yn3FJ ? Or is there someway you could link to updated versions for the latest Salmon? ; thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-603455186
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-603455186:503,Testability,log,log,503,I just tried to index the GRCh38 transcriptome with decoys on a local machine (macOS Catalina 16GB) and it seemed to be doing ok for a while but after ~3hrs it got stuck (see log attached). I waited a good long while before killing it. Any suggestions?; These are the files I'm using:; ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/gencode.v33.transcripts.fa.gz; ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/GRCh38.primary_assembly.genome.fa.gz. [ref_indexing.log](https://github.com/COMBINE-lab/salmon/files/4377206/ref_indexing.log). EDIT: works fine without using decoys.; EDIT2: do you recommend using the indexes you link to here: http://bit.ly/30yn3FJ ? Or is there someway you could link to updated versions for the latest Salmon? ; thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-603455186
https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-603455186:573,Testability,log,log,573,I just tried to index the GRCh38 transcriptome with decoys on a local machine (macOS Catalina 16GB) and it seemed to be doing ok for a while but after ~3hrs it got stuck (see log attached). I waited a good long while before killing it. Any suggestions?; These are the files I'm using:; ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/gencode.v33.transcripts.fa.gz; ftp://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_33/GRCh38.primary_assembly.genome.fa.gz. [ref_indexing.log](https://github.com/COMBINE-lab/salmon/files/4377206/ref_indexing.log). EDIT: works fine without using decoys.; EDIT2: do you recommend using the indexes you link to here: http://bit.ly/30yn3FJ ? Or is there someway you could link to updated versions for the latest Salmon? ; thanks!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/476#issuecomment-603455186
https://github.com/COMBINE-lab/salmon/issues/478#issuecomment-578326494:564,Deployability,update,updates,564,"Hi @lauraht,. Sure; let me answer these in order. (1) Salmon does not really make use of the read id (we assume the fasta files are synchronized in the input). The one place that not using the `-l` option might raise a complication is if you wish to dump the mappings and examine them (using `--writeMappings`), in which case it will be difficult in the SAM file to discern which read is which just from the names (though the flags should still be valid). (2) Yes, this is normal and expected behavior. Since the online phase of salmon is asynchronous, the online updates may arrive in a slightly different order. This can lead to a slightly different starting condition for the offline phase and, subsequently, small differences in the abundances. However, these differences are generally small and are _inherent_ to the uncertainty in the inference itself. That is, the estimates have inherent uncertainty that is greater than the variation you might see between runs. To assess this you can (and, perhaps should) estimate this uncertainty by asking salmon to draw inferential samples (e.g. Gibbs samples with `--numGibbsSamples`). Best,; Rob. P.S. As a general note, I'd recommending upgrading to the latest version of salmon. We update salmon quite regularly with both small and large improvements (and bug fixes where relevant). There has been quite a lot of progress since version 0.9.1.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/478#issuecomment-578326494
https://github.com/COMBINE-lab/salmon/issues/478#issuecomment-578326494:1233,Deployability,update,update,1233,"Hi @lauraht,. Sure; let me answer these in order. (1) Salmon does not really make use of the read id (we assume the fasta files are synchronized in the input). The one place that not using the `-l` option might raise a complication is if you wish to dump the mappings and examine them (using `--writeMappings`), in which case it will be difficult in the SAM file to discern which read is which just from the names (though the flags should still be valid). (2) Yes, this is normal and expected behavior. Since the online phase of salmon is asynchronous, the online updates may arrive in a slightly different order. This can lead to a slightly different starting condition for the offline phase and, subsequently, small differences in the abundances. However, these differences are generally small and are _inherent_ to the uncertainty in the inference itself. That is, the estimates have inherent uncertainty that is greater than the variation you might see between runs. To assess this you can (and, perhaps should) estimate this uncertainty by asking salmon to draw inferential samples (e.g. Gibbs samples with `--numGibbsSamples`). Best,; Rob. P.S. As a general note, I'd recommending upgrading to the latest version of salmon. We update salmon quite regularly with both small and large improvements (and bug fixes where relevant). There has been quite a lot of progress since version 0.9.1.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/478#issuecomment-578326494
https://github.com/COMBINE-lab/salmon/issues/478#issuecomment-578326494:132,Integrability,synchroniz,synchronized,132,"Hi @lauraht,. Sure; let me answer these in order. (1) Salmon does not really make use of the read id (we assume the fasta files are synchronized in the input). The one place that not using the `-l` option might raise a complication is if you wish to dump the mappings and examine them (using `--writeMappings`), in which case it will be difficult in the SAM file to discern which read is which just from the names (though the flags should still be valid). (2) Yes, this is normal and expected behavior. Since the online phase of salmon is asynchronous, the online updates may arrive in a slightly different order. This can lead to a slightly different starting condition for the offline phase and, subsequently, small differences in the abundances. However, these differences are generally small and are _inherent_ to the uncertainty in the inference itself. That is, the estimates have inherent uncertainty that is greater than the variation you might see between runs. To assess this you can (and, perhaps should) estimate this uncertainty by asking salmon to draw inferential samples (e.g. Gibbs samples with `--numGibbsSamples`). Best,; Rob. P.S. As a general note, I'd recommending upgrading to the latest version of salmon. We update salmon quite regularly with both small and large improvements (and bug fixes where relevant). There has been quite a lot of progress since version 0.9.1.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/478#issuecomment-578326494
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:502,Availability,redundant,redundant,502,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:411,Performance,perform,perform,411,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:502,Safety,redund,redundant,502,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:462,Security,validat,validateMappings,462,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875:203,Usability,clear,clear,203,"Hi Kivanc,. Thanks for the kind words, and thank you for the _extremely detailed_ report. Reports like this are a model of what every developer wishes a user did before filing an issue :). First, let me clear up what seems like might be a small source of confusion. Since both of the salmon runs are from v1.1.0, _neither_ of these are making use of quasi-mapping. Specifically, newer versions of salmon _only_ perform selective alignment (and this makes the `--validateMappings` command line argument redundant in newer versions, though we keep it so as to maximize backward compatibility with command line parameters people may be using). So, the main difference between your two salmon runs is inclusion of the decoy set. This almost certainly means that the reads that map in your second set of salmon runs but not your first are being assigned to decoys in the first case. To try and get a better handle on this, could you upload a `meta_info.json` file from both runs? This file lives in the `aux_info` directory, and it will provide information about e.g. how many reads were best mapped to decoys and were discarded for this reason. The guarantee you get from the selective alignment is that, if the fragment is discarded by decoy mapping, it maps _strictly better_ to the decoy than to the non-decoy sequence. There are many reasons this could happen. One is rRNA contamination, another could be that reads are coming from processed pseudogenes that are not properly in your annotation, yet a third is that your sample has a considerable fraction of reads spanning exon-intron junctions (in this case, the read will map better to the corresponding location on the genome, and worse to the annotate transcript where the intronic sequence is not present). Now, figuring out exactly which of these cases you are in is a bit more difficult, but one approach would be to pick one of the samples with the biggest differences and map to the reads to the genome with e.g. STAR or HISAT2 to see what y",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578848875
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578980727:357,Security,validat,validateMappings,357,"Hi Rob,. Thank you for really quick response. Also thanks for your comment on the report. Always trying to pay attention on documenting/reproducibility, and great to hear that it was helpful. . I was not so sure about writing ""quasi-mapping"". It sometimes gets confusing with all that terms so thank you for clarifying that. It's also good to know about `--validateMappings` argument and selective alignment. I will be keeping that in mind. Please find below meta_info.json files from both runs for two different samples:. _**From a sample with low mapping rate**_:; <details><summary>meta_info for salmon run with decoys</summary>; <p>. ```json; {; ""salmon_version"": ""1.1.0"",; ""samp_type"": ""bootstrap"",; ""opt_type"": ""vb"",; ""quant_errors"": [],; ""num_libraries"": 1,; ""library_types"": [; ""ISF""; ],; ""frag_dist_length"": 1001,; ""seq_bias_correct"": true,; ""gc_bias_correct"": true,; ""num_bias_bins"": 4096,; ""mapping_type"": ""mapping"",; ""num_valid_targets"": 82785,; ""num_decoy_targets"": 384,; ""num_eq_classes"": 14170,; ""serialized_eq_classes"": false,; ""eq_class_properties"": [; ""range_factorized""; ],; ""length_classes"": [; 657,; 1257,; 1868,; 2701,; 18586; ],; ""index_seq_hash"": ""4f69fdca155ff281a9051bf6f31831edd7e1f71ec8cf630564a59346d82a9791"",; ""index_name_hash"": ""7da07d00d0c1890fdedeb25e843c19981ff38fa5cdb3c2f1af25c0ff0b7aeb3e"",; ""index_seq_hash512"": ""f7410be132f10bc0aa56a9513037be738d843ec5c1326aee3eefc7af479d138630673b84705982fbbbd4783c66b40b6eccd83c4933a87220efd3ee5f3ff84d62"",; ""index_name_hash512"": ""6eb2d9569579e6fc9ecf832b8ce8b67482edaf6b6773afff0660644bda1f7ea27585ffbd53fcc50bf35495cad2a21ebde2b300e6954cdcf30be05d4b49538925"",; ""index_decoy_seq_hash"": ""b766133ec97b898f1cc1d25ec9240e9e8a54ae82cb5fa7494fe8347b6ea60b21"",; ""index_decoy_name_hash"": ""39a1e89e06638787f331ee368746cac8f09ab519442650f2bfdd6606dffa5e24"",; ""num_bootstraps"": 100,; ""num_processed"": 11225446,; ""num_mapped"": 3069202,; ""num_decoy_fragments"": 6809189,; ""num_dovetail_fragments"": 52039,; ""num_fragments_filtered_vm"": 68778",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-578980727
https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-579314493:38,Testability,log,logs,38,"Hi Kivanc,. Indeed, it looks from the logs as if, in the low mapping-rate samples, SAF with decoys is confidently assigning a lot of reads to decoy sequence. For example, out of `11225446` fragments, `6809189` map best to decoys and `3069202` map best to the annotation. This is compared to without decoys where `6166065` reads map to the annotation. Interestingly, you can see that with the decoys, the total number of reads accounted for is considerably higher. I agree that the results of SAF may be closer to that of STAR. One thing I'd be curious to know is how many of those reads aligned by STAR can be confidently assigned to exons. That's the number that you'd want to most-directly compare against the mapping rate of salmon to non-decoy targets. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/479#issuecomment-579314493
https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579829090:201,Deployability,install,installs,201,"it's not clear if the right fix is to pin the boost version [here](https://github.com/bioconda/bioconda-recipes/blob/master/recipes/salmon/meta.yaml), or just to go with the ""always use conda-forge on installs"" strategy in the documentation. I will consult with experts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579829090
https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579829090:9,Usability,clear,clear,9,"it's not clear if the right fix is to pin the boost version [here](https://github.com/bioconda/bioconda-recipes/blob/master/recipes/salmon/meta.yaml), or just to go with the ""always use conda-forge on installs"" strategy in the documentation. I will consult with experts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579829090
https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579842099:287,Availability,error,error,287,"experts tell me that you just need to be darned sure that conda-forge is a high priority channel, above bioconda, as [per instructions](https://bioconda.github.io/user/install.html#set-up-channels); that specifying multiple channels with `-c` explicitly is poor practice because it's so error prone; and that if I'm teaching people that, I'm a bad teacher. :). I'll look at the salmon documentation to make sure it's accurately represented there and then close this issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579842099
https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579842099:168,Deployability,install,install,168,"experts tell me that you just need to be darned sure that conda-forge is a high priority channel, above bioconda, as [per instructions](https://bioconda.github.io/user/install.html#set-up-channels); that specifying multiple channels with `-c` explicitly is poor practice because it's so error prone; and that if I'm teaching people that, I'm a bad teacher. :). I'll look at the salmon documentation to make sure it's accurately represented there and then close this issue.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-579842099
https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-580259997:138,Availability,error,error,138,*shrug* conda isn't mentioned in the salmon docs so I'll just leave this here for people who are trying to use bioconda and run into this error. peace out!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/480#issuecomment-580259997
https://github.com/COMBINE-lab/salmon/issues/481#issuecomment-583888968:597,Availability,error,error,597,"Hi Rob!; I think I got it, I might have posted too soon, I was too eager to run the latest Salmon :). Basically, for the first part of my question, if anyone else happens to run into here:; To successfully run Salmon Index 1.0.0+, three inputs are required:; 1. Transriptome FASTA; 2. Concatenated Transcriptome+Genome FASTA; 3. List of decoys (most likely the list of all contigs from the Genome FASTA); So on from version 1.0.0, any supplementary scripts aren't needed (as you properly specified in your latest instructions). . Regarding the second part of my question, the ERCC contigs and the error that Salmon Index gave:; The reason for this was that my ERCC entries look like this:; `ERCC-00002 ERCC exon 1 1061 0.000000 + . gene_id ""ERCC-00002""; transcript_id ""ERCC-00002"";`; i.e. the contig name and the transcript name is identical. ; This means that there will exist an entry in the Transcriptome FASTA file labeled as ERCC-00002, and an entry in the Genome FASTA file labeled as ERCC-00002.; Then, in the concatenated Transcriptome+Genome FASTA, there will first be transcript sequence ERCC-00002, and then a genome sequence ERCC-00002, which is labeled as decoy. ; Hence the error: non-decoy, then some decoys, then again the ERCC-00002 as non-decoy. So my solution here was just to omit the ERCC entries both from the decoy list and from the genome FASTA that gets concatenated to the transcriptome. They're still kept in the transcriptome FASTA, as this allows to look for expression results of these transcripts for downstream absolute normalization. Thanks for the answer Rob!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/481#issuecomment-583888968
https://github.com/COMBINE-lab/salmon/issues/481#issuecomment-583888968:1188,Availability,error,error,1188,"Hi Rob!; I think I got it, I might have posted too soon, I was too eager to run the latest Salmon :). Basically, for the first part of my question, if anyone else happens to run into here:; To successfully run Salmon Index 1.0.0+, three inputs are required:; 1. Transriptome FASTA; 2. Concatenated Transcriptome+Genome FASTA; 3. List of decoys (most likely the list of all contigs from the Genome FASTA); So on from version 1.0.0, any supplementary scripts aren't needed (as you properly specified in your latest instructions). . Regarding the second part of my question, the ERCC contigs and the error that Salmon Index gave:; The reason for this was that my ERCC entries look like this:; `ERCC-00002 ERCC exon 1 1061 0.000000 + . gene_id ""ERCC-00002""; transcript_id ""ERCC-00002"";`; i.e. the contig name and the transcript name is identical. ; This means that there will exist an entry in the Transcriptome FASTA file labeled as ERCC-00002, and an entry in the Genome FASTA file labeled as ERCC-00002.; Then, in the concatenated Transcriptome+Genome FASTA, there will first be transcript sequence ERCC-00002, and then a genome sequence ERCC-00002, which is labeled as decoy. ; Hence the error: non-decoy, then some decoys, then again the ERCC-00002 as non-decoy. So my solution here was just to omit the ERCC entries both from the decoy list and from the genome FASTA that gets concatenated to the transcriptome. They're still kept in the transcriptome FASTA, as this allows to look for expression results of these transcripts for downstream absolute normalization. Thanks for the answer Rob!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/481#issuecomment-583888968
https://github.com/COMBINE-lab/salmon/issues/481#issuecomment-583888968:1532,Availability,down,downstream,1532,"Hi Rob!; I think I got it, I might have posted too soon, I was too eager to run the latest Salmon :). Basically, for the first part of my question, if anyone else happens to run into here:; To successfully run Salmon Index 1.0.0+, three inputs are required:; 1. Transriptome FASTA; 2. Concatenated Transcriptome+Genome FASTA; 3. List of decoys (most likely the list of all contigs from the Genome FASTA); So on from version 1.0.0, any supplementary scripts aren't needed (as you properly specified in your latest instructions). . Regarding the second part of my question, the ERCC contigs and the error that Salmon Index gave:; The reason for this was that my ERCC entries look like this:; `ERCC-00002 ERCC exon 1 1061 0.000000 + . gene_id ""ERCC-00002""; transcript_id ""ERCC-00002"";`; i.e. the contig name and the transcript name is identical. ; This means that there will exist an entry in the Transcriptome FASTA file labeled as ERCC-00002, and an entry in the Genome FASTA file labeled as ERCC-00002.; Then, in the concatenated Transcriptome+Genome FASTA, there will first be transcript sequence ERCC-00002, and then a genome sequence ERCC-00002, which is labeled as decoy. ; Hence the error: non-decoy, then some decoys, then again the ERCC-00002 as non-decoy. So my solution here was just to omit the ERCC entries both from the decoy list and from the genome FASTA that gets concatenated to the transcriptome. They're still kept in the transcriptome FASTA, as this allows to look for expression results of these transcripts for downstream absolute normalization. Thanks for the answer Rob!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/481#issuecomment-583888968
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:3023,Availability,robust,robust,3023,"h is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because they map better to a decoy sequence (this information can also be obtained from `meta_info.json`). This will tell you if something strange might be going on in those samples, like extensive rRNA contamination, that would lead to the observed mapping rates. You could also get this information using a tool like STAR, by asking it to produce both genomic and transcriptomi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2780,Energy Efficiency,adapt,adapters,2780," described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because they map better to a decoy sequence (this information can also be obtained from `meta_info.json`). Thi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2780,Integrability,adapter,adapters,2780," described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because they map better to a decoy sequence (this information can also be obtained from `meta_info.json`). Thi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2780,Modifiability,adapt,adapters,2780," described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because they map better to a decoy sequence (this information can also be obtained from `meta_info.json`). Thi",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2690,Performance,perform,performs,2690,"efore used for quantification by default. Specifically, starting with 0.14, ""dovetail"" alignments [(as described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This of course, should not affect the mapping rate. However, I'd be quite curious to see if you index the reference using the _whole genome_ as decoy (i.e. the SAF method from the pre-print), how many reads are discarded because ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:1147,Security,validat,validate,1147," to generally be similar to that of quasi-mapping, but there are some important exceptions. You can find some aggregate statistics in supplementary figure 1 of the [pre-print that introduces selective alignment](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf); ; ![image](https://user-images.githubusercontent.com/361470/73905651-08d4fd00-486e-11ea-91f4-9f167f54f676.png). Here, ""orcale"" is a method that aligns the reads both to the transcriptome with Bowtie2 and the genome with STAR, and removes reads from the Bowtie2 `bam` that seem to be spuriously aligned to the transcriptome (they align to the genome outside of an annotated transcriptome with a better score than that assigned by Bowtie2 within the transcriptome). Here, you can see that in most cases most methods map a similar number of reads, but there are definitely samples where methods map more reads than the oracle, and sometimes quasi-mapping maps quite a few more. This is, to a large extent, because it doesn't validate those mappings and some of them may be spurious (i.e. the exact matches used to find the mapping in the given location would not support a high quality alignment at that location). (2) This is certainly possible that some samples get very little to no mapping. _However_, there are a few points worth noting about how the data are processed that is worth being aware of before you write such samples off. * There is a change in default behavior between salmon < 0.13 and >= 0.13 with which mappings are considered as ""concordant"" and therefore used for quantification by default. Specifically, starting with 0.14, ""dovetail"" alignments [(as described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you c",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798:2407,Usability,simpl,simply,2407,"how the data are processed that is worth being aware of before you write such samples off. * There is a change in default behavior between salmon < 0.13 and >= 0.13 with which mappings are considered as ""concordant"" and therefore used for quantification by default. Specifically, starting with 0.14, ""dovetail"" alignments [(as described in the Bowtie2 manual)](http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml#bowtie2-options-dovetail) are considered discordant. This is the same default behavior imposed by Bowtie2. If you look in the `meta_info.json` file for some of these samples (which is in the `aux_info` subdirectory of the quantification directory for a sample), you can see how many mappings are being discarded by virtue of being dovetail mappings. It is possible to allow such alignments (consider them as concordant) by passing the `--allowDovetail` flag. It is not the case that such alignments are always ""bad"", its simply that one would not expect many fragments to align in such a way, and if these constitute the overwhelming majority of the mappings, one might be suspicious about the underlying data. * Selective alignment actually _aligns_ the reads to the transcriptome. For this purpose, it performs end-to-end alignment. This means that if you suspect that the sample may contain adapters or very low-quality read ends, the reads should be trimmed prior to quantification. It is, therefore, worth checking how the mapping rate changes for some of these samples if the reads are trimmed first. * Selective alignment is more robust than quasi-mapping to the chosen value of `-k`, the minimum match length used when searching for alignments. I noticed that some of the samples contain relatively short reads, so you might see if the mapping rate changes if you adopt a smaller value of `-k` in the index (e.g. we use `23` in the [pre-print](https://www.biorxiv.org/content/10.1101/657874v2.full.pdf)). * You mention that this index doesn't contain any decoy sequence. This ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-582734798
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583784767:2035,Deployability,update,update,2035,"(with statistics) and advice! ; I really appreciate it!. I have looked into the `meta_info.json` files of these datasets. It looks like none of them has dovetail fragments. The following are the relevant information from the `meta_info.json` for each dataset:. For the 5 datasets with minimal mapping rates:. SRR9007475:; ```; ""num_processed"": 64991581,; ""num_mapped"": 1356,; ""num_decoy_fragments"": 0,; ""num_dovetail_fragments"": 0,; ""num_fragments_filtered_vm"": 1272836,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 3823916,. ```; SRR5866113:; ```; ""num_processed"": 8065000,; ""num_mapped"": 1384,; ""num_decoy_fragments"": 0,; ""num_dovetail_fragments"": 0,; ""num_fragments_filtered_vm"": 5154685,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 165001284,. ```; SRR448056:; ```; ""num_processed"": 13530942,; ""num_mapped"": 7101,; ""num_decoy_fragments"": 0,; ""num_dovetail_fragments"": 0,; ""num_fragments_filtered_vm"": 54,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 14420,; ```. SRR1535539:; ```; ""num_processed"": 8045873,; ""num_mapped"": 245,; ""num_decoy_fragments"": 0,; ""num_dovetail_fragments"": 0,; ""num_fragments_filtered_vm"": 111743,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 593521,; ```. SRR3129941:; ```; ""num_processed"": 57495682,; ""num_mapped"": 360,; ""num_decoy_fragments"": 0,; ""num_dovetail_fragments"": 0,; ""num_fragments_filtered_vm"": 358659,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 6631423,; ```. For the 4 datasets with 0 mapping rate (they do not have other info besides `""num_processed""` and `""num_mapped""`):. SRR764657:; ```; ""num_processed"": 28342632,; ""num_mapped"": 0,; ```. SRR067901:; ```; ""num_processed"": 3571366,; ""num_mapped"": 0,; ```. SRR2913241:; ```; ""num_processed"": 40070874,; ""num_mapped"": 8,; ```. SRR1182629:; ```; ""num_processed"": 15381872,; ""num_mapped"": 0,; ```. I will dig into the other possibilities that you suggested when I get time, and post the update. Thank you very much for all your help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583784767
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668:518,Energy Efficiency,adapt,adapter,518,"Hi @lauraht,. So I decided to explore just one of these to see if I could figure out what might be going on. The below is with respect to `SRR9007475`. So first, even though I processed the data with the latest version of the develop branch (which will become 1.2.0), I got basically identical results to what you reported. Simply aligning the data against an index built on a human Gencode v26 transcriptome (with no decoys) gives me a mapping rate of `0.00378202832148367%`. The first thing I did was to quality and adapter trim the data (using `fastp -i SRR9007475.fastq.gz -o SRR9007475_trimmed.fastq.gz -q 10 -w 8`) and ... whoa. This is the fastp html report [fastp.html.zip](https://github.com/COMBINE-lab/salmon/files/4176345/fastp.html.zip). So the first astounding statistic, the mean read length before trimming is 51bp (these are relatively short single-end reads). The mean read length after trimming is 21bp! So, the average read length is, in fact, less than the k-mer length used for indexing (default is k=31). On the trimmed data, the mapping rate goes up to `2.3545475882931305%`, still very low, but now there's somewhat of an explanation, the average read is shorter than a single k-mer. So, the next thing I tried was indexing with a smaller k; a _really_ small one in this case,`k=15`. Then, I re-ran on the _trimmed_ reads (the fact that the trimming took us from 51-21bp suggests that the reads had a lot of low quality bases, adapter contamination, or both). Under this setting, I still get a very low mapping rate, but it was _much_ higher — `16.766993524863488%`. The final thing I tried was seeing how the mapping rate changed as I altered `--minScoreFraction`, which is the salmon parameter that determines the alignment score that a read must achieve in order to be mapped validly. The default is 0.65. This means that the read cannot have a score < 0.65 * the maximum achievable score for the read given it's length. In the case of a 21bp read, the best score would be ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668:1452,Energy Efficiency,adapt,adapter,1452,"ranscriptome (with no decoys) gives me a mapping rate of `0.00378202832148367%`. The first thing I did was to quality and adapter trim the data (using `fastp -i SRR9007475.fastq.gz -o SRR9007475_trimmed.fastq.gz -q 10 -w 8`) and ... whoa. This is the fastp html report [fastp.html.zip](https://github.com/COMBINE-lab/salmon/files/4176345/fastp.html.zip). So the first astounding statistic, the mean read length before trimming is 51bp (these are relatively short single-end reads). The mean read length after trimming is 21bp! So, the average read length is, in fact, less than the k-mer length used for indexing (default is k=31). On the trimmed data, the mapping rate goes up to `2.3545475882931305%`, still very low, but now there's somewhat of an explanation, the average read is shorter than a single k-mer. So, the next thing I tried was indexing with a smaller k; a _really_ small one in this case,`k=15`. Then, I re-ran on the _trimmed_ reads (the fact that the trimming took us from 51-21bp suggests that the reads had a lot of low quality bases, adapter contamination, or both). Under this setting, I still get a very low mapping rate, but it was _much_ higher — `16.766993524863488%`. The final thing I tried was seeing how the mapping rate changed as I altered `--minScoreFraction`, which is the salmon parameter that determines the alignment score that a read must achieve in order to be mapped validly. The default is 0.65. This means that the read cannot have a score < 0.65 * the maximum achievable score for the read given it's length. In the case of a 21bp read, the best score would be a score of 42, so a read must obtain a score >= 27 in order to be mapped. This is already a pretty poor mapping, but I reduced it even more to 0.3 (so any read with a score > 12 would pass). This led to a mapping rate of `~46%`. However, at this point, I'm not sure I would be confident in such mappings. For example, the situation here would be a 21bp read with multiple mismatches and, much of",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668:2120,Energy Efficiency,reduce,reduced,2120," low, but now there's somewhat of an explanation, the average read is shorter than a single k-mer. So, the next thing I tried was indexing with a smaller k; a _really_ small one in this case,`k=15`. Then, I re-ran on the _trimmed_ reads (the fact that the trimming took us from 51-21bp suggests that the reads had a lot of low quality bases, adapter contamination, or both). Under this setting, I still get a very low mapping rate, but it was _much_ higher — `16.766993524863488%`. The final thing I tried was seeing how the mapping rate changed as I altered `--minScoreFraction`, which is the salmon parameter that determines the alignment score that a read must achieve in order to be mapped validly. The default is 0.65. This means that the read cannot have a score < 0.65 * the maximum achievable score for the read given it's length. In the case of a 21bp read, the best score would be a score of 42, so a read must obtain a score >= 27 in order to be mapped. This is already a pretty poor mapping, but I reduced it even more to 0.3 (so any read with a score > 12 would pass). This led to a mapping rate of `~46%`. However, at this point, I'm not sure I would be confident in such mappings. For example, the situation here would be a 21bp read with multiple mismatches and, much of the time, one or more indels. So, my conclusion, at least on this sample, is that the main issue is data quality. Trimming the reads and indexing with a smaller k can lead to a mapping rate `~16%`, and then allowing _really bad_ alignments can take it up to `~45%` (and even more — when I set `--minScoreFraction` to 0.1, I get a mapping rate of `57%`). But the level of confidence that one might derive from poorly-aligned 21bp reads is (and probably should be) quite low. I can't say, of course, that this is the situation with the other samples, as I've not looked at them. However, for this sample, and likely for some or all of the others, there is likely a data quality issue. So, perhaps the first order of",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668:518,Integrability,adapter,adapter,518,"Hi @lauraht,. So I decided to explore just one of these to see if I could figure out what might be going on. The below is with respect to `SRR9007475`. So first, even though I processed the data with the latest version of the develop branch (which will become 1.2.0), I got basically identical results to what you reported. Simply aligning the data against an index built on a human Gencode v26 transcriptome (with no decoys) gives me a mapping rate of `0.00378202832148367%`. The first thing I did was to quality and adapter trim the data (using `fastp -i SRR9007475.fastq.gz -o SRR9007475_trimmed.fastq.gz -q 10 -w 8`) and ... whoa. This is the fastp html report [fastp.html.zip](https://github.com/COMBINE-lab/salmon/files/4176345/fastp.html.zip). So the first astounding statistic, the mean read length before trimming is 51bp (these are relatively short single-end reads). The mean read length after trimming is 21bp! So, the average read length is, in fact, less than the k-mer length used for indexing (default is k=31). On the trimmed data, the mapping rate goes up to `2.3545475882931305%`, still very low, but now there's somewhat of an explanation, the average read is shorter than a single k-mer. So, the next thing I tried was indexing with a smaller k; a _really_ small one in this case,`k=15`. Then, I re-ran on the _trimmed_ reads (the fact that the trimming took us from 51-21bp suggests that the reads had a lot of low quality bases, adapter contamination, or both). Under this setting, I still get a very low mapping rate, but it was _much_ higher — `16.766993524863488%`. The final thing I tried was seeing how the mapping rate changed as I altered `--minScoreFraction`, which is the salmon parameter that determines the alignment score that a read must achieve in order to be mapped validly. The default is 0.65. This means that the read cannot have a score < 0.65 * the maximum achievable score for the read given it's length. In the case of a 21bp read, the best score would be ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668:1452,Integrability,adapter,adapter,1452,"ranscriptome (with no decoys) gives me a mapping rate of `0.00378202832148367%`. The first thing I did was to quality and adapter trim the data (using `fastp -i SRR9007475.fastq.gz -o SRR9007475_trimmed.fastq.gz -q 10 -w 8`) and ... whoa. This is the fastp html report [fastp.html.zip](https://github.com/COMBINE-lab/salmon/files/4176345/fastp.html.zip). So the first astounding statistic, the mean read length before trimming is 51bp (these are relatively short single-end reads). The mean read length after trimming is 21bp! So, the average read length is, in fact, less than the k-mer length used for indexing (default is k=31). On the trimmed data, the mapping rate goes up to `2.3545475882931305%`, still very low, but now there's somewhat of an explanation, the average read is shorter than a single k-mer. So, the next thing I tried was indexing with a smaller k; a _really_ small one in this case,`k=15`. Then, I re-ran on the _trimmed_ reads (the fact that the trimming took us from 51-21bp suggests that the reads had a lot of low quality bases, adapter contamination, or both). Under this setting, I still get a very low mapping rate, but it was _much_ higher — `16.766993524863488%`. The final thing I tried was seeing how the mapping rate changed as I altered `--minScoreFraction`, which is the salmon parameter that determines the alignment score that a read must achieve in order to be mapped validly. The default is 0.65. This means that the read cannot have a score < 0.65 * the maximum achievable score for the read given it's length. In the case of a 21bp read, the best score would be a score of 42, so a read must obtain a score >= 27 in order to be mapped. This is already a pretty poor mapping, but I reduced it even more to 0.3 (so any read with a score > 12 would pass). This led to a mapping rate of `~46%`. However, at this point, I'm not sure I would be confident in such mappings. For example, the situation here would be a 21bp read with multiple mismatches and, much of",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668:518,Modifiability,adapt,adapter,518,"Hi @lauraht,. So I decided to explore just one of these to see if I could figure out what might be going on. The below is with respect to `SRR9007475`. So first, even though I processed the data with the latest version of the develop branch (which will become 1.2.0), I got basically identical results to what you reported. Simply aligning the data against an index built on a human Gencode v26 transcriptome (with no decoys) gives me a mapping rate of `0.00378202832148367%`. The first thing I did was to quality and adapter trim the data (using `fastp -i SRR9007475.fastq.gz -o SRR9007475_trimmed.fastq.gz -q 10 -w 8`) and ... whoa. This is the fastp html report [fastp.html.zip](https://github.com/COMBINE-lab/salmon/files/4176345/fastp.html.zip). So the first astounding statistic, the mean read length before trimming is 51bp (these are relatively short single-end reads). The mean read length after trimming is 21bp! So, the average read length is, in fact, less than the k-mer length used for indexing (default is k=31). On the trimmed data, the mapping rate goes up to `2.3545475882931305%`, still very low, but now there's somewhat of an explanation, the average read is shorter than a single k-mer. So, the next thing I tried was indexing with a smaller k; a _really_ small one in this case,`k=15`. Then, I re-ran on the _trimmed_ reads (the fact that the trimming took us from 51-21bp suggests that the reads had a lot of low quality bases, adapter contamination, or both). Under this setting, I still get a very low mapping rate, but it was _much_ higher — `16.766993524863488%`. The final thing I tried was seeing how the mapping rate changed as I altered `--minScoreFraction`, which is the salmon parameter that determines the alignment score that a read must achieve in order to be mapped validly. The default is 0.65. This means that the read cannot have a score < 0.65 * the maximum achievable score for the read given it's length. In the case of a 21bp read, the best score would be ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668:1452,Modifiability,adapt,adapter,1452,"ranscriptome (with no decoys) gives me a mapping rate of `0.00378202832148367%`. The first thing I did was to quality and adapter trim the data (using `fastp -i SRR9007475.fastq.gz -o SRR9007475_trimmed.fastq.gz -q 10 -w 8`) and ... whoa. This is the fastp html report [fastp.html.zip](https://github.com/COMBINE-lab/salmon/files/4176345/fastp.html.zip). So the first astounding statistic, the mean read length before trimming is 51bp (these are relatively short single-end reads). The mean read length after trimming is 21bp! So, the average read length is, in fact, less than the k-mer length used for indexing (default is k=31). On the trimmed data, the mapping rate goes up to `2.3545475882931305%`, still very low, but now there's somewhat of an explanation, the average read is shorter than a single k-mer. So, the next thing I tried was indexing with a smaller k; a _really_ small one in this case,`k=15`. Then, I re-ran on the _trimmed_ reads (the fact that the trimming took us from 51-21bp suggests that the reads had a lot of low quality bases, adapter contamination, or both). Under this setting, I still get a very low mapping rate, but it was _much_ higher — `16.766993524863488%`. The final thing I tried was seeing how the mapping rate changed as I altered `--minScoreFraction`, which is the salmon parameter that determines the alignment score that a read must achieve in order to be mapped validly. The default is 0.65. This means that the read cannot have a score < 0.65 * the maximum achievable score for the read given it's length. In the case of a 21bp read, the best score would be a score of 42, so a read must obtain a score >= 27 in order to be mapped. This is already a pretty poor mapping, but I reduced it even more to 0.3 (so any read with a score > 12 would pass). This led to a mapping rate of `~46%`. However, at this point, I'm not sure I would be confident in such mappings. For example, the situation here would be a 21bp read with multiple mismatches and, much of",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668
https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668:324,Usability,Simpl,Simply,324,"Hi @lauraht,. So I decided to explore just one of these to see if I could figure out what might be going on. The below is with respect to `SRR9007475`. So first, even though I processed the data with the latest version of the develop branch (which will become 1.2.0), I got basically identical results to what you reported. Simply aligning the data against an index built on a human Gencode v26 transcriptome (with no decoys) gives me a mapping rate of `0.00378202832148367%`. The first thing I did was to quality and adapter trim the data (using `fastp -i SRR9007475.fastq.gz -o SRR9007475_trimmed.fastq.gz -q 10 -w 8`) and ... whoa. This is the fastp html report [fastp.html.zip](https://github.com/COMBINE-lab/salmon/files/4176345/fastp.html.zip). So the first astounding statistic, the mean read length before trimming is 51bp (these are relatively short single-end reads). The mean read length after trimming is 21bp! So, the average read length is, in fact, less than the k-mer length used for indexing (default is k=31). On the trimmed data, the mapping rate goes up to `2.3545475882931305%`, still very low, but now there's somewhat of an explanation, the average read is shorter than a single k-mer. So, the next thing I tried was indexing with a smaller k; a _really_ small one in this case,`k=15`. Then, I re-ran on the _trimmed_ reads (the fact that the trimming took us from 51-21bp suggests that the reads had a lot of low quality bases, adapter contamination, or both). Under this setting, I still get a very low mapping rate, but it was _much_ higher — `16.766993524863488%`. The final thing I tried was seeing how the mapping rate changed as I altered `--minScoreFraction`, which is the salmon parameter that determines the alignment score that a read must achieve in order to be mapped validly. The default is 0.65. This means that the read cannot have a score < 0.65 * the maximum achievable score for the read given it's length. In the case of a 21bp read, the best score would be ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/482#issuecomment-583799668
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738:111,Deployability,install,installing,111,"Me too! . I tried the commands they suggest at anaconda (https://anaconda.org/bioconda/salmon), but even after installing everything I got with these commands, salmon still wants its update... (my current version after installing and updating via conda (with the channels conda-forge, bioconda, and default) is salmon 0.12.0). Is there an easy way to install it ""manually"" (I just started using Linux and haven't quite figured out how to install stuff on my own yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738:183,Deployability,update,update,183,"Me too! . I tried the commands they suggest at anaconda (https://anaconda.org/bioconda/salmon), but even after installing everything I got with these commands, salmon still wants its update... (my current version after installing and updating via conda (with the channels conda-forge, bioconda, and default) is salmon 0.12.0). Is there an easy way to install it ""manually"" (I just started using Linux and haven't quite figured out how to install stuff on my own yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738:219,Deployability,install,installing,219,"Me too! . I tried the commands they suggest at anaconda (https://anaconda.org/bioconda/salmon), but even after installing everything I got with these commands, salmon still wants its update... (my current version after installing and updating via conda (with the channels conda-forge, bioconda, and default) is salmon 0.12.0). Is there an easy way to install it ""manually"" (I just started using Linux and haven't quite figured out how to install stuff on my own yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738:351,Deployability,install,install,351,"Me too! . I tried the commands they suggest at anaconda (https://anaconda.org/bioconda/salmon), but even after installing everything I got with these commands, salmon still wants its update... (my current version after installing and updating via conda (with the channels conda-forge, bioconda, and default) is salmon 0.12.0). Is there an easy way to install it ""manually"" (I just started using Linux and haven't quite figured out how to install stuff on my own yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738:438,Deployability,install,install,438,"Me too! . I tried the commands they suggest at anaconda (https://anaconda.org/bioconda/salmon), but even after installing everything I got with these commands, salmon still wants its update... (my current version after installing and updating via conda (with the channels conda-forge, bioconda, and default) is salmon 0.12.0). Is there an easy way to install it ""manually"" (I just started using Linux and haven't quite figured out how to install stuff on my own yet)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774540738
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774550817:33,Availability,down,download,33,"Hi @DobbyLikesPenguins,. You can download a pre-compiled executable for 1.4.0 [here](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz). You should be able to just decompress it and run the binary in that folder. Let me know if that works.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774550817
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774550817:132,Availability,down,download,132,"Hi @DobbyLikesPenguins,. You can download a pre-compiled executable for 1.4.0 [here](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz). You should be able to just decompress it and run the binary in that folder. Let me know if that works.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774550817
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774550817:123,Deployability,release,releases,123,"Hi @DobbyLikesPenguins,. You can download a pre-compiled executable for 1.4.0 [here](https://github.com/COMBINE-lab/salmon/releases/download/v1.4.0/salmon-1.4.0_linux_x86_64.tar.gz). You should be able to just decompress it and run the binary in that folder. Let me know if that works.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774550817
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774555943:106,Deployability,install,install,106,"Hi @rob-p . Thank you very much for your help! I can run the binary in the folder, but is there a way to ""install"" it? ; I'm using miniconda and would like to install the current version of salmon ""permanently"" so that it just works whenever I type salmon ...; I've opened the directory via the terminal using the environment I'd like to run salmon in and run the binary and it opened v1.4.0 . However, when I type salmon (in the same terminal or a new terminal) it still opens v0.13.1 (I somehow managed to install a newer version from bioconda, I think it was $ conda install -c bioconda/label/cf201901 salmon )",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774555943
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774555943:159,Deployability,install,install,159,"Hi @rob-p . Thank you very much for your help! I can run the binary in the folder, but is there a way to ""install"" it? ; I'm using miniconda and would like to install the current version of salmon ""permanently"" so that it just works whenever I type salmon ...; I've opened the directory via the terminal using the environment I'd like to run salmon in and run the binary and it opened v1.4.0 . However, when I type salmon (in the same terminal or a new terminal) it still opens v0.13.1 (I somehow managed to install a newer version from bioconda, I think it was $ conda install -c bioconda/label/cf201901 salmon )",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774555943
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774555943:508,Deployability,install,install,508,"Hi @rob-p . Thank you very much for your help! I can run the binary in the folder, but is there a way to ""install"" it? ; I'm using miniconda and would like to install the current version of salmon ""permanently"" so that it just works whenever I type salmon ...; I've opened the directory via the terminal using the environment I'd like to run salmon in and run the binary and it opened v1.4.0 . However, when I type salmon (in the same terminal or a new terminal) it still opens v0.13.1 (I somehow managed to install a newer version from bioconda, I think it was $ conda install -c bioconda/label/cf201901 salmon )",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774555943
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774555943:570,Deployability,install,install,570,"Hi @rob-p . Thank you very much for your help! I can run the binary in the folder, but is there a way to ""install"" it? ; I'm using miniconda and would like to install the current version of salmon ""permanently"" so that it just works whenever I type salmon ...; I've opened the directory via the terminal using the environment I'd like to run salmon in and run the binary and it opened v1.4.0 . However, when I type salmon (in the same terminal or a new terminal) it still opens v0.13.1 (I somehow managed to install a newer version from bioconda, I think it was $ conda install -c bioconda/label/cf201901 salmon )",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-774555943
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:303,Deployability,install,install,303,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:342,Deployability,install,install,342,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:414,Deployability,install,install,414,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:793,Testability,log,logout,793,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:484,Usability,simpl,simplest,484,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715:511,Usability,simpl,simply,511,"Hi @DobbyLikesPenguins,. ### conda idea. So, if you want to try with conda again, I would first recommend that you create a new environment for salmon. ```; conda create --name salmon; ```. which you can then activate with . ```; conda activate salmon; ```. From this environment, you should be able to install the latest version. ```; conda install salmon; ```. or specifying version explicitly like . ```; conda install salmon=1.4.0; ```. ### using the pre-compiled executable. The simplest thing would be to simply add it to your PATH. Assuming you are using bash or a similar shell, you can do something like:. ```; export PATH=<path_to_salmon_directory>/bin:$PATH; ```. to add salmon to your path. It should choose this version when you use `salmon`. However, this will be reset when you logout. To make the change permanent, then you add this command to your bash profile (usually `~/.bash_profile`). It's a little bit different (but very similar) if you are using a different shell. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-775273715
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-778208335:77,Deployability,install,installing,77,"Hi @rob-p ; Thank you very much for your help! I finally had the time to try installing salmon again and followed your instructions for conda. It worked perfectly! I tried adding it to my PATH as well (I went to the directory where conda installed salmon and copied its ""address"" in the place of ""path_to_salmon"", and salmon works perfectly fine even in new terminals (after activating the conda environment). Since I'm new to Linux, I don't know if I managed to change the PATH permanently, but I'm happy as long as it works in its conda environment :D; Thank you very much again :D",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-778208335
https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-778208335:238,Deployability,install,installed,238,"Hi @rob-p ; Thank you very much for your help! I finally had the time to try installing salmon again and followed your instructions for conda. It worked perfectly! I tried adding it to my PATH as well (I went to the directory where conda installed salmon and copied its ""address"" in the place of ""path_to_salmon"", and salmon works perfectly fine even in new terminals (after activating the conda environment). Since I'm new to Linux, I don't know if I managed to change the PATH permanently, but I'm happy as long as it works in its conda environment :D; Thank you very much again :D",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/483#issuecomment-778208335
https://github.com/COMBINE-lab/salmon/issues/484#issuecomment-588455026:163,Testability,log,log,163,Thanks for the report @bernt-matthias. It's interesting that the exception propagation ends up at this place in the code. Would it be possible to also provide the log file for the indexing at the point you catch the indexing in this state? That will help us figure out which allocation could be going awry and why it is propagating to this location rather than causing the program to exit more abruptly.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/484#issuecomment-588455026
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586473052:852,Energy Efficiency,adapt,adapter,852,"@zhangchipku,. Yes, it seems that the biggest culprit here is `num_fragments_filtered_vm`. That is the number of fragments filtered because the best alignment failed to reach the threshold for a ""valid"" alignment. Here, `47,470,013` fragments are discarded entirely because they didn't have an alignment meeting the required quality. If these fragments (which do have matching MEMs, because alignment was carried out for them) were mapped, then the overall mapping rate would go up to `50,729,814 + 47,470,013 = 98,199,827 / 107,275,750 = ~91.5%`. Now, I wouldn't expect _all_ of these to be mappable, and some alignments might not be feasible at any reasonable quality whatsoever. My recommendation would be as follows. First, have you trimmed these reads (using e.g. `fastp` or `TrimGalore` or some such)? Very low quality read ends or (more likely) adapter contamination could cause the reads that have matching MEMs to fail to align within the required score threshold. My first recommendation would be to trim the reads and see how the mapping rate changes. Second, the required alignment score is a user-alterable parameter. By changing `--minScoreFraction` to be lower, you can allow reads with even lower alignment scores to be counted for quantification. The default value is `0.65`, so you could explore what happens if you lower this number. The number represents the fraction of the maximum achievable alignment score that a read must obtain to be considered a valid alignment. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586473052
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586473052:852,Integrability,adapter,adapter,852,"@zhangchipku,. Yes, it seems that the biggest culprit here is `num_fragments_filtered_vm`. That is the number of fragments filtered because the best alignment failed to reach the threshold for a ""valid"" alignment. Here, `47,470,013` fragments are discarded entirely because they didn't have an alignment meeting the required quality. If these fragments (which do have matching MEMs, because alignment was carried out for them) were mapped, then the overall mapping rate would go up to `50,729,814 + 47,470,013 = 98,199,827 / 107,275,750 = ~91.5%`. Now, I wouldn't expect _all_ of these to be mappable, and some alignments might not be feasible at any reasonable quality whatsoever. My recommendation would be as follows. First, have you trimmed these reads (using e.g. `fastp` or `TrimGalore` or some such)? Very low quality read ends or (more likely) adapter contamination could cause the reads that have matching MEMs to fail to align within the required score threshold. My first recommendation would be to trim the reads and see how the mapping rate changes. Second, the required alignment score is a user-alterable parameter. By changing `--minScoreFraction` to be lower, you can allow reads with even lower alignment scores to be counted for quantification. The default value is `0.65`, so you could explore what happens if you lower this number. The number represents the fraction of the maximum achievable alignment score that a read must obtain to be considered a valid alignment. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586473052
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586473052:852,Modifiability,adapt,adapter,852,"@zhangchipku,. Yes, it seems that the biggest culprit here is `num_fragments_filtered_vm`. That is the number of fragments filtered because the best alignment failed to reach the threshold for a ""valid"" alignment. Here, `47,470,013` fragments are discarded entirely because they didn't have an alignment meeting the required quality. If these fragments (which do have matching MEMs, because alignment was carried out for them) were mapped, then the overall mapping rate would go up to `50,729,814 + 47,470,013 = 98,199,827 / 107,275,750 = ~91.5%`. Now, I wouldn't expect _all_ of these to be mappable, and some alignments might not be feasible at any reasonable quality whatsoever. My recommendation would be as follows. First, have you trimmed these reads (using e.g. `fastp` or `TrimGalore` or some such)? Very low quality read ends or (more likely) adapter contamination could cause the reads that have matching MEMs to fail to align within the required score threshold. My first recommendation would be to trim the reads and see how the mapping rate changes. Second, the required alignment score is a user-alterable parameter. By changing `--minScoreFraction` to be lower, you can allow reads with even lower alignment scores to be counted for quantification. The default value is `0.65`, so you could explore what happens if you lower this number. The number represents the fraction of the maximum achievable alignment score that a read must obtain to be considered a valid alignment. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586473052
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673:404,Energy Efficiency,adapt,adapter,404,"Thanks, I usually do not trim reads. I am surprised to see such a difference from version 0.8.3. Do you have a recommendation for --minScoreFraction if I do not trim reads? Or maybe I should go back to NOT using --validateMappings?; For testing purposes, I will try trimming the reads for this sample. Will report back.; Oh, and this sample was prepared by ultra-low RNA input protocol, so the issues of adapter contamination could be present.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673:377,Integrability,protocol,protocol,377,"Thanks, I usually do not trim reads. I am surprised to see such a difference from version 0.8.3. Do you have a recommendation for --minScoreFraction if I do not trim reads? Or maybe I should go back to NOT using --validateMappings?; For testing purposes, I will try trimming the reads for this sample. Will report back.; Oh, and this sample was prepared by ultra-low RNA input protocol, so the issues of adapter contamination could be present.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673:404,Integrability,adapter,adapter,404,"Thanks, I usually do not trim reads. I am surprised to see such a difference from version 0.8.3. Do you have a recommendation for --minScoreFraction if I do not trim reads? Or maybe I should go back to NOT using --validateMappings?; For testing purposes, I will try trimming the reads for this sample. Will report back.; Oh, and this sample was prepared by ultra-low RNA input protocol, so the issues of adapter contamination could be present.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673:404,Modifiability,adapt,adapter,404,"Thanks, I usually do not trim reads. I am surprised to see such a difference from version 0.8.3. Do you have a recommendation for --minScoreFraction if I do not trim reads? Or maybe I should go back to NOT using --validateMappings?; For testing purposes, I will try trimming the reads for this sample. Will report back.; Oh, and this sample was prepared by ultra-low RNA input protocol, so the issues of adapter contamination could be present.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673:214,Security,validat,validateMappings,214,"Thanks, I usually do not trim reads. I am surprised to see such a difference from version 0.8.3. Do you have a recommendation for --minScoreFraction if I do not trim reads? Or maybe I should go back to NOT using --validateMappings?; For testing purposes, I will try trimming the reads for this sample. Will report back.; Oh, and this sample was prepared by ultra-low RNA input protocol, so the issues of adapter contamination could be present.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673:237,Testability,test,testing,237,"Thanks, I usually do not trim reads. I am surprised to see such a difference from version 0.8.3. Do you have a recommendation for --minScoreFraction if I do not trim reads? Or maybe I should go back to NOT using --validateMappings?; For testing purposes, I will try trimming the reads for this sample. Will report back.; Oh, and this sample was prepared by ultra-low RNA input protocol, so the issues of adapter contamination could be present.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586475673
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:486,Deployability,release,releases,486,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:869,Deployability,release,release,869,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:1440,Deployability,release,release,1440,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:90,Integrability,depend,depends,90,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704:1231,Usability,responsiv,responsive,1231,"@zhangchipku,. The default value of `--minScoreFraction` is quite reasonable, I think. It depends on the read length, but for a 100-bp read, it corresponds to 8 mismatches under the default scoring parameters. So the read pair could have up to 16 mismatches before being discarded. I understand that the recommendation to trim reads is a new one, but I think it is a standard best-practice anyway. However, we are looking at the possibility of allowing read-end soft-clipping in future releases, which could mitigate this need in the most common case. It is worth noting that, if you *don't* want to use selective alignment, then the last version of salmon that you can use is the one tagged as `0.15.0`. As of version 1.0.0, the index structure and default mapping algorithm changed, so that selective alignment is ""always on"". This is discussed in some detail in the release notes for version 1.0.0. Generally, we think that the benefits offered by selective-alignment are important, and, unless there is a very strong reason not to, one should generally ensure that reads sharing some exact matches with the reference also produce reasonable quality alignments at the implied loci. However, we also try to be very receptive and responsive to our users' workflows and desiderata, so if the soft-clipping feature is something that would make your experience much smoother, we will certainly consider prioritizing that feature for a future release.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586478704
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586530740:560,Energy Efficiency,charge,charged,560,"You are right on the spot. ; After trimming, every problem went away:; ""num_processed"": 102482661,; ""num_mapped"": 85812375,; ""num_decoy_fragments"": 760387,; ""num_dovetail_fragments"": 1265734,; ""num_fragments_filtered_vm"": 7722295,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 293676436,; ""percent_mapped"": 83.7335546937057,. I would really like to have the soft clipping feature though. With salmon being so fast, trimming step basically takes more time than the salmon quantification step. A lot of us are now turning to cloud platforms and are charged by the the computing time. Some other questions unrelated to this topic:; For snRNA-seq like 10X platform, do you recommend just trimming read2?; From what I read out of documentation, decoy enhanced index would only work with --validateMapping. Would Alevin only work with non-decoy index then?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586530740
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586530740:759,Modifiability,enhance,enhanced,759,"You are right on the spot. ; After trimming, every problem went away:; ""num_processed"": 102482661,; ""num_mapped"": 85812375,; ""num_decoy_fragments"": 760387,; ""num_dovetail_fragments"": 1265734,; ""num_fragments_filtered_vm"": 7722295,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 293676436,; ""percent_mapped"": 83.7335546937057,. I would really like to have the soft clipping feature though. With salmon being so fast, trimming step basically takes more time than the salmon quantification step. A lot of us are now turning to cloud platforms and are charged by the the computing time. Some other questions unrelated to this topic:; For snRNA-seq like 10X platform, do you recommend just trimming read2?; From what I read out of documentation, decoy enhanced index would only work with --validateMapping. Would Alevin only work with non-decoy index then?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586530740
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586530740:797,Security,validat,validateMapping,797,"You are right on the spot. ; After trimming, every problem went away:; ""num_processed"": 102482661,; ""num_mapped"": 85812375,; ""num_decoy_fragments"": 760387,; ""num_dovetail_fragments"": 1265734,; ""num_fragments_filtered_vm"": 7722295,; ""num_alignments_below_threshold_for_mapped_fragments_vm"": 293676436,; ""percent_mapped"": 83.7335546937057,. I would really like to have the soft clipping feature though. With salmon being so fast, trimming step basically takes more time than the salmon quantification step. A lot of us are now turning to cloud platforms and are charged by the the computing time. Some other questions unrelated to this topic:; For snRNA-seq like 10X platform, do you recommend just trimming read2?; From what I read out of documentation, decoy enhanced index would only work with --validateMapping. Would Alevin only work with non-decoy index then?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586530740
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443:211,Deployability,release,releases,211,"Thank you for verifying @zhangchipku, and thank you very much for the kind words! We appreciate the feedback and input from our users like yourself. We'll prioritize the soft-clipping functionality for upcoming releases (maybe even the next if we can make that work in time). For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443:332,Energy Efficiency,efficient,efficient,332,"Thank you for verifying @zhangchipku, and thank you very much for the kind words! We appreciate the feedback and input from our users like yourself. We'll prioritize the soft-clipping functionality for upcoming releases (maybe even the next if we can make that work in time). For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443:100,Usability,feedback,feedback,100,"Thank you for verifying @zhangchipku, and thank you very much for the kind words! We appreciate the feedback and input from our users like yourself. We'll prioritize the soft-clipping functionality for upcoming releases (maybe even the next if we can make that work in time). For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-586548443
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-592995074:68,Energy Efficiency,adapt,adapter,68,"@rob-p I would request that you try out bbduk and bbmap for quality/adapter trimming and contaminant removal.; > Thank you for verifying @zhangchipku, For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-592995074
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-592995074:207,Energy Efficiency,efficient,efficient,207,"@rob-p I would request that you try out bbduk and bbmap for quality/adapter trimming and contaminant removal.; > Thank you for verifying @zhangchipku, For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-592995074
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-592995074:68,Integrability,adapter,adapter,68,"@rob-p I would request that you try out bbduk and bbmap for quality/adapter trimming and contaminant removal.; > Thank you for verifying @zhangchipku, For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-592995074
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-592995074:68,Modifiability,adapt,adapter,68,"@rob-p I would request that you try out bbduk and bbmap for quality/adapter trimming and contaminant removal.; > Thank you for verifying @zhangchipku, For the time being, I can recommend `fastp` as a fairly efficient / fast trimmer that. It might even be able to work in a streaming fashion so that you could pipe the trimmed reads directly to salmon.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-592995074
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597344801:85,Energy Efficiency,adapt,adapter,85,"Hi @tamuanand,. Sure; is there anything specific about bbduk and bbmap for quality / adapter trimming that you think would be provided beyond or in addition to what fastp provides? Also, we have a beta implementation of soft-clipping and are looking for a wide net of testing data. Any suggestions to that end would be welcome!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597344801
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597344801:85,Integrability,adapter,adapter,85,"Hi @tamuanand,. Sure; is there anything specific about bbduk and bbmap for quality / adapter trimming that you think would be provided beyond or in addition to what fastp provides? Also, we have a beta implementation of soft-clipping and are looking for a wide net of testing data. Any suggestions to that end would be welcome!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597344801
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597344801:85,Modifiability,adapt,adapter,85,"Hi @tamuanand,. Sure; is there anything specific about bbduk and bbmap for quality / adapter trimming that you think would be provided beyond or in addition to what fastp provides? Also, we have a beta implementation of soft-clipping and are looking for a wide net of testing data. Any suggestions to that end would be welcome!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597344801
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597344801:268,Testability,test,testing,268,"Hi @tamuanand,. Sure; is there anything specific about bbduk and bbmap for quality / adapter trimming that you think would be provided beyond or in addition to what fastp provides? Also, we have a beta implementation of soft-clipping and are looking for a wide net of testing data. Any suggestions to that end would be welcome!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597344801
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3155,Availability,down,downstream,3155,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3382,Availability,down,downstream,3382,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:266,Energy Efficiency,adapt,adapter,266,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:529,Energy Efficiency,adapt,adapter,529,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:754,Energy Efficiency,adapt,adapter-trimming,754,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:853,Energy Efficiency,adapt,adapters,853,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:929,Energy Efficiency,adapt,adapter,929,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:997,Energy Efficiency,adapt,adapter-trimming,997,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1042,Energy Efficiency,adapt,adapter-trimming,1042,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1161,Energy Efficiency,adapt,adapter,1161,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1255,Energy Efficiency,adapt,adapter,1255,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1324,Energy Efficiency,adapt,adapters,1324,"mming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1401,Energy Efficiency,adapt,adapter,1401,"**_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1583,Energy Efficiency,adapt,adapter,1583,"/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1805,Energy Efficiency,adapt,adapter,1805,"quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mappin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3423,Energy Efficiency,adapt,adapter,3423,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3494,Energy Efficiency,adapt,adapters,3494,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:266,Integrability,adapter,adapter,266,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:529,Integrability,adapter,adapter,529,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:754,Integrability,adapter,adapter-trimming,754,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:853,Integrability,adapter,adapters,853,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:929,Integrability,adapter,adapter,929,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:997,Integrability,adapter,adapter-trimming,997,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1042,Integrability,adapter,adapter-trimming,1042,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1161,Integrability,adapter,adapter,1161,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1255,Integrability,adapter,adapter,1255,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1324,Integrability,adapter,adapters,1324,"mming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1401,Integrability,adapter,adapter,1401,"**_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1583,Integrability,adapter,adapter,1583,"/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1805,Integrability,adapter,adapter,1805,"quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mappin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3423,Integrability,adapter,adapter,3423,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3494,Integrability,adapter,adapters,3494,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:266,Modifiability,adapt,adapter,266,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:529,Modifiability,adapt,adapter,529,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:754,Modifiability,adapt,adapter-trimming,754,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:853,Modifiability,adapt,adapters,853,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:929,Modifiability,adapt,adapter,929,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:997,Modifiability,adapt,adapter-trimming,997,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1042,Modifiability,adapt,adapter-trimming,1042,"e I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1161,Modifiability,adapt,adapter,1161,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1255,Modifiability,adapt,adapter,1255,"ing incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondri",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1324,Modifiability,adapt,adapters,1324,"mming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmo",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1401,Modifiability,adapt,adapter,1401,"**_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1583,Modifiability,adapt,adapter,1583,"/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1805,Modifiability,adapt,adapter,1805,"quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mappin",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3423,Modifiability,adapt,adapter,3423,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3494,Modifiability,adapt,adapters,3494,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1917,Performance,throughput,throughput,1917,"trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.bios",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1409,Safety,detect,detection,1409,"**_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1431,Safety,detect,detect,1431,"igure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:1591,Safety,detect,detection,1591,"/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:56,Testability,log,logic,56,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:226,Testability,log,logic,226,"Hi @rob-p . Before I answer your question and layout my logic, I want to mention that I am **_not_** suggesting fastp is not doing its job, **_neither am I stating that fastp is working incorrectly_**. Now to my answer(s) and logic:; 1. With fastp, I am not sure if adapter trimming happens first and then quality trimming OR vice-versa. I could not find info on this from their README and **_I could be wrong here with my next line_** - [Based on Figure 1 of this paper, it looks to me as though quality trimming is done before adapter trimming](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6129281/figure/bty560-F1/). - [To quote Brian Bushnell (author of BBTools)]( http://seqanswers.com/forums/showpost.php?p=140819&postcount=5), . > It's best to do adapter-trimming first, then quality-trimming, because if you do quality-trimming first, sometimes adapters will be partially trimmed and become too short to be recognized as adapter sequence. When you run BBDuk with both quality-trimming and adapter-trimming in the same run, it will do adapter-trimming first, then quality-trimming. 2. I very well know that the advantage of using fastp is that it can do adapter trimming, etc in a automatic fashion - no need to provide external sequences (example adapter sequences). Yes, I know one can also provide a fasta file of adapters and fatsp will work off it. There are many fatsp issues in GH about adapter detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Not",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:3297,Testability,log,logic,3297,"omatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I use bbduk with adapter trimming and quality trimming in same command line - also, the adapters.fa file that ships with BBTools can be used in all runs. Hope that helps.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209:2391,Usability,simpl,simply,2391,"detection:. - fail to detect adpaters automatically - (https://github.com/OpenGene/fastp/issues/222 and https://github.com/OpenGene/fastp/issues/205). - [incorrect automatic adapter detection](https://github.com/OpenGene/fastp/issues/240). - [inconsistent behavior with different runs](https://github.com/OpenGene/fastp/issues/231). 3. When I see the above, I am bit skeptical using fastp for qc/adapter trimming. . After lot of trials with bbduk and fastp, I have decided to use bbduk and bbmap for my high throughput analysis. Below, I explain my reasoning behind those choices: . **_```Note - edited this on 02-Apr-2020 to have the correct order of operations ```_** . 1. **_STEP 1 - run bbmap.sh on raw fatsq file to remove contaminants as appropriate (rRNA, mitochondrial, chloroplast)_**; - You have alluded to the importance of removing contaminants [in this post](https://github.com/COMBINE-lab/salmon/issues/160#issuecomment-334762498); >However, the other thing to try is simply to align one of these samples to the genome with a tool like STAR or HISAT2 and look at their mapping rate to known features. If it's similar, then the other reads could be accounted for by e.g. intron retention or even contamination. Finally, [@vals has an excellent series of blog posts on investigating and addressing low mapping rates](http://www.nxn.se/valent/2017/9/18/low-mapping-rate-5-human-dna-contamination); - bbmap Command ([based of this biostars post](https://www.biostars.org/p/143019/#210890)):; `bbmap.sh in=read_1.fq.gz ref=rRNA_Chlor_Mito.fa maxindel=1 minid=0.95 outu=clean_read_1.fq.gz nodisk`; - Strategy:; `use the rRNA+Mito+Chloroplast file and map the reads using bbmap, then collect the unmapped reads (clean_read_1.fq.gz) for my downstream analysis`. 2. **_STEP 2 - run bbduk.sh on the outu files from bbmap step -- the outu stands for output unmapped - as stated in the logic above, anything that is unmapped to the rRNA_Chlor_Mito.fa is a clean read for downstream analysis_**. I us",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-597393209
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756:286,Integrability,depend,depending,286,"Thanks @rob-p I am glad you and others have found it useful. The actual bbmap.sh and bbduk.sh commands for SE data are in the links to Phil Ewels' multiqc GH. . Just like salmon indexing kmer size choice, one can tinker with the **_```k, hdist, minq and other parameters```_** of bbduk depending on how good/bad the data is. Needless to state, bbduk is the swiss-army-knife for sequence reads quality assessment with whole range of parameters to tweak . https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/ suggests; ![image](https://user-images.githubusercontent.com/8467214/78302368-a3695980-7508-11ea-990d-4b6e008e3f07.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756:511,Usability,guid,guide,511,"Thanks @rob-p I am glad you and others have found it useful. The actual bbmap.sh and bbduk.sh commands for SE data are in the links to Phil Ewels' multiqc GH. . Just like salmon indexing kmer size choice, one can tinker with the **_```k, hdist, minq and other parameters```_** of bbduk depending on how good/bad the data is. Needless to state, bbduk is the swiss-army-knife for sequence reads quality assessment with whole range of parameters to tweak . https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/ suggests; ![image](https://user-images.githubusercontent.com/8467214/78302368-a3695980-7508-11ea-990d-4b6e008e3f07.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756
https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756:523,Usability,guid,guide,523,"Thanks @rob-p I am glad you and others have found it useful. The actual bbmap.sh and bbduk.sh commands for SE data are in the links to Phil Ewels' multiqc GH. . Just like salmon indexing kmer size choice, one can tinker with the **_```k, hdist, minq and other parameters```_** of bbduk depending on how good/bad the data is. Needless to state, bbduk is the swiss-army-knife for sequence reads quality assessment with whole range of parameters to tweak . https://jgi.doe.gov/data-and-tools/bbtools/bb-tools-user-guide/bbduk-guide/ suggests; ![image](https://user-images.githubusercontent.com/8467214/78302368-a3695980-7508-11ea-990d-4b6e008e3f07.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/485#issuecomment-608105756
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-590911530:95,Deployability,install,install,95,"The same with version 1.2.0. Is there a way to disable version check by default, completely? I install Salmon as a module and I wouldn't let it update itself automatically, anyway.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-590911530
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-590911530:144,Deployability,update,update,144,"The same with version 1.2.0. Is there a way to disable version check by default, completely? I install Salmon as a module and I wouldn't let it update itself automatically, anyway.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-590911530
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-592203836:210,Availability,reliab,reliable,210,"Hi @cihanerkut,. First, thanks for reporting this, and going through the trouble to give the `strace` information. Second, salmon won't auto-update anyway. That would be quite slick, but there's not a good and reliable way to do that with natively-compiled programs that I know of. However, the most surprising thing is that you are finding the call to the version check ip to be hanging for any significant amount of time. The timeout should be pretty quick. How long does it hang when you do `salmon index --help`? I'll look into it on our end as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-592203836
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-592203836:141,Deployability,update,update,141,"Hi @cihanerkut,. First, thanks for reporting this, and going through the trouble to give the `strace` information. Second, salmon won't auto-update anyway. That would be quite slick, but there's not a good and reliable way to do that with natively-compiled programs that I know of. However, the most surprising thing is that you are finding the call to the version check ip to be hanging for any significant amount of time. The timeout should be pretty quick. How long does it hang when you do `salmon index --help`? I'll look into it on our end as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-592203836
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-592203836:428,Safety,timeout,timeout,428,"Hi @cihanerkut,. First, thanks for reporting this, and going through the trouble to give the `strace` information. Second, salmon won't auto-update anyway. That would be quite slick, but there's not a good and reliable way to do that with natively-compiled programs that I know of. However, the most surprising thing is that you are finding the call to the version check ip to be hanging for any significant amount of time. The timeout should be pretty quick. How long does it hang when you do `salmon index --help`? I'll look into it on our end as well.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-592203836
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617250297:104,Deployability,install,install,104,"> The same with version 1.2.0.; > ; > Is there a way to disable version check by default, completely? I install Salmon as a module and I wouldn't let it update itself automatically, anyway. ```; --- Salmon.cpp.ori 2020-04-21 15:12:29.916219870 +0000; +++ Salmon.cpp 2020-04-21 15:16:48.488926415 +0000; @@ -53,7 +53,7 @@; ""Usage: salmon -h|--help or \n""; "" salmon -v|--version or \n""; "" salmon -c|--cite or \n""; - "" salmon [--no-version-check] <COMMAND> [-h | options]\n\n"");; + "" salmon <COMMAND> [-h | options]\n\n"");; helpMsg.write(""Commands:\n"");; helpMsg.write("" index Create a salmon index\n"");; helpMsg.write("" quant Quantify a sample\n"");; @@ -171,8 +171,6 @@; // https://gist.github.com/randomphrase/10801888; po::options_description sfopts(""Allowed Options"");; sfopts.add_options()(""version,v"", ""print version string"")(; - ""no-version-check"",; - ""don't check with the server to see if this is the latest version"")(; ""cite,c"", ""show citation information"")(; ""help,h"", ""produce help message"")(""command"", po::value<string>(),; ""command to run {index, quant, sf}"")(; @@ -209,11 +207,6 @@; std::exit(0);; }. - if (!vm.count(""no-version-check"")) {; - std::string versionMessage = getVersionMessage();; - std::cerr << versionMessage;; - }; -; // po::notify(vm);. std::string cmd = vm[""command""].as<std::string>();; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617250297
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617250297:153,Deployability,update,update,153,"> The same with version 1.2.0.; > ; > Is there a way to disable version check by default, completely? I install Salmon as a module and I wouldn't let it update itself automatically, anyway. ```; --- Salmon.cpp.ori 2020-04-21 15:12:29.916219870 +0000; +++ Salmon.cpp 2020-04-21 15:16:48.488926415 +0000; @@ -53,7 +53,7 @@; ""Usage: salmon -h|--help or \n""; "" salmon -v|--version or \n""; "" salmon -c|--cite or \n""; - "" salmon [--no-version-check] <COMMAND> [-h | options]\n\n"");; + "" salmon <COMMAND> [-h | options]\n\n"");; helpMsg.write(""Commands:\n"");; helpMsg.write("" index Create a salmon index\n"");; helpMsg.write("" quant Quantify a sample\n"");; @@ -171,8 +171,6 @@; // https://gist.github.com/randomphrase/10801888; po::options_description sfopts(""Allowed Options"");; sfopts.add_options()(""version,v"", ""print version string"")(; - ""no-version-check"",; - ""don't check with the server to see if this is the latest version"")(; ""cite,c"", ""show citation information"")(; ""help,h"", ""produce help message"")(""command"", po::value<string>(),; ""command to run {index, quant, sf}"")(; @@ -209,11 +207,6 @@; std::exit(0);; }. - if (!vm.count(""no-version-check"")) {; - std::string versionMessage = getVersionMessage();; - std::cerr << versionMessage;; - }; -; // po::notify(vm);. std::string cmd = vm[""command""].as<std::string>();; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617250297
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617250297:991,Integrability,message,message,991,"> The same with version 1.2.0.; > ; > Is there a way to disable version check by default, completely? I install Salmon as a module and I wouldn't let it update itself automatically, anyway. ```; --- Salmon.cpp.ori 2020-04-21 15:12:29.916219870 +0000; +++ Salmon.cpp 2020-04-21 15:16:48.488926415 +0000; @@ -53,7 +53,7 @@; ""Usage: salmon -h|--help or \n""; "" salmon -v|--version or \n""; "" salmon -c|--cite or \n""; - "" salmon [--no-version-check] <COMMAND> [-h | options]\n\n"");; + "" salmon <COMMAND> [-h | options]\n\n"");; helpMsg.write(""Commands:\n"");; helpMsg.write("" index Create a salmon index\n"");; helpMsg.write("" quant Quantify a sample\n"");; @@ -171,8 +171,6 @@; // https://gist.github.com/randomphrase/10801888; po::options_description sfopts(""Allowed Options"");; sfopts.add_options()(""version,v"", ""print version string"")(; - ""no-version-check"",; - ""don't check with the server to see if this is the latest version"")(; ""cite,c"", ""show citation information"")(; ""help,h"", ""produce help message"")(""command"", po::value<string>(),; ""command to run {index, quant, sf}"")(; @@ -209,11 +207,6 @@; std::exit(0);; }. - if (!vm.count(""no-version-check"")) {; - std::string versionMessage = getVersionMessage();; - std::cerr << versionMessage;; - }; -; // po::notify(vm);. std::string cmd = vm[""command""].as<std::string>();; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617250297
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617251989:61,Availability,avail,available,61,I would have prefered an option to check if a new version is available instead of having a software that systematicly phone home . it will be problematic on our cluster as compute nodes does not have network acces to internet. ; did not checked yet how this case in handled by salmon. regards. Eric,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617251989
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617286729:370,Security,access,access,370,"@EricDeveaud,. Does passing `--no-version-check` resolve the issue? This flag goes before any other command and disables this behavior. For example:. ```; salmon --no-version-check index -t <gentrome.fa> -d decoys.txt -i index; ```. or . ```; salmon --no-version-check quant -i index -la -1 reads_1.fq.gz -2 reads_2.fq.gz -o quant_dir; ```; There should be _no_ network access when using this flag.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617286729
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617310271:139,Deployability,install,installation,139,"yes `--no-version-check` does the trick, but among all the users of the cluster I'm pretty sure some of them will forgot ;-). on our local installation I disabled the getVersionMessage even if salmon handle the no network cleanly. (I tested using `unshare -n salmon whatever you want`); NB debian maintainer also disabled the phone home call in their packages. sorry if it it may sound harsh",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617310271
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617310271:234,Testability,test,tested,234,"yes `--no-version-check` does the trick, but among all the users of the cluster I'm pretty sure some of them will forgot ;-). on our local installation I disabled the getVersionMessage even if salmon handle the no network cleanly. (I tested using `unshare -n salmon whatever you want`); NB debian maintainer also disabled the phone home call in their packages. sorry if it it may sound harsh",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617310271
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919:60,Modifiability,variab,variable,60,"Just an idea. Would it be possible to assign an environment variable, such as SALMON_NO_VERSION_CHECK, whose existence overrides version checking? This wouldn't break compatibility with older scripts because they wouldn't have the variable in the first place. In non-networked nodes, an admin can simply set this variable and users will run salmon as usual.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919:231,Modifiability,variab,variable,231,"Just an idea. Would it be possible to assign an environment variable, such as SALMON_NO_VERSION_CHECK, whose existence overrides version checking? This wouldn't break compatibility with older scripts because they wouldn't have the variable in the first place. In non-networked nodes, an admin can simply set this variable and users will run salmon as usual.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919:313,Modifiability,variab,variable,313,"Just an idea. Would it be possible to assign an environment variable, such as SALMON_NO_VERSION_CHECK, whose existence overrides version checking? This wouldn't break compatibility with older scripts because they wouldn't have the variable in the first place. In non-networked nodes, an admin can simply set this variable and users will run salmon as usual.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919:297,Usability,simpl,simply,297,"Just an idea. Would it be possible to assign an environment variable, such as SALMON_NO_VERSION_CHECK, whose existence overrides version checking? This wouldn't break compatibility with older scripts because they wouldn't have the variable in the first place. In non-networked nodes, an admin can simply set this variable and users will run salmon as usual.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617326919
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617830312:39,Deployability,release,released,39,"@cihanerkut and @EricDeveaud,. We just released 1.2.1, which is on the release page, and dockerhub, and should propagate to bioconda soon. It adds support for the `SALMON_NO_VERSION_CHECK` environment variable. If you set `SALMON_NO_VERSION_CHECK` to either `1` or `TRUE` in the environment where salmon is running, it will skip the version check. I hope this helps!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617830312
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617830312:71,Deployability,release,release,71,"@cihanerkut and @EricDeveaud,. We just released 1.2.1, which is on the release page, and dockerhub, and should propagate to bioconda soon. It adds support for the `SALMON_NO_VERSION_CHECK` environment variable. If you set `SALMON_NO_VERSION_CHECK` to either `1` or `TRUE` in the environment where salmon is running, it will skip the version check. I hope this helps!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617830312
https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617830312:201,Modifiability,variab,variable,201,"@cihanerkut and @EricDeveaud,. We just released 1.2.1, which is on the release page, and dockerhub, and should propagate to bioconda soon. It adds support for the `SALMON_NO_VERSION_CHECK` environment variable. If you set `SALMON_NO_VERSION_CHECK` to either `1` or `TRUE` in the environment where salmon is running, it will skip the version check. I hope this helps!. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/486#issuecomment-617830312
https://github.com/COMBINE-lab/salmon/issues/488#issuecomment-591733839:22,Performance,perform,performs,22,"Hi,; Basically Alevin performs CB sequence correction within 1 distance hamming ball, the intuition being the set of real CB should ideally be more than 1 edit distance away.; Here I think the x axis gives you the count of reads for a CB before sequence correction and on y axis post sequence correction. Hope it helps",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/488#issuecomment-591733839
https://github.com/COMBINE-lab/salmon/issues/488#issuecomment-591733839:90,Usability,intuit,intuition,90,"Hi,; Basically Alevin performs CB sequence correction within 1 distance hamming ball, the intuition being the set of real CB should ideally be more than 1 edit distance away.; Here I think the x axis gives you the count of reads for a CB before sequence correction and on y axis post sequence correction. Hope it helps",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/488#issuecomment-591733839
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738804437:180,Safety,detect,detect,180,"Hi,; I'm having a similar issue with specification of library type. I'm quantifying a single-end library of type SF with salmon 1.3.0. ; The two commands being compared are:. auto detect:. salmon --no-version-check quant --writeMappings -i target --incompatPrior 0.0 --validateMappings -l A -r d218056_dedup.fastq -p 4 -o d218056_A.quant > d218056_A.sam. specify SF:. salmon --no-version-check quant --writeMappings -i target --incompatPrior 0.0 --validateMappings -l SF -r d218056_dedup.fastq -p 4 -o d218056_SF.quant > d218056_SF.sam. The log files indicate that salmon correctly identifies the library as SF in the auto case. I noticed the issue when examining a pair of genes with overlapping 3'UTRs. The forward strand gene (GQ67_03478) is expressed at a much lower level than the reverse strand gene (GQ67_03479). The sam files contain the same number of reads mapped to each transcript without regard to how the libtype is specified:. egrep -v '^@' d218056_A.sam|grep -c GQ67_03478 ; 382; egrep -v '^@' d218056_A.sam|grep -c GQ67_03479; 399; egrep -v '^@' d218056_SF.sam|grep -c GQ67_03478 ; 382; egrep -v '^@' d218056_SF.sam|grep -c GQ67_03479; 399. The quantitation is very different with 120 counts assigned to the forward strand gene in the A case and a much more accurate (based on examination of the sam file) 10 counts in the SF case:. grep GQ67_03478 d218056_A.quant/quant.sf ; GQ67_03478T0 2914 2664.000 202.831978 119.926. grep GQ67_03478 d218056_SF.quant/quant.sf ; GQ67_03478T0 2914 2664.000 17.066270 10.000. For the reverse strand gene, the auto case undercounts due to reads being assigned to the forward strand gene. grep GQ67_03479 d218056_A.quant/quant.sf ; GQ67_03479T0 1383 1133.000 1245.013842 313.074. grep GQ67_03479 d218056_SF.quant/quant.sf ; GQ67_03479T0 1383 1133.000 1589.051981 396.000. I've been using salmon with -l A thinking that if the software correctly recognizes the libtype, the results would be nearly identical to explicitly specifying the libtype but th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738804437
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738804437:269,Security,validat,validateMappings,269,"Hi,; I'm having a similar issue with specification of library type. I'm quantifying a single-end library of type SF with salmon 1.3.0. ; The two commands being compared are:. auto detect:. salmon --no-version-check quant --writeMappings -i target --incompatPrior 0.0 --validateMappings -l A -r d218056_dedup.fastq -p 4 -o d218056_A.quant > d218056_A.sam. specify SF:. salmon --no-version-check quant --writeMappings -i target --incompatPrior 0.0 --validateMappings -l SF -r d218056_dedup.fastq -p 4 -o d218056_SF.quant > d218056_SF.sam. The log files indicate that salmon correctly identifies the library as SF in the auto case. I noticed the issue when examining a pair of genes with overlapping 3'UTRs. The forward strand gene (GQ67_03478) is expressed at a much lower level than the reverse strand gene (GQ67_03479). The sam files contain the same number of reads mapped to each transcript without regard to how the libtype is specified:. egrep -v '^@' d218056_A.sam|grep -c GQ67_03478 ; 382; egrep -v '^@' d218056_A.sam|grep -c GQ67_03479; 399; egrep -v '^@' d218056_SF.sam|grep -c GQ67_03478 ; 382; egrep -v '^@' d218056_SF.sam|grep -c GQ67_03479; 399. The quantitation is very different with 120 counts assigned to the forward strand gene in the A case and a much more accurate (based on examination of the sam file) 10 counts in the SF case:. grep GQ67_03478 d218056_A.quant/quant.sf ; GQ67_03478T0 2914 2664.000 202.831978 119.926. grep GQ67_03478 d218056_SF.quant/quant.sf ; GQ67_03478T0 2914 2664.000 17.066270 10.000. For the reverse strand gene, the auto case undercounts due to reads being assigned to the forward strand gene. grep GQ67_03479 d218056_A.quant/quant.sf ; GQ67_03479T0 1383 1133.000 1245.013842 313.074. grep GQ67_03479 d218056_SF.quant/quant.sf ; GQ67_03479T0 1383 1133.000 1589.051981 396.000. I've been using salmon with -l A thinking that if the software correctly recognizes the libtype, the results would be nearly identical to explicitly specifying the libtype but th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738804437
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738804437:448,Security,validat,validateMappings,448,"Hi,; I'm having a similar issue with specification of library type. I'm quantifying a single-end library of type SF with salmon 1.3.0. ; The two commands being compared are:. auto detect:. salmon --no-version-check quant --writeMappings -i target --incompatPrior 0.0 --validateMappings -l A -r d218056_dedup.fastq -p 4 -o d218056_A.quant > d218056_A.sam. specify SF:. salmon --no-version-check quant --writeMappings -i target --incompatPrior 0.0 --validateMappings -l SF -r d218056_dedup.fastq -p 4 -o d218056_SF.quant > d218056_SF.sam. The log files indicate that salmon correctly identifies the library as SF in the auto case. I noticed the issue when examining a pair of genes with overlapping 3'UTRs. The forward strand gene (GQ67_03478) is expressed at a much lower level than the reverse strand gene (GQ67_03479). The sam files contain the same number of reads mapped to each transcript without regard to how the libtype is specified:. egrep -v '^@' d218056_A.sam|grep -c GQ67_03478 ; 382; egrep -v '^@' d218056_A.sam|grep -c GQ67_03479; 399; egrep -v '^@' d218056_SF.sam|grep -c GQ67_03478 ; 382; egrep -v '^@' d218056_SF.sam|grep -c GQ67_03479; 399. The quantitation is very different with 120 counts assigned to the forward strand gene in the A case and a much more accurate (based on examination of the sam file) 10 counts in the SF case:. grep GQ67_03478 d218056_A.quant/quant.sf ; GQ67_03478T0 2914 2664.000 202.831978 119.926. grep GQ67_03478 d218056_SF.quant/quant.sf ; GQ67_03478T0 2914 2664.000 17.066270 10.000. For the reverse strand gene, the auto case undercounts due to reads being assigned to the forward strand gene. grep GQ67_03479 d218056_A.quant/quant.sf ; GQ67_03479T0 1383 1133.000 1245.013842 313.074. grep GQ67_03479 d218056_SF.quant/quant.sf ; GQ67_03479T0 1383 1133.000 1589.051981 396.000. I've been using salmon with -l A thinking that if the software correctly recognizes the libtype, the results would be nearly identical to explicitly specifying the libtype but th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738804437
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738804437:541,Testability,log,log,541,"Hi,; I'm having a similar issue with specification of library type. I'm quantifying a single-end library of type SF with salmon 1.3.0. ; The two commands being compared are:. auto detect:. salmon --no-version-check quant --writeMappings -i target --incompatPrior 0.0 --validateMappings -l A -r d218056_dedup.fastq -p 4 -o d218056_A.quant > d218056_A.sam. specify SF:. salmon --no-version-check quant --writeMappings -i target --incompatPrior 0.0 --validateMappings -l SF -r d218056_dedup.fastq -p 4 -o d218056_SF.quant > d218056_SF.sam. The log files indicate that salmon correctly identifies the library as SF in the auto case. I noticed the issue when examining a pair of genes with overlapping 3'UTRs. The forward strand gene (GQ67_03478) is expressed at a much lower level than the reverse strand gene (GQ67_03479). The sam files contain the same number of reads mapped to each transcript without regard to how the libtype is specified:. egrep -v '^@' d218056_A.sam|grep -c GQ67_03478 ; 382; egrep -v '^@' d218056_A.sam|grep -c GQ67_03479; 399; egrep -v '^@' d218056_SF.sam|grep -c GQ67_03478 ; 382; egrep -v '^@' d218056_SF.sam|grep -c GQ67_03479; 399. The quantitation is very different with 120 counts assigned to the forward strand gene in the A case and a much more accurate (based on examination of the sam file) 10 counts in the SF case:. grep GQ67_03478 d218056_A.quant/quant.sf ; GQ67_03478T0 2914 2664.000 202.831978 119.926. grep GQ67_03478 d218056_SF.quant/quant.sf ; GQ67_03478T0 2914 2664.000 17.066270 10.000. For the reverse strand gene, the auto case undercounts due to reads being assigned to the forward strand gene. grep GQ67_03479 d218056_A.quant/quant.sf ; GQ67_03479T0 1383 1133.000 1245.013842 313.074. grep GQ67_03479 d218056_SF.quant/quant.sf ; GQ67_03479T0 1383 1133.000 1589.051981 396.000. I've been using salmon with -l A thinking that if the software correctly recognizes the libtype, the results would be nearly identical to explicitly specifying the libtype but th",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738804437
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213:2669,Deployability,pipeline,pipeline,2669,"ble difference is because (1) the reads are assumed to arrive in a random order (2) this is only a very small fraction of the total data and (3) if the unoriented reads map to a target in the ""wrong"" direction, they will also align to the proper target in the ""right"" direction and, once the orientation filter is applied for future reads, the weight of evidence should turn the probability of assignment of these unoriented reads toward the proper target. However, I'm guessing there is an edge case you're seeing here where the conditions don't induce this behavior. So, there are 2 immediate solutions to the problem. First, if you know the library type explicitly, you can use that. Second (and some other folks have discussed this here for other reasons), you can do a ""throw-away"" run of salmon on a small prefix of the read file (e.g. `salmon quant ... -lA --skipQuant -r <(gunzip -c reads.fq.gz | head -n 400000)`) to get the output of the automatic library type determination, and then run the full dataset with that library type. Finally, moving forward, I'm happy to consider working on modifying this default behavior. That is, we could (though it would be a little bit of work) modify the default behavior. The idea here is to basically run as we do now for the first 10,000 aligned reads to get the library type and then ""reset"" the whole quantification pipeline. The main challenge here is that salmon is designed to work with streaming FASTQ input, and we don't want to break that. So we can't do something as easy as ""reset the file pointer"". I think the best option is to make a copy of the first X reads in memory, detection the library type with them, and then start quantifying them and continue with the rest of the file. That complicates the logic a bit, because now the input source for reads changes dynamically during quantification --- but I think it could be done. Please let me know if you both have interest in this feature and it's worth putting on the list. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213:1076,Performance,concurren,concurrent,1076,"mp3,. Thanks for raising this issue. So, I _am_ surprised at these particular differences that you found, but the behavior you are observing is consistent with how automatic library type detection works. Let me explain what it's doing, and then I'm open to discussing if we should focus on changing that behavior going forward. The standard library type detection works by looking at the total number of compatible mappings in both possible orientations for the first x=10,000 aligned reads. These 10,000 reads are themselves mapped with an `IU` orientation in paired data and a `U` orientation in unpaired data. Once the 10,000 data points have been processed, a heuristic chooses the most likely library type and applies it (and salmon issues a warning if, at the end of quantification,, there are too many reads that disagree). So, the explanation of what could be happening here is that the reads that are different between your runs are coming within the first set of 10,000 aligned reads (note, this may not be the first 10k reads of the file, because concurrent processing means that reads from different threads are being aligned in an essentially random order ... but of course maintaining pairing information). The argument for why this should usually not cause a considerable difference is because (1) the reads are assumed to arrive in a random order (2) this is only a very small fraction of the total data and (3) if the unoriented reads map to a target in the ""wrong"" direction, they will also align to the proper target in the ""right"" direction and, once the orientation filter is applied for future reads, the weight of evidence should turn the probability of assignment of these unoriented reads toward the proper target. However, I'm guessing there is an edge case you're seeing here where the conditions don't induce this behavior. So, there are 2 immediate solutions to the problem. First, if you know the library type explicitly, you can use that. Second (and some other folks h",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213:205,Safety,detect,detection,205,"Hi @bumproo and @mmp3,. Thanks for raising this issue. So, I _am_ surprised at these particular differences that you found, but the behavior you are observing is consistent with how automatic library type detection works. Let me explain what it's doing, and then I'm open to discussing if we should focus on changing that behavior going forward. The standard library type detection works by looking at the total number of compatible mappings in both possible orientations for the first x=10,000 aligned reads. These 10,000 reads are themselves mapped with an `IU` orientation in paired data and a `U` orientation in unpaired data. Once the 10,000 data points have been processed, a heuristic chooses the most likely library type and applies it (and salmon issues a warning if, at the end of quantification,, there are too many reads that disagree). So, the explanation of what could be happening here is that the reads that are different between your runs are coming within the first set of 10,000 aligned reads (note, this may not be the first 10k reads of the file, because concurrent processing means that reads from different threads are being aligned in an essentially random order ... but of course maintaining pairing information). The argument for why this should usually not cause a considerable difference is because (1) the reads are assumed to arrive in a random order (2) this is only a very small fraction of the total data and (3) if the unoriented reads map to a target in the ""wrong"" direction, they will also align to the proper target in the ""right"" direction and, once the orientation filter is applied for future reads, the weight of evidence should turn the probability of assignment of these unoriented reads toward the proper target. However, I'm guessing there is an edge case you're seeing here where the conditions don't induce this behavior. So, there are 2 immediate solutions to the problem. First, if you know the library type explicitly, you can use that. Second (and s",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213:372,Safety,detect,detection,372,"Hi @bumproo and @mmp3,. Thanks for raising this issue. So, I _am_ surprised at these particular differences that you found, but the behavior you are observing is consistent with how automatic library type detection works. Let me explain what it's doing, and then I'm open to discussing if we should focus on changing that behavior going forward. The standard library type detection works by looking at the total number of compatible mappings in both possible orientations for the first x=10,000 aligned reads. These 10,000 reads are themselves mapped with an `IU` orientation in paired data and a `U` orientation in unpaired data. Once the 10,000 data points have been processed, a heuristic chooses the most likely library type and applies it (and salmon issues a warning if, at the end of quantification,, there are too many reads that disagree). So, the explanation of what could be happening here is that the reads that are different between your runs are coming within the first set of 10,000 aligned reads (note, this may not be the first 10k reads of the file, because concurrent processing means that reads from different threads are being aligned in an essentially random order ... but of course maintaining pairing information). The argument for why this should usually not cause a considerable difference is because (1) the reads are assumed to arrive in a random order (2) this is only a very small fraction of the total data and (3) if the unoriented reads map to a target in the ""wrong"" direction, they will also align to the proper target in the ""right"" direction and, once the orientation filter is applied for future reads, the weight of evidence should turn the probability of assignment of these unoriented reads toward the proper target. However, I'm guessing there is an edge case you're seeing here where the conditions don't induce this behavior. So, there are 2 immediate solutions to the problem. First, if you know the library type explicitly, you can use that. Second (and s",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213:2935,Safety,detect,detection,2935,"ble difference is because (1) the reads are assumed to arrive in a random order (2) this is only a very small fraction of the total data and (3) if the unoriented reads map to a target in the ""wrong"" direction, they will also align to the proper target in the ""right"" direction and, once the orientation filter is applied for future reads, the weight of evidence should turn the probability of assignment of these unoriented reads toward the proper target. However, I'm guessing there is an edge case you're seeing here where the conditions don't induce this behavior. So, there are 2 immediate solutions to the problem. First, if you know the library type explicitly, you can use that. Second (and some other folks have discussed this here for other reasons), you can do a ""throw-away"" run of salmon on a small prefix of the read file (e.g. `salmon quant ... -lA --skipQuant -r <(gunzip -c reads.fq.gz | head -n 400000)`) to get the output of the automatic library type determination, and then run the full dataset with that library type. Finally, moving forward, I'm happy to consider working on modifying this default behavior. That is, we could (though it would be a little bit of work) modify the default behavior. The idea here is to basically run as we do now for the first 10,000 aligned reads to get the library type and then ""reset"" the whole quantification pipeline. The main challenge here is that salmon is designed to work with streaming FASTQ input, and we don't want to break that. So we can't do something as easy as ""reset the file pointer"". I think the best option is to make a copy of the first X reads in memory, detection the library type with them, and then start quantifying them and continue with the rest of the file. That complicates the logic a bit, because now the input source for reads changes dynamically during quantification --- but I think it could be done. Please let me know if you both have interest in this feature and it's worth putting on the list. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213:3066,Testability,log,logic,3066,"ble difference is because (1) the reads are assumed to arrive in a random order (2) this is only a very small fraction of the total data and (3) if the unoriented reads map to a target in the ""wrong"" direction, they will also align to the proper target in the ""right"" direction and, once the orientation filter is applied for future reads, the weight of evidence should turn the probability of assignment of these unoriented reads toward the proper target. However, I'm guessing there is an edge case you're seeing here where the conditions don't induce this behavior. So, there are 2 immediate solutions to the problem. First, if you know the library type explicitly, you can use that. Second (and some other folks have discussed this here for other reasons), you can do a ""throw-away"" run of salmon on a small prefix of the read file (e.g. `salmon quant ... -lA --skipQuant -r <(gunzip -c reads.fq.gz | head -n 400000)`) to get the output of the automatic library type determination, and then run the full dataset with that library type. Finally, moving forward, I'm happy to consider working on modifying this default behavior. That is, we could (though it would be a little bit of work) modify the default behavior. The idea here is to basically run as we do now for the first 10,000 aligned reads to get the library type and then ""reset"" the whole quantification pipeline. The main challenge here is that salmon is designed to work with streaming FASTQ input, and we don't want to break that. So we can't do something as easy as ""reset the file pointer"". I think the best option is to make a copy of the first X reads in memory, detection the library type with them, and then start quantifying them and continue with the rest of the file. That complicates the logic a bit, because now the input source for reads changes dynamically during quantification --- but I think it could be done. Please let me know if you both have interest in this feature and it's worth putting on the list. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738830213
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738896890:662,Safety,detect,detected,662,"Thank you for the clear and thorough explanation, @rob-p . Now I understand exactly why this is happening. I like your idea for the “throw-away” run for Salmon, and the short example command you sketched out is exactly what I had said in mind as I read your words. Reworking the core Salmon algorithm to do some gymnastics with re-processing the first 10,000 reads would not be elegant or worth your time. I think the workaround you proposed is a perfectly good solution. If in the long run many other people find this useful, perhaps an easier fix would be to make a new command in Salmon that just bails after the first 10k reads automatically and returns the detected library orientation upon termination of the command; e.g. in Bash:. `mylibtype=$(salmon quant —getLibType -r reads.fq.gz)`; `salmon quant —libType $mylibtype -r reads.fq.gz`. Thank you for the great software and for being so attentive to detail and our questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738896890
https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738896890:18,Usability,clear,clear,18,"Thank you for the clear and thorough explanation, @rob-p . Now I understand exactly why this is happening. I like your idea for the “throw-away” run for Salmon, and the short example command you sketched out is exactly what I had said in mind as I read your words. Reworking the core Salmon algorithm to do some gymnastics with re-processing the first 10,000 reads would not be elegant or worth your time. I think the workaround you proposed is a perfectly good solution. If in the long run many other people find this useful, perhaps an easier fix would be to make a new command in Salmon that just bails after the first 10k reads automatically and returns the detected library orientation upon termination of the command; e.g. in Bash:. `mylibtype=$(salmon quant —getLibType -r reads.fq.gz)`; `salmon quant —libType $mylibtype -r reads.fq.gz`. Thank you for the great software and for being so attentive to detail and our questions.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/489#issuecomment-738896890
https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597349699:626,Safety,avoid,avoid,626,"Hi @mej54,. First, thanks for using salmon and for providing detailed feedback! There are two main points I'd like to make in response to the points you raise. . First, v0.9.1 is _very_ old, and there have been a large number of bug fixes and substantial improvements to salmon since that version (though it's much better than people who are still using 0.8.2 from, like, 3 years ago!). Specifically, I'd highly recommend upgrading to the latest version (1.1.0, with 1.2.0 coming out shortly). We've added (and made standard) selective-alignment, which is a procedure that provides alignment scoring for the assigned reads to avoid spurious mappings that arise with fast lightweight mapping procedures. Second, the observation of mismatching bases at the provided alignment location is the expected behavior with the mappings written by salmon with the `--writeMappings` option. Specifically, while newer versions of salmon (0.15.0 and greater) will do alignment scoring and removal of low score alignments by default, salmon still does not compute or write out a full CIGAR string for its alignments. Instead, it uses a _score-only_ dynamic program to compute the optimal alignment score at the given location, but it ""spoofs"" the CIGAR string. Thus, if there is e.g. a small indel in the read, this will show up in an IGV visualization as a large number of mismatches after that indeed location. I'm not sure that is what is happening in the screenshot you show above, and, in fact, may of these mappings may disappear with selective-alignment. However, it will definitely still be possible to see a cigar string showing full matches, where there are mismatches in IGV. This is intended behavior due to score-only alignment. However, it's also true that newer versions of salmon will report the alignment score in an `AS` tag, so that you can see how high the alignment quality was at the particular location.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597349699
https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597349699:70,Usability,feedback,feedback,70,"Hi @mej54,. First, thanks for using salmon and for providing detailed feedback! There are two main points I'd like to make in response to the points you raise. . First, v0.9.1 is _very_ old, and there have been a large number of bug fixes and substantial improvements to salmon since that version (though it's much better than people who are still using 0.8.2 from, like, 3 years ago!). Specifically, I'd highly recommend upgrading to the latest version (1.1.0, with 1.2.0 coming out shortly). We've added (and made standard) selective-alignment, which is a procedure that provides alignment scoring for the assigned reads to avoid spurious mappings that arise with fast lightweight mapping procedures. Second, the observation of mismatching bases at the provided alignment location is the expected behavior with the mappings written by salmon with the `--writeMappings` option. Specifically, while newer versions of salmon (0.15.0 and greater) will do alignment scoring and removal of low score alignments by default, salmon still does not compute or write out a full CIGAR string for its alignments. Instead, it uses a _score-only_ dynamic program to compute the optimal alignment score at the given location, but it ""spoofs"" the CIGAR string. Thus, if there is e.g. a small indel in the read, this will show up in an IGV visualization as a large number of mismatches after that indeed location. I'm not sure that is what is happening in the screenshot you show above, and, in fact, may of these mappings may disappear with selective-alignment. However, it will definitely still be possible to see a cigar string showing full matches, where there are mismatches in IGV. This is intended behavior due to score-only alignment. However, it's also true that newer versions of salmon will report the alignment score in an `AS` tag, so that you can see how high the alignment quality was at the particular location.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597349699
https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597755020:248,Deployability,update,update,248,"Hi @rob-p,. Thank you for your response/explanation about the CIGAR strings. I had a feeling this was the case based on previous issues I had been reading, but I just wanted to make sure I was understanding correctly about the CIGAR output. I will update to the latest version of Salmon and will check if the selective-alignment helps with these false mappings. Thanks again!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-597755020
https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-1204244129:158,Performance,cache,cache,158,"@rob-p - How much effort would it be to run a DP with tracing of the alignment? I remember reading that Brian Bushnell managed to fit everything into the CPU cache for BBMap's alignment algorithm, at least for regular read lengths, so the performance impact should be acceptable if done right. Not sure about licenses, but I think there was an optional native C implementation for it in his code base. This would be great to have so the SAM can be used for e.g. basic genotyping for QC purposes. Alternatively, maybe add a line to the docs of --writeMappings to make sure everyone understands the read alignments will have a score and position, but lack actual alignment and appear as if they were perfect matches.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-1204244129
https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-1204244129:239,Performance,perform,performance,239,"@rob-p - How much effort would it be to run a DP with tracing of the alignment? I remember reading that Brian Bushnell managed to fit everything into the CPU cache for BBMap's alignment algorithm, at least for regular read lengths, so the performance impact should be acceptable if done right. Not sure about licenses, but I think there was an optional native C implementation for it in his code base. This would be great to have so the SAM can be used for e.g. basic genotyping for QC purposes. Alternatively, maybe add a line to the docs of --writeMappings to make sure everyone understands the read alignments will have a score and position, but lack actual alignment and appear as if they were perfect matches.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/491#issuecomment-1204244129
https://github.com/COMBINE-lab/salmon/issues/492#issuecomment-620232036:1282,Availability,avail,available,1282,"Hi @lijing28101,. Thanks for your interest in salmon. The answer to your first question:. > I want to know how did you estimate the final counts to combine the unique and ambiguous counts?. is actually a huge part of the method behind salmon. Salmon uses a statistical estimation procedure to determine what abundances for the transcripts are most likely given the observed data (read alignments). You can read about the method in detail [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5600148/pdf/nihms849351.pdf) and about some subsequent improvements [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5870700/) and [here](https://www.biorxiv.org/content/10.1101/657874v2). > For the unstranded library, did you use the same algorithm to estimate the final counts as the stranded library? . The difference between stranded and unstranded is that in the stranded library, we set the conditional probability of alignments that don't agree with the specified strandedness to be a very-low, user-adjustable value (`--incompatPrior` : 0 by default). The underlying inference methodology is identical. Strand specificity, if the library was sequenced that way, provides extra signal to the quantification algorithm. However, that is an external source of information, and is only available if the library itself was strand-specific.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/492#issuecomment-620232036
https://github.com/COMBINE-lab/salmon/issues/494#issuecomment-601087573:189,Usability,clear,clear,189,"> Yep use --end 5 --umiLength 8 --barcodeLength 18. Thanks again, I forgot, however, to specify that the sequence is composed first of BC and then of UMI (BC + UMI) (I'm not sure if it was clear in the issue).; Does the command remain the same?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/494#issuecomment-601087573
https://github.com/COMBINE-lab/salmon/issues/496#issuecomment-603335132:57,Deployability,patch,patching,57,"Rob, thank you very much for your response. I am already patching the source as i need to work; with stable and not development versions. Best regards, Nadya.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/496#issuecomment-603335132
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738:162,Deployability,release,releases,162,"Hi @citron96,. This is what is covered in https://github.com/COMBINE-lab/salmon/issues/496. This is because an upstream package changed the SHA of their *tagged* releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases. I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738:405,Deployability,release,releases,405,"Hi @citron96,. This is what is covered in https://github.com/COMBINE-lab/salmon/issues/496. This is because an upstream package changed the SHA of their *tagged* releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases. I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738:454,Deployability,release,releases,454,"Hi @citron96,. This is what is covered in https://github.com/COMBINE-lab/salmon/issues/496. This is because an upstream package changed the SHA of their *tagged* releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases. I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738:503,Deployability,patch,patch,503,"Hi @citron96,. This is what is covered in https://github.com/COMBINE-lab/salmon/issues/496. This is because an upstream package changed the SHA of their *tagged* releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases. I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738:359,Safety,avoid,avoid,359,"Hi @citron96,. This is what is covered in https://github.com/COMBINE-lab/salmon/issues/496. This is because an upstream package changed the SHA of their *tagged* releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases. I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603902738
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:19,Deployability,patch,patch,19,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:131,Deployability,patch,patch,131,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:502,Deployability,install,install,502,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:733,Deployability,install,install,733,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1383,Deployability,install,install,1383,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1514,Deployability,release,releases,1514,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1578,Deployability,release,releases,1578,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1685,Deployability,patch,patches,1685,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:1712,Deployability,release,releases,1712,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:311,Integrability,message,message,311,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:389,Integrability,message,message,389,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394:34,Usability,simpl,simple,34,"Hi @citron96,. The patch is quite simple and i have verified it and it works for salmon-1.1.0 version that i compiled. Here is the patch content:. --- salmon-1.1.0/CMakeLists.txt.orig 2020-03-24 08:50:22.681000000 -0700; +++ salmon-1.1.0/CMakeLists.txt 2020-03-24 08:51:41.786000000 -0700; @@ -596,7 +596,7 @@; message(""Build system will fetch and build Intel Threading Building Blocks""); message(""==================================================================""); # These are useful for the custom install step we'll do later; -set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8); +set(TBB_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8); set(TBB_INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install). if(""${TBB_COMPILER}"" STREQUAL ""gcc""); @@ -610,9 +610,9 @@; externalproject_add(libtbb; DOWNLOAD_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external; DOWNLOAD_COMMAND curl -k -L https://github.com/intel/tbb/archive/2019_U8.tar.gz -o tbb-2019_U8.tgz &&; - ${SHASUM} 7b1fd8caea14be72ae4175896510bf99c809cd7031306a1917565e6de7382fba tbb-2019_U8.tgz &&; + ${SHASUM} 6b540118cbc79f9cbc06a35033c18156c21b84ab7b6cf56d773b168ad2b68566 tbb-2019_U8.tgz &&; tar -xzvf tbb-2019_U8.tgz; - SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/tbb-2019_U8; + SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/oneTBB-2019_U8; INSTALL_DIR ${CMAKE_CURRENT_SOURCE_DIR}/external/install; PATCH_COMMAND ""${TBB_PATCH_STEP}""; CONFIGURE_COMMAND """". Rob, ; I understand that you don't want to push changes to older releases but perhaps one; can issue a README/NOTE for all prior releases that are affected by this. The explanation of; what changed will allow people to create their own patches for their specific releases. Regards,; Nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603916394
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603936569:20,Deployability,patch,patch,20,Please see attached patch here. I did not realized my writing was stylized which; wiped out correct syntax. Nadya; [salmon-1.1.0.patch.txt](https://github.com/COMBINE-lab/salmon/files/4382067/salmon-1.1.0.patch.txt),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603936569
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603936569:129,Deployability,patch,patch,129,Please see attached patch here. I did not realized my writing was stylized which; wiped out correct syntax. Nadya; [salmon-1.1.0.patch.txt](https://github.com/COMBINE-lab/salmon/files/4382067/salmon-1.1.0.patch.txt),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603936569
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603936569:205,Deployability,patch,patch,205,Please see attached patch here. I did not realized my writing was stylized which; wiped out correct syntax. Nadya; [salmon-1.1.0.patch.txt](https://github.com/COMBINE-lab/salmon/files/4382067/salmon-1.1.0.patch.txt),MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603936569
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603952681:22,Deployability,patch,patch,22,"> Please see attached patch here. I did not realized my writing was stylized which; > wiped out correct syntax. Nadya; > [salmon-1.1.0.patch.txt](https://github.com/COMBINE-lab/salmon/files/4382067/salmon-1.1.0.patch.txt). Hello, @nadyawilliams ; Thank you for your kind support. Since I'm pretty new here and even know nothing about how to applying this patch to fix the problem, could you give me some extra advice on it? Many thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603952681
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603952681:135,Deployability,patch,patch,135,"> Please see attached patch here. I did not realized my writing was stylized which; > wiped out correct syntax. Nadya; > [salmon-1.1.0.patch.txt](https://github.com/COMBINE-lab/salmon/files/4382067/salmon-1.1.0.patch.txt). Hello, @nadyawilliams ; Thank you for your kind support. Since I'm pretty new here and even know nothing about how to applying this patch to fix the problem, could you give me some extra advice on it? Many thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603952681
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603952681:211,Deployability,patch,patch,211,"> Please see attached patch here. I did not realized my writing was stylized which; > wiped out correct syntax. Nadya; > [salmon-1.1.0.patch.txt](https://github.com/COMBINE-lab/salmon/files/4382067/salmon-1.1.0.patch.txt). Hello, @nadyawilliams ; Thank you for your kind support. Since I'm pretty new here and even know nothing about how to applying this patch to fix the problem, could you give me some extra advice on it? Many thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603952681
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603952681:355,Deployability,patch,patch,355,"> Please see attached patch here. I did not realized my writing was stylized which; > wiped out correct syntax. Nadya; > [salmon-1.1.0.patch.txt](https://github.com/COMBINE-lab/salmon/files/4382067/salmon-1.1.0.patch.txt). Hello, @nadyawilliams ; Thank you for your kind support. Since I'm pretty new here and even know nothing about how to applying this patch to fix the problem, could you give me some extra advice on it? Many thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-603952681
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006:21,Deployability,patch,patch,21,"@citron96 ; Applying patch is very standard. The way i created it was to make a copy of; salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; CMakeLists.txt. ; Then apply diff command to create patch:; diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch. Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; cd salmon-1.1.0/; patch -p1 < ../salmon-1.1.0.patch. nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006:227,Deployability,patch,patch,227,"@citron96 ; Applying patch is very standard. The way i created it was to make a copy of; salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; CMakeLists.txt. ; Then apply diff command to create patch:; diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch. Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; cd salmon-1.1.0/; patch -p1 < ../salmon-1.1.0.patch. nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006:322,Deployability,patch,patch,322,"@citron96 ; Applying patch is very standard. The way i created it was to make a copy of; salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; CMakeLists.txt. ; Then apply diff command to create patch:; diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch. Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; cd salmon-1.1.0/; patch -p1 < ../salmon-1.1.0.patch. nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006:347,Deployability,patch,patch,347,"@citron96 ; Applying patch is very standard. The way i created it was to make a copy of; salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; CMakeLists.txt. ; Then apply diff command to create patch:; diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch. Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; cd salmon-1.1.0/; patch -p1 < ../salmon-1.1.0.patch. nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006:372,Deployability,patch,patch,372,"@citron96 ; Applying patch is very standard. The way i created it was to make a copy of; salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; CMakeLists.txt. ; Then apply diff command to create patch:; diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch. Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; cd salmon-1.1.0/; patch -p1 < ../salmon-1.1.0.patch. nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006:466,Deployability,patch,patch,466,"@citron96 ; Applying patch is very standard. The way i created it was to make a copy of; salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; CMakeLists.txt. ; Then apply diff command to create patch:; diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch. Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; cd salmon-1.1.0/; patch -p1 < ../salmon-1.1.0.patch. nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006:494,Deployability,patch,patch,494,"@citron96 ; Applying patch is very standard. The way i created it was to make a copy of; salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; CMakeLists.txt. ; Then apply diff command to create patch:; diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch. Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; cd salmon-1.1.0/; patch -p1 < ../salmon-1.1.0.patch. nadya",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604029006
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081:24,Deployability,patch,patch,24,"> @citron96; > Applying patch is very standard. The way i created it was to make a copy of; > salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; > CMakeLists.txt.; > Then apply diff command to create patch:; > diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch; > ; > Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; > cd salmon-1.1.0/; > patch -p1 < ../salmon-1.1.0.patch; > ; > nadya. get it, and already solved the problem, thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081:235,Deployability,patch,patch,235,"> @citron96; > Applying patch is very standard. The way i created it was to make a copy of; > salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; > CMakeLists.txt.; > Then apply diff command to create patch:; > diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch; > ; > Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; > cd salmon-1.1.0/; > patch -p1 < ../salmon-1.1.0.patch; > ; > nadya. get it, and already solved the problem, thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081:332,Deployability,patch,patch,332,"> @citron96; > Applying patch is very standard. The way i created it was to make a copy of; > salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; > CMakeLists.txt.; > Then apply diff command to create patch:; > diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch; > ; > Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; > cd salmon-1.1.0/; > patch -p1 < ../salmon-1.1.0.patch; > ; > nadya. get it, and already solved the problem, thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081:363,Deployability,patch,patch,363,"> @citron96; > Applying patch is very standard. The way i created it was to make a copy of; > salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; > CMakeLists.txt.; > Then apply diff command to create patch:; > diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch; > ; > Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; > cd salmon-1.1.0/; > patch -p1 < ../salmon-1.1.0.patch; > ; > nadya. get it, and already solved the problem, thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081:388,Deployability,patch,patch,388,"> @citron96; > Applying patch is very standard. The way i created it was to make a copy of; > salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; > CMakeLists.txt.; > Then apply diff command to create patch:; > diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch; > ; > Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; > cd salmon-1.1.0/; > patch -p1 < ../salmon-1.1.0.patch; > ; > nadya. get it, and already solved the problem, thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081:486,Deployability,patch,patch,486,"> @citron96; > Applying patch is very standard. The way i created it was to make a copy of; > salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; > CMakeLists.txt.; > Then apply diff command to create patch:; > diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch; > ; > Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; > cd salmon-1.1.0/; > patch -p1 < ../salmon-1.1.0.patch; > ; > nadya. get it, and already solved the problem, thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081:514,Deployability,patch,patch,514,"> @citron96; > Applying patch is very standard. The way i created it was to make a copy of; > salmon-1.1.0/CMakeLists.txt as salmon-1.1.0/CMakeLists.txt.orig and to make edits to; > CMakeLists.txt.; > Then apply diff command to create patch:; > diff -Naur salmon-1.1.0/CMakeLists.txt.orig salmon-1.1.0/CMakeLists.txt > salmon-1.1.0.patch; > ; > Now, to apply the patch one needs to put a patch file on the same level where your untar'ed salmon-1.1.0/ is and then; > cd salmon-1.1.0/; > patch -p1 < ../salmon-1.1.0.patch; > ; > nadya. get it, and already solved the problem, thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461081
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264:126,Deployability,release,releases,126,"> Hi @citron96,; > ; > This is what is covered in #496. This is because an upstream package changed the SHA of their _tagged_ releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases.; > ; > I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?. thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264:369,Deployability,release,releases,369,"> Hi @citron96,; > ; > This is what is covered in #496. This is because an upstream package changed the SHA of their _tagged_ releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases.; > ; > I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?. thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264:418,Deployability,release,releases,418,"> Hi @citron96,; > ; > This is what is covered in #496. This is because an upstream package changed the SHA of their _tagged_ releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases.; > ; > I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?. thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264:474,Deployability,patch,patch,474,"> Hi @citron96,; > ; > This is what is covered in #496. This is because an upstream package changed the SHA of their _tagged_ releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases.; > ; > I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?. thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264:323,Safety,avoid,avoid,323,"> Hi @citron96,; > ; > This is what is covered in #496. This is because an upstream package changed the SHA of their _tagged_ releases (which is really not ideal). Are you pulling from the master branch? If you pull from develop, everything should build. I can pull the changes that fix this into master. Unfortunately, to avoid changing the signatures of _our_ tagged releases, I can't push this change back to older releases.; > ; > I think @nadyawilliams may also have a patch for the CMakeLists.txt file, which, perhaps, can be shared?. thank you so much.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-604461264
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:339,Availability,error,error,339,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:393,Availability,down,download,393,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:729,Availability,error,error,729,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:209,Deployability,install,install,209,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:286,Deployability,install,installing,286,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:439,Deployability,update,update,439,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:479,Deployability,update,update,479,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:543,Deployability,install,install,543,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:693,Deployability,install,installation,693,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:581,Performance,cache,cache,581,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958:37,Usability,simpl,simple,37,"I faced the same problem and found a simple solution. The trick is at line 585 of the cMakeList.txt. ""if (${TBB_VERSION} VERSION_GREATER_EQUAL 2018.0)"". It checks if you have tbb version 2018 or above. If you install tbb BEFORE running cmake, it will fulfill the requirement and bypass installing tbb in the make command, hence bypass the error. The solution:; 1. Delete the salmon folder and download a fresh one from github; 2. sudo apt update (this step is very important, to update the packages to be above version 2018) ; 3. sudo apt-get install libtbb-dev; 4. (Optional) apt-cache policy libtbb-dev (check the version of libtbb, it should be 2019 or above); 5. Then follows the standard installation (cmake, make etc.) The error should disappear and compile successfully. Thanks!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/497#issuecomment-610977958
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474:612,Availability,down,downstream,612,"@tamuanand,. Right now, I added the code to dump the _salmon_ version into versionInfo.json. Which is a standard json file that goes in the index directory. Actually, that file already contains an index version key, which is simply a number that is incremented every time there is a change made that alters the binary representation of the index on disk. That is particularly useful because not every salmon version requires re-building the index. Regarding the feature I've added. It's fairly standard practice for us to put information that is meant to be read by both humans and machines (scripts, R packages downstream, etc.) into a JSON file. This makes it easy to access it simply from many languages, and to have _some_ (but not too much) structure to this data. There are even slick command line tools for pulling info out of JSON files (like [jq](https://stedolan.github.io/jq/)). If there is a strong reason that you need the _salmon_ version in its own text file, I'm willing to oblige and duplicate the information there. Just let me know.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474:670,Security,access,access,670,"@tamuanand,. Right now, I added the code to dump the _salmon_ version into versionInfo.json. Which is a standard json file that goes in the index directory. Actually, that file already contains an index version key, which is simply a number that is incremented every time there is a change made that alters the binary representation of the index on disk. That is particularly useful because not every salmon version requires re-building the index. Regarding the feature I've added. It's fairly standard practice for us to put information that is meant to be read by both humans and machines (scripts, R packages downstream, etc.) into a JSON file. This makes it easy to access it simply from many languages, and to have _some_ (but not too much) structure to this data. There are even slick command line tools for pulling info out of JSON files (like [jq](https://stedolan.github.io/jq/)). If there is a strong reason that you need the _salmon_ version in its own text file, I'm willing to oblige and duplicate the information there. Just let me know.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474:225,Usability,simpl,simply,225,"@tamuanand,. Right now, I added the code to dump the _salmon_ version into versionInfo.json. Which is a standard json file that goes in the index directory. Actually, that file already contains an index version key, which is simply a number that is incremented every time there is a change made that alters the binary representation of the index on disk. That is particularly useful because not every salmon version requires re-building the index. Regarding the feature I've added. It's fairly standard practice for us to put information that is meant to be read by both humans and machines (scripts, R packages downstream, etc.) into a JSON file. This makes it easy to access it simply from many languages, and to have _some_ (but not too much) structure to this data. There are even slick command line tools for pulling info out of JSON files (like [jq](https://stedolan.github.io/jq/)). If there is a strong reason that you need the _salmon_ version in its own text file, I'm willing to oblige and duplicate the information there. Just let me know.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474:680,Usability,simpl,simply,680,"@tamuanand,. Right now, I added the code to dump the _salmon_ version into versionInfo.json. Which is a standard json file that goes in the index directory. Actually, that file already contains an index version key, which is simply a number that is incremented every time there is a change made that alters the binary representation of the index on disk. That is particularly useful because not every salmon version requires re-building the index. Regarding the feature I've added. It's fairly standard practice for us to put information that is meant to be read by both humans and machines (scripts, R packages downstream, etc.) into a JSON file. This makes it easy to access it simply from many languages, and to have _some_ (but not too much) structure to this data. There are even slick command line tools for pulling info out of JSON files (like [jq](https://stedolan.github.io/jq/)). If there is a strong reason that you need the _salmon_ version in its own text file, I'm willing to oblige and duplicate the information there. Just let me know.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605694474
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208:182,Availability,avail,available,182,"Hi @rob-p . A json file would be fine - I was just suggesting a text file. I myself use jq. So, adding it to 'versionInfo.json"" file is fine. Would you know when this update will be available?. On a different note - is salmon 1.2.0 available? See attached screenshot from the ""readthedocs"" page - that seems to suggest that salmon 1.2.0 is already available. I checked out the releases page and could only see 1.1.0. Even bioconda has salmon 1.1.0 - am I missing anything?. ![image](https://user-images.githubusercontent.com/8467214/77860517-d399bb00-71dd-11ea-9f07-d8173789cd6f.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208:232,Availability,avail,available,232,"Hi @rob-p . A json file would be fine - I was just suggesting a text file. I myself use jq. So, adding it to 'versionInfo.json"" file is fine. Would you know when this update will be available?. On a different note - is salmon 1.2.0 available? See attached screenshot from the ""readthedocs"" page - that seems to suggest that salmon 1.2.0 is already available. I checked out the releases page and could only see 1.1.0. Even bioconda has salmon 1.1.0 - am I missing anything?. ![image](https://user-images.githubusercontent.com/8467214/77860517-d399bb00-71dd-11ea-9f07-d8173789cd6f.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208:348,Availability,avail,available,348,"Hi @rob-p . A json file would be fine - I was just suggesting a text file. I myself use jq. So, adding it to 'versionInfo.json"" file is fine. Would you know when this update will be available?. On a different note - is salmon 1.2.0 available? See attached screenshot from the ""readthedocs"" page - that seems to suggest that salmon 1.2.0 is already available. I checked out the releases page and could only see 1.1.0. Even bioconda has salmon 1.1.0 - am I missing anything?. ![image](https://user-images.githubusercontent.com/8467214/77860517-d399bb00-71dd-11ea-9f07-d8173789cd6f.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208:167,Deployability,update,update,167,"Hi @rob-p . A json file would be fine - I was just suggesting a text file. I myself use jq. So, adding it to 'versionInfo.json"" file is fine. Would you know when this update will be available?. On a different note - is salmon 1.2.0 available? See attached screenshot from the ""readthedocs"" page - that seems to suggest that salmon 1.2.0 is already available. I checked out the releases page and could only see 1.1.0. Even bioconda has salmon 1.1.0 - am I missing anything?. ![image](https://user-images.githubusercontent.com/8467214/77860517-d399bb00-71dd-11ea-9f07-d8173789cd6f.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208
https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208:377,Deployability,release,releases,377,"Hi @rob-p . A json file would be fine - I was just suggesting a text file. I myself use jq. So, adding it to 'versionInfo.json"" file is fine. Would you know when this update will be available?. On a different note - is salmon 1.2.0 available? See attached screenshot from the ""readthedocs"" page - that seems to suggest that salmon 1.2.0 is already available. I checked out the releases page and could only see 1.1.0. Even bioconda has salmon 1.1.0 - am I missing anything?. ![image](https://user-images.githubusercontent.com/8467214/77860517-d399bb00-71dd-11ea-9f07-d8173789cd6f.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/498#issuecomment-605700208
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610578989:169,Integrability,depend,dependency,169,"Hi, Rob.; I've been trying to compile ""salmon"" 1.1.0 from source under Ubuntu 18.04 and managed to get a newer version of ""cmake"" from KitWare, but there are many other dependency problems...",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610578989
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610580721:402,Testability,test,test,402,"We shy away from using `native` arch anywhere, and instead try to be explicit about the instruction sets used. My current best guess is that we assume SSE4 (at least [here](https://github.com/COMBINE-lab/pufferfish/blob/master/CMakeLists.txt#L77)). I believe this processor does not fully support SSE4.0. I can try and see if there are any other places we make this assumption, and then perhaps make a test pre-compiled binary without it.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610580721
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162:221,Deployability,release,release,221,"Excellent! Now we should do some internal testing to see if this has any negative performance impact on machines that _do_ have SSE4. Then we can determine if we can just make this the default, or if it's worth cutting a release under 2 configurations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162:237,Deployability,configurat,configurations,237,"Excellent! Now we should do some internal testing to see if this has any negative performance impact on machines that _do_ have SSE4. Then we can determine if we can just make this the default, or if it's worth cutting a release under 2 configurations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162:237,Modifiability,config,configurations,237,"Excellent! Now we should do some internal testing to see if this has any negative performance impact on machines that _do_ have SSE4. Then we can determine if we can just make this the default, or if it's worth cutting a release under 2 configurations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162:82,Performance,perform,performance,82,"Excellent! Now we should do some internal testing to see if this has any negative performance impact on machines that _do_ have SSE4. Then we can determine if we can just make this the default, or if it's worth cutting a release under 2 configurations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162:42,Testability,test,testing,42,"Excellent! Now we should do some internal testing to see if this has any negative performance impact on machines that _do_ have SSE4. Then we can determine if we can just make this the default, or if it's worth cutting a release under 2 configurations.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-610602162
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-612273525:9,Performance,perform,performance,9,"I saw no performance regressions, so 1.2.0 is built without the offending flag. Thanks for the heads up.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-612273525
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-612423808:317,Integrability,Bridg,Bridge,317,"On 11/04/2020 01:04, Rob Patro wrote:; > I saw no performance regressions, so 1.2.0 is built without the ; > offending flag. Thanks for the heads up. Hi, Rob. Thanks for fixing the problem so quickly!. Tony. -- ; Minke Informatics Limited, Registered in Scotland - Company No. SC419028; Registered Office: 3 Donview, Bridge of Alford, AB33 8QJ, Scotland (UK); tel. +44(0)19755 63548 http://minke-informatics.co.uk; mob. +44(0)7985 078324 mailto:tony.travis@minke-informatics.co.uk",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-612423808
https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-612423808:50,Performance,perform,performance,50,"On 11/04/2020 01:04, Rob Patro wrote:; > I saw no performance regressions, so 1.2.0 is built without the ; > offending flag. Thanks for the heads up. Hi, Rob. Thanks for fixing the problem so quickly!. Tony. -- ; Minke Informatics Limited, Registered in Scotland - Company No. SC419028; Registered Office: 3 Donview, Bridge of Alford, AB33 8QJ, Scotland (UK); tel. +44(0)19755 63548 http://minke-informatics.co.uk; mob. +44(0)7985 078324 mailto:tony.travis@minke-informatics.co.uk",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/500#issuecomment-612423808
https://github.com/COMBINE-lab/salmon/issues/501#issuecomment-611822688:95,Deployability,release,release,95,Thanks for the bug report. This has been fixed in develop and should work properly in the next release.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/501#issuecomment-611822688
https://github.com/COMBINE-lab/salmon/issues/502#issuecomment-989318706:207,Deployability,install,install,207,"Looks like some progress has been made leading up to 1.6.0. The only thing cmake insists on embedding in the build now is pufferfish. Seems like it should be relatively easy to tweak cmake to use a separate install of this since it's already a separate Github project. However, it appears to use a COMBINE-lab fork of staden-io, which makes it impractical to install that as a separate package, since it would conflict with the mainstream version. The Debian package for staden-io_lib is from [https://github.com/jkbonfield/io_lib](https://github.com/jkbonfield/io_lib). I did get a successful build under FreeBSD ports: [https://github.com/outpaddling/freebsd-ports-wip/tree/master/salmon](https://github.com/outpaddling/freebsd-ports-wip/tree/master/salmon).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/502#issuecomment-989318706
https://github.com/COMBINE-lab/salmon/issues/502#issuecomment-989318706:359,Deployability,install,install,359,"Looks like some progress has been made leading up to 1.6.0. The only thing cmake insists on embedding in the build now is pufferfish. Seems like it should be relatively easy to tweak cmake to use a separate install of this since it's already a separate Github project. However, it appears to use a COMBINE-lab fork of staden-io, which makes it impractical to install that as a separate package, since it would conflict with the mainstream version. The Debian package for staden-io_lib is from [https://github.com/jkbonfield/io_lib](https://github.com/jkbonfield/io_lib). I did get a successful build under FreeBSD ports: [https://github.com/outpaddling/freebsd-ports-wip/tree/master/salmon](https://github.com/outpaddling/freebsd-ports-wip/tree/master/salmon).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/502#issuecomment-989318706
https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612193221:97,Deployability,release,release,97,"The virtual memory should also be greatly reduced in 1.2.0 (which I am working on finalizing the release of at the moment). There will be detailed release notes describing the improvements. However, getting the pre-built index is probably worth it if it's the right organism and annotation.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612193221
https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612193221:147,Deployability,release,release,147,"The virtual memory should also be greatly reduced in 1.2.0 (which I am working on finalizing the release of at the moment). There will be detailed release notes describing the improvements. However, getting the pre-built index is probably worth it if it's the right organism and annotation.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612193221
https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612193221:42,Energy Efficiency,reduce,reduced,42,"The virtual memory should also be greatly reduced in 1.2.0 (which I am working on finalizing the release of at the moment). There will be detailed release notes describing the improvements. However, getting the pre-built index is probably worth it if it's the right organism and annotation.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612193221
https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612220894:47,Deployability,release,releases,47,@nickhir https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0 is now released by @rob-p,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612220894
https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612220894:74,Deployability,release,released,74,@nickhir https://github.com/COMBINE-lab/salmon/releases/tag/v1.2.0 is now released by @rob-p,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/503#issuecomment-612220894
https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080:823,Deployability,release,released,823,"Hi @tamuanand,. Nope; these values are right. The `indexVersion` is a global identifier with respect to previous versions of salmon. It is a global number that is incremented each time (a) a backward-incompatible change to the index is introduced or (b) a fundamentally new piece of information is contained in the index. This field took a value of `1` way back when we started versioning the salmon index a number of years ago, and version `1` was based on the RapMap index (rather than pufferfish like the current one). This is simply a global identifier that we can use internally to determine whether the version of salmon reading this index can be expected to make use of it. The other field `indexType` corresponds to the value from an internal enumeration used in the salmon code. Over the years (since it was first released in 2014), salmon has used a number of different data structures for its underlying index. First, it used a modified version of the FMD index that BWA is based upon, then, it used the RapMap index (based upon a sparse hash map and an uncompressed suffix array), and now it uses the pufferfish index. This `indexType` filed just records the type of this index. In modern (post 1.0.0) versions of salmon, the pufferfish index (`2`) is the only valid version. There's a lot of history to these values, but they all make sense internally within salmon, which is how the contents of this file are primarily used (i.e. to make sure there is compatibility between the version of salmon being run and the index we are trying to consume). Hopefully, this description clears things up a bit. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080
https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080:1049,Security,hash,hash,1049,"Hi @tamuanand,. Nope; these values are right. The `indexVersion` is a global identifier with respect to previous versions of salmon. It is a global number that is incremented each time (a) a backward-incompatible change to the index is introduced or (b) a fundamentally new piece of information is contained in the index. This field took a value of `1` way back when we started versioning the salmon index a number of years ago, and version `1` was based on the RapMap index (rather than pufferfish like the current one). This is simply a global identifier that we can use internally to determine whether the version of salmon reading this index can be expected to make use of it. The other field `indexType` corresponds to the value from an internal enumeration used in the salmon code. Over the years (since it was first released in 2014), salmon has used a number of different data structures for its underlying index. First, it used a modified version of the FMD index that BWA is based upon, then, it used the RapMap index (based upon a sparse hash map and an uncompressed suffix array), and now it uses the pufferfish index. This `indexType` filed just records the type of this index. In modern (post 1.0.0) versions of salmon, the pufferfish index (`2`) is the only valid version. There's a lot of history to these values, but they all make sense internally within salmon, which is how the contents of this file are primarily used (i.e. to make sure there is compatibility between the version of salmon being run and the index we are trying to consume). Hopefully, this description clears things up a bit. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080
https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080:530,Usability,simpl,simply,530,"Hi @tamuanand,. Nope; these values are right. The `indexVersion` is a global identifier with respect to previous versions of salmon. It is a global number that is incremented each time (a) a backward-incompatible change to the index is introduced or (b) a fundamentally new piece of information is contained in the index. This field took a value of `1` way back when we started versioning the salmon index a number of years ago, and version `1` was based on the RapMap index (rather than pufferfish like the current one). This is simply a global identifier that we can use internally to determine whether the version of salmon reading this index can be expected to make use of it. The other field `indexType` corresponds to the value from an internal enumeration used in the salmon code. Over the years (since it was first released in 2014), salmon has used a number of different data structures for its underlying index. First, it used a modified version of the FMD index that BWA is based upon, then, it used the RapMap index (based upon a sparse hash map and an uncompressed suffix array), and now it uses the pufferfish index. This `indexType` filed just records the type of this index. In modern (post 1.0.0) versions of salmon, the pufferfish index (`2`) is the only valid version. There's a lot of history to these values, but they all make sense internally within salmon, which is how the contents of this file are primarily used (i.e. to make sure there is compatibility between the version of salmon being run and the index we are trying to consume). Hopefully, this description clears things up a bit. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080
https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080:1589,Usability,clear,clears,1589,"Hi @tamuanand,. Nope; these values are right. The `indexVersion` is a global identifier with respect to previous versions of salmon. It is a global number that is incremented each time (a) a backward-incompatible change to the index is introduced or (b) a fundamentally new piece of information is contained in the index. This field took a value of `1` way back when we started versioning the salmon index a number of years ago, and version `1` was based on the RapMap index (rather than pufferfish like the current one). This is simply a global identifier that we can use internally to determine whether the version of salmon reading this index can be expected to make use of it. The other field `indexType` corresponds to the value from an internal enumeration used in the salmon code. Over the years (since it was first released in 2014), salmon has used a number of different data structures for its underlying index. First, it used a modified version of the FMD index that BWA is based upon, then, it used the RapMap index (based upon a sparse hash map and an uncompressed suffix array), and now it uses the pufferfish index. This `indexType` filed just records the type of this index. In modern (post 1.0.0) versions of salmon, the pufferfish index (`2`) is the only valid version. There's a lot of history to these values, but they all make sense internally within salmon, which is how the contents of this file are primarily used (i.e. to make sure there is compatibility between the version of salmon being run and the index we are trying to consume). Hopefully, this description clears things up a bit. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/504#issuecomment-613217080
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613224352:309,Testability,log,log,309,Hi @rob-p . _**Salmon index command used**_; ```; salmon index -t rnor_gentrome.fa -d decoys.txt -k 17 --keepFixedFasta --keepDuplicates -p 16 -i rnor_ENSEMBL_96_index; ```. **_Directory size after indexing completes_**; ```; du -sh .; 45G; ```. **_Listing of files and their sizes_**; ```; 4.2K ref_indexing.log; 115 pre_indexing.log; 126 versionInfo.json; 944M mphf.bin; 6.0G pos.bin; 1004 info.json; 256K refAccumLengths.bin; 701M refseq.bin; 15G ctable.bin; 2.5G ctg_offsets.bin; 128K reflengths.bin; 2.9G seq.bin; 1.5G rank.bin; 128K complete_ref_lens.bin; 2.8G ref_k17_fixed.fa; 22K duplicate_clusters.tsv; ```,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613224352
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613224352:331,Testability,log,log,331,Hi @rob-p . _**Salmon index command used**_; ```; salmon index -t rnor_gentrome.fa -d decoys.txt -k 17 --keepFixedFasta --keepDuplicates -p 16 -i rnor_ENSEMBL_96_index; ```. **_Directory size after indexing completes_**; ```; du -sh .; 45G; ```. **_Listing of files and their sizes_**; ```; 4.2K ref_indexing.log; 115 pre_indexing.log; 126 versionInfo.json; 944M mphf.bin; 6.0G pos.bin; 1004 info.json; 256K refAccumLengths.bin; 701M refseq.bin; 15G ctable.bin; 2.5G ctg_offsets.bin; 128K reflengths.bin; 2.9G seq.bin; 1.5G rank.bin; 128K complete_ref_lens.bin; 2.8G ref_k17_fixed.fa; 22K duplicate_clusters.tsv; ```,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613224352
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613226163:507,Performance,optimiz,optimization,507,"So, I am still very surprised by the 45G number, but one big difference here is that the refgenomes indices are with the default value of k (`k=31`). As you can see, the dominant element of the index here is the `ctable.bin`, which stores where unitigs of the dBG start within each reference. As the k-met size gets smaller, unitigs get shorter, and they appear more places. Further, the increase here is not linear as `k` decreases. I’d suspect most of the size difference is due to that. There is also an optimization for the `ctable.bin` that we have been working on that wastes fewer bits, and will make this part of the index somewhat smaller in such cases. However, I am fairly certain that is not the dominant factor here. You could see what this is with the default `k`; I’d expect that to be closer to the refgenomes number (but perhaps a bit different due to version differences and the `—keepDuplicates` flag).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613226163
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613226597:77,Deployability,update,update,77,Ok - I will give this a try with` k 31` and `not have keepDuplicates` . Will update soon,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613226597
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613229118:393,Availability,error,errors,393,"When I try it this way, the program exits. ```; salmon index -t rnor_gentrome.fa -d decoys.txt -k 31 -p 16 -i rnor_ENSEMBL_96_SA_index_k31; ```. These are the last few lines. ```; [2020-04-14 01:07:52.750] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by seq; uences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta; file that is being indexed.; [2020-04-14 01:07:52.895] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613229118
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613229118:400,Availability,down,downstream,400,"When I try it this way, the program exits. ```; salmon index -t rnor_gentrome.fa -d decoys.txt -k 31 -p 16 -i rnor_ENSEMBL_96_SA_index_k31; ```. These are the last few lines. ```; [2020-04-14 01:07:52.750] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by seq; uences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta; file that is being indexed.; [2020-04-14 01:07:52.895] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613229118
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613229118:558,Availability,error,error,558,"When I try it this way, the program exits. ```; salmon index -t rnor_gentrome.fa -d decoys.txt -k 31 -p 16 -i rnor_ENSEMBL_96_SA_index_k31; ```. These are the last few lines. ```; [2020-04-14 01:07:52.750] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by seq; uences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta; file that is being indexed.; [2020-04-14 01:07:52.895] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613229118
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:430,Availability,error,errors,430,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:437,Availability,down,downstream,437,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:594,Availability,error,error,594,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:129,Testability,log,log,129,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792:39,Usability,simpl,simple,39,"Hi @tamuanand,. Ok, it seems something simple with the preparation of the decoys.txt file. I'm looking into it. If you watch the log, you see the following output before the (intentional exit with status code 1):. ```; [2020-04-14 09:44:12.991] [puff::index::jointLog] [critical] The decoy file contained the names of 955 decoy sequences, but 953 were matched by sequences in the reference file provided. To prevent unintentional errors downstream, please ensure that the decoy file exactly matches with the fasta file that is being indexed.; [2020-04-14 09:44:13.304] [puff::index::jointLog] [error] The fixFasta phase failed with exit code 1; Command exited with non-zero status 1; 56.66user 9.14system 1:04.69elapsed 101%CPU (0avgtext+0avgdata 6902936maxresident)k; 3792inputs+16outputs (30major+3629051minor)pagefaults 0swaps; ```. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613451792
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613480649:925,Availability,ping,pinging,925,"Ok, I figured it out :) — these two *decoy* references are (1) identical with each other and (2) collide with *another* decoy reference. Currently, the way we process decoys, we don't allow duplicate decoys (it makes even less sense to allow duplicate decoys than to allow duplicate transcripts). However, the reason indexing worked with `k=17` is not because of `k` but because of the `--keepDuplicates` flag. With that flag, these decoys get added. I think the right thing for us to do on our part is to remove duplicate *decoys* if they appear in the reference and the user has not passed `--keepDuplicates`. . However, for the time being, I think the best thing to do is simply to remove `AABR07022993.1` and `AABR07023006.1` from the `toplevel` file and from `decoys.txt`, since the sequence they contain is already represented in the decoy part of the index. This will represent a full and comprehensive SAF index. I'm pinging @k3yavi to see if he has any good idea about the easiest way to cull these duplicate refs from the input files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613480649
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613480649:675,Usability,simpl,simply,675,"Ok, I figured it out :) — these two *decoy* references are (1) identical with each other and (2) collide with *another* decoy reference. Currently, the way we process decoys, we don't allow duplicate decoys (it makes even less sense to allow duplicate decoys than to allow duplicate transcripts). However, the reason indexing worked with `k=17` is not because of `k` but because of the `--keepDuplicates` flag. With that flag, these decoys get added. I think the right thing for us to do on our part is to remove duplicate *decoys* if they appear in the reference and the user has not passed `--keepDuplicates`. . However, for the time being, I think the best thing to do is simply to remove `AABR07022993.1` and `AABR07023006.1` from the `toplevel` file and from `decoys.txt`, since the sequence they contain is already represented in the decoy part of the index. This will represent a full and comprehensive SAF index. I'm pinging @k3yavi to see if he has any good idea about the easiest way to cull these duplicate refs from the input files.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-613480649
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617830355:49,Usability,feedback,feedback,49,"Hi @tamuanand and @uros-sipetic,. Thanks for the feedback on this! I just cut v1.2.1 which ""fixes"" the behavior. It will simply discard any duplicate _decoy_ sequences, which resolves this problem without requiring manual intervention.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617830355
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617830355:121,Usability,simpl,simply,121,"Hi @tamuanand and @uros-sipetic,. Thanks for the feedback on this! I just cut v1.2.1 which ""fixes"" the behavior. It will simply discard any duplicate _decoy_ sequences, which resolves this problem without requiring manual intervention.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617830355
https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617940367:121,Deployability,release,release,121,"No, there are no changes here. Further, indices built from version 1.0.0 are *forward-compatible* up through the current release. There is no need to rebuild any indices. Also, though 1.2.1 added a new flag, it made no changes to defaults, so quantifications between 1.2.0 and 1.2.1 are directly comparable.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/505#issuecomment-617940367
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614198287:42,Deployability,upgrade,upgrade,42,"@diegoalzatec86 Also, you should probably upgrade to 1.2.0 as there have been many memory related issues that @rob-p and @k3yavi have incorporated in 1.2.0",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614198287
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614218653:93,Availability,error,error,93,"Thanks, Isn't Sailfish required to run Salmon?, because the first time I run salmon I got an error:. `bash: sailfish: command not found_`. I dowloaded sailfish and when got back to use salmon the malloc error appears.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614218653
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614218653:203,Availability,error,error,203,"Thanks, Isn't Sailfish required to run Salmon?, because the first time I run salmon I got an error:. `bash: sailfish: command not found_`. I dowloaded sailfish and when got back to use salmon the malloc error appears.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614218653
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614225512:80,Availability,down,downloading,80,@diegoalzatec86 Probably you can try with salmon 1.2.0 and post back here . Try downloading via the bioconda channel - https://bioconda.github.io/recipes/salmon/README.html,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614225512
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614227322:296,Availability,error,error,296,"@diegoalzatec86,. Sailfish is *not* required to run salmon. Salmon is a completely different tool, and it has subsumed and largely obviated sailfish. I would recommend using salmon rather than sailfish to do any RNA-seq quantification. Can you show what command you were running when you got the error:. ```; bash: sailfish: command not found_; ```. ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614227322
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614279802:278,Availability,echo,echo,278,"Hi @rob-p ; Yes, for now I'm just trying to run the getting started tutorial, I dowloaded and indexed the A. thaliana transcriptome, then I dowloaded the sequnecing data and run the quantification code:. `#!/bin/bash; for fn in data/DRR0161{25..40};; do; samp=`basename ${fn}`; echo ""Processing sample ${samp}""; sailfish quant -i athal_index -l A \; -1 ${fn}/${samp}_1.fastq.gz \; -2 ${samp}_2.fastq.gz \; 		 -p 8 -o quants/${samp}_quant; done `. `_Processing sample DRR016125. -bash: sailfish: command not found_`",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614279802
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614289282:381,Availability,echo,echo,381,"Hello again @rob-p . I was calling using the wrong code for quantification and calling sailfish .; The code that I just sent you is included in the tutorial and is saved as shell script . `quant_tut_samples.sh`. However this doesn't match with the code described immediately below in the tutorial. ; >#!/bin/bash; >for fn in data/DRR0161{25..40};; > do; > samp=`basename ${fn}`; > echo ""Processing sample ${samp}""; > salmon quant -i athal_index -l A \; > -1 ${fn}/${samp}_1.fastq.gz \; > -2 ${fn}/${samp}_2.fastq.gz \; > -p 8 --validateMappings -o quants/${samp}_quant; >done. This is something that should be corrected in the tutorial. Thank you so much. I apologize.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614289282
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614289282:528,Security,validat,validateMappings,528,"Hello again @rob-p . I was calling using the wrong code for quantification and calling sailfish .; The code that I just sent you is included in the tutorial and is saved as shell script . `quant_tut_samples.sh`. However this doesn't match with the code described immediately below in the tutorial. ; >#!/bin/bash; >for fn in data/DRR0161{25..40};; > do; > samp=`basename ${fn}`; > echo ""Processing sample ${samp}""; > salmon quant -i athal_index -l A \; > -1 ${fn}/${samp}_1.fastq.gz \; > -2 ${fn}/${samp}_2.fastq.gz \; > -p 8 --validateMappings -o quants/${samp}_quant; >done. This is something that should be corrected in the tutorial. Thank you so much. I apologize.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614289282
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614298125:160,Availability,error,errors,160,"@diegoalzatec86 I think you are referring to this page - https://combine-lab.github.io/salmon/getting_started/. I myself have used that I did not encounter any errors or anything related to sailfish when I used the very same shell script you are alluding to.. I don't know which tutorial has the sailfish code that you state - https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614279802. @rob-p Check this https://raw.githubusercontent.com/COMBINE-lab/salmon/gh-pages/assets/quant_tut_samples.sh . Probably you landed on a wrong or outdated tutorial page . As previously mentioned many times in this GH issue, most of sailfish code is within salmon . Either way, I think you should use bioconda to upgrade to 1.2.0 and start using salmon as the quantification tool",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614298125
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614298125:710,Deployability,upgrade,upgrade,710,"@diegoalzatec86 I think you are referring to this page - https://combine-lab.github.io/salmon/getting_started/. I myself have used that I did not encounter any errors or anything related to sailfish when I used the very same shell script you are alluding to.. I don't know which tutorial has the sailfish code that you state - https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614279802. @rob-p Check this https://raw.githubusercontent.com/COMBINE-lab/salmon/gh-pages/assets/quant_tut_samples.sh . Probably you landed on a wrong or outdated tutorial page . As previously mentioned many times in this GH issue, most of sailfish code is within salmon . Either way, I think you should use bioconda to upgrade to 1.2.0 and start using salmon as the quantification tool",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614298125
https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614386127:5,Deployability,update,updated,5,I've updated the linked shell script. Thanks for pointing it ou!,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/508#issuecomment-614386127
https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186:1318,Energy Efficiency,efficient,efficiently,1318,"input gentrome file, replace ambiguous characters (e.g. `N`) with pseudo-random nucleotides. It will also report any transcripts smaller than the chosen k-mer size, and it will detect and remove (unless `--keepDuplicates` is passed) any identical / duplicate sequences. After all of this, it will begin constructing the index in earnest. This is done by running a modified version of [TwoPaCo](https://github.com/medvedevgroup/TwoPaCo) to construct the compacted colored de Bruijn graph on the gentrome, which is then indexed using our [pufferfish index](https://github.com/COMBINE-lab/pufferfish). The TwoPaCo algorithm that generates the compacted colored de Bruijn graph from the input sequence is based on a very elegant algorithm that couples a Bloom filter with an exact hash table, and makes two (or more) passes over the input to identify all of the junctions in the reference (which directly implies all unitigs). To make the algorithm work efficiently, one needs to have an estimate for the number of distinct k-mers that will be encountered in the reference sequence. If the estimate is too big, one wastes memory. If the estimate is too small, the Bloom filter is not big enough, it doesn't filter efficiently, and the algorithm ends up putting way too much data in the exact hash table. In order to determine how to set the Bloom filter size appropriately, we take the following approach. If the Bloom filter size isn't provided directly (_note_: this is _not_ the same as the k-mer size, this is an estimate of the total number of distinct k-mers in the entire input data), then we make a call to a function defined in the [ntCard](https://github.com/bcgsc/ntCard) library. This is a program designed specifically for cardinality estimation of k-mers in sequencing data. Based on the estimated number of distinct k-mers, we use the standard equations (derived from the theory behind Bloom filters) to set the Bloom filter to be of the smallest possible size that still achieves a relati",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186
https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186:1578,Energy Efficiency,efficient,efficiently,1578,"plicates` is passed) any identical / duplicate sequences. After all of this, it will begin constructing the index in earnest. This is done by running a modified version of [TwoPaCo](https://github.com/medvedevgroup/TwoPaCo) to construct the compacted colored de Bruijn graph on the gentrome, which is then indexed using our [pufferfish index](https://github.com/COMBINE-lab/pufferfish). The TwoPaCo algorithm that generates the compacted colored de Bruijn graph from the input sequence is based on a very elegant algorithm that couples a Bloom filter with an exact hash table, and makes two (or more) passes over the input to identify all of the junctions in the reference (which directly implies all unitigs). To make the algorithm work efficiently, one needs to have an estimate for the number of distinct k-mers that will be encountered in the reference sequence. If the estimate is too big, one wastes memory. If the estimate is too small, the Bloom filter is not big enough, it doesn't filter efficiently, and the algorithm ends up putting way too much data in the exact hash table. In order to determine how to set the Bloom filter size appropriately, we take the following approach. If the Bloom filter size isn't provided directly (_note_: this is _not_ the same as the k-mer size, this is an estimate of the total number of distinct k-mers in the entire input data), then we make a call to a function defined in the [ntCard](https://github.com/bcgsc/ntCard) library. This is a program designed specifically for cardinality estimation of k-mers in sequencing data. Based on the estimated number of distinct k-mers, we use the standard equations (derived from the theory behind Bloom filters) to set the Bloom filter to be of the smallest possible size that still achieves a relatively low, pre-specified, false positive rate. The message you are seeing is that the estimates suggest the Bloom filter should be of size 2^28 *bits*, which is ~ 33.55MB — pretty small, actually. This is because ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186
https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186:2418,Integrability,message,message,2418,"nerates the compacted colored de Bruijn graph from the input sequence is based on a very elegant algorithm that couples a Bloom filter with an exact hash table, and makes two (or more) passes over the input to identify all of the junctions in the reference (which directly implies all unitigs). To make the algorithm work efficiently, one needs to have an estimate for the number of distinct k-mers that will be encountered in the reference sequence. If the estimate is too big, one wastes memory. If the estimate is too small, the Bloom filter is not big enough, it doesn't filter efficiently, and the algorithm ends up putting way too much data in the exact hash table. In order to determine how to set the Bloom filter size appropriately, we take the following approach. If the Bloom filter size isn't provided directly (_note_: this is _not_ the same as the k-mer size, this is an estimate of the total number of distinct k-mers in the entire input data), then we make a call to a function defined in the [ntCard](https://github.com/bcgsc/ntCard) library. This is a program designed specifically for cardinality estimation of k-mers in sequencing data. Based on the estimated number of distinct k-mers, we use the standard equations (derived from the theory behind Bloom filters) to set the Bloom filter to be of the smallest possible size that still achieves a relatively low, pre-specified, false positive rate. The message you are seeing is that the estimates suggest the Bloom filter should be of size 2^28 *bits*, which is ~ 33.55MB — pretty small, actually. This is because ntCard estimated 12,754,610 distinct k-mers (31-mers) in your input dataset, which doesn't seem unreasonable. In short, I think the output you observe here seems completely reasonable and in line with the data you are providing. However, I understand how all of this output might to make sense if you're not familiar with everything going on behind the scenes. Please let me know if this answers your question. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186
https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186:545,Safety,detect,detect,545,"Hi @gtollefson,. To understand the output here a little bit better, it is necessary to understand what salmon is doing during indexing. My understanding is that you provided salmon with a transcriptome and (small) decoy reference genome, and asked it to build an index with k-mer size `k=31`. When you do this, salmon will do a few things. First, it will go over your input gentrome file, replace ambiguous characters (e.g. `N`) with pseudo-random nucleotides. It will also report any transcripts smaller than the chosen k-mer size, and it will detect and remove (unless `--keepDuplicates` is passed) any identical / duplicate sequences. After all of this, it will begin constructing the index in earnest. This is done by running a modified version of [TwoPaCo](https://github.com/medvedevgroup/TwoPaCo) to construct the compacted colored de Bruijn graph on the gentrome, which is then indexed using our [pufferfish index](https://github.com/COMBINE-lab/pufferfish). The TwoPaCo algorithm that generates the compacted colored de Bruijn graph from the input sequence is based on a very elegant algorithm that couples a Bloom filter with an exact hash table, and makes two (or more) passes over the input to identify all of the junctions in the reference (which directly implies all unitigs). To make the algorithm work efficiently, one needs to have an estimate for the number of distinct k-mers that will be encountered in the reference sequence. If the estimate is too big, one wastes memory. If the estimate is too small, the Bloom filter is not big enough, it doesn't filter efficiently, and the algorithm ends up putting way too much data in the exact hash table. In order to determine how to set the Bloom filter size appropriately, we take the following approach. If the Bloom filter size isn't provided directly (_note_: this is _not_ the same as the k-mer size, this is an estimate of the total number of distinct k-mers in the entire input data), then we make a call to a function defined in ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186
https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186:1145,Security,hash,hash,1145,"dexing. My understanding is that you provided salmon with a transcriptome and (small) decoy reference genome, and asked it to build an index with k-mer size `k=31`. When you do this, salmon will do a few things. First, it will go over your input gentrome file, replace ambiguous characters (e.g. `N`) with pseudo-random nucleotides. It will also report any transcripts smaller than the chosen k-mer size, and it will detect and remove (unless `--keepDuplicates` is passed) any identical / duplicate sequences. After all of this, it will begin constructing the index in earnest. This is done by running a modified version of [TwoPaCo](https://github.com/medvedevgroup/TwoPaCo) to construct the compacted colored de Bruijn graph on the gentrome, which is then indexed using our [pufferfish index](https://github.com/COMBINE-lab/pufferfish). The TwoPaCo algorithm that generates the compacted colored de Bruijn graph from the input sequence is based on a very elegant algorithm that couples a Bloom filter with an exact hash table, and makes two (or more) passes over the input to identify all of the junctions in the reference (which directly implies all unitigs). To make the algorithm work efficiently, one needs to have an estimate for the number of distinct k-mers that will be encountered in the reference sequence. If the estimate is too big, one wastes memory. If the estimate is too small, the Bloom filter is not big enough, it doesn't filter efficiently, and the algorithm ends up putting way too much data in the exact hash table. In order to determine how to set the Bloom filter size appropriately, we take the following approach. If the Bloom filter size isn't provided directly (_note_: this is _not_ the same as the k-mer size, this is an estimate of the total number of distinct k-mers in the entire input data), then we make a call to a function defined in the [ntCard](https://github.com/bcgsc/ntCard) library. This is a program designed specifically for cardinality estimation of k-",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186
https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186:1656,Security,hash,hash,1656,"plicates` is passed) any identical / duplicate sequences. After all of this, it will begin constructing the index in earnest. This is done by running a modified version of [TwoPaCo](https://github.com/medvedevgroup/TwoPaCo) to construct the compacted colored de Bruijn graph on the gentrome, which is then indexed using our [pufferfish index](https://github.com/COMBINE-lab/pufferfish). The TwoPaCo algorithm that generates the compacted colored de Bruijn graph from the input sequence is based on a very elegant algorithm that couples a Bloom filter with an exact hash table, and makes two (or more) passes over the input to identify all of the junctions in the reference (which directly implies all unitigs). To make the algorithm work efficiently, one needs to have an estimate for the number of distinct k-mers that will be encountered in the reference sequence. If the estimate is too big, one wastes memory. If the estimate is too small, the Bloom filter is not big enough, it doesn't filter efficiently, and the algorithm ends up putting way too much data in the exact hash table. In order to determine how to set the Bloom filter size appropriately, we take the following approach. If the Bloom filter size isn't provided directly (_note_: this is _not_ the same as the k-mer size, this is an estimate of the total number of distinct k-mers in the entire input data), then we make a call to a function defined in the [ntCard](https://github.com/bcgsc/ntCard) library. This is a program designed specifically for cardinality estimation of k-mers in sequencing data. Based on the estimated number of distinct k-mers, we use the standard equations (derived from the theory behind Bloom filters) to set the Bloom filter to be of the smallest possible size that still achieves a relatively low, pre-specified, false positive rate. The message you are seeing is that the estimates suggest the Bloom filter should be of size 2^28 *bits*, which is ~ 33.55MB — pretty small, actually. This is because ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-616713186
https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-617275380:35,Usability,clear,clear,35,@rob-p Thank you very much for the clear explanation. That answers my question.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/510#issuecomment-617275380
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-616870047:169,Integrability,message,message,169,"Hi @summerrfair ,. I can't see anything obviously wrong with the command line. Do you have a small example of the transcripts.fa and myseq.bam file you could share? The message indicates that salmon thinks its running in mapping-based mode (with input fastq files), but you are clearly running in alignment based mode. Is the behavior any different if you put the -a argument first?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-616870047
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-616870047:278,Usability,clear,clearly,278,"Hi @summerrfair ,. I can't see anything obviously wrong with the command line. Do you have a small example of the transcripts.fa and myseq.bam file you could share? The message indicates that salmon thinks its running in mapping-based mode (with input fastq files), but you are clearly running in alignment based mode. Is the behavior any different if you put the -a argument first?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-616870047
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094:1930,Performance,queue,queue,1930," => salmon; # [ command ] => quant; # [ libType ] => { IU }; # [ alignments ] => { sample_alignments.sam }; # [ targets ] => { ../sample_data/transcripts.fasta }; # [ output ] => { sample_aln_quant }; Logs will be written to sample_aln_quant/logs; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2020-04-21 10:11:42.553] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 10:11:42.553] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-21 10:11:42.553] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""sample_alignments.sam"", fasta = ""../sample_data/transcripts.fasta"" . . .done; [2020-04-21 10:11:43.180] [jointLog] [info] replaced 0 non-ACGT nucleotides with random nucleotides. processed 0 reads in current round; killing thread 3 . . . done. Freeing memory used by read queue . . . 00; Joined parsing thread . . . ""sample_alignments.sam""; Closed all files . . .; Emptied frag queue. . . [2020-04-21 10:11:43.477] [jointLog] [info]. Completed first pass through the alignment file.; Total # of mapped reads : 10000; # of uniquely mapped reads : 6913; # ambiguously mapped reads : 3087. [2020-04-21 10:11:43.489] [jointLog] [info] Computed 27 rich equivalence classes for further processing; [2020-04-21 10:11:43.489] [jointLog] [info] Counted 10,000 total reads in the equivalence classes; [2020-04-21 10:11:43.490] [jointLog] [warning] Only 10000 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2020-04-21 10:11:43.492] [jointLog] [info] starting optimizer; [2020-04-21 10:11:43.493] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2020-04-21 10:11:43.493] [jointLog] [info] iteration = 0 | max rel diff. = 14.87; [2020-04-21 10:11:",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094:2036,Performance,queue,queue,2036,"targets ] => { ../sample_data/transcripts.fasta }; # [ output ] => { sample_aln_quant }; Logs will be written to sample_aln_quant/logs; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2020-04-21 10:11:42.553] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 10:11:42.553] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-21 10:11:42.553] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""sample_alignments.sam"", fasta = ""../sample_data/transcripts.fasta"" . . .done; [2020-04-21 10:11:43.180] [jointLog] [info] replaced 0 non-ACGT nucleotides with random nucleotides. processed 0 reads in current round; killing thread 3 . . . done. Freeing memory used by read queue . . . 00; Joined parsing thread . . . ""sample_alignments.sam""; Closed all files . . .; Emptied frag queue. . . [2020-04-21 10:11:43.477] [jointLog] [info]. Completed first pass through the alignment file.; Total # of mapped reads : 10000; # of uniquely mapped reads : 6913; # ambiguously mapped reads : 3087. [2020-04-21 10:11:43.489] [jointLog] [info] Computed 27 rich equivalence classes for further processing; [2020-04-21 10:11:43.489] [jointLog] [info] Counted 10,000 total reads in the equivalence classes; [2020-04-21 10:11:43.490] [jointLog] [warning] Only 10000 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2020-04-21 10:11:43.492] [jointLog] [info] starting optimizer; [2020-04-21 10:11:43.493] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2020-04-21 10:11:43.493] [jointLog] [info] iteration = 0 | max rel diff. = 14.87; [2020-04-21 10:11:43.495] [jointLog] [info] iteration = 100 | max rel diff. = 9.59592e-05; [2020-04-21 10:11:43.495] [jointLog] [in",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094:2710,Performance,optimiz,optimizer,2710," strandedness:unstranded }; [2020-04-21 10:11:42.553] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 10:11:42.553] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-21 10:11:42.553] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""sample_alignments.sam"", fasta = ""../sample_data/transcripts.fasta"" . . .done; [2020-04-21 10:11:43.180] [jointLog] [info] replaced 0 non-ACGT nucleotides with random nucleotides. processed 0 reads in current round; killing thread 3 . . . done. Freeing memory used by read queue . . . 00; Joined parsing thread . . . ""sample_alignments.sam""; Closed all files . . .; Emptied frag queue. . . [2020-04-21 10:11:43.477] [jointLog] [info]. Completed first pass through the alignment file.; Total # of mapped reads : 10000; # of uniquely mapped reads : 6913; # ambiguously mapped reads : 3087. [2020-04-21 10:11:43.489] [jointLog] [info] Computed 27 rich equivalence classes for further processing; [2020-04-21 10:11:43.489] [jointLog] [info] Counted 10,000 total reads in the equivalence classes; [2020-04-21 10:11:43.490] [jointLog] [warning] Only 10000 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2020-04-21 10:11:43.492] [jointLog] [info] starting optimizer; [2020-04-21 10:11:43.493] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2020-04-21 10:11:43.493] [jointLog] [info] iteration = 0 | max rel diff. = 14.87; [2020-04-21 10:11:43.495] [jointLog] [info] iteration = 100 | max rel diff. = 9.59592e-05; [2020-04-21 10:11:43.495] [jointLog] [info] finished optimizer; [2020-04-21 10:11:43.495] [jointLog] [info] writing output. Emptied Alignemnt Group Pool. .; Emptied Alignment Group Queue. . . done; ```. Let me know if this works for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094:3045,Performance,optimiz,optimizer,3045," strandedness:unstranded }; [2020-04-21 10:11:42.553] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 10:11:42.553] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-21 10:11:42.553] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""sample_alignments.sam"", fasta = ""../sample_data/transcripts.fasta"" . . .done; [2020-04-21 10:11:43.180] [jointLog] [info] replaced 0 non-ACGT nucleotides with random nucleotides. processed 0 reads in current round; killing thread 3 . . . done. Freeing memory used by read queue . . . 00; Joined parsing thread . . . ""sample_alignments.sam""; Closed all files . . .; Emptied frag queue. . . [2020-04-21 10:11:43.477] [jointLog] [info]. Completed first pass through the alignment file.; Total # of mapped reads : 10000; # of uniquely mapped reads : 6913; # ambiguously mapped reads : 3087. [2020-04-21 10:11:43.489] [jointLog] [info] Computed 27 rich equivalence classes for further processing; [2020-04-21 10:11:43.489] [jointLog] [info] Counted 10,000 total reads in the equivalence classes; [2020-04-21 10:11:43.490] [jointLog] [warning] Only 10000 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2020-04-21 10:11:43.492] [jointLog] [info] starting optimizer; [2020-04-21 10:11:43.493] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2020-04-21 10:11:43.493] [jointLog] [info] iteration = 0 | max rel diff. = 14.87; [2020-04-21 10:11:43.495] [jointLog] [info] iteration = 100 | max rel diff. = 9.59592e-05; [2020-04-21 10:11:43.495] [jointLog] [info] finished optimizer; [2020-04-21 10:11:43.495] [jointLog] [info] writing output. Emptied Alignemnt Group Pool. .; Emptied Alignment Group Queue. . . done; ```. Let me know if this works for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094:3173,Performance,Queue,Queue,3173," strandedness:unstranded }; [2020-04-21 10:11:42.553] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 10:11:42.553] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-21 10:11:42.553] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""sample_alignments.sam"", fasta = ""../sample_data/transcripts.fasta"" . . .done; [2020-04-21 10:11:43.180] [jointLog] [info] replaced 0 non-ACGT nucleotides with random nucleotides. processed 0 reads in current round; killing thread 3 . . . done. Freeing memory used by read queue . . . 00; Joined parsing thread . . . ""sample_alignments.sam""; Closed all files . . .; Emptied frag queue. . . [2020-04-21 10:11:43.477] [jointLog] [info]. Completed first pass through the alignment file.; Total # of mapped reads : 10000; # of uniquely mapped reads : 6913; # ambiguously mapped reads : 3087. [2020-04-21 10:11:43.489] [jointLog] [info] Computed 27 rich equivalence classes for further processing; [2020-04-21 10:11:43.489] [jointLog] [info] Counted 10,000 total reads in the equivalence classes; [2020-04-21 10:11:43.490] [jointLog] [warning] Only 10000 fragments were mapped, but the number of burn-in fragments was set to 5000000.; The effective lengths have been computed using the observed mappings. [2020-04-21 10:11:43.492] [jointLog] [info] starting optimizer; [2020-04-21 10:11:43.493] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2020-04-21 10:11:43.493] [jointLog] [info] iteration = 0 | max rel diff. = 14.87; [2020-04-21 10:11:43.495] [jointLog] [info] iteration = 100 | max rel diff. = 9.59592e-05; [2020-04-21 10:11:43.495] [jointLog] [info] finished optimizer; [2020-04-21 10:11:43.495] [jointLog] [info] writing output. Emptied Alignemnt Group Pool. .; Emptied Alignment Group Queue. . . done; ```. Let me know if this works for you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094:167,Testability,test,test,167,"Hi @summerrfair,. So, your BAM file looks like a SAM file (which is still OK), *but*, it's missing a header. I'm attaching here a sample SAM file you can use with the test data from the repository. Note, this SAM file is zipped (GitHub made me zip it before attaching it, so unzip it before you process it):. [sample_alignments.sam.zip](https://github.com/COMBINE-lab/salmon/files/4510467/sample_alignments.sam.zip). Once you've unzipped this file, you can run salmon as:. ```~bash; ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; ```. where the `transcripts.fa` is the sample transcriptome distributed with salmon that you can get by unzipping this file : [transcripts.fasta.zip](https://github.com/COMBINE-lab/salmon/files/4510488/transcripts.fasta.zip). When I run this with the latest salmon, I get the following output:. ```; # salmon (alignment-based) v1.2.0; # [ program ] => salmon; # [ command ] => quant; # [ libType ] => { IU }; # [ alignments ] => { sample_alignments.sam }; # [ targets ] => { ../sample_data/transcripts.fasta }; # [ output ] => { sample_aln_quant }; Logs will be written to sample_aln_quant/logs; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2020-04-21 10:11:42.553] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 10:11:42.553] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-21 10:11:42.553] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""sample_alignments.sam"", fasta = ""../sample_data/transcripts.fasta"" . . .done; [2020-04-21 10:11:43.180] [jointLog] [info] replaced 0 non-ACGT nucleotides with random nucleotides. processed 0 reads in current round; killing thread 3 . . . done. Freeing memory used by read queue . . . 00; Joined parsing thread . . . ""sample_alignments.sam""; Cl",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094:1120,Testability,Log,Logs,1120,"a from the repository. Note, this SAM file is zipped (GitHub made me zip it before attaching it, so unzip it before you process it):. [sample_alignments.sam.zip](https://github.com/COMBINE-lab/salmon/files/4510467/sample_alignments.sam.zip). Once you've unzipped this file, you can run salmon as:. ```~bash; ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; ```. where the `transcripts.fa` is the sample transcriptome distributed with salmon that you can get by unzipping this file : [transcripts.fasta.zip](https://github.com/COMBINE-lab/salmon/files/4510488/transcripts.fasta.zip). When I run this with the latest salmon, I get the following output:. ```; # salmon (alignment-based) v1.2.0; # [ program ] => salmon; # [ command ] => quant; # [ libType ] => { IU }; # [ alignments ] => { sample_alignments.sam }; # [ targets ] => { ../sample_data/transcripts.fasta }; # [ output ] => { sample_aln_quant }; Logs will be written to sample_aln_quant/logs; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2020-04-21 10:11:42.553] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 10:11:42.553] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-21 10:11:42.553] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""sample_alignments.sam"", fasta = ""../sample_data/transcripts.fasta"" . . .done; [2020-04-21 10:11:43.180] [jointLog] [info] replaced 0 non-ACGT nucleotides with random nucleotides. processed 0 reads in current round; killing thread 3 . . . done. Freeing memory used by read queue . . . 00; Joined parsing thread . . . ""sample_alignments.sam""; Closed all files . . .; Emptied frag queue. . . [2020-04-21 10:11:43.477] [jointLog] [info]. Completed first pass through the alignment file.; Total # of mapped reads : 10000; #",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094:1161,Testability,log,logs,1161,"a from the repository. Note, this SAM file is zipped (GitHub made me zip it before attaching it, so unzip it before you process it):. [sample_alignments.sam.zip](https://github.com/COMBINE-lab/salmon/files/4510467/sample_alignments.sam.zip). Once you've unzipped this file, you can run salmon as:. ```~bash; ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; ```. where the `transcripts.fa` is the sample transcriptome distributed with salmon that you can get by unzipping this file : [transcripts.fasta.zip](https://github.com/COMBINE-lab/salmon/files/4510488/transcripts.fasta.zip). When I run this with the latest salmon, I get the following output:. ```; # salmon (alignment-based) v1.2.0; # [ program ] => salmon; # [ command ] => quant; # [ libType ] => { IU }; # [ alignments ] => { sample_alignments.sam }; # [ targets ] => { ../sample_data/transcripts.fasta }; # [ output ] => { sample_aln_quant }; Logs will be written to sample_aln_quant/logs; Library format { type:paired end, relative orientation:inward, strandedness:unstranded }; [2020-04-21 10:11:42.553] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 10:11:42.553] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-21 10:11:42.553] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""sample_alignments.sam"", fasta = ""../sample_data/transcripts.fasta"" . . .done; [2020-04-21 10:11:43.180] [jointLog] [info] replaced 0 non-ACGT nucleotides with random nucleotides. processed 0 reads in current round; killing thread 3 . . . done. Freeing memory used by read queue . . . 00; Joined parsing thread . . . ""sample_alignments.sam""; Closed all files . . .; Emptied frag queue. . . [2020-04-21 10:11:43.477] [jointLog] [info]. Completed first pass through the alignment file.; Total # of mapped reads : 10000; #",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617206094
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834:95,Availability,down,downloaded,95,"@rob-p . I did notice that the header was missing so I am looking into getting the original. I downloaded/unzipped the files you sent and seem to still have the same issue, though. ; ```; $ conda activate salmon; $ cd ~/opt/anaconda2/envs/salmon; $ ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { IU }; # [ targets ] => { transcripts.fa }; # [ alignments ] => { sample_alignments.sam }; # [ output ] => { quant_directory }; Logs will be written to quant_directory/logs; [2020-04-21 11:46:41.365] [jointLog] [critical] Note: Alignment-free mapping (i.e. mapping without subsequent selective-alignment) has not yet been throughly tested under the pufferfish-based index and using the pufferfish-based mapping strategies. Thus, disabling of selective-alignment is not currently allowed. We may, potentially explore re-enabling this option in future versions of salmon. ```. To set up Salmon, I entered the following per the Getting Started Guide:; `$ conda config --add channels conda-forge`; `$ conda config --add channels bioconda`; `$ conda create -n salmon salmon`. Then, set the wd to `~opt/anaconda2/envs/salmon`. To run, I dropped the `transcripts.fa` and `seq.bam`/`seq.sam` file into the ~opt/anaconda2/envs/salmon and ran it. I noticed that if I moved the files to an entirely separate directory or deleted them all together and ran `./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory`, the same error came up. Is it possible that there is an issue with Salmon reading the files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834:1642,Availability,error,error,1642,"@rob-p . I did notice that the header was missing so I am looking into getting the original. I downloaded/unzipped the files you sent and seem to still have the same issue, though. ; ```; $ conda activate salmon; $ cd ~/opt/anaconda2/envs/salmon; $ ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { IU }; # [ targets ] => { transcripts.fa }; # [ alignments ] => { sample_alignments.sam }; # [ output ] => { quant_directory }; Logs will be written to quant_directory/logs; [2020-04-21 11:46:41.365] [jointLog] [critical] Note: Alignment-free mapping (i.e. mapping without subsequent selective-alignment) has not yet been throughly tested under the pufferfish-based index and using the pufferfish-based mapping strategies. Thus, disabling of selective-alignment is not currently allowed. We may, potentially explore re-enabling this option in future versions of salmon. ```. To set up Salmon, I entered the following per the Getting Started Guide:; `$ conda config --add channels conda-forge`; `$ conda config --add channels bioconda`; `$ conda create -n salmon salmon`. Then, set the wd to `~opt/anaconda2/envs/salmon`. To run, I dropped the `transcripts.fa` and `seq.bam`/`seq.sam` file into the ~opt/anaconda2/envs/salmon and ran it. I noticed that if I moved the files to an entirely separate directory or deleted them all together and ran `./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory`, the same error came up. Is it possible that there is an issue with Salmon reading the files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834:1157,Modifiability,config,config,1157,"@rob-p . I did notice that the header was missing so I am looking into getting the original. I downloaded/unzipped the files you sent and seem to still have the same issue, though. ; ```; $ conda activate salmon; $ cd ~/opt/anaconda2/envs/salmon; $ ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { IU }; # [ targets ] => { transcripts.fa }; # [ alignments ] => { sample_alignments.sam }; # [ output ] => { quant_directory }; Logs will be written to quant_directory/logs; [2020-04-21 11:46:41.365] [jointLog] [critical] Note: Alignment-free mapping (i.e. mapping without subsequent selective-alignment) has not yet been throughly tested under the pufferfish-based index and using the pufferfish-based mapping strategies. Thus, disabling of selective-alignment is not currently allowed. We may, potentially explore re-enabling this option in future versions of salmon. ```. To set up Salmon, I entered the following per the Getting Started Guide:; `$ conda config --add channels conda-forge`; `$ conda config --add channels bioconda`; `$ conda create -n salmon salmon`. Then, set the wd to `~opt/anaconda2/envs/salmon`. To run, I dropped the `transcripts.fa` and `seq.bam`/`seq.sam` file into the ~opt/anaconda2/envs/salmon and ran it. I noticed that if I moved the files to an entirely separate directory or deleted them all together and ran `./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory`, the same error came up. Is it possible that there is an issue with Salmon reading the files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834:1202,Modifiability,config,config,1202,"@rob-p . I did notice that the header was missing so I am looking into getting the original. I downloaded/unzipped the files you sent and seem to still have the same issue, though. ; ```; $ conda activate salmon; $ cd ~/opt/anaconda2/envs/salmon; $ ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { IU }; # [ targets ] => { transcripts.fa }; # [ alignments ] => { sample_alignments.sam }; # [ output ] => { quant_directory }; Logs will be written to quant_directory/logs; [2020-04-21 11:46:41.365] [jointLog] [critical] Note: Alignment-free mapping (i.e. mapping without subsequent selective-alignment) has not yet been throughly tested under the pufferfish-based index and using the pufferfish-based mapping strategies. Thus, disabling of selective-alignment is not currently allowed. We may, potentially explore re-enabling this option in future versions of salmon. ```. To set up Salmon, I entered the following per the Getting Started Guide:; `$ conda config --add channels conda-forge`; `$ conda config --add channels bioconda`; `$ conda create -n salmon salmon`. Then, set the wd to `~opt/anaconda2/envs/salmon`. To run, I dropped the `transcripts.fa` and `seq.bam`/`seq.sam` file into the ~opt/anaconda2/envs/salmon and ran it. I noticed that if I moved the files to an entirely separate directory or deleted them all together and ran `./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory`, the same error came up. Is it possible that there is an issue with Salmon reading the files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834:627,Testability,Log,Logs,627,"@rob-p . I did notice that the header was missing so I am looking into getting the original. I downloaded/unzipped the files you sent and seem to still have the same issue, though. ; ```; $ conda activate salmon; $ cd ~/opt/anaconda2/envs/salmon; $ ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { IU }; # [ targets ] => { transcripts.fa }; # [ alignments ] => { sample_alignments.sam }; # [ output ] => { quant_directory }; Logs will be written to quant_directory/logs; [2020-04-21 11:46:41.365] [jointLog] [critical] Note: Alignment-free mapping (i.e. mapping without subsequent selective-alignment) has not yet been throughly tested under the pufferfish-based index and using the pufferfish-based mapping strategies. Thus, disabling of selective-alignment is not currently allowed. We may, potentially explore re-enabling this option in future versions of salmon. ```. To set up Salmon, I entered the following per the Getting Started Guide:; `$ conda config --add channels conda-forge`; `$ conda config --add channels bioconda`; `$ conda create -n salmon salmon`. Then, set the wd to `~opt/anaconda2/envs/salmon`. To run, I dropped the `transcripts.fa` and `seq.bam`/`seq.sam` file into the ~opt/anaconda2/envs/salmon and ran it. I noticed that if I moved the files to an entirely separate directory or deleted them all together and ran `./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory`, the same error came up. Is it possible that there is an issue with Salmon reading the files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834:667,Testability,log,logs,667,"@rob-p . I did notice that the header was missing so I am looking into getting the original. I downloaded/unzipped the files you sent and seem to still have the same issue, though. ; ```; $ conda activate salmon; $ cd ~/opt/anaconda2/envs/salmon; $ ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { IU }; # [ targets ] => { transcripts.fa }; # [ alignments ] => { sample_alignments.sam }; # [ output ] => { quant_directory }; Logs will be written to quant_directory/logs; [2020-04-21 11:46:41.365] [jointLog] [critical] Note: Alignment-free mapping (i.e. mapping without subsequent selective-alignment) has not yet been throughly tested under the pufferfish-based index and using the pufferfish-based mapping strategies. Thus, disabling of selective-alignment is not currently allowed. We may, potentially explore re-enabling this option in future versions of salmon. ```. To set up Salmon, I entered the following per the Getting Started Guide:; `$ conda config --add channels conda-forge`; `$ conda config --add channels bioconda`; `$ conda create -n salmon salmon`. Then, set the wd to `~opt/anaconda2/envs/salmon`. To run, I dropped the `transcripts.fa` and `seq.bam`/`seq.sam` file into the ~opt/anaconda2/envs/salmon and ran it. I noticed that if I moved the files to an entirely separate directory or deleted them all together and ran `./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory`, the same error came up. Is it possible that there is an issue with Salmon reading the files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834:831,Testability,test,tested,831,"@rob-p . I did notice that the header was missing so I am looking into getting the original. I downloaded/unzipped the files you sent and seem to still have the same issue, though. ; ```; $ conda activate salmon; $ cd ~/opt/anaconda2/envs/salmon; $ ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { IU }; # [ targets ] => { transcripts.fa }; # [ alignments ] => { sample_alignments.sam }; # [ output ] => { quant_directory }; Logs will be written to quant_directory/logs; [2020-04-21 11:46:41.365] [jointLog] [critical] Note: Alignment-free mapping (i.e. mapping without subsequent selective-alignment) has not yet been throughly tested under the pufferfish-based index and using the pufferfish-based mapping strategies. Thus, disabling of selective-alignment is not currently allowed. We may, potentially explore re-enabling this option in future versions of salmon. ```. To set up Salmon, I entered the following per the Getting Started Guide:; `$ conda config --add channels conda-forge`; `$ conda config --add channels bioconda`; `$ conda create -n salmon salmon`. Then, set the wd to `~opt/anaconda2/envs/salmon`. To run, I dropped the `transcripts.fa` and `seq.bam`/`seq.sam` file into the ~opt/anaconda2/envs/salmon and ran it. I noticed that if I moved the files to an entirely separate directory or deleted them all together and ran `./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory`, the same error came up. Is it possible that there is an issue with Salmon reading the files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834:1140,Usability,Guid,Guide,1140,"@rob-p . I did notice that the header was missing so I am looking into getting the original. I downloaded/unzipped the files you sent and seem to still have the same issue, though. ; ```; $ conda activate salmon; $ cd ~/opt/anaconda2/envs/salmon; $ ./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory; Version Info: This is the most recent version of salmon.; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { IU }; # [ targets ] => { transcripts.fa }; # [ alignments ] => { sample_alignments.sam }; # [ output ] => { quant_directory }; Logs will be written to quant_directory/logs; [2020-04-21 11:46:41.365] [jointLog] [critical] Note: Alignment-free mapping (i.e. mapping without subsequent selective-alignment) has not yet been throughly tested under the pufferfish-based index and using the pufferfish-based mapping strategies. Thus, disabling of selective-alignment is not currently allowed. We may, potentially explore re-enabling this option in future versions of salmon. ```. To set up Salmon, I entered the following per the Getting Started Guide:; `$ conda config --add channels conda-forge`; `$ conda config --add channels bioconda`; `$ conda create -n salmon salmon`. Then, set the wd to `~opt/anaconda2/envs/salmon`. To run, I dropped the `transcripts.fa` and `seq.bam`/`seq.sam` file into the ~opt/anaconda2/envs/salmon and ran it. I noticed that if I moved the files to an entirely separate directory or deleted them all together and ran `./bin/salmon quant -l IU -t transcripts.fa -a sample_alignments.sam -o quant_directory`, the same error came up. Is it possible that there is an issue with Salmon reading the files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617263834
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617454680:1147,Availability,fault,fault,1147,"@rob-p . That's odd! Well, what you sent above works perfect the sample files you provided earlier. I am still not able to process my own - perhaps because of the header issue?. ```; $ ./bin/salmon --no-version-check quant -l OSR -t sequence.fasta -a myseq.sam -o quant; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { OSR }; # [ targets ] => { sequence.fasta }; # [ alignments ] => { myseq.sam }; # [ output ] => { quant }. Logs will be written to auts2_quant/logs; [2020-04-21 18:36:46.762] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 18:36:46.762] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:outward, strandedness:(antisense, sense) }; [2020-04-21 18:36:46.764] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""myseq.sam"", fasta = ""sequence.fasta"" . . .done; Reference seq chr7 unknown; processed 0 reads in current roundSegmentation fault: 11; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617454680
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617454680:482,Testability,Log,Logs,482,"@rob-p . That's odd! Well, what you sent above works perfect the sample files you provided earlier. I am still not able to process my own - perhaps because of the header issue?. ```; $ ./bin/salmon --no-version-check quant -l OSR -t sequence.fasta -a myseq.sam -o quant; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { OSR }; # [ targets ] => { sequence.fasta }; # [ alignments ] => { myseq.sam }; # [ output ] => { quant }. Logs will be written to auts2_quant/logs; [2020-04-21 18:36:46.762] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 18:36:46.762] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:outward, strandedness:(antisense, sense) }; [2020-04-21 18:36:46.764] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""myseq.sam"", fasta = ""sequence.fasta"" . . .done; Reference seq chr7 unknown; processed 0 reads in current roundSegmentation fault: 11; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617454680
https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617454680:518,Testability,log,logs,518,"@rob-p . That's odd! Well, what you sent above works perfect the sample files you provided earlier. I am still not able to process my own - perhaps because of the header issue?. ```; $ ./bin/salmon --no-version-check quant -l OSR -t sequence.fasta -a myseq.sam -o quant; # salmon (alignment-based) v1.2.0; # [ program ] => salmon ; # [ command ] => quant ; # [ libType ] => { OSR }; # [ targets ] => { sequence.fasta }; # [ alignments ] => { myseq.sam }; # [ output ] => { quant }. Logs will be written to auts2_quant/logs; [2020-04-21 18:36:46.762] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-21 18:36:46.762] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; Library format { type:paired end, relative orientation:outward, strandedness:(antisense, sense) }; [2020-04-21 18:36:46.764] [jointLog] [info] numQuantThreads = 4; parseThreads = 4; Checking that provided alignment files have consistent headers . . . done; Populating targets from aln = ""myseq.sam"", fasta = ""sequence.fasta"" . . .done; Reference seq chr7 unknown; processed 0 reads in current roundSegmentation fault: 11; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/511#issuecomment-617454680
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618021381:15,Availability,error,error,15,"Does it always error out on the _same_ samples? Is there anything different about the how the commands are run (e.g. are they running on different machines etc.)?. The error suggests that the process is not able to properly read the index. When salmon cannot read the index, it propagates an exception, which is what you are seeing here. The question is why this would happen for some samples and not others, so I'd look to find differences between the invocations, or the machines where samples are running / not running properly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618021381
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618021381:168,Availability,error,error,168,"Does it always error out on the _same_ samples? Is there anything different about the how the commands are run (e.g. are they running on different machines etc.)?. The error suggests that the process is not able to properly read the index. When salmon cannot read the index, it propagates an exception, which is what you are seeing here. The question is why this would happen for some samples and not others, so I'd look to find differences between the invocations, or the machines where samples are running / not running properly.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618021381
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626:34,Availability,error,error,34,"Strangely enough - with the above error message of mine. when I go to the logs directory and look up salmon_quant.log, it has correct info (last line below); ```; [2020-04-22 19:45:18.487] [jointLog] [info] Finished Bootstrapping; ```. And the output directory has a `quant.sf` file and it has all the records I want -- however, salmon is exiting with the above error message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626:362,Availability,error,error,362,"Strangely enough - with the above error message of mine. when I go to the logs directory and look up salmon_quant.log, it has correct info (last line below); ```; [2020-04-22 19:45:18.487] [jointLog] [info] Finished Bootstrapping; ```. And the output directory has a `quant.sf` file and it has all the records I want -- however, salmon is exiting with the above error message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626:40,Integrability,message,message,40,"Strangely enough - with the above error message of mine. when I go to the logs directory and look up salmon_quant.log, it has correct info (last line below); ```; [2020-04-22 19:45:18.487] [jointLog] [info] Finished Bootstrapping; ```. And the output directory has a `quant.sf` file and it has all the records I want -- however, salmon is exiting with the above error message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626:368,Integrability,message,message,368,"Strangely enough - with the above error message of mine. when I go to the logs directory and look up salmon_quant.log, it has correct info (last line below); ```; [2020-04-22 19:45:18.487] [jointLog] [info] Finished Bootstrapping; ```. And the output directory has a `quant.sf` file and it has all the records I want -- however, salmon is exiting with the above error message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626:74,Testability,log,logs,74,"Strangely enough - with the above error message of mine. when I go to the logs directory and look up salmon_quant.log, it has correct info (last line below); ```; [2020-04-22 19:45:18.487] [jointLog] [info] Finished Bootstrapping; ```. And the output directory has a `quant.sf` file and it has all the records I want -- however, salmon is exiting with the above error message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626:114,Testability,log,log,114,"Strangely enough - with the above error message of mine. when I go to the logs directory and look up salmon_quant.log, it has correct info (last line below); ```; [2020-04-22 19:45:18.487] [jointLog] [info] Finished Bootstrapping; ```. And the output directory has a `quant.sf` file and it has all the records I want -- however, salmon is exiting with the above error message",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618027626
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618029055:125,Testability,log,log,125,Ok; that is _super_ strange since (obviously) it cannot both complete successfully and throw an exception. It looks like the log points to a sample that completed successfully at `19:45:18.487` before the sample at the top of the post started `19:51:56.392`. Is this the quant directory for the same sample?,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618029055
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618044865:326,Availability,error,error,326,"@rob-p This is not running twice on same sample. I can see that this run generates a exit code of 1 for that run - however all files are there as needed. Other samples have a exit code 0. I looked up sample runs before and after - they seem to have correct exit codes and ran fine. Even this runs fine, but what triggers that error message - I am not sure. ```; failed to read 8 bytes; salmon quant invoked improperly; ```. I also reran my whole pipeline (qc_trimming etc and finally salmon) - this time with 5 samples only (and included the above sample) - the pipeline runs successfully. Not sure where to investigate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618044865
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618044865:446,Deployability,pipeline,pipeline,446,"@rob-p This is not running twice on same sample. I can see that this run generates a exit code of 1 for that run - however all files are there as needed. Other samples have a exit code 0. I looked up sample runs before and after - they seem to have correct exit codes and ran fine. Even this runs fine, but what triggers that error message - I am not sure. ```; failed to read 8 bytes; salmon quant invoked improperly; ```. I also reran my whole pipeline (qc_trimming etc and finally salmon) - this time with 5 samples only (and included the above sample) - the pipeline runs successfully. Not sure where to investigate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618044865
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618044865:562,Deployability,pipeline,pipeline,562,"@rob-p This is not running twice on same sample. I can see that this run generates a exit code of 1 for that run - however all files are there as needed. Other samples have a exit code 0. I looked up sample runs before and after - they seem to have correct exit codes and ran fine. Even this runs fine, but what triggers that error message - I am not sure. ```; failed to read 8 bytes; salmon quant invoked improperly; ```. I also reran my whole pipeline (qc_trimming etc and finally salmon) - this time with 5 samples only (and included the above sample) - the pipeline runs successfully. Not sure where to investigate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618044865
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618044865:332,Integrability,message,message,332,"@rob-p This is not running twice on same sample. I can see that this run generates a exit code of 1 for that run - however all files are there as needed. Other samples have a exit code 0. I looked up sample runs before and after - they seem to have correct exit codes and ran fine. Even this runs fine, but what triggers that error message - I am not sure. ```; failed to read 8 bytes; salmon quant invoked improperly; ```. I also reran my whole pipeline (qc_trimming etc and finally salmon) - this time with 5 samples only (and included the above sample) - the pipeline runs successfully. Not sure where to investigate",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618044865
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618093803:345,Performance,load,loading,345,"I'm also at a loss for exactly what could bre going on here. Specifically, this bit confused me:. > It looks like the log points to a sample that completed successfully at 19:45:18.487 before the sample at the top of the post started 19:51:56.392. So, unless the clock is messed up, it seems the successful completion (which, obviously required loading the complete index for alignment) happens *before* the exception. Further, the output you printed around the exception happens at the start of program execution, so I don't understand the timeline of events here for a single run / execution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618093803
https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618093803:118,Testability,log,log,118,"I'm also at a loss for exactly what could bre going on here. Specifically, this bit confused me:. > It looks like the log points to a sample that completed successfully at 19:45:18.487 before the sample at the top of the post started 19:51:56.392. So, unless the clock is messed up, it seems the successful completion (which, obviously required loading the complete index for alignment) happens *before* the exception. Further, the output you printed around the exception happens at the start of program execution, so I don't understand the timeline of events here for a single run / execution.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/512#issuecomment-618093803
https://github.com/COMBINE-lab/salmon/issues/513#issuecomment-619584519:600,Performance,load,loading,600,"Hi @Lordlitong,. The name of a FASTA record is whatever appears up to the first white space character in its header line. This is why you get these long names in the index as a sequence name. If you pass the `—gencode` flag when building the salmon index, it will treat `|` as an additional separator, and your names will just be e.g. `ENST00000456328.2`. That is the easiest way to avoid this issue going forward. If you don’t want to rebuild the index and re-process the data (if you’ve already processed a ton of samples), then you would have to write some code to strip the sequence names before loading them.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/513#issuecomment-619584519
https://github.com/COMBINE-lab/salmon/issues/513#issuecomment-619584519:383,Safety,avoid,avoid,383,"Hi @Lordlitong,. The name of a FASTA record is whatever appears up to the first white space character in its header line. This is why you get these long names in the index as a sequence name. If you pass the `—gencode` flag when building the salmon index, it will treat `|` as an additional separator, and your names will just be e.g. `ENST00000456328.2`. That is the easiest way to avoid this issue going forward. If you don’t want to rebuild the index and re-process the data (if you’ve already processed a ton of samples), then you would have to write some code to strip the sequence names before loading them.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/513#issuecomment-619584519
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-622566744:1360,Usability,feedback,feedback,1360,"Hi Jason,. Thanks for the kind words, and for the detailed issue! This is sort of a tough one, since salmon tries to work out the relative abundance of these different transcripts in the way that maximizes the likelihood of the observed data (or, more specifically, maximized the ELBO in the variational Bayesian framework). Of course, you seem to know that already :). One thought that comes to mind, though, is the following. The default settings for salmon favor sparsity of the solution pretty strongly — it is important to explain the data with as few distinct transcripts as possible. While this often seems a nice thing to do, it can tend to lead to the type of behavior that you are seeing. The way to modify this is to alter the `--vbPrior` parameter to salmon. Basically, this number encodes the prior observation weight that should be attributed to each isoform _before_ accounting for the data. The default value for this parameter is 0.01. Small values (<< 1) are sparsity inducing, while larger values are not (and values close to 1 and above actually penalize sparsity). You could try quantifying with a few (larger) values of this parameter to see if any of them give allocations among these isoforms that seem to make more sense to you (or that agree more strongly with any alternative assays). Actually, I'd be very interested in hearing any feedback you have about this if you find a setting that is more in line with what you expect!. Best,; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-622566744
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:1011,Availability,down,downstream,1011,"Thanks for the feedback!. I tried playing with the vbPrior setting and observed that, as you noted, higher increases of the vbPrior tended to flatten out the read apportionment, such that as the vbPrior increased the two transcripts became increasingly similar in their final expression (presumably they would eventually hit 50/50). It's good to know how that settings affects my data, but this is not quite what I was hoping for... . Ideally, the short transcript would get nearly *all* of the reads, rather than splitting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large de",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:2158,Deployability,pipeline,pipeline,2158,"tting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. So, for now my workaround is to just modify the transcripts so they are non-overlapping in the transcriptome fasta or to manually count reads after looking at the alignments, but I'd love to hear any more thoughts you have on this problem. Thanks,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:1814,Safety,detect,detect,1814,"tting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. So, for now my workaround is to just modify the transcripts so they are non-overlapping in the transcriptome fasta or to manually count reads after looking at the alignments, but I'd love to hear any more thoughts you have on this problem. Thanks,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:15,Usability,feedback,feedback,15,"Thanks for the feedback!. I tried playing with the vbPrior setting and observed that, as you noted, higher increases of the vbPrior tended to flatten out the read apportionment, such that as the vbPrior increased the two transcripts became increasingly similar in their final expression (presumably they would eventually hit 50/50). It's good to know how that settings affects my data, but this is not quite what I was hoping for... . Ideally, the short transcript would get nearly *all* of the reads, rather than splitting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large de",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:1049,Usability,intuit,intuition,1049," read apportionment, such that as the vbPrior increased the two transcripts became increasingly similar in their final expression (presumably they would eventually hit 50/50). It's good to know how that settings affects my data, but this is not quite what I was hoping for... . Ideally, the short transcript would get nearly *all* of the reads, rather than splitting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651:1631,Usability,intuit,intuition,1631,"tting the reads 50/50 or, with the default settings, giving nearly all the reads to the longer transcript. I realized that, as a human, the reason the short transcript is obviously the dominant one is how the reads pileup in the alignment. There are hundreds of reads mapping to both transcripts, but NO reads map to the 5' of the long transcript. As I understand the selective alignment, the alignment scores are passed to the quantification step, but the *position* of the reads is not used downstream. In order to pass my human intuition along here, the software would need to pay attention to the coverage bias of the reads mapping to the transcripts and assign a penalty when two otherwise identical transcripts have a different coverage variance across the transcript. This sounds like what the --posBias flag should incorporate into the effective lengths, but it doesn't have much effect on these transcripts for me (FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine). . Also, my intuition for these transcripts is not really a coverage ""bias"" as much as the read depth absolutely plummeting at the 5' end of the long transcript. It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. So, for now my workaround is to just modify the transcripts so they are non-overlapping in the transcriptome fasta or to manually count reads after looking at the alignments, but I'd love to hear any more thoughts you have on this problem. Thanks,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623043651
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:510,Availability,down,down,510,"Hi Jason,. Thanks for the super-detailed feedback! A couple of thoughts / questions:. > FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:684,Availability,down,downstream,684,"Hi Jason,. Thanks for the super-detailed feedback! A couple of thoughts / questions:. > FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:2001,Deployability,pipeline,pipeline,2001,"er, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and nei",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:4212,Energy Efficiency,efficient,efficiently,4212,"ion](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would be a big win for a quantification tool to include some sort of built-in lightweight model for things like this. The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently? I'm _very_ interested in pursuing something like this if it can be made to work efficiently. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:4305,Energy Efficiency,efficient,efficiently,4305,"ion](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would be a big win for a quantification tool to include some sort of built-in lightweight model for things like this. The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently? I'm _very_ interested in pursuing something like this if it can be made to work efficiently. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1657,Safety,detect,detect,1657," not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitiga",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:2324,Safety,Detect,Detection,2324," term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:41,Usability,feedback,feedback,41,"Hi Jason,. Thanks for the super-detailed feedback! A couple of thoughts / questions:. > FYI, I am getting a segfault when I run only --posBias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1113,Usability,intuit,intuition,1113,"ias in the current salmon version, but if I run all the models together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A cou",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1182,Usability,intuit,intuition,1182,"together like --gcBias --seqBias --posBias, it completes fine. ~~Do you have a small example (ref / read pair) that reproduces this? It would be great to figure out why and fix it. We could split that into a new issue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1464,Usability,learn,learn,1464,"sue if you'd rather.~~. P.S. Nevermind; thanks to you mentioning this, I was able to track it down and fix it in develop!. > As I understand the selective alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-sy",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:1575,Usability,learn,learns,1575,"alignment, the alignment scores are passed to the quantification step, but the position of the reads is not used downstream. . Well, yes and no. We make extensive use of the position when estimate the implied fragment length (distance between paired end reads) and then model the conditional probability of this fragment based on the global fragment length distribution. This is just as much as is done by e.g. RSEM. However, you are right that there is no notion of using the coverage profile in estimation (more on this below)!. > Also, my intuition for these transcripts is not really a coverage ""bias"" . My intuition agrees with yours here completely. First, this isn't really a coverage bias as we use the normal definition of the term. Second, the positional bias modeling in salmon is not on a per-transcript level (since that would be an astronomical number of different parameters to learn, and any procedure would almost certainly overfit). Instead, it groups transcripts into length bins, and learns a distinct coverage bias model per-bin. > It would be neat if Salmon could detect these kinds of dramatic dropoffs and add a warning or something... even if not incorporating the information into the quants... it could even be a good QC step to identify large deletions/insertions over a gene body. As far as I know, there are NO rnaseq quant programs that would handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:2911,Usability,intuit,intuitions,2911,"d handle this, because even something like a STAR -> RSEM pipeline just projects read counts to the transcriptome and doesn't incorporate the coverage information. These are **great** points! A couple of thoughts. First, you are right that salmon, RSEM, etc. don't use coverage information in the way you describe here. One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would b",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:3922,Usability,clear,clear,3922,"ion](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would be a big win for a quantification tool to include some sort of built-in lightweight model for things like this. The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently? I'm _very_ interested in pursuing something like this if it can be made to work efficiently. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035:4109,Usability,intuit,intuition,4109,"ion](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). While both of these approaches get at some of the core intuitions you raise in your response, they are both rather ""heavyweight"", and neither, of course, is built into salmon. So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. While I don't know of any widely-used and actively maintained quantification tools that do this, the idea for this was proposed in the [iReckon paper](https://www.ncbi.nlm.nih.gov/pubmed/23204306) and a coverage-based heuristic was introduced. However, the coverage was not directly incorporated into the likelihood. Rather, a variant of the normal likelihood function was used and then coverage was used to select between different potential solutions that were otherwise of similar likelihood. Given issues like the one you see here, and the ones that we observed in the JCC paper and that Cong and Carl observed in the SAD paper, it seems clear that it would be a big win for a quantification tool to include some sort of built-in lightweight model for things like this. The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently? I'm _very_ interested in pursuing something like this if it can be made to work efficiently. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623047035
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2001,Availability,down,downstream,2001,"nto_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. Yes! I've been analyzing a large dataset and my real motivating problem was not really the example I posted above, but distinguishing between pre-processed and fully-processed non-coding RNA transcripts. I'm attaching an image showing an example ncRNA; the two tracks are the same data, but the lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:4650,Availability,down,downstream,4650,"the unique-region-derived prior... . But as I think about it... I realize I don't *really* know the underlying algorithmic details of the existing implements. But it would be **amazing** if you could incorporate this type of information into Salmon. I really hope some progress can be made here! . Thanks again for helping me out and showing interest in the motivating problem!. P.S.,; As a total aside, I've been working with this large yeast RNAseq dataset and eventually reached the same conclusions as the selective alignment paper and other recent ones; that is, the most important aspect for getting good transcript-level quantifications is not aligning to the genome vs. aligning to the transcriptome, but rather having an accurate transcriptome annotation to begin with. I saw **huge** gains from updating my transcriptome annotation to include UTRs, especially given differences in coverage bias between samples... for example, if the actual transcript is 500 bp but the gene body is only 200 bp, slight coverage biases can propagate non-linearly and cause huge problems downstream. This got me thinking... if the end goal is differential expression analysis (and obviously this is not *always* the end goal), what if we just discard the notion of a pre-defined transcriptome and stick with equivalence classes, then do differential expression analysis on the equivalence classes themselves (perhaps calculated against the whole genome... this is feasible in yeast, maybe not in humans), then only after discovering significant differential expression one could work backwards to interpret the changes. Is this a crazy idea? Or not crazy at all and already routine? I know salmon can dump the counts to each equivalence class already so it's not hard for me to *do* what I just described, but I'm wondering if you have any opinions/insights into this idea. Thanks again!. ![snr40_isoforms](https://user-images.githubusercontent.com/10292386/81047610-a17c1880-8e6f-11ea-8012-a6695afd68db.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:3445,Deployability,update,updates,3445,"e of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth variance per isoform). The problem here is actual coverage biases may then masquerade as alternative transcript isoforms... 2) Use the information from the unique sequences between the transcripts... the read depth over unique regions updates the prior on the overall transcript abundance, and the otherwise non-unique reads get apportioned in accordance with the unique-region-derived prior... . But as I think about it... I realize I don't *really* know the underlying algorithmic details of the existing implements. But it would be **amazing** if you could incorporate this type of information into Salmon. I really hope some progress can be made here! . Thanks again for helping me out and showing interest in the motivating problem!. P.S.,; As a total aside, I've been working with this large yeast RNAseq dataset and eventually reached the same conclusions as the selective alignment paper and other recent ones; that is, the most important aspect for getting good transcript-level quantifications is not aligning to the genome vs. aligning to the transcriptome, but rather having an accurate transcriptome annotation to begin with. I saw **huge** gains from updating my transcriptome annotation to include UTRs, especially given differences in coverage bias between samples... for example,",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2613,Energy Efficiency,efficient,efficiently,2613,"l have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth variance per isoform). The problem here is actual coverage biases may then masquerade as alternative transcript isoforms... 2) Use the information from the unique sequences between the transcripts... the read depth over unique regions updates the prior on the overall transcript abundance, and the otherwise non-unique reads get ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:5236,Integrability,rout,routine,5236,"the unique-region-derived prior... . But as I think about it... I realize I don't *really* know the underlying algorithmic details of the existing implements. But it would be **amazing** if you could incorporate this type of information into Salmon. I really hope some progress can be made here! . Thanks again for helping me out and showing interest in the motivating problem!. P.S.,; As a total aside, I've been working with this large yeast RNAseq dataset and eventually reached the same conclusions as the selective alignment paper and other recent ones; that is, the most important aspect for getting good transcript-level quantifications is not aligning to the genome vs. aligning to the transcriptome, but rather having an accurate transcriptome annotation to begin with. I saw **huge** gains from updating my transcriptome annotation to include UTRs, especially given differences in coverage bias between samples... for example, if the actual transcript is 500 bp but the gene body is only 200 bp, slight coverage biases can propagate non-linearly and cause huge problems downstream. This got me thinking... if the end goal is differential expression analysis (and obviously this is not *always* the end goal), what if we just discard the notion of a pre-defined transcriptome and stick with equivalence classes, then do differential expression analysis on the equivalence classes themselves (perhaps calculated against the whole genome... this is feasible in yeast, maybe not in humans), then only after discovering significant differential expression one could work backwards to interpret the changes. Is this a crazy idea? Or not crazy at all and already routine? I know salmon can dump the counts to each equivalence class already so it's not hard for me to *do* what I just described, but I'm wondering if you have any opinions/insights into this idea. Thanks again!. ![snr40_isoforms](https://user-images.githubusercontent.com/10292386/81047610-a17c1880-8e6f-11ea-8012-a6695afd68db.png)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:228,Safety,Detect,Detection,228,"Hi Rob,. Thanks for being so interested in this! I'm blown away by your support. And thank you for preemptively fixing the bug before I could even post an example!. > One piece of software you might look into is [Salmon Anomaly Detection](https://github.com/Kingsford-Group/sad) (paper [here](https://www.cell.com/cell-systems/pdf/S2405-4712(19)30381-3.pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). Excellent papers! Definitely going to give those tools a try on my data. > So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. Yes! I've been analyzing a large dataset and my real motivating problem was not really the example I posted above, but distinguishing between pre-processed and fully-processed non-coding RNA transcripts. I'm attaching an image showing an example ncRNA; the two tracks are the same data, but the lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2406,Safety,detect,detect,2406,"he lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth variance per isoform). The problem here is actual coverage biases may then masquerade as alternative transcript isoforms... 2) Use the information from the unique seq",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:1409,Testability,log,log,1409,".pdf)). This is sort of akin to what you are suggesting, and post-processes salmon output by looking for anomalous coverage profiles. It can both flag ""suspicious"" transcripts, and can also sometimes move reads around to mitigate anomalous coverage. Another tool / metric you might consider is the junction coverage compatibility (paper [here](https://www.life-science-alliance.org/content/2/1/e201800175)). Excellent papers! Definitely going to give those tools a try on my data. > So, I **completely agree** that a lightweight version of something like SAD would be great to have built _into_ salmon. Specifically, it makes a lot of sense to have some component of the likelihood account for the coverage profile of transcripts. Yes! I've been analyzing a large dataset and my real motivating problem was not really the example I posted above, but distinguishing between pre-processed and fully-processed non-coding RNA transcripts. I'm attaching an image showing an example ncRNA; the two tracks are the same data, but the lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity mea",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2256,Testability,benchmark,benchmark,2256," between pre-processed and fully-processed non-coding RNA transcripts. I'm attaching an image showing an example ncRNA; the two tracks are the same data, but the lower one shows abundance on a log scale. In this particular sample, it's easy to estimate that ~5-10% of the transcripts are pre-processed (the transcripts still have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth varianc",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851:2510,Usability,intuit,intuition,2510,"l have neighboring genomic DNA). I wanted to see how this ratio changes between samples (for example, in a snoRNA processing-defective mutant strain), but quickly realized this is not easily done in salmon or any other quant tools because the processed transcript is entirely a subset of the sequence of the pre-processed transcript. The only way to accurately quantify it is to use the coverage information, which as you agreed is not really taken into account downstream. If Salmon could incorporate the coverage information to solve this class of problem, that would indeed be a **huge win**. I think the ncRNA example would be both a great biologically-interesting motivating problem, as well as a good technical benchmark for implementing any new methods. It could even be used as a secondary RNA velocity measure in scRNA seq data, provided the method used can detect these (non-polyadenylated) transcripts. > The big questions are (1) how do you fold this type of intuition formally into the probabilistic model and (2) is it possible to incorporate this information efficiently?. This is definitely your domain of expertise (and I know it's a rhetorical question but I'd love to throw some ideas out here)... I can think of a few mostly heuristical approaches.... . 1) when apportioning reads to transcripts, after the mapping phase, incorporate a notion of ""evenness"" into the EM step... reads that decrease the variance in coverage are favored over reads that increase the variance (so, define read depth per 10 bp window or something and calculate the variance across all windows for the transcript, then try to assign reads such that they minimize read depth variance per isoform). The problem here is actual coverage biases may then masquerade as alternative transcript isoforms... 2) Use the information from the unique sequences between the transcripts... the read depth over unique regions updates the prior on the overall transcript abundance, and the otherwise non-unique reads get ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-623963851
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642307073:358,Availability,down,downstream,358,"Hi Mohsen,. Thanks for looking into this!. I'm attaching a small dataset to reproduce the issue. I extracted the reads aligned to the region in the screenshot above (the snR40 transcript) for that sample. I've included two fasta files, one with the standard snR40 transcript, and one that has a second transcript called snR40_genomic which includes upstream/downstream sequences. Based on the image above, you would expect >90% of the reads to apportion to the smaller transcript, but instead I see the reads going ~50/50 when I run salmon on the fasta file with both indexes. . I've included an ""index"" folder with a script to build each index and a salmon.sh script to reproduce my results. My salmon binary version is 1.2.1. Please let me know if this works for you, or if you need anything else from me. Happy to share the full fastq files if that's helpful too. Thanks,; Jason. [snr40_example_github.gz](https://github.com/COMBINE-lab/salmon/files/4761633/snr40_example_github.gz)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642307073
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:322,Energy Efficiency,adapt,adapter,322,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:394,Energy Efficiency,adapt,adapter,394,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:618,Energy Efficiency,adapt,adapter,618,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:832,Energy Efficiency,adapt,adapter,832,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:322,Integrability,adapter,adapter,322,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:394,Integrability,adapter,adapter,394,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:618,Integrability,adapter,adapter,618,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:832,Integrability,adapter,adapter,832,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:322,Modifiability,adapt,adapter,322,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:394,Modifiability,adapt,adapter,394,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:618,Modifiability,adapt,adapter,618,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815:832,Modifiability,adapt,adapter,832,"Hi Mohsen and Rob,. So sorry if you've already been troubleshooting the example data I gave you. I realized that that is not a good example of the problem. In this example, there are snR40 and snR40_genomic transcripts, representing processed and pre-processed isoforms. However, it just so happens that there is residual adapter on some of the reads I provided and the first nucleotide of the adapter sequence actually matches the first nucleotide of the longer, genomic version of this transcript, therefore, the genomic variant gets a slightly better alignment score, as it should. After hard trimming any residual adapter the results for this transcript were a lot better (although still not quite the ratio I would expect). I have quite a few examples like this and I'm fairly sure they are not *all* explained by alignment of adapter sequences. However, I just wanted to let you know in case you were already troubleshooting my example data. I'm aggregating a handful more general examples of the same problem, but ones without a trivial solution like the one I provided. The files are too large to attach on github directly, though, do you have a preferred way to share the files? Maybe a google drive?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-642954815
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:517,Availability,Failure,Failure,517,"Hi @mohsenzakeri ,. Sorry for the slow reply! I’ve done some sleuthing and have (seemingly) figured out what’s going on. I compared tons of overlapping transcript scenarios and played with the salmon options and concluded the following:. 1. **Success scenarios:** Generally, with default options, Salmon does an excellent job at assigning reads to overlapping transcripts the same way that a human would. Whether or not the transcripts overlap slightly or one is entirely contained within another is irrelevant. 2. **Failure scenarios:** In some scenarios with overlapping transcripts, read assignment can be very strange and unintuitive, especially when one of the transcript isoforms is much more abundant than the other (the abundant transcript tends to “steal” reads from the less abundant one). 3. Both the success and failure scenarios are **due to the length bias model** used when estimating the abundance of transcripts. **Totally disabling this with the —noLengthCorrection flag** (NOT the —no**Effective**LengthCorrection flag) **actually *creates* the transcript within a transcript failure scenario that I mistakenly thought was the original issue.** That is, when length bias modeling is turned off, then longer transcripts will always get assigned *all* of the reads that multimap to shorter transcripts.; -Therefore... if you *did* want to tackle the transcript within a transcript scenario to build a coverage bias model, you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:824,Availability,failure,failure,824,"Hi @mohsenzakeri ,. Sorry for the slow reply! I’ve done some sleuthing and have (seemingly) figured out what’s going on. I compared tons of overlapping transcript scenarios and played with the salmon options and concluded the following:. 1. **Success scenarios:** Generally, with default options, Salmon does an excellent job at assigning reads to overlapping transcripts the same way that a human would. Whether or not the transcripts overlap slightly or one is entirely contained within another is irrelevant. 2. **Failure scenarios:** In some scenarios with overlapping transcripts, read assignment can be very strange and unintuitive, especially when one of the transcript isoforms is much more abundant than the other (the abundant transcript tends to “steal” reads from the less abundant one). 3. Both the success and failure scenarios are **due to the length bias model** used when estimating the abundance of transcripts. **Totally disabling this with the —noLengthCorrection flag** (NOT the —no**Effective**LengthCorrection flag) **actually *creates* the transcript within a transcript failure scenario that I mistakenly thought was the original issue.** That is, when length bias modeling is turned off, then longer transcripts will always get assigned *all* of the reads that multimap to shorter transcripts.; -Therefore... if you *did* want to tackle the transcript within a transcript scenario to build a coverage bias model, you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:1095,Availability,failure,failure,1095," I’ve done some sleuthing and have (seemingly) figured out what’s going on. I compared tons of overlapping transcript scenarios and played with the salmon options and concluded the following:. 1. **Success scenarios:** Generally, with default options, Salmon does an excellent job at assigning reads to overlapping transcripts the same way that a human would. Whether or not the transcripts overlap slightly or one is entirely contained within another is irrelevant. 2. **Failure scenarios:** In some scenarios with overlapping transcripts, read assignment can be very strange and unintuitive, especially when one of the transcript isoforms is much more abundant than the other (the abundant transcript tends to “steal” reads from the less abundant one). 3. Both the success and failure scenarios are **due to the length bias model** used when estimating the abundance of transcripts. **Totally disabling this with the —noLengthCorrection flag** (NOT the —no**Effective**LengthCorrection flag) **actually *creates* the transcript within a transcript failure scenario that I mistakenly thought was the original issue.** That is, when length bias modeling is turned off, then longer transcripts will always get assigned *all* of the reads that multimap to shorter transcripts.; -Therefore... if you *did* want to tackle the transcript within a transcript scenario to build a coverage bias model, you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive/folders/1LcJNa4PHNoYqGsnkRx0YxvNXnNJCVyq9?u",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:5138,Availability,Failure,Failure,5138,"5"" alt=""KCC4_table"" src=""https://user-images.githubusercontent.com/10292386/86509801-6d55a300-bd9f-11ea-9c69-8e269fc3ab1c.png"">. In this example, there are two regions in KCC4 with obviously different coverage. Ideally we would be able to have a default KCC4 transcript and a truncated isoform in the salmon index, and it would assign the reads appropriately, even though all of the reads that map to the truncated form would also multimap to the long form. Again, you can see in the table that salmon assigns reads parsimoniously to both transcripts with the default options, but with the length bias modeling turned off ALL of the reads are assigned to the long transcript. I also added a third transcript to the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you r",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6637,Availability,down,down,6637," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6417,Integrability,depend,depends,6417," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:1952,Security,access,access,1952,"otally disabling this with the —noLengthCorrection flag** (NOT the —no**Effective**LengthCorrection flag) **actually *creates* the transcript within a transcript failure scenario that I mistakenly thought was the original issue.** That is, when length bias modeling is turned off, then longer transcripts will always get assigned *all* of the reads that multimap to shorter transcripts.; -Therefore... if you *did* want to tackle the transcript within a transcript scenario to build a coverage bias model, you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive/folders/1LcJNa4PHNoYqGsnkRx0YxvNXnNJCVyq9?usp=sharing. 1. **Success scenarios with default options:** . In the below IGV snapshots, I show the read alignments for one sample. The top GTF annotation is the default gene annotation, and the GTF at the bottom shows the new transcript isoforms I made and quantified on (this index is called ""extras""). For each example I ran salmon on the transcripts from the default or extra index, with standard options (only --validateMappings), with or without the --noLengthCorrection flag. **I'm showing only the number of reads** assigned to each transcript, not the TPM. I also tried this on more samples and transcript scenarios and saw the same trends. **Nested transcript isoforms:** ; ![AGP1_example](https://user-images.githubusercontent.com/10292386/86509506-45654000-bd9d-11ea-839f-6637620c3247.png). <img width=""430"" alt=""AGP1_table"" src=""https://user-images.githubusercontent.com/1029",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:2462,Security,validat,validateMappings,2462," you probably want to disable the length bias modeling or at least consider how it would interact with coverage modeling. With that said, I'm sharing an example that illustrates each of the above points and a link to a toy dataset that you can use to recreate the examples or explore this further. If you'd like to dig deeper into this, free free to e-mail me at jason@calicolabs.com, I have tons more notes and data that I'm willing to share. Dataset is in google drive (you'll have to click the link and request access to view it) https://drive.google.com/drive/folders/1LcJNa4PHNoYqGsnkRx0YxvNXnNJCVyq9?usp=sharing. 1. **Success scenarios with default options:** . In the below IGV snapshots, I show the read alignments for one sample. The top GTF annotation is the default gene annotation, and the GTF at the bottom shows the new transcript isoforms I made and quantified on (this index is called ""extras""). For each example I ran salmon on the transcripts from the default or extra index, with standard options (only --validateMappings), with or without the --noLengthCorrection flag. **I'm showing only the number of reads** assigned to each transcript, not the TPM. I also tried this on more samples and transcript scenarios and saw the same trends. **Nested transcript isoforms:** ; ![AGP1_example](https://user-images.githubusercontent.com/10292386/86509506-45654000-bd9d-11ea-839f-6637620c3247.png). <img width=""430"" alt=""AGP1_table"" src=""https://user-images.githubusercontent.com/10292386/86509511-48f8c700-bd9d-11ea-8203-c37eeaf4820d.png"">. Looking first at the ""extras"" index + default options, almost all the reads are assigned to AGP1_long1, which indeed seems to be the best fit for the actual gene body + UTRs (the default AGP1 transcript is only protein coding ORF, no UTRs). Even though all of the reads from AGP1_long1 would also multimap to AGP1_long2, etc., AGP1_long1 is assigned all the reads because it has the highest reads per kb. Presumably the few reads assigned to AGP1_",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:5122,Usability,intuit,intuition,5122,"thubusercontent.com/10292386/86509800-6b8bdf80-bd9f-11ea-9a9f-bbfea99e2703.png); <img width=""485"" alt=""KCC4_table"" src=""https://user-images.githubusercontent.com/10292386/86509801-6d55a300-bd9f-11ea-9c69-8e269fc3ab1c.png"">. In this example, there are two regions in KCC4 with obviously different coverage. Ideally we would be able to have a default KCC4 transcript and a truncated isoform in the salmon index, and it would assign the reads appropriately, even though all of the reads that map to the truncated form would also multimap to the long form. Again, you can see in the table that salmon assigns reads parsimoniously to both transcripts with the default options, but with the length bias modeling turned off ALL of the reads are assigned to the long transcript. I also added a third transcript to the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% o",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6540,Usability,intuit,intuition,6540," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6647,Usability,simpl,simple,6647," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847:6811,Usability,clear,cleared,6811," the right end of the transcript which is inconsistent with the coverage profile and, as hoped, salmon did not assign any reads to that variant. So, in these two scenarios the default options produce nice results in line with our human intuition. 2. **Failure scenario with default options:** ; ![PDI1_example](https://user-images.githubusercontent.com/10292386/86509895-3df36600-bda0-11ea-8f0b-df0de4fefa31.png); <img width=""383"" alt=""PDI1_table"" src=""https://user-images.githubusercontent.com/10292386/86509897-40ee5680-bda0-11ea-9566-9f2bdab464f0.png"">. In this example there are four genes (oriented in the same direction) with wildly different expression levels. I added a ""PDI1_SuperTranscript"" which stretches from the 5' end of PDI1 to the 3' end of POF1 (so, all reads from all 4 genes would multimap to the super transcript). This is a contrived example to illustrate the technical details, but you could imagine similar biological scenarios, especially regarding splicing isoforms. With the default options, you get the counterintuitive result that all of the reads from just MGR1 and POF1 (the two lowest abundance transcripts) are assigned to the super transcript. EMC1 loses ~50% of its reads to the super transcript, and PDI1 only loses ~10%. I'm not showing it, but if you remove the default PDI1 transcript from the index (so it's just the super transcript + the 3 genes MGR1/EMC1/POF1), all three of them lose all of their reads to the super transcript... meaning that whether or not EMC1 gets assigned any reads depends entirely on the presence of a non-overlapping gene, PDI1, in the salmon index. This is definitely at odds with our intuition from looking at the coverage plots, but makes sense when you break all the transcripts down to a simple reads per kb equation. As before, if you turn off length modeling then all of the reads get assigned to the super transcript. I hope this was insightful and cleared up the issue a bit. Feel free to e-mail or reply here. Best,; Jason",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-653747847
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:556,Availability,recover,recovering,556,"Hi @jasonvrogers,. First of all thank you so much for sharing this thorough analysis with us, it was very clear and helpful for understanding the details of the problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:1113,Availability,failure,failure,1113,"problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in treating those reads as un-mapped. Furthermore, this problem was more evident when we tried that approach on other larger samples, it seemed that could effect the",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:414,Deployability,update,updates,414,"Hi @jasonvrogers,. First of all thank you so much for sharing this thorough analysis with us, it was very clear and helpful for understanding the details of the problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:2553,Deployability,update,update,2553,"ection, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in treating those reads as un-mapped. Furthermore, this problem was more evident when we tried that approach on other larger samples, it seemed that could effect the expression of a lot transcripts very significantly. Specially, on the real samples where the coverage are often not uniform and detecting a zero coverage region on a transcript is more common due to un-annotated transcripts in the samples and etc. Currently, we are actively looking for more thorough solutions for this problem to deal with the coverage profile of transcripts. I'll try to update you more as we make more progress about this. Thank you again for the detailed explanation, hope to get back at you soon. Best,; Mohsen",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:556,Safety,recover,recovering,556,"Hi @jasonvrogers,. First of all thank you so much for sharing this thorough analysis with us, it was very clear and helpful for understanding the details of the problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:2291,Safety,detect,detecting,2291,"ection, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in treating those reads as un-mapped. Furthermore, this problem was more evident when we tried that approach on other larger samples, it seemed that could effect the expression of a lot transcripts very significantly. Specially, on the real samples where the coverage are often not uniform and detecting a zero coverage region on a transcript is more common due to un-annotated transcripts in the samples and etc. Currently, we are actively looking for more thorough solutions for this problem to deal with the coverage profile of transcripts. I'll try to update you more as we make more progress about this. Thank you again for the detailed explanation, hope to get back at you soon. Best,; Mohsen",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703:106,Usability,clear,clear,106,"Hi @jasonvrogers,. First of all thank you so much for sharing this thorough analysis with us, it was very clear and helpful for understanding the details of the problem you are referring to. Secondly, I apologize for the long time it took me to get back at you. I would like you to know that we have been and are still working on possible solutions for addressing this problem, and here I would like to share some updates with you. . About the success cases, it was nice to know that the current model of Salmon with length correction works pretty well in recovering the right estimates for those ""easier"" cases where one transcript is fully contained in another one. Turning off the length correction, tells the Salmon model not to consider the effective length of each transcript for computing the conditional probabilities of originating a fragment from a transcript. So, for the RNA-seq data there is no reason to turn off this term of the model, and we highly recommend not to use that flag for the bulk RNA-seq abundance estimation with Salmon. Looking more carefully at the 2nd case you have posted as the failure case, it is interesting to see that there is a very nice visual evidence on the super transcript that the long transcript might not be expressed at all. I am referring to the zero coverage regions on the Super Transcript between the regions corresponding to the smaller transcripts, e. g., between POF1 and EMC1. So, we tried a solution that inspects the coverage profile of all transcripts and calculates the probability of observing a zero coverage region on each transcript. If this probability is too low, this would be counted as an evidence for a transcript not being expressed at all. This approach seems to be working fine on this example that you have shared here. however, one problem was that there were considerable number of reads in the sample that were uniquely mapping only to the Super Transcript and turning of the expression of that transcript would result in t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-666512703
https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-670495465:1233,Modifiability,extend,extending,1233," arise in some instances (like with many splicing isoforms or alternative transcriptional start/stop sites). . I'm curious to know how things progress on the issue. It seems like using the coverage as evidence of the transcript ""not being expressed at all"" may be too binary for this problem, certainly in real data, including my own, there are serious coverage dropoffs with long genes or low sequencing depth, and it doesn't mean the transcript is not expressed. Second, regarding unique mappers to the ""super transcript,"" this highlights the problem with using gene bodies that do not incorporate UTRs as in my examples above. The quantification can only be as good as the ground truth transcriptome you're working with, and so every gene is going to be a bit wrong if UTRs are not included in the transcriptome sequence. For my real data analysis, I've dealt with this by 1) adding UTRs to all genes whenever possible, and if no UTR data is given, then extending each gene body by ~100 bp in either direction (if two genes end up overlapping a bit, this ends up not being problematic at all because of how salmon apportions reads in accordance with the length bias model). Second, I set the mapping flags to --softclip and --minScoreFraction 0.3; this helps a LOT since if one read mate pair maps perfectly within a gene and the other is in an unannotated UTR region, it will still assign the read correctly to the gene. . Lastly, for solving the problem at hand with the ""super transcript"" scenario and using coverage... perhaps instead of looking for regions of zero coverage, you could keep track of the overall variance in read depth over a gene and assign reads in a way that minimizes variance; in my example, since the super transcripts has regions of very high read depth, zero read depth, and intermediate, the total variance in read depth across the gene would be quite high, while the variance over the sub-transcripts would be much lower. Similarly, in a scenario with a gene that has",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/514#issuecomment-670495465
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621834194:144,Testability,log,log,144,"That is totally unexpected. There is nothing obvious from the output above. Could you share the output directory itself? It will also contain a log file with time stamps so we can see where the time went. Regarding the mapping rate; that is a bit on the low end, but not catastrophically so (May be worth trimming to see if that changes things at all). The number of decoy fragments is also quite high, meaning a good number of reads arising outside of annotated transcripts.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621834194
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756:756,Performance,Load,Loading,756,"Thanks for the quick answer!; Here is the log file:. [2020-04-22 12:53:21.437] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-22 12:53:21.437] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-04-22 12:53:21.437] [jointLog] [info] parsing read library format; [2020-04-22 12:53:21.437] [jointLog] [info] There is 1 library.; [2020-04-22 12:53:21.501] [jointLog] [info] Loading pufferfish index; [2020-04-22 12:53:21.503] [jointLog] [info] Loading dense pufferfish index.; [2020-04-22 12:54:13.540] [jointLog] [info] done; [2020-04-22 12:54:13.713] [jointLog] [info] Index contained 228,799 targets; [2020-04-22 12:54:29.422] [jointLog] [info] Number of decoys : 84; [2020-04-22 12:54:29.466] [jointLog] [info] First decoy index : 228,673 ; [2020-04-22 13:00:24.946] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-04-23 00:06:31.287] [jointLog] [info] Thread saw mini-batch with a maximum of 1.06% zero probability fragments; [2020-04-23 00:06:41.198] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:50.741] [jointLog] [info] Thread saw mini-batch with a maximum of 1.02% zero probability fragments; [2020-04-23 00:06:56.260] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:56.781] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.636] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.759] [jointLog] [info] Thread saw mini-batch ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756:826,Performance,Load,Loading,826,"Thanks for the quick answer!; Here is the log file:. [2020-04-22 12:53:21.437] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-22 12:53:21.437] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-04-22 12:53:21.437] [jointLog] [info] parsing read library format; [2020-04-22 12:53:21.437] [jointLog] [info] There is 1 library.; [2020-04-22 12:53:21.501] [jointLog] [info] Loading pufferfish index; [2020-04-22 12:53:21.503] [jointLog] [info] Loading dense pufferfish index.; [2020-04-22 12:54:13.540] [jointLog] [info] done; [2020-04-22 12:54:13.713] [jointLog] [info] Index contained 228,799 targets; [2020-04-22 12:54:29.422] [jointLog] [info] Number of decoys : 84; [2020-04-22 12:54:29.466] [jointLog] [info] First decoy index : 228,673 ; [2020-04-22 13:00:24.946] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-04-23 00:06:31.287] [jointLog] [info] Thread saw mini-batch with a maximum of 1.06% zero probability fragments; [2020-04-23 00:06:41.198] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:50.741] [jointLog] [info] Thread saw mini-batch with a maximum of 1.02% zero probability fragments; [2020-04-23 00:06:56.260] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:56.781] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.636] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.759] [jointLog] [info] Thread saw mini-batch ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756:3228,Performance,optimiz,optimizer,3228,"258 rich equivalence classes for further processing; [2020-04-23 00:10:07.647] [jointLog] [info] Counted 11,112,281 total reads in the equivalence classes ; [2020-04-23 00:10:07.660] [jointLog] [info] Number of mappings discarded because of alignment score : 26,561,460; [2020-04-23 00:10:07.660] [jointLog] [info] Number of fragments entirely discarded because of alignment score : 2,134,945; [2020-04-23 00:10:07.660] [jointLog] [info] Number of fragments discarded because they are best-mapped to decoys : 1,953,200; [2020-04-23 00:10:07.660] [jointLog] [info] Number of fragments discarded because they have only dovetail (discordant) mappings to valid targets : 33,267; [2020-04-23 00:10:07.660] [jointLog] [info] Mapping rate = 55.9852%. [2020-04-23 00:10:07.670] [jointLog] [info] finished quantifyLibrary(); [2020-04-23 00:10:07.648] [fileLog] [info] ; At end of round 0; Observed 19848610 total fragments (19848610 in most recent round). [2020-04-23 00:10:11.274] [jointLog] [info] Starting optimizer; [2020-04-23 00:10:57.432] [jointLog] [info] Marked 0 weighted equivalence classes as degenerate; [2020-04-23 00:10:57.563] [jointLog] [info] iteration = 0 | max rel diff. = 3769.01; [2020-04-23 00:10:57.705] [jointLog] [info] iteration 11, adjusting effective lengths to account for biases; [2020-04-23 00:13:04.343] [jointLog] [info] Computed expected counts (for bias correction); [2020-04-23 00:13:04.582] [jointLog] [info] processed bias for 0.0% of the transcripts; [2020-04-23 00:13:06.971] [jointLog] [info] processed bias for 10.0% of the transcripts; [2020-04-23 00:13:09.342] [jointLog] [info] processed bias for 20.0% of the transcripts; [2020-04-23 00:13:11.459] [jointLog] [info] processed bias for 30.0% of the transcripts; [2020-04-23 00:13:13.690] [jointLog] [info] processed bias for 40.0% of the transcripts; [2020-04-23 00:13:15.869] [jointLog] [info] processed bias for 50.0% of the transcripts; [2020-04-23 00:13:18.159] [jointLog] [info] processed bias for 60.0% of t",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756:6349,Performance,optimiz,optimizer,6349,"ts; [2020-04-23 00:13:24.793] [jointLog] [info] processed bias for 90.0% of the transcripts; [2020-04-23 00:13:26.996] [jointLog] [info] processed bias for 100.0% of the transcripts; [2020-04-23 00:13:28.195] [jointLog] [info] iteration = 100 | max rel diff. = 18.2995; [2020-04-23 00:13:29.515] [jointLog] [info] iteration = 200 | max rel diff. = 9.0865; [2020-04-23 00:13:30.800] [jointLog] [info] iteration = 300 | max rel diff. = 4.01818; [2020-04-23 00:13:32.083] [jointLog] [info] iteration = 400 | max rel diff. = 4.55608; [2020-04-23 00:13:33.364] [jointLog] [info] iteration = 500 | max rel diff. = 0.520451; [2020-04-23 00:13:34.643] [jointLog] [info] iteration = 600 | max rel diff. = 2.54118; [2020-04-23 00:13:35.923] [jointLog] [info] iteration = 700 | max rel diff. = 3.03814; [2020-04-23 00:13:37.202] [jointLog] [info] iteration = 800 | max rel diff. = 1.03192; [2020-04-23 00:13:38.483] [jointLog] [info] iteration = 900 | max rel diff. = 0.0895496; [2020-04-23 00:13:39.763] [jointLog] [info] iteration = 1,000 | max rel diff. = 0.114555; [2020-04-23 00:13:41.046] [jointLog] [info] iteration = 1,100 | max rel diff. = 0.0141693; [2020-04-23 00:13:42.326] [jointLog] [info] iteration = 1,200 | max rel diff. = 0.0828263; [2020-04-23 00:13:43.604] [jointLog] [info] iteration = 1,300 | max rel diff. = 0.0393046; [2020-04-23 00:13:44.883] [jointLog] [info] iteration = 1,400 | max rel diff. = 0.0200319; [2020-04-23 00:13:46.163] [jointLog] [info] iteration = 1,500 | max rel diff. = 0.0299069; [2020-04-23 00:13:47.443] [jointLog] [info] iteration = 1,600 | max rel diff. = 0.0153416; [2020-04-23 00:13:48.723] [jointLog] [info] iteration = 1,700 | max rel diff. = 0.0219194; [2020-04-23 00:13:50.003] [jointLog] [info] iteration = 1,800 | max rel diff. = 0.0886396; [2020-04-23 00:13:50.706] [jointLog] [info] iteration = 1,856 | max rel diff. = 0.00570505; [2020-04-23 00:13:50.714] [jointLog] [info] Finished optimizer; [2020-04-23 00:13:50.714] [jointLog] [info] writing output",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756:1185,Safety,detect,detected,1185,"ty prior below threshold. Incompatible fragments will be ignored.; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-04-22 12:53:21.437] [jointLog] [info] parsing read library format; [2020-04-22 12:53:21.437] [jointLog] [info] There is 1 library.; [2020-04-22 12:53:21.501] [jointLog] [info] Loading pufferfish index; [2020-04-22 12:53:21.503] [jointLog] [info] Loading dense pufferfish index.; [2020-04-22 12:54:13.540] [jointLog] [info] done; [2020-04-22 12:54:13.713] [jointLog] [info] Index contained 228,799 targets; [2020-04-22 12:54:29.422] [jointLog] [info] Number of decoys : 84; [2020-04-22 12:54:29.466] [jointLog] [info] First decoy index : 228,673 ; [2020-04-22 13:00:24.946] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-04-23 00:06:31.287] [jointLog] [info] Thread saw mini-batch with a maximum of 1.06% zero probability fragments; [2020-04-23 00:06:41.198] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:50.741] [jointLog] [info] Thread saw mini-batch with a maximum of 1.02% zero probability fragments; [2020-04-23 00:06:56.260] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:56.781] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.636] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.759] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:47.416] [jointLog] [info] Thread saw mini-batch with a maximum of 1.24% zero probability fragments; [2020-04-23 00:10:07.526] [",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756:320,Security,validat,validateMappings,320,"Thanks for the quick answer!; Here is the log file:. [2020-04-22 12:53:21.437] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-22 12:53:21.437] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-04-22 12:53:21.437] [jointLog] [info] parsing read library format; [2020-04-22 12:53:21.437] [jointLog] [info] There is 1 library.; [2020-04-22 12:53:21.501] [jointLog] [info] Loading pufferfish index; [2020-04-22 12:53:21.503] [jointLog] [info] Loading dense pufferfish index.; [2020-04-22 12:54:13.540] [jointLog] [info] done; [2020-04-22 12:54:13.713] [jointLog] [info] Index contained 228,799 targets; [2020-04-22 12:54:29.422] [jointLog] [info] Number of decoys : 84; [2020-04-22 12:54:29.466] [jointLog] [info] First decoy index : 228,673 ; [2020-04-22 13:00:24.946] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-04-23 00:06:31.287] [jointLog] [info] Thread saw mini-batch with a maximum of 1.06% zero probability fragments; [2020-04-23 00:06:41.198] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:50.741] [jointLog] [info] Thread saw mini-batch with a maximum of 1.02% zero probability fragments; [2020-04-23 00:06:56.260] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:56.781] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.636] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.759] [jointLog] [info] Thread saw mini-batch ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756:482,Security,validat,validateMappings,482,"Thanks for the quick answer!; Here is the log file:. [2020-04-22 12:53:21.437] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-22 12:53:21.437] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-04-22 12:53:21.437] [jointLog] [info] parsing read library format; [2020-04-22 12:53:21.437] [jointLog] [info] There is 1 library.; [2020-04-22 12:53:21.501] [jointLog] [info] Loading pufferfish index; [2020-04-22 12:53:21.503] [jointLog] [info] Loading dense pufferfish index.; [2020-04-22 12:54:13.540] [jointLog] [info] done; [2020-04-22 12:54:13.713] [jointLog] [info] Index contained 228,799 targets; [2020-04-22 12:54:29.422] [jointLog] [info] Number of decoys : 84; [2020-04-22 12:54:29.466] [jointLog] [info] First decoy index : 228,673 ; [2020-04-22 13:00:24.946] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-04-23 00:06:31.287] [jointLog] [info] Thread saw mini-batch with a maximum of 1.06% zero probability fragments; [2020-04-23 00:06:41.198] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:50.741] [jointLog] [info] Thread saw mini-batch with a maximum of 1.02% zero probability fragments; [2020-04-23 00:06:56.260] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:56.781] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.636] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.759] [jointLog] [info] Thread saw mini-batch ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756:42,Testability,log,log,42,"Thanks for the quick answer!; Here is the log file:. [2020-04-22 12:53:21.437] [jointLog] [info] setting maxHashResizeThreads to 8; [2020-04-22 12:53:21.437] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-04-22 12:53:21.437] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-04-22 12:53:21.437] [jointLog] [info] parsing read library format; [2020-04-22 12:53:21.437] [jointLog] [info] There is 1 library.; [2020-04-22 12:53:21.501] [jointLog] [info] Loading pufferfish index; [2020-04-22 12:53:21.503] [jointLog] [info] Loading dense pufferfish index.; [2020-04-22 12:54:13.540] [jointLog] [info] done; [2020-04-22 12:54:13.713] [jointLog] [info] Index contained 228,799 targets; [2020-04-22 12:54:29.422] [jointLog] [info] Number of decoys : 84; [2020-04-22 12:54:29.466] [jointLog] [info] First decoy index : 228,673 ; [2020-04-22 13:00:24.946] [jointLog] [info] Automatically detected most likely library type as ISR; [2020-04-23 00:06:31.287] [jointLog] [info] Thread saw mini-batch with a maximum of 1.06% zero probability fragments; [2020-04-23 00:06:41.198] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:50.741] [jointLog] [info] Thread saw mini-batch with a maximum of 1.02% zero probability fragments; [2020-04-23 00:06:56.260] [jointLog] [info] Thread saw mini-batch with a maximum of 1.08% zero probability fragments; [2020-04-23 00:06:56.781] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.636] [jointLog] [info] Thread saw mini-batch with a maximum of 1.04% zero probability fragments; [2020-04-23 00:07:03.759] [jointLog] [info] Thread saw mini-batch ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621872756
https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621987962:187,Safety,detect,detected,187,"Thanks for the quick response yourself :). Ok, so it looks like there is just an ungodly amount of time between this line:; ```; [2020-04-22 13:00:24.946] [jointLog] [info] Automatically detected most likely library type as ISR; ```. and this one. ```; [2020-04-23 00:06:31.287] [jointLog] [info] Thread saw mini-batch with a maximum of 1.06% zero probability fragments; ```. This is an absolutely outrageous amount of time spent on trying to align ~19M reads. Was there any other console output that is missing here? . However, even other parts of the run are outrageously slow. For example. ```; [2020-04-23 00:07:47.416] [jointLog] [info] Thread saw mini-batch with a maximum of 1.24% zero probability fragments; [2020-04-23 00:10:07.526] [jointLog] [info] Computed 414,258 rich equivalence classes for further processing; ```. literally minutes passed between these two lines but almost nothing is done in this time. This makes me think there is, perhaps, a resource problem. Is this a shared machine? Was someone else, perhaps, using all of the cores?. I assume the data can't be easily shared, right? I could see how long it takes to run on a different machine. Otherwise, do you have a different machine on which you could run?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/516#issuecomment-621987962
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623050187:86,Availability,Down,Downloads,86,"Can you try doing:. ```; export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/Users/maysonlin/Downloads/salmon-1.2.1-h2072146_0 2/lib; ```. before running salmon? The problem is that the executable is looking for `libtbb` and `libtbb_proxy`, but they are not in the library path.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623050187
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623073440:96,Availability,Down,Downloads,96,"> Can you try doing:; > ; > ```; > export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/Users/maysonlin/Downloads/salmon-1.2.1-h2072146_0 2/lib; > ```; > ; > before running salmon? The problem is that the executable is looking for `libtbb` and `libtbb_proxy`, but they are not in the library path. Hi, Rob, thank you for replying, do you mean type those code in ""Terminal""? ; I tried it, and I got this message:. -bash: export: `2/lib': not a valid identifier",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623073440
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623073440:395,Integrability,message,message,395,"> Can you try doing:; > ; > ```; > export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/Users/maysonlin/Downloads/salmon-1.2.1-h2072146_0 2/lib; > ```; > ; > before running salmon? The problem is that the executable is looking for `libtbb` and `libtbb_proxy`, but they are not in the library path. Hi, Rob, thank you for replying, do you mean type those code in ""Terminal""? ; I tried it, and I got this message:. -bash: export: `2/lib': not a valid identifier",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623073440
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623118024:187,Availability,Down,Downloads,187,"It looks like it's having trouble parsing this folder name ""salmon-1.2.1-h2072146_0 2/"", what if you put it in quotes?. ```; export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:""/Users/maysonlin/Downloads/salmon-1.2.1-h2072146_0 2/lib""; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623118024
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623133092:197,Availability,Down,Downloads,197,"> It looks like it's having trouble parsing this folder name ""salmon-1.2.1-h2072146_0 2/"", what if you put it in quotes?; > ; > ```; > export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:""/Users/maysonlin/Downloads/salmon-1.2.1-h2072146_0 2/lib""; > ```. Hi, Rob, I tried that, nothing happened",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623133092
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623164361:262,Deployability,install,install,262,"Hrmm; can you say a bit about where you got the executable from? What's missing in that directory are the key shared library files `libtbbmalloc.dylib`, `libtbbmalloc_proxy.dylib` and `libtbb.dylib`. These are included in precompiled linux excutable, and if you install via Conda for linux or OSX, they should be taken care of. Also, if you build from source, they are installed into the install target `lib` subdirectory by the `make install` command.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623164361
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623164361:369,Deployability,install,installed,369,"Hrmm; can you say a bit about where you got the executable from? What's missing in that directory are the key shared library files `libtbbmalloc.dylib`, `libtbbmalloc_proxy.dylib` and `libtbb.dylib`. These are included in precompiled linux excutable, and if you install via Conda for linux or OSX, they should be taken care of. Also, if you build from source, they are installed into the install target `lib` subdirectory by the `make install` command.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623164361
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623164361:388,Deployability,install,install,388,"Hrmm; can you say a bit about where you got the executable from? What's missing in that directory are the key shared library files `libtbbmalloc.dylib`, `libtbbmalloc_proxy.dylib` and `libtbb.dylib`. These are included in precompiled linux excutable, and if you install via Conda for linux or OSX, they should be taken care of. Also, if you build from source, they are installed into the install target `lib` subdirectory by the `make install` command.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623164361
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623164361:435,Deployability,install,install,435,"Hrmm; can you say a bit about where you got the executable from? What's missing in that directory are the key shared library files `libtbbmalloc.dylib`, `libtbbmalloc_proxy.dylib` and `libtbb.dylib`. These are included in precompiled linux excutable, and if you install via Conda for linux or OSX, they should be taken care of. Also, if you build from source, they are installed into the install target `lib` subdirectory by the `make install` command.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623164361
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623175345:2,Availability,down,downloaded,2,"I downloaded the package from here ; https://anaconda.org/bioconda/salmon/files; then I opened the package up with ""The Unarchiver.app"", so I downloaded linux one, and open up the package with ""Unarchiver.app"", I didn't see any library files -libtbbmalloc.dylib, libtbbmalloc_proxy.dylib and libtbb.dylib. ; ![Screen Shot 2020-05-03 at 4 17 25 PM](https://user-images.githubusercontent.com/64718774/80924960-3ee52880-8d5a-11ea-8fbc-5cf4537f0129.png); ; Then I just installed Conda, but I have no idea how to install the package, sorry, I have ground zero coding knowledge",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623175345
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623175345:142,Availability,down,downloaded,142,"I downloaded the package from here ; https://anaconda.org/bioconda/salmon/files; then I opened the package up with ""The Unarchiver.app"", so I downloaded linux one, and open up the package with ""Unarchiver.app"", I didn't see any library files -libtbbmalloc.dylib, libtbbmalloc_proxy.dylib and libtbb.dylib. ; ![Screen Shot 2020-05-03 at 4 17 25 PM](https://user-images.githubusercontent.com/64718774/80924960-3ee52880-8d5a-11ea-8fbc-5cf4537f0129.png); ; Then I just installed Conda, but I have no idea how to install the package, sorry, I have ground zero coding knowledge",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623175345
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623175345:465,Deployability,install,installed,465,"I downloaded the package from here ; https://anaconda.org/bioconda/salmon/files; then I opened the package up with ""The Unarchiver.app"", so I downloaded linux one, and open up the package with ""Unarchiver.app"", I didn't see any library files -libtbbmalloc.dylib, libtbbmalloc_proxy.dylib and libtbb.dylib. ; ![Screen Shot 2020-05-03 at 4 17 25 PM](https://user-images.githubusercontent.com/64718774/80924960-3ee52880-8d5a-11ea-8fbc-5cf4537f0129.png); ; Then I just installed Conda, but I have no idea how to install the package, sorry, I have ground zero coding knowledge",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623175345
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623175345:508,Deployability,install,install,508,"I downloaded the package from here ; https://anaconda.org/bioconda/salmon/files; then I opened the package up with ""The Unarchiver.app"", so I downloaded linux one, and open up the package with ""Unarchiver.app"", I didn't see any library files -libtbbmalloc.dylib, libtbbmalloc_proxy.dylib and libtbb.dylib. ; ![Screen Shot 2020-05-03 at 4 17 25 PM](https://user-images.githubusercontent.com/64718774/80924960-3ee52880-8d5a-11ea-8fbc-5cf4537f0129.png); ; Then I just installed Conda, but I have no idea how to install the package, sorry, I have ground zero coding knowledge",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623175345
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623179953:30,Deployability,install,install,30,"Ahh, that's not how the conda install works. Take a look at the bioconda instructions here (http://combine-lab.github.io/salmon/getting_started/), and click the link therein to see how to install conda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623179953
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623179953:188,Deployability,install,install,188,"Ahh, that's not how the conda install works. Take a look at the bioconda instructions here (http://combine-lab.github.io/salmon/getting_started/), and click the link therein to see how to install conda.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623179953
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623635133:32,Deployability,install,install,32,"> Ahh, that's not how the conda install works. Take a look at the bioconda instructions here (http://combine-lab.github.io/salmon/getting_started/), and click the link therein to see how to install conda. Hi, problem solved, I can finally install and activate Salmon now, thank you very much!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623635133
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623635133:190,Deployability,install,install,190,"> Ahh, that's not how the conda install works. Take a look at the bioconda instructions here (http://combine-lab.github.io/salmon/getting_started/), and click the link therein to see how to install conda. Hi, problem solved, I can finally install and activate Salmon now, thank you very much!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623635133
https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623635133:239,Deployability,install,install,239,"> Ahh, that's not how the conda install works. Take a look at the bioconda instructions here (http://combine-lab.github.io/salmon/getting_started/), and click the link therein to see how to install conda. Hi, problem solved, I can finally install and activate Salmon now, thank you very much!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/517#issuecomment-623635133
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623679572:91,Performance,Load,Loading,91,"The reason I bring this up is this line:. ```; [2020-05-04 21:30:58.701] [jointLog] [info] Loading Quasi index; ```. suggests salmon is trying to load the wrong type of index (the `quasi` index), since the new versions only support pufferfish-based indexes (and the 16G index to which you refer is a pufferfish-based index).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623679572
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623679572:146,Performance,load,load,146,"The reason I bring this up is this line:. ```; [2020-05-04 21:30:58.701] [jointLog] [info] Loading Quasi index; ```. suggests salmon is trying to load the wrong type of index (the `quasi` index), since the new versions only support pufferfish-based indexes (and the 16G index to which you refer is a pufferfish-based index).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623679572
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623868804:58,Deployability,update,update,58,salmon 0.14.1. Maybe that is the problem... I am going to update now and try again.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623868804
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623897373:271,Availability,down,downloaded,271,"Done!, I had few problems removing old versions and salmon command was still associated with the old version. I remove any salmon piece using `whereis` command to find them. After that the path in $PATH was right;; salmon -v; salmon 1.2.1. Now, I am using the as index I downloaded from refgenie and it says this;; [2020-05-05 09:21:31.276] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information. I guess the index was made with some previous version which doesn't include this option (or they didn't use it). . Let's see how it works :) and how long it takes (the normal index spend like 3h to finish 15 quantifications of paired fastq files i7 (6threads) and 16GB ram :))). Thank you!!!!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623897373
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:1429,Performance,Load,Loading,1429,"APSa16_1P.fq.gz }; ### [ mates2 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_2P.fq.gz }; ### [ threads ] => { 7 }; ### [ output ] => { /media/usr/quantification/APSa16.fq.gz_quant }; Logs will be written to /media/usr/quantification/APSa16.fq.gz_quant/logs; [2020-05-05 09:19:06.171] [jointLog] [info] setting maxHashResizeThreads to 7; [2020-05-05 09:19:06.171] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:1710,Performance,Load,Loading,1710,"quant/logs; [2020-05-05 09:19:06.171] [jointLog] [info] setting maxHashResizeThreads to 7; [2020-05-05 09:19:06.171] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | Loading contig boundaries | Time = 11.467 s; -----------------------------------------; size = 3784352032; -----------------------------------------; | Loading sequence | Time = 9.5665 s; -----------------------------------------; size = 2674916722; -----------------------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:1788,Performance,Load,Loading,1788,"HashResizeThreads to 7; [2020-05-05 09:19:06.171] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | Loading contig boundaries | Time = 11.467 s; -----------------------------------------; size = 3784352032; -----------------------------------------; | Loading sequence | Time = 9.5665 s; -----------------------------------------; size = 2674916722; -----------------------------------------; | Loading positions | Time = 4.3912 ms; -------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:1933,Performance,Load,Loading,1933,"d. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | Loading contig boundaries | Time = 11.467 s; -----------------------------------------; size = 3784352032; -----------------------------------------; | Loading sequence | Time = 9.5665 s; -----------------------------------------; size = 2674916722; -----------------------------------------; | Loading positions | Time = 4.3912 ms; -----------------------------------------; Exception : [std::bad_alloc]; salmon quant was invoked improperly.; For usage",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:2063,Performance,Load,Loading,2063,"Fraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | Loading contig boundaries | Time = 11.467 s; -----------------------------------------; size = 3784352032; -----------------------------------------; | Loading sequence | Time = 9.5665 s; -----------------------------------------; size = 2674916722; -----------------------------------------; | Loading positions | Time = 4.3912 ms; -----------------------------------------; Exception : [std::bad_alloc]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting. Not sure why it happens... memory doesn't reach the max. ![Screenshot at 2020-05-05 09-45-37",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:2197,Performance,Load,Loading,2197,"appings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | Loading contig boundaries | Time = 11.467 s; -----------------------------------------; size = 3784352032; -----------------------------------------; | Loading sequence | Time = 9.5665 s; -----------------------------------------; size = 2674916722; -----------------------------------------; | Loading positions | Time = 4.3912 ms; -----------------------------------------; Exception : [std::bad_alloc]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting. Not sure why it happens... memory doesn't reach the max. ![Screenshot at 2020-05-05 09-45-37](https://user-images.githubusercontent.com/61701461/81045096-d14e0f00-8eb5-11ea-97ed-b4f4454ba042.png). Than you so much in advan",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:2441,Performance,Load,Loading,2441,"implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | Loading contig boundaries | Time = 11.467 s; -----------------------------------------; size = 3784352032; -----------------------------------------; | Loading sequence | Time = 9.5665 s; -----------------------------------------; size = 2674916722; -----------------------------------------; | Loading positions | Time = 4.3912 ms; -----------------------------------------; Exception : [std::bad_alloc]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting. Not sure why it happens... memory doesn't reach the max. ![Screenshot at 2020-05-05 09-45-37](https://user-images.githubusercontent.com/61701461/81045096-d14e0f00-8eb5-11ea-97ed-b4f4454ba042.png). Than you so much in advance!; Fer",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:2593,Performance,Load,Loading,2593,"implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | Loading contig boundaries | Time = 11.467 s; -----------------------------------------; size = 3784352032; -----------------------------------------; | Loading sequence | Time = 9.5665 s; -----------------------------------------; size = 2674916722; -----------------------------------------; | Loading positions | Time = 4.3912 ms; -----------------------------------------; Exception : [std::bad_alloc]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting. Not sure why it happens... memory doesn't reach the max. ![Screenshot at 2020-05-05 09-45-37](https://user-images.githubusercontent.com/61701461/81045096-d14e0f00-8eb5-11ea-97ed-b4f4454ba042.png). Than you so much in advance!; Fer",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:2736,Performance,Load,Loading,2736,"implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; -----------------------------------------; | Loading mphf table | Time = 20.002 s; -----------------------------------------; size = 3784352032; Number of ones: 36981177; Number of ones per inventory item: 512; Inventory entries filled: 72229; -----------------------------------------; | Loading contig boundaries | Time = 11.467 s; -----------------------------------------; size = 3784352032; -----------------------------------------; | Loading sequence | Time = 9.5665 s; -----------------------------------------; size = 2674916722; -----------------------------------------; | Loading positions | Time = 4.3912 ms; -----------------------------------------; Exception : [std::bad_alloc]; salmon quant was invoked improperly.; For usage information, try salmon quant --help; Exiting. Not sure why it happens... memory doesn't reach the max. ![Screenshot at 2020-05-05 09-45-37](https://user-images.githubusercontent.com/61701461/81045096-d14e0f00-8eb5-11ea-97ed-b4f4454ba042.png). Than you so much in advance!; Fer",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:336,Security,validat,validateMappings,336,"Sorry :// Another issue... . Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v1.2.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /media/usr/Hybrid_02/Unidad_Bioinf/FER_Scripts/Index/hg38/salmon_sa_index/default }; ### [ libType ] => { A }; ### [ gcBias ] => { }; ### [ validateMappings ] => { }; ### [ mates1 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_1P.fq.gz }; ### [ mates2 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_2P.fq.gz }; ### [ threads ] => { 7 }; ### [ output ] => { /media/usr/quantification/APSa16.fq.gz_quant }; Logs will be written to /media/usr/quantification/APSa16.fq.gz_quant/logs; [2020-05-05 09:19:06.171] [jointLog] [info] setting maxHashResizeThreads to 7; [2020-05-05 09:19:06.171] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; --------------------------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:993,Security,validat,validateMappings,993,"Sorry :// Another issue... . Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v1.2.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /media/usr/Hybrid_02/Unidad_Bioinf/FER_Scripts/Index/hg38/salmon_sa_index/default }; ### [ libType ] => { A }; ### [ gcBias ] => { }; ### [ validateMappings ] => { }; ### [ mates1 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_1P.fq.gz }; ### [ mates2 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_2P.fq.gz }; ### [ threads ] => { 7 }; ### [ output ] => { /media/usr/quantification/APSa16.fq.gz_quant }; Logs will be written to /media/usr/quantification/APSa16.fq.gz_quant/logs; [2020-05-05 09:19:06.171] [jointLog] [info] setting maxHashResizeThreads to 7; [2020-05-05 09:19:06.171] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; --------------------------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:1155,Security,validat,validateMappings,1155,"=> quant ; ### [ index ] => { /media/usr/Hybrid_02/Unidad_Bioinf/FER_Scripts/Index/hg38/salmon_sa_index/default }; ### [ libType ] => { A }; ### [ gcBias ] => { }; ### [ validateMappings ] => { }; ### [ mates1 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_1P.fq.gz }; ### [ mates2 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_2P.fq.gz }; ### [ threads ] => { 7 }; ### [ output ] => { /media/usr/quantification/APSa16.fq.gz_quant }; Logs will be written to /media/usr/quantification/APSa16.fq.gz_quant/logs; [2020-05-05 09:19:06.171] [jointLog] [info] setting maxHashResizeThreads to 7; [2020-05-05 09:19:06.171] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; -----------------------------------------; -----------------------------------------; | Loading reference lengths | Time = 5.6842 ms; -----------------------------------------; --------------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:651,Testability,Log,Logs,651,"Sorry :// Another issue... . Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v1.2.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /media/usr/Hybrid_02/Unidad_Bioinf/FER_Scripts/Index/hg38/salmon_sa_index/default }; ### [ libType ] => { A }; ### [ gcBias ] => { }; ### [ validateMappings ] => { }; ### [ mates1 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_1P.fq.gz }; ### [ mates2 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_2P.fq.gz }; ### [ threads ] => { 7 }; ### [ output ] => { /media/usr/quantification/APSa16.fq.gz_quant }; Logs will be written to /media/usr/quantification/APSa16.fq.gz_quant/logs; [2020-05-05 09:19:06.171] [jointLog] [info] setting maxHashResizeThreads to 7; [2020-05-05 09:19:06.171] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; --------------------------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021:720,Testability,log,logs,720,"Sorry :// Another issue... . Version Info: This is the most recent version of salmon.; ### salmon (mapping-based) v1.2.1; ### [ program ] => salmon ; ### [ command ] => quant ; ### [ index ] => { /media/usr/Hybrid_02/Unidad_Bioinf/FER_Scripts/Index/hg38/salmon_sa_index/default }; ### [ libType ] => { A }; ### [ gcBias ] => { }; ### [ validateMappings ] => { }; ### [ mates1 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_1P.fq.gz }; ### [ mates2 ] => { /media/usr/trimmed_fastq_files/PAIRED_trimmed_fastq_files/APSa16_2P.fq.gz }; ### [ threads ] => { 7 }; ### [ output ] => { /media/usr/quantification/APSa16.fq.gz_quant }; Logs will be written to /media/usr/quantification/APSa16.fq.gz_quant/logs; [2020-05-05 09:19:06.171] [jointLog] [info] setting maxHashResizeThreads to 7; [2020-05-05 09:19:06.171] [jointLog] [info] Fragment incompatibility prior below threshold. Incompatible fragments will be ignored.; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies use of minScoreFraction. Since not explicitly specified, it is being set to 0.65; [2020-05-05 09:19:06.171] [jointLog] [info] Usage of --validateMappings implies a default consensus slack of 0.2. Setting consensusSlack to 0.35.; [2020-05-05 09:19:06.171] [jointLog] [info] parsing read library format; [2020-05-05 09:19:06.171] [jointLog] [info] There is 1 library.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading pufferfish index; [2020-05-05 09:19:06.278] [jointLog] [warning] The index did not record if the `--keepDuplicates` flag was used. Please consider re-indexing with a newer version of salmon that will propagate this information.; [2020-05-05 09:19:06.278] [jointLog] [info] Loading dense pufferfish index.; -----------------------------------------; | Loading contig table | Time = 30.609 s; -----------------------------------------; size = 36981178; -----------------------------------------; | Loading contig offsets | Time = 1.3312 s; --------------------------",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-623910021
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-629842095:132,Energy Efficiency,allocate,allocate,132,"How big is the index directory? The fact that it hasn't used up all of the memory doesn't mean that the allocator should be able to allocate more. If the next chunk it needs to allocate is large (e.g. the position table), and it needs that memory in contiguous space, the allocation might fail. I'd recommend either trying this on a larger memory machine, or trying a smaller index (e.g. the selective alignment index with custom decoys rather than the whole genome) on this machine. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-629842095
https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-629842095:177,Energy Efficiency,allocate,allocate,177,"How big is the index directory? The fact that it hasn't used up all of the memory doesn't mean that the allocator should be able to allocate more. If the next chunk it needs to allocate is large (e.g. the position table), and it needs that memory in contiguous space, the allocation might fail. I'd recommend either trying this on a larger memory machine, or trying a smaller index (e.g. the selective alignment index with custom decoys rather than the whole genome) on this machine. --Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/518#issuecomment-629842095
https://github.com/COMBINE-lab/salmon/issues/520#issuecomment-625604855:38,Usability,guid,guidance,38,"Dear Rob,. Thank you so much for your guidance. I appreciate you taking the time to help me. I was able to run salmon, no problem. All the best,; Craig",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/520#issuecomment-625604855
https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627508722:130,Availability,fault,fault,130,"Hi @gambardella,. Thank you for the detailed bug report. I agree that, though the issue seems to be arising from the input, a seg fault should not happen in any case. Judging from the place in the log from which this is arising, it seems to be happening during the TwoPaCo construction of the compacted de Bruijn graph. We'll dig into this and see if we can figure out how to handle this in a better way.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627508722
https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627508722:197,Testability,log,log,197,"Hi @gambardella,. Thank you for the detailed bug report. I agree that, though the issue seems to be arising from the input, a seg fault should not happen in any case. Judging from the place in the log from which this is arising, it seems to be happening during the TwoPaCo construction of the compacted de Bruijn graph. We'll dig into this and see if we can figure out how to handle this in a better way.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627508722
https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627671880:19,Availability,down,downloaded,19,"Thanks Nicolas,. I downloaded the files from this url and (unfortunately?) was unable to reproduce the segfault. I'd just like to check that we are working from the same source files and there wasn't e.g. a corruption during your download or some such. I have the following as the MD5 hash sums for the files input into the indexing. ```; $ md5sum GRCm38.primary_assembly.genome.fa.gz; 3bc591be24b77f710b6ba5d41022fc5a GRCm38.primary_assembly.genome.fa.gz; $ md5sum gencode.vM25.transcripts.fa.gz; a821c0dde39c48b9d2c4b48d36b0180c gencode.vM25.transcripts.fa.gz; $ md5sum decoys.txt; fdfb8e3ea371649a7ec2c39fcd8bb8f4 decoys.txt; $ md5sum gentrome.fa.gz; db7022ecc40483f105aedcdc4e113304 gentrome.fa.gz; ```. could you let me know if the signatures for your files are the same?. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627671880
https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627671880:230,Availability,down,download,230,"Thanks Nicolas,. I downloaded the files from this url and (unfortunately?) was unable to reproduce the segfault. I'd just like to check that we are working from the same source files and there wasn't e.g. a corruption during your download or some such. I have the following as the MD5 hash sums for the files input into the indexing. ```; $ md5sum GRCm38.primary_assembly.genome.fa.gz; 3bc591be24b77f710b6ba5d41022fc5a GRCm38.primary_assembly.genome.fa.gz; $ md5sum gencode.vM25.transcripts.fa.gz; a821c0dde39c48b9d2c4b48d36b0180c gencode.vM25.transcripts.fa.gz; $ md5sum decoys.txt; fdfb8e3ea371649a7ec2c39fcd8bb8f4 decoys.txt; $ md5sum gentrome.fa.gz; db7022ecc40483f105aedcdc4e113304 gentrome.fa.gz; ```. could you let me know if the signatures for your files are the same?. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627671880
https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627671880:285,Security,hash,hash,285,"Thanks Nicolas,. I downloaded the files from this url and (unfortunately?) was unable to reproduce the segfault. I'd just like to check that we are working from the same source files and there wasn't e.g. a corruption during your download or some such. I have the following as the MD5 hash sums for the files input into the indexing. ```; $ md5sum GRCm38.primary_assembly.genome.fa.gz; 3bc591be24b77f710b6ba5d41022fc5a GRCm38.primary_assembly.genome.fa.gz; $ md5sum gencode.vM25.transcripts.fa.gz; a821c0dde39c48b9d2c4b48d36b0180c gencode.vM25.transcripts.fa.gz; $ md5sum decoys.txt; fdfb8e3ea371649a7ec2c39fcd8bb8f4 decoys.txt; $ md5sum gentrome.fa.gz; db7022ecc40483f105aedcdc4e113304 gentrome.fa.gz; ```. could you let me know if the signatures for your files are the same?. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627671880
https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627835939:12,Security,checksum,checksums,12,"Hello,. The checksums were correct. I now refined the problem. It does not happen when I set the number of threads to 4 with ""-p 4"", in which case the procedure goes until completion. ; With ""-p 12"" The ""invalid character"" is sometimes different, and found in different chromosomes.; It is likely that the number of threads exceeding the number of cores (8) causes the problem. NB: I used the procedure with or without ""-k 31"" to follow; https://salmon.readthedocs.io/en/latest/salmon.html#preparing-transcriptome-indices-mapping-based-mode; or; https://combine-lab.github.io/alevin-tutorial/2019/selective-alignment/ . Would you now advise to drop the ""-k 31""?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-627835939
https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-628067346:57,Availability,down,down,57,"Hi @gambardella,. Thanks for the effort in tracking this down further. So, I tried running locally with more threads than we have physically, but the bug wasn't immediately triggered, so I'm guessing there is some stochasticity there. One thing that would be really helpful (given our lack of success in reproducing locally yet) is to find out if the issue is being caused by the compacted De Bruijn graph construction library, or prior to that. One way to do that is to ask the indexer to keep around the ""fixed fasta"", and then to see if that differs from we're producing locally on our machine. If you run the following command:. ```; salmon index -t gentrome.fa.gz -d decoys.txt -p 12 -i salmon_index --gencode --keepFixedFasta; ```. then there should be a file `salmon_index/ref_k31_fixed.fa` that is generated in the index directory. This is a fixed fasta file (removing ambiguous nucleotides, trimming polyA tails, removing sequences that are too short (<= k), etc.). In my runs, I get the same md5sum for this file, regardless of the number of threads:. ```; $ md5sum salmon_index/ref_k31_fixed.fa; ceaf3a96bfc2a65e70b6f3094d02491c salmon_index/ref_k31_fixed.fa; ```. could you see if you get something different (on a passing run, on a failing run, or on both)? If so, I might be able to replicate the bug by starting from your fixed fasta file, rather than from the original fasta files. Regarding your other question :. > Would you now advise to drop the ""-k 31""?. This should make no difference to the indexing procedure. While `-k` is an important parameter, 31 is the default value. Therefore, if you run the command without providing `-k` explicitly, it will use k=31. However, some folks like to keep it in the command line anyway, as a form of self documentation (though it is also recorded in the index), and that seems fine to me.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/521#issuecomment-628067346
https://github.com/COMBINE-lab/salmon/issues/522#issuecomment-632647047:181,Integrability,protocol,protocol,181,"I just noticed there is something wrong with my input file, sorry for my mistake. . The basic question stays the same: Are unmated reads supported by Alevin? Is there a single-cell protocol that produces 'single-end' files?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/522#issuecomment-632647047
https://github.com/COMBINE-lab/salmon/issues/522#issuecomment-634378224:276,Integrability,protocol,protocol,276,"Hey @TobiTekath ,. Thanks for the very interesting question. You are right, the unmated reads are not supported yet, but the basic idea was to consume a file with cb and umi attached to the read name, however it's a wip and is not properly supported. . re: any SE single-cell protocol - I am not aware of any.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/522#issuecomment-634378224
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-632953613:1212,Availability,down,down-weight,1212,"ne should expect transcript (or even gene) abundances to stay the same under a change of annotation. The estimates computed by salmon (and by all transcript-level abundance estimation tools) is one that maximizes the likelihood of the data (or maximizes the ELBO in the case of VI) _conditioned on_ the observed fragments and the _transcripts_. When one changes the transcripts, they change the variable upon which the inference is conditioned, and the results, in general, can change (a lot, or a little bit). This is specifically most prone to happen when transcripts / genes are added to the annotation that are similar to other transcripts or genes in the annotation. Now, my specific thought based on your settings of parameters. They are _quite_ different, but the three big factors I see here are (1) the setting for `--scoreExp`, (2) the setting(s) for dovetail and softclipOverhangs and (3) the setting for `--consensusSlack` Why are they a big deal?. * `--scoreExp` determines how much we down-weight scores sub-optimal alignments. Setting `--scoreExp` to 0 says that a sub-optimal alignment, at least in terms of the alignment probability is _just as good as the optimal alignment_. So, imagine you had a few read length regions of a pair of genes that each differed by 1 or 2 SNPs. When `--scoreExp` is 0, then the model considers alignments (say to transcript 2) with 2 substitutions to be just as likely as alignments (say to transcript 1) that are perfect (with no substitutions). While you can play around with different values of `--scoreExp` to determine how differences from the optimal alignment should be weighted, I'd strongly suggest against setting `--scoreExp` equal to 0. * `--allowDovetail` and `--softclipOverhangs` may or may not have a significant effect based on the quality of your library and annotation. Ideally, you would have no dovetailed mappings and no reads overhanging annotated transcripts. However, if you have an incomplete assembly or a library of question",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-632953613
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-632953613:608,Modifiability,variab,variable,608,"Hi @PlantDr430,. Thanks for the detailed report. I have a general thought, and then a more specific thought given your use case and parameter settings. The general thought is that it is _not_ true, broadly, that one should expect transcript (or even gene) abundances to stay the same under a change of annotation. The estimates computed by salmon (and by all transcript-level abundance estimation tools) is one that maximizes the likelihood of the data (or maximizes the ELBO in the case of VI) _conditioned on_ the observed fragments and the _transcripts_. When one changes the transcripts, they change the variable upon which the inference is conditioned, and the results, in general, can change (a lot, or a little bit). This is specifically most prone to happen when transcripts / genes are added to the annotation that are similar to other transcripts or genes in the annotation. Now, my specific thought based on your settings of parameters. They are _quite_ different, but the three big factors I see here are (1) the setting for `--scoreExp`, (2) the setting(s) for dovetail and softclipOverhangs and (3) the setting for `--consensusSlack` Why are they a big deal?. * `--scoreExp` determines how much we down-weight scores sub-optimal alignments. Setting `--scoreExp` to 0 says that a sub-optimal alignment, at least in terms of the alignment probability is _just as good as the optimal alignment_. So, imagine you had a few read length regions of a pair of genes that each differed by 1 or 2 SNPs. When `--scoreExp` is 0, then the model considers alignments (say to transcript 2) with 2 substitutions to be just as likely as alignments (say to transcript 1) that are perfect (with no substitutions). While you can play around with different values of `--scoreExp` to determine how differences from the optimal alignment should be weighted, I'd strongly suggest against setting `--scoreExp` equal to 0. * `--allowDovetail` and `--softclipOverhangs` may or may not have a significant effect bas",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-632953613
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-632953613:2592,Security,validat,validation,2592,"Exp` is 0, then the model considers alignments (say to transcript 2) with 2 substitutions to be just as likely as alignments (say to transcript 1) that are perfect (with no substitutions). While you can play around with different values of `--scoreExp` to determine how differences from the optimal alignment should be weighted, I'd strongly suggest against setting `--scoreExp` equal to 0. * `--allowDovetail` and `--softclipOverhangs` may or may not have a significant effect based on the quality of your library and annotation. Ideally, you would have no dovetailed mappings and no reads overhanging annotated transcripts. However, if you have an incomplete assembly or a library of questionable quality, these can both occur in practice. The `meta_info.json` file in the `aux_info` directory will give you stats about the number of dovetailed reads, so you can see if this is likely to have an effect here or not. * `--consensusSlack` determines which reads pass through the mapping-based filtering based on their chaining score and are therefore subject to alignment validation. While the chaining score is a decent proxy for alignment score, it's not perfect (otherwise, we would not really waste compute cycles computing the optimal alignment). Setting the `--consensusSlack` to a large value allows many things to be subject to mapping validation, while setting it to a smaller value doesn't. If the value is too small, then you may see situations where the mapping that yields the optimal _alignment_ doesn't have a chance to be counted because its chain score is too low. Now, it _is_ true that the only reads used will be those surpassing `--minScoreFraction` in terms of their alignment score, so changing the `--consensusSlack` won't allow through poor alignments, but if you set it too conservatively, you might miss some mappings that could have yielded the optimal alignment to mappings that are sub-optimal (personally, though, I'd guess this flag is probably the least likely to be ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-632953613
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-632953613:2864,Security,validat,validation,2864,"ed, I'd strongly suggest against setting `--scoreExp` equal to 0. * `--allowDovetail` and `--softclipOverhangs` may or may not have a significant effect based on the quality of your library and annotation. Ideally, you would have no dovetailed mappings and no reads overhanging annotated transcripts. However, if you have an incomplete assembly or a library of questionable quality, these can both occur in practice. The `meta_info.json` file in the `aux_info` directory will give you stats about the number of dovetailed reads, so you can see if this is likely to have an effect here or not. * `--consensusSlack` determines which reads pass through the mapping-based filtering based on their chaining score and are therefore subject to alignment validation. While the chaining score is a decent proxy for alignment score, it's not perfect (otherwise, we would not really waste compute cycles computing the optimal alignment). Setting the `--consensusSlack` to a large value allows many things to be subject to mapping validation, while setting it to a smaller value doesn't. If the value is too small, then you may see situations where the mapping that yields the optimal _alignment_ doesn't have a chance to be counted because its chain score is too low. Now, it _is_ true that the only reads used will be those surpassing `--minScoreFraction` in terms of their alignment score, so changing the `--consensusSlack` won't allow through poor alignments, but if you set it too conservatively, you might miss some mappings that could have yielded the optimal alignment to mappings that are sub-optimal (personally, though, I'd guess this flag is probably the least likely to be having an effect here). So, where to go from here? I'd try these flags 1-by-1, roughly in the order I listed them above, to see which ones are having the most drastic effect on your result. Also, if you'd be willing to share the `meta_info.json` files in each quantification directory, I'd be happy to look into them and see i",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-632953613
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633062608:449,Modifiability,variab,variables,449,"Thanks @rob-p,. Your explanations are helpful, and I think it may my concern may just be more associated with your general thought as I've tested this with multiple parameters. The one I represented here was just an example, but I also can see how the parameters can be affecting these results. It was just strange to see such a huge shift with the addition/removal of one gene, which makes me think it more associated with how the inference of the variables are conditioned. . As for providing the meta_info.json files, I currently have thousands of them as I am running triplicates of ~150 parameter combinations for multiple tissue types and stages. In the end I don't think it will be necessary as we will likely be changing our approach a bit, which should be fine with the system I have in place. . Also, as for `--scoreExp` our main goal is to try and use Salmon to get quantification of individual genes (primary versus spliced forms). From my analysis, it appears that some genes perform better with scores > 0, however, some genes do perform better with a `--scoreExp` of 0. Although, this could be a factor in running Salmon with such a narrow view (i.e. two transcripts and some housekeeping genes) and might not be the case as more genes are added to the run.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633062608
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633062608:989,Performance,perform,perform,989,"Thanks @rob-p,. Your explanations are helpful, and I think it may my concern may just be more associated with your general thought as I've tested this with multiple parameters. The one I represented here was just an example, but I also can see how the parameters can be affecting these results. It was just strange to see such a huge shift with the addition/removal of one gene, which makes me think it more associated with how the inference of the variables are conditioned. . As for providing the meta_info.json files, I currently have thousands of them as I am running triplicates of ~150 parameter combinations for multiple tissue types and stages. In the end I don't think it will be necessary as we will likely be changing our approach a bit, which should be fine with the system I have in place. . Also, as for `--scoreExp` our main goal is to try and use Salmon to get quantification of individual genes (primary versus spliced forms). From my analysis, it appears that some genes perform better with scores > 0, however, some genes do perform better with a `--scoreExp` of 0. Although, this could be a factor in running Salmon with such a narrow view (i.e. two transcripts and some housekeeping genes) and might not be the case as more genes are added to the run.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633062608
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633062608:1044,Performance,perform,perform,1044,"Thanks @rob-p,. Your explanations are helpful, and I think it may my concern may just be more associated with your general thought as I've tested this with multiple parameters. The one I represented here was just an example, but I also can see how the parameters can be affecting these results. It was just strange to see such a huge shift with the addition/removal of one gene, which makes me think it more associated with how the inference of the variables are conditioned. . As for providing the meta_info.json files, I currently have thousands of them as I am running triplicates of ~150 parameter combinations for multiple tissue types and stages. In the end I don't think it will be necessary as we will likely be changing our approach a bit, which should be fine with the system I have in place. . Also, as for `--scoreExp` our main goal is to try and use Salmon to get quantification of individual genes (primary versus spliced forms). From my analysis, it appears that some genes perform better with scores > 0, however, some genes do perform better with a `--scoreExp` of 0. Although, this could be a factor in running Salmon with such a narrow view (i.e. two transcripts and some housekeeping genes) and might not be the case as more genes are added to the run.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633062608
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633062608:139,Testability,test,tested,139,"Thanks @rob-p,. Your explanations are helpful, and I think it may my concern may just be more associated with your general thought as I've tested this with multiple parameters. The one I represented here was just an example, but I also can see how the parameters can be affecting these results. It was just strange to see such a huge shift with the addition/removal of one gene, which makes me think it more associated with how the inference of the variables are conditioned. . As for providing the meta_info.json files, I currently have thousands of them as I am running triplicates of ~150 parameter combinations for multiple tissue types and stages. In the end I don't think it will be necessary as we will likely be changing our approach a bit, which should be fine with the system I have in place. . Also, as for `--scoreExp` our main goal is to try and use Salmon to get quantification of individual genes (primary versus spliced forms). From my analysis, it appears that some genes perform better with scores > 0, however, some genes do perform better with a `--scoreExp` of 0. Although, this could be a factor in running Salmon with such a narrow view (i.e. two transcripts and some housekeeping genes) and might not be the case as more genes are added to the run.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633062608
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633091638:261,Performance,perform,perform,261,"Hi @PlantDr430,. Thanks for the context! As always, we'd be interested in learning anything interesting you find about the general behavior of salmon in different contexts and with different parameter settings etc. Out of curiosity, when you mention that genes perform ""better"" with one or another `--scoreExp`, is it the case that this is data where you have some sort of ground truth expectation for the abundance of the primary vs. spliced forms? If so, super interesting!. One other thought I had about this. While it is true, as I mentioned in my original post, that the conditioning on the transcripts is _fundamental_ in the case of salmon and other transcript expression tools that don't, themselves, try to assemble new transcripts, it's not necessarily true that there is no evidence in the quantifications that something my be awry. Specifically, I noticed that you are using posterior confidence estimation (bootstrapping). We actually have a [recent paper](https://www.biorxiv.org/content/10.1101/2020.04.07.029967v1.full) that discusses how to use the uncertainty estimates from salmon (though we rely on the Gibbs sampler rather than bootstrapping) to group together transcripts whose abundances cannot be individually estimated with confidence (with evidenced provided by the posterior samples). It might be useful to identify such cases in your analysis. Let me know if there's any other way I can help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633091638
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633091638:74,Usability,learn,learning,74,"Hi @PlantDr430,. Thanks for the context! As always, we'd be interested in learning anything interesting you find about the general behavior of salmon in different contexts and with different parameter settings etc. Out of curiosity, when you mention that genes perform ""better"" with one or another `--scoreExp`, is it the case that this is data where you have some sort of ground truth expectation for the abundance of the primary vs. spliced forms? If so, super interesting!. One other thought I had about this. While it is true, as I mentioned in my original post, that the conditioning on the transcripts is _fundamental_ in the case of salmon and other transcript expression tools that don't, themselves, try to assemble new transcripts, it's not necessarily true that there is no evidence in the quantifications that something my be awry. Specifically, I noticed that you are using posterior confidence estimation (bootstrapping). We actually have a [recent paper](https://www.biorxiv.org/content/10.1101/2020.04.07.029967v1.full) that discusses how to use the uncertainty estimates from salmon (though we rely on the Gibbs sampler rather than bootstrapping) to group together transcripts whose abundances cannot be individually estimated with confidence (with evidenced provided by the posterior samples). It might be useful to identify such cases in your analysis. Let me know if there's any other way I can help!",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633091638
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633099524:804,Availability,avail,available,804,"Hi @rob-p,. As for our main goal, we are looking at the precision of Salmon versus biological PCR expectations of abundances. However, it is on a small number of genes as there have been some initial challenges with using ""accurate"" transcripts versus ""computational predicted transcripts"". Anyway, we hope to have this paper ready this year, but again I think we need to look at it from a different angle. While this project is a bit of a side project, my hope it that it will accomplish our initial goal and to provide the community with the general behavior of Salmon with different spliced types (i.e. Exon skipping, IR, AA, AD, etc). Again, it'll be on a small set of genes, but still should be interesting and hopeful can improve the future use of Salmon. I'll make sure to let you know when it is available.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633099524
https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633099524:267,Safety,predict,predicted,267,"Hi @rob-p,. As for our main goal, we are looking at the precision of Salmon versus biological PCR expectations of abundances. However, it is on a small number of genes as there have been some initial challenges with using ""accurate"" transcripts versus ""computational predicted transcripts"". Anyway, we hope to have this paper ready this year, but again I think we need to look at it from a different angle. While this project is a bit of a side project, my hope it that it will accomplish our initial goal and to provide the community with the general behavior of Salmon with different spliced types (i.e. Exon skipping, IR, AA, AD, etc). Again, it'll be on a small set of genes, but still should be interesting and hopeful can improve the future use of Salmon. I'll make sure to let you know when it is available.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/523#issuecomment-633099524
https://github.com/COMBINE-lab/salmon/issues/525#issuecomment-639000029:150,Availability,down,downstream,150,"Hi @rob-p ,; I used the latest version of salmon, and you were right, the difference between two database decreased a little bit, but still affect my downstream analysis. So I'm wondering should I change or add some options when run salmon quantification step to count for the different reads between two databases? Now I'm using the same script for both NCBI and ENSEMBL quantification except using related index file as below:. ./salmon-latest_linux_x86_64/bin/salmon quant -i index -l A -r SRR.fastq -p 8 --numBootstraps 100 --validateMappings --writeMappings=../mapinfo.sam -o pat1 . As you mentioned the NCBI find the best locus to assign reads and ENSEMBL match the reads better, so should I use different parameters when quantify reads for these two databases? Thank you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/525#issuecomment-639000029
https://github.com/COMBINE-lab/salmon/issues/525#issuecomment-639000029:530,Security,validat,validateMappings,530,"Hi @rob-p ,; I used the latest version of salmon, and you were right, the difference between two database decreased a little bit, but still affect my downstream analysis. So I'm wondering should I change or add some options when run salmon quantification step to count for the different reads between two databases? Now I'm using the same script for both NCBI and ENSEMBL quantification except using related index file as below:. ./salmon-latest_linux_x86_64/bin/salmon quant -i index -l A -r SRR.fastq -p 8 --numBootstraps 100 --validateMappings --writeMappings=../mapinfo.sam -o pat1 . As you mentioned the NCBI find the best locus to assign reads and ENSEMBL match the reads better, so should I use different parameters when quantify reads for these two databases? Thank you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/525#issuecomment-639000029
https://github.com/COMBINE-lab/salmon/issues/525#issuecomment-639057440:1505,Availability,avail,available,1505,"Hi @silvernano,. Though alignment stringency parameters can have an effect here, the problem of comparing the abundance of a specific transcript within the context of different annotations is more fundamental. That is, apart from the transcript of interest, the ""background"" of related transcripts may be different. Consider the following case:. You have an annotation with 2 transcripts annotated for your gene:. ```; [A, B, C]; [A,C]; ```. Where `A`, `B`, and `C` are exons and the `[A,B,C]` notation means the transcript is exon `A`, followed by `B`, followed by `C`. Now, imagine that in annotation 2, you just have:. ```; [A,C]; ```. That is, the other isoform `[A, B, C]` is missing from annotation 2. Now, imagine that, in reality, `[A,B,C]` is highly expressed in your sample and `[A,C]` is lowly expressed. Under annotation 1, you get (correct) high expression for `[A,B,C]` and low expression for `[A,C]`. However, in annotation 2, since you still have to describe the `A` reads and the `C` reads, you get high expression for `[A,C]` since the reads contained within `A` and within `C` still map well there — the reads that map to `B` and to the `A-B` and `B-C` junction will likely just go un-mapped (which explains a difference in mapping rate). Now if you compare `[A,C]` across databases ... you will see big differences. This is not really related to the alignments that are observed, but rather how the data (reads) must be explained conditioned on the transcriptome (annotation) you have available. In other words, in loci with substantial unannotated transcription, the annotation used can definitely have a considerable effect on the quantification results. How to best assess, detect, and mitigate these effects is an active area of research, but I can point you at papers like [this](https://www.life-science-alliance.org/content/2/1/e201800175) and [this](https://www.cell.com/cell-systems/pdfExtended/S2405-4712(19)30381-3).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/525#issuecomment-639057440
https://github.com/COMBINE-lab/salmon/issues/525#issuecomment-639057440:1697,Safety,detect,detect,1697,"Hi @silvernano,. Though alignment stringency parameters can have an effect here, the problem of comparing the abundance of a specific transcript within the context of different annotations is more fundamental. That is, apart from the transcript of interest, the ""background"" of related transcripts may be different. Consider the following case:. You have an annotation with 2 transcripts annotated for your gene:. ```; [A, B, C]; [A,C]; ```. Where `A`, `B`, and `C` are exons and the `[A,B,C]` notation means the transcript is exon `A`, followed by `B`, followed by `C`. Now, imagine that in annotation 2, you just have:. ```; [A,C]; ```. That is, the other isoform `[A, B, C]` is missing from annotation 2. Now, imagine that, in reality, `[A,B,C]` is highly expressed in your sample and `[A,C]` is lowly expressed. Under annotation 1, you get (correct) high expression for `[A,B,C]` and low expression for `[A,C]`. However, in annotation 2, since you still have to describe the `A` reads and the `C` reads, you get high expression for `[A,C]` since the reads contained within `A` and within `C` still map well there — the reads that map to `B` and to the `A-B` and `B-C` junction will likely just go un-mapped (which explains a difference in mapping rate). Now if you compare `[A,C]` across databases ... you will see big differences. This is not really related to the alignments that are observed, but rather how the data (reads) must be explained conditioned on the transcriptome (annotation) you have available. In other words, in loci with substantial unannotated transcription, the annotation used can definitely have a considerable effect on the quantification results. How to best assess, detect, and mitigate these effects is an active area of research, but I can point you at papers like [this](https://www.life-science-alliance.org/content/2/1/e201800175) and [this](https://www.cell.com/cell-systems/pdfExtended/S2405-4712(19)30381-3).",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/525#issuecomment-639057440
https://github.com/COMBINE-lab/salmon/issues/526#issuecomment-635472672:223,Availability,error,error,223,"@brejnev . ```; 280 if not isinstance(X, AnnData):; 281 raise ValueError(""X has to be an AnnData object.""); --> 282 self._init_as_view(X, oidx, vidx); 283 else:; 284 self._init_as_actual(; ```. This could be an informative error, is your adata objec an `AnnData` object ?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/526#issuecomment-635472672
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636410253:51,Testability,log,log,51,"FInally, it took like 25 hours, I am attaching the log. ; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4706992/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636410253
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636410253:72,Testability,log,log,72,"FInally, it took like 25 hours, I am attaching the log. ; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4706992/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636410253
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636410253:142,Testability,log,log,142,"FInally, it took like 25 hours, I am attaching the log. ; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4706992/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636410253
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636410492:96,Deployability,upgrade,upgraded,96,"@red-plant ,. Thanks for the detailed report. Has anything else changed on the system since you upgraded? Does this same dataset finish much faster with 1.0.0? There have been no large-scale changes to alignment since 1.0.0 that should affect the runtime, so this is definitely completely unexpected behavior. We will also try to reproduce locally on this data.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636410492
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:477,Deployability,update,updates,477,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:252,Energy Efficiency,adapt,adapters,252,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:398,Energy Efficiency,adapt,adapter,398,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:252,Integrability,adapter,adapters,252,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:398,Integrability,adapter,adapter,398,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:252,Modifiability,adapt,adapters,252,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:398,Modifiability,adapt,adapter,398,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:204,Testability,log,log,204,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:792,Testability,log,log,792,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127:862,Testability,log,log,862,"Ok, salmon V1.0.0 finished in 5H 15 min, so about 5 times faster, the exact same library and parameters, and achieved almost the same mapping rate (85.1058% with V1.2.0 vs 84.6341% with V1.0.0) attaching log. I must add I did not trim this library for adapters nor quality, nor did anything to it. Just mapped as is. But fastQC showed excellent levels of quality even at the ends and no or minimal adapter content. ; Also no changes have been done one my OS other than regular updates, but still Ubuntu 18.04. I don't remember any specific changes I've done to it. ; Pearson's correlation in transcript abundance (isoform lelvel) is 0.9984013. Spearman's is 0.9899048. ; Also, I did checked that salmon was actually using 4 threads in both cases, and it was fully using those.; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4707443/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636447127
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636605261:795,Performance,perform,performance,795,"Hi @red-plant,. Thank you for providing the follow-up here. This behavior is very strange! However, I am not sure what is stranger; that 1.2.1 takes ~5x longer for you, or that 1.0.0 is taking > 5H in the first place! I can confirm that I am getting really strange (i.e. slow) behavior on this sample under both versions. It's unexpected because I've never seen a sample take this long before. We are looking into it, and will report back. However, since digging into the data to figure out precisely what is going on may take a little bit, I wanted to ask if you can try something. I think the issue may be resulting from some highly-expressed, ""pathologically"" repetitive sequence. Could you run salmon with the extra command line option `--hitFilterPolicy BOTH`, and let me know if / how the performance profile changes for you?. Thanks!; Rob",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636605261
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636790517:129,Testability,log,log,129,"So, it took ~30 min (impressively fast) for V1.0.0 with ```--hitFilterPolicy BOTH```, it seems that you are right. Attaching the log and running again in V1.2.1. Thanks again; José ; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4710898/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636790517
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636790517:197,Testability,log,log,197,"So, it took ~30 min (impressively fast) for V1.0.0 with ```--hitFilterPolicy BOTH```, it seems that you are right. Attaching the log and running again in V1.2.1. Thanks again; José ; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4710898/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636790517
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636790517:267,Testability,log,log,267,"So, it took ~30 min (impressively fast) for V1.0.0 with ```--hitFilterPolicy BOTH```, it seems that you are right. Attaching the log and running again in V1.2.1. Thanks again; José ; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4710898/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636790517
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636837126:251,Performance,perform,performance,251,"It took ~1.5H for V1.2.1 with ```--hitFilterPolicy BOTH```. Attaching log and fastp report, which shows normal tetramer over-representation. By the insert size determined by fastp I suspect there's quite a bit of dovetailing, I did had extremely slow performance with dovetailed libraries (for example SRR7945268, which is insert size 100, and its a PE 150 [not my data]) even allowing dovetails, to the point I ended up mapping them as single end and not using one of the pairs. Even then it took its time. Edit: allowing dovetails only increased the mapping rate by 0.0277%. As an additional note, not ```--minAlnProb 0.1``` nor ```--hardFilter``` help. . [fastp.pdf](https://github.com/COMBINE-lab/salmon/files/4711278/fastp.pdf); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4711259/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636837126
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636837126:70,Testability,log,log,70,"It took ~1.5H for V1.2.1 with ```--hitFilterPolicy BOTH```. Attaching log and fastp report, which shows normal tetramer over-representation. By the insert size determined by fastp I suspect there's quite a bit of dovetailing, I did had extremely slow performance with dovetailed libraries (for example SRR7945268, which is insert size 100, and its a PE 150 [not my data]) even allowing dovetails, to the point I ended up mapping them as single end and not using one of the pairs. Even then it took its time. Edit: allowing dovetails only increased the mapping rate by 0.0277%. As an additional note, not ```--minAlnProb 0.1``` nor ```--hardFilter``` help. . [fastp.pdf](https://github.com/COMBINE-lab/salmon/files/4711278/fastp.pdf); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4711259/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636837126
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636837126:748,Testability,log,log,748,"It took ~1.5H for V1.2.1 with ```--hitFilterPolicy BOTH```. Attaching log and fastp report, which shows normal tetramer over-representation. By the insert size determined by fastp I suspect there's quite a bit of dovetailing, I did had extremely slow performance with dovetailed libraries (for example SRR7945268, which is insert size 100, and its a PE 150 [not my data]) even allowing dovetails, to the point I ended up mapping them as single end and not using one of the pairs. Even then it took its time. Edit: allowing dovetails only increased the mapping rate by 0.0277%. As an additional note, not ```--minAlnProb 0.1``` nor ```--hardFilter``` help. . [fastp.pdf](https://github.com/COMBINE-lab/salmon/files/4711278/fastp.pdf); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4711259/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636837126
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636837126:818,Testability,log,log,818,"It took ~1.5H for V1.2.1 with ```--hitFilterPolicy BOTH```. Attaching log and fastp report, which shows normal tetramer over-representation. By the insert size determined by fastp I suspect there's quite a bit of dovetailing, I did had extremely slow performance with dovetailed libraries (for example SRR7945268, which is insert size 100, and its a PE 150 [not my data]) even allowing dovetails, to the point I ended up mapping them as single end and not using one of the pairs. Even then it took its time. Edit: allowing dovetails only increased the mapping rate by 0.0277%. As an additional note, not ```--minAlnProb 0.1``` nor ```--hardFilter``` help. . [fastp.pdf](https://github.com/COMBINE-lab/salmon/files/4711278/fastp.pdf); [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4711259/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-636837126
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013:32,Deployability,update,update,32,"Hi @red-plant,. So, I have some update from our end. @mohsenzakeri dug into the data a bit (specifically `SRR7985407`). What he found is that there are a considerable number of reads (~13%) have long stretches of polyA or polyT that are matching in a hyper-repetitive manner internally within a certain set of transcripts (i.e. these are not matching polyA tails, because those are already trimmed). These matches are, obviously, minimally informative, but we had not special-cased ignoring them yet. Specifically, what seems to be prevalent in these reads are read pairs where one read has polyA, the other has polyT, and the keep matching to the same positions. However, the rest of the reads don't match the transcript, so a bunch of time is wasted on validating (and discarding) these mappings. To test this hypothesis, we made a small change to the mapping algorithm to special case and ignore k-mers that are purely homopolymers. I'll note that in this data, this has no effect on the mapping rate. I get the following performance profile running the trimmed version of this data (having trimmed with `fastp`) using 4 threads, and _without_ the additional `--hitFilterPolicy BOTH` flag. ```; 1306.86user 4.79system 4:42.54elapsed 464%CPU (0avgtext+0avgdata 592704maxresident)k; ```. I was wondering if you might test this altered version out and see if it has a similarly beneficial effect for you as well. Probably, the time will be different, since the processors themselves are, and since I elided all non-essential flags here, but I would hope this version is faster than the current (even with the altered `hitFilterPolicy`). You can find a tarball with the pre-compiled binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). It should work on any relatively recent linux system.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013:1025,Performance,perform,performance,1025,"Hi @red-plant,. So, I have some update from our end. @mohsenzakeri dug into the data a bit (specifically `SRR7985407`). What he found is that there are a considerable number of reads (~13%) have long stretches of polyA or polyT that are matching in a hyper-repetitive manner internally within a certain set of transcripts (i.e. these are not matching polyA tails, because those are already trimmed). These matches are, obviously, minimally informative, but we had not special-cased ignoring them yet. Specifically, what seems to be prevalent in these reads are read pairs where one read has polyA, the other has polyT, and the keep matching to the same positions. However, the rest of the reads don't match the transcript, so a bunch of time is wasted on validating (and discarding) these mappings. To test this hypothesis, we made a small change to the mapping algorithm to special case and ignore k-mers that are purely homopolymers. I'll note that in this data, this has no effect on the mapping rate. I get the following performance profile running the trimmed version of this data (having trimmed with `fastp`) using 4 threads, and _without_ the additional `--hitFilterPolicy BOTH` flag. ```; 1306.86user 4.79system 4:42.54elapsed 464%CPU (0avgtext+0avgdata 592704maxresident)k; ```. I was wondering if you might test this altered version out and see if it has a similarly beneficial effect for you as well. Probably, the time will be different, since the processors themselves are, and since I elided all non-essential flags here, but I would hope this version is faster than the current (even with the altered `hitFilterPolicy`). You can find a tarball with the pre-compiled binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). It should work on any relatively recent linux system.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013:755,Security,validat,validating,755,"Hi @red-plant,. So, I have some update from our end. @mohsenzakeri dug into the data a bit (specifically `SRR7985407`). What he found is that there are a considerable number of reads (~13%) have long stretches of polyA or polyT that are matching in a hyper-repetitive manner internally within a certain set of transcripts (i.e. these are not matching polyA tails, because those are already trimmed). These matches are, obviously, minimally informative, but we had not special-cased ignoring them yet. Specifically, what seems to be prevalent in these reads are read pairs where one read has polyA, the other has polyT, and the keep matching to the same positions. However, the rest of the reads don't match the transcript, so a bunch of time is wasted on validating (and discarding) these mappings. To test this hypothesis, we made a small change to the mapping algorithm to special case and ignore k-mers that are purely homopolymers. I'll note that in this data, this has no effect on the mapping rate. I get the following performance profile running the trimmed version of this data (having trimmed with `fastp`) using 4 threads, and _without_ the additional `--hitFilterPolicy BOTH` flag. ```; 1306.86user 4.79system 4:42.54elapsed 464%CPU (0avgtext+0avgdata 592704maxresident)k; ```. I was wondering if you might test this altered version out and see if it has a similarly beneficial effect for you as well. Probably, the time will be different, since the processors themselves are, and since I elided all non-essential flags here, but I would hope this version is faster than the current (even with the altered `hitFilterPolicy`). You can find a tarball with the pre-compiled binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). It should work on any relatively recent linux system.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013:802,Testability,test,test,802,"Hi @red-plant,. So, I have some update from our end. @mohsenzakeri dug into the data a bit (specifically `SRR7985407`). What he found is that there are a considerable number of reads (~13%) have long stretches of polyA or polyT that are matching in a hyper-repetitive manner internally within a certain set of transcripts (i.e. these are not matching polyA tails, because those are already trimmed). These matches are, obviously, minimally informative, but we had not special-cased ignoring them yet. Specifically, what seems to be prevalent in these reads are read pairs where one read has polyA, the other has polyT, and the keep matching to the same positions. However, the rest of the reads don't match the transcript, so a bunch of time is wasted on validating (and discarding) these mappings. To test this hypothesis, we made a small change to the mapping algorithm to special case and ignore k-mers that are purely homopolymers. I'll note that in this data, this has no effect on the mapping rate. I get the following performance profile running the trimmed version of this data (having trimmed with `fastp`) using 4 threads, and _without_ the additional `--hitFilterPolicy BOTH` flag. ```; 1306.86user 4.79system 4:42.54elapsed 464%CPU (0avgtext+0avgdata 592704maxresident)k; ```. I was wondering if you might test this altered version out and see if it has a similarly beneficial effect for you as well. Probably, the time will be different, since the processors themselves are, and since I elided all non-essential flags here, but I would hope this version is faster than the current (even with the altered `hitFilterPolicy`). You can find a tarball with the pre-compiled binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). It should work on any relatively recent linux system.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013:1318,Testability,test,test,1318,"Hi @red-plant,. So, I have some update from our end. @mohsenzakeri dug into the data a bit (specifically `SRR7985407`). What he found is that there are a considerable number of reads (~13%) have long stretches of polyA or polyT that are matching in a hyper-repetitive manner internally within a certain set of transcripts (i.e. these are not matching polyA tails, because those are already trimmed). These matches are, obviously, minimally informative, but we had not special-cased ignoring them yet. Specifically, what seems to be prevalent in these reads are read pairs where one read has polyA, the other has polyT, and the keep matching to the same positions. However, the rest of the reads don't match the transcript, so a bunch of time is wasted on validating (and discarding) these mappings. To test this hypothesis, we made a small change to the mapping algorithm to special case and ignore k-mers that are purely homopolymers. I'll note that in this data, this has no effect on the mapping rate. I get the following performance profile running the trimmed version of this data (having trimmed with `fastp`) using 4 threads, and _without_ the additional `--hitFilterPolicy BOTH` flag. ```; 1306.86user 4.79system 4:42.54elapsed 464%CPU (0avgtext+0avgdata 592704maxresident)k; ```. I was wondering if you might test this altered version out and see if it has a similarly beneficial effect for you as well. Probably, the time will be different, since the processors themselves are, and since I elided all non-essential flags here, but I would hope this version is faster than the current (even with the altered `hitFilterPolicy`). You can find a tarball with the pre-compiled binary [here](https://drive.google.com/file/d/1tPyOPW3Y8l86RS0-zBRLh0wCt3VTpkNw/view?usp=sharing). It should work on any relatively recent linux system.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637568013
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637581730:16,Security,validat,validateMappings,16,"Ran with only --validateMappings and -p 4 options. Wow, it took less than 15 minutes! Thanks a lot to you and your team for looking into this.; Best,; Jose; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4717767/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637581730
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637581730:171,Testability,log,log,171,"Ran with only --validateMappings and -p 4 options. Wow, it took less than 15 minutes! Thanks a lot to you and your team for looking into this.; Best,; Jose; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4717767/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637581730
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637581730:241,Testability,log,log,241,"Ran with only --validateMappings and -p 4 options. Wow, it took less than 15 minutes! Thanks a lot to you and your team for looking into this.; Best,; Jose; [salmon_quant.log](https://github.com/COMBINE-lab/salmon/files/4717767/salmon_quant.log)",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637581730
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637601951:183,Deployability,release,release,183,Awesome! Thank you so much for the detailed report and for finding this data that exposed this strange (but interesting) performance case. We'll fold these improvements into the next release as well.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637601951
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637601951:121,Performance,perform,performance,121,Awesome! Thank you so much for the detailed report and for finding this data that exposed this strange (but interesting) performance case. We'll fold these improvements into the next release as well.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637601951
https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637601951:82,Security,expose,exposed,82,Awesome! Thank you so much for the detailed report and for finding this data that exposed this strange (but interesting) performance case. We'll fold these improvements into the next release as well.,MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/527#issuecomment-637601951
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-638026422:235,Performance,load,load,235,"I am also confused about it. It seems that peudosam can not be converted into bam because of lacking of location. If I just use the. ```bash; samtools sort -O bam -@ 30 -o sort.bam Mapping.sam . samtools index sort.bam; ```. I can not load the sort.bam into IGV. But I did find the two issue: #475 and #38 , which mentioned bam file.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-638026422
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639065653:2438,Modifiability,variab,variables,2438,"here, for instance. What do the flags and qualities represent?. It is just a SAM file without CIGAR strings. The flags have the same (normal) interpretation for SAM records. However the CIGAR strings are not meaningful (apart from what is required for the file to undergo valid processing with samtools). The records additionally contain tags about the number of targets to which a fragment multi-maps, and the alignment score of the read pair to the current target (in the `AS` flag). The records themself are just normal SAM records, but with a trivial CIGAR strong. > More importantly, is there a way to filter the pseudobam files to find the reads corresponding to the counts/NumReads in the quant.sf output file? Do the normal samtools quality and flag filters work to subset e.g. uniquely-mapped reads, or do those concepts not really apply to these pseudobams?. There is no easy way to filter so the above condition is satisfied, as the NumReads are obtained by proportional allocation of the reads according to the underlying probabilistic model of salmon. Specifically, the NumReads column of the quantification file corresponds to summing over the _expectation_ of all of the latent variables that represent fragment to transcript assignment so that, apart from uniquely-mapped reads, there is no way to say that a fragment _definitely_ came from a transcript. However, you should still be able to easily filter out uniquely mapped reads, and you can interpret them in a relatively unambiguous way. Also, you could filter on the `AS` tag as well. For a given read, if there is a single transcript where the `AS` value is much larger than the others for this read, it is overwhelmingly likely that the read originated from the transcript with the unique best `AS` score. @shangguandong1996 : The SAM file _does_ contain positions (and orientations, and alignment scores) for each read. It is simply that the positions are with respect to the transcriptome and not with respect to the genome.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639065653
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639065653:3146,Usability,simpl,simply,3146,"here, for instance. What do the flags and qualities represent?. It is just a SAM file without CIGAR strings. The flags have the same (normal) interpretation for SAM records. However the CIGAR strings are not meaningful (apart from what is required for the file to undergo valid processing with samtools). The records additionally contain tags about the number of targets to which a fragment multi-maps, and the alignment score of the read pair to the current target (in the `AS` flag). The records themself are just normal SAM records, but with a trivial CIGAR strong. > More importantly, is there a way to filter the pseudobam files to find the reads corresponding to the counts/NumReads in the quant.sf output file? Do the normal samtools quality and flag filters work to subset e.g. uniquely-mapped reads, or do those concepts not really apply to these pseudobams?. There is no easy way to filter so the above condition is satisfied, as the NumReads are obtained by proportional allocation of the reads according to the underlying probabilistic model of salmon. Specifically, the NumReads column of the quantification file corresponds to summing over the _expectation_ of all of the latent variables that represent fragment to transcript assignment so that, apart from uniquely-mapped reads, there is no way to say that a fragment _definitely_ came from a transcript. However, you should still be able to easily filter out uniquely mapped reads, and you can interpret them in a relatively unambiguous way. Also, you could filter on the `AS` tag as well. For a given read, if there is a single transcript where the `AS` value is much larger than the others for this read, it is overwhelmingly likely that the read originated from the transcript with the unique best `AS` score. @shangguandong1996 : The SAM file _does_ contain positions (and orientations, and alignment scores) for each read. It is simply that the positions are with respect to the transcriptome and not with respect to the genome.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639065653
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639153957:867,Availability,error,error,867,"Thanks for the response. . The transcriptional variants I've been interested often are quite similar (e.g. only differ for a small part of one exon). Therefore, many of the reads (especially when they map to parts of the genes that don't differ) show up as pseudoaligned to multiple variants, as you'd expect. In that case, do you suggest only looking at the uniquely mapped reads, or only looking at primary alignments for each read, or still looking at all reads (perhaps with a certain `AS` score) for a given transcript? I'm mostly interested in performing sanity checks that transcriptional variants identified by Salmon/Swish are differentially used across conditions. Or would it be better to use a tool like DEXSeq to asks these questions directly? . Also, when filtering by the `AS`, I found some reads with `AS:i:-2147483648`, which I assume is an overflow error.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639153957
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639153957:550,Performance,perform,performing,550,"Thanks for the response. . The transcriptional variants I've been interested often are quite similar (e.g. only differ for a small part of one exon). Therefore, many of the reads (especially when they map to parts of the genes that don't differ) show up as pseudoaligned to multiple variants, as you'd expect. In that case, do you suggest only looking at the uniquely mapped reads, or only looking at primary alignments for each read, or still looking at all reads (perhaps with a certain `AS` score) for a given transcript? I'm mostly interested in performing sanity checks that transcriptional variants identified by Salmon/Swish are differentially used across conditions. Or would it be better to use a tool like DEXSeq to asks these questions directly? . Also, when filtering by the `AS`, I found some reads with `AS:i:-2147483648`, which I assume is an overflow error.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639153957
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639153957:561,Safety,sanity check,sanity checks,561,"Thanks for the response. . The transcriptional variants I've been interested often are quite similar (e.g. only differ for a small part of one exon). Therefore, many of the reads (especially when they map to parts of the genes that don't differ) show up as pseudoaligned to multiple variants, as you'd expect. In that case, do you suggest only looking at the uniquely mapped reads, or only looking at primary alignments for each read, or still looking at all reads (perhaps with a certain `AS` score) for a given transcript? I'm mostly interested in performing sanity checks that transcriptional variants identified by Salmon/Swish are differentially used across conditions. Or would it be better to use a tool like DEXSeq to asks these questions directly? . Also, when filtering by the `AS`, I found some reads with `AS:i:-2147483648`, which I assume is an overflow error.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639153957
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639157711:226,Availability,ping,ping,226,"The `AS:i:-2147483648` is a sentinel value basically meaning the alignment was below the minimum acceptable quality. You can simply ignore those (its the min signed 32-bit integer). Let me think about your other question (and ping @mikelove), and get back to you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639157711
https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639157711:125,Usability,simpl,simply,125,"The `AS:i:-2147483648` is a sentinel value basically meaning the alignment was below the minimum acceptable quality. You can simply ignore those (its the min signed 32-bit integer). Let me think about your other question (and ping @mikelove), and get back to you.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/528#issuecomment-639157711
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-637975180:262,Availability,down,download,262,"> Could you share the quant directory for one of the samples that has NANs in the bootstraps?. Hi Rob,. Thanks for your quick reply. Sleuth didn't show which sample caused the issue therefore I have to upload all samples used in this analysis. Here is a link to download it. https://send.firefox.com/download/ddf555a8eec9b6c7/#AUxMdiugHH6PSXJ7sayUlA. Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-637975180
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-637975180:300,Availability,down,download,300,"> Could you share the quant directory for one of the samples that has NANs in the bootstraps?. Hi Rob,. Thanks for your quick reply. Sleuth didn't show which sample caused the issue therefore I have to upload all samples used in this analysis. Here is a link to download it. https://send.firefox.com/download/ddf555a8eec9b6c7/#AUxMdiugHH6PSXJ7sayUlA. Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-637975180
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638254418:126,Performance,load,loaded,126,"Hi @litongda007,. Thanks for providing the data; it's very useful. I will continue to poke here a bit, but in a first pass, I loaded up all of the bootstrap samples and did `frame.isna().sum().sum()` for all 8 samples and got 0 on each. So, at least loading the bootstraps this ways, I can't see any NaNs.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638254418
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638254418:250,Performance,load,loading,250,"Hi @litongda007,. Thanks for providing the data; it's very useful. I will continue to poke here a bit, but in a first pass, I loaded up all of the bootstrap samples and did `frame.isna().sum().sum()` for all 8 samples and got 0 on each. So, at least loading the bootstraps this ways, I can't see any NaNs.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638254418
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638496616:230,Availability,down,download,230,"Hi @rob-p ,. Thanks for your reply. I just confirmed with my colleague, and realised the data I uploaded previously was related to another issue instead of the ""NA"" issue. I just re-uploaded the data. Please use the link below to download it. I am sorry for any inconvenience caused by this. https://send.firefox.com/download/8344911f0baae7d0/#xIWOt34AlY4XENr23IkBJA. Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638496616
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638496616:317,Availability,down,download,317,"Hi @rob-p ,. Thanks for your reply. I just confirmed with my colleague, and realised the data I uploaded previously was related to another issue instead of the ""NA"" issue. I just re-uploaded the data. Please use the link below to download it. I am sorry for any inconvenience caused by this. https://send.firefox.com/download/8344911f0baae7d0/#xIWOt34AlY4XENr23IkBJA. Thanks.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638496616
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711:346,Availability,error,error,346,"Hi @litongda007,. Thanks for the updated. I grabbed these and ... basically saw the same thing:. ```; 23R1F : has null = False; 23R2F : has null = False; 23R3F : has null = False; 23R4F : has null = False; R1ST1 : has null = False; R2ST1 : has null = False; R3ST1 : has null = False; R4ST1 : has null = False; ```. I am thinking that perhaps the error is popping up somewhere _downstream_ of salmon. I presume you are using the `salmon` -> `wasabi` -> `sleuth` pipeline, is that correct? If so, I can try and see if I get the same thing importing in R. The tests above were using the python importer from [here](https://github.com/COMBINE-lab/pluribus). **Update**: Ok, that, too, has failed. I converted all of the quantifications to hdf5 files using wasabi, and then checked for nans in the converted files:. ```python; import h5py; import numpy as np; def get_num_nan(x):; nbs = int(x['aux']['num_bootstrap'].value[0]); s = 0; for i in range(nbs):; s += np.isnan(x['bootstrap']['bs{}'.format(i)].value).sum(); return s. samps = ['23R1F', '23R2F', '23R3F', '23R4F', 'R1ST1', 'R2ST1', 'R3ST1', 'R4ST1']; for s in samps:; d = h5py.File('quant/{}/abundance.h5'.format(s)) # abundance.h5 created by wasabi; null_count = get_num_nan(d); print(""{} : null count = {}"".format(s, null_count)); d.close(); ```. The output, as above, is : . ```; 23R1F : null count = 0; 23R2F : null count = 0; 23R3F : null count = 0; 23R4F : null count = 0; R1ST1 : null count = 0; R2ST1 : null count = 0; R3ST1 : null count = 0; R4ST1 : null count = 0; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711:33,Deployability,update,updated,33,"Hi @litongda007,. Thanks for the updated. I grabbed these and ... basically saw the same thing:. ```; 23R1F : has null = False; 23R2F : has null = False; 23R3F : has null = False; 23R4F : has null = False; R1ST1 : has null = False; R2ST1 : has null = False; R3ST1 : has null = False; R4ST1 : has null = False; ```. I am thinking that perhaps the error is popping up somewhere _downstream_ of salmon. I presume you are using the `salmon` -> `wasabi` -> `sleuth` pipeline, is that correct? If so, I can try and see if I get the same thing importing in R. The tests above were using the python importer from [here](https://github.com/COMBINE-lab/pluribus). **Update**: Ok, that, too, has failed. I converted all of the quantifications to hdf5 files using wasabi, and then checked for nans in the converted files:. ```python; import h5py; import numpy as np; def get_num_nan(x):; nbs = int(x['aux']['num_bootstrap'].value[0]); s = 0; for i in range(nbs):; s += np.isnan(x['bootstrap']['bs{}'.format(i)].value).sum(); return s. samps = ['23R1F', '23R2F', '23R3F', '23R4F', 'R1ST1', 'R2ST1', 'R3ST1', 'R4ST1']; for s in samps:; d = h5py.File('quant/{}/abundance.h5'.format(s)) # abundance.h5 created by wasabi; null_count = get_num_nan(d); print(""{} : null count = {}"".format(s, null_count)); d.close(); ```. The output, as above, is : . ```; 23R1F : null count = 0; 23R2F : null count = 0; 23R3F : null count = 0; 23R4F : null count = 0; R1ST1 : null count = 0; R2ST1 : null count = 0; R3ST1 : null count = 0; R4ST1 : null count = 0; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711:461,Deployability,pipeline,pipeline,461,"Hi @litongda007,. Thanks for the updated. I grabbed these and ... basically saw the same thing:. ```; 23R1F : has null = False; 23R2F : has null = False; 23R3F : has null = False; 23R4F : has null = False; R1ST1 : has null = False; R2ST1 : has null = False; R3ST1 : has null = False; R4ST1 : has null = False; ```. I am thinking that perhaps the error is popping up somewhere _downstream_ of salmon. I presume you are using the `salmon` -> `wasabi` -> `sleuth` pipeline, is that correct? If so, I can try and see if I get the same thing importing in R. The tests above were using the python importer from [here](https://github.com/COMBINE-lab/pluribus). **Update**: Ok, that, too, has failed. I converted all of the quantifications to hdf5 files using wasabi, and then checked for nans in the converted files:. ```python; import h5py; import numpy as np; def get_num_nan(x):; nbs = int(x['aux']['num_bootstrap'].value[0]); s = 0; for i in range(nbs):; s += np.isnan(x['bootstrap']['bs{}'.format(i)].value).sum(); return s. samps = ['23R1F', '23R2F', '23R3F', '23R4F', 'R1ST1', 'R2ST1', 'R3ST1', 'R4ST1']; for s in samps:; d = h5py.File('quant/{}/abundance.h5'.format(s)) # abundance.h5 created by wasabi; null_count = get_num_nan(d); print(""{} : null count = {}"".format(s, null_count)); d.close(); ```. The output, as above, is : . ```; 23R1F : null count = 0; 23R2F : null count = 0; 23R3F : null count = 0; 23R4F : null count = 0; R1ST1 : null count = 0; R2ST1 : null count = 0; R3ST1 : null count = 0; R4ST1 : null count = 0; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711:656,Deployability,Update,Update,656,"Hi @litongda007,. Thanks for the updated. I grabbed these and ... basically saw the same thing:. ```; 23R1F : has null = False; 23R2F : has null = False; 23R3F : has null = False; 23R4F : has null = False; R1ST1 : has null = False; R2ST1 : has null = False; R3ST1 : has null = False; R4ST1 : has null = False; ```. I am thinking that perhaps the error is popping up somewhere _downstream_ of salmon. I presume you are using the `salmon` -> `wasabi` -> `sleuth` pipeline, is that correct? If so, I can try and see if I get the same thing importing in R. The tests above were using the python importer from [here](https://github.com/COMBINE-lab/pluribus). **Update**: Ok, that, too, has failed. I converted all of the quantifications to hdf5 files using wasabi, and then checked for nans in the converted files:. ```python; import h5py; import numpy as np; def get_num_nan(x):; nbs = int(x['aux']['num_bootstrap'].value[0]); s = 0; for i in range(nbs):; s += np.isnan(x['bootstrap']['bs{}'.format(i)].value).sum(); return s. samps = ['23R1F', '23R2F', '23R3F', '23R4F', 'R1ST1', 'R2ST1', 'R3ST1', 'R4ST1']; for s in samps:; d = h5py.File('quant/{}/abundance.h5'.format(s)) # abundance.h5 created by wasabi; null_count = get_num_nan(d); print(""{} : null count = {}"".format(s, null_count)); d.close(); ```. The output, as above, is : . ```; 23R1F : null count = 0; 23R2F : null count = 0; 23R3F : null count = 0; 23R4F : null count = 0; R1ST1 : null count = 0; R2ST1 : null count = 0; R3ST1 : null count = 0; R4ST1 : null count = 0; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711:557,Testability,test,tests,557,"Hi @litongda007,. Thanks for the updated. I grabbed these and ... basically saw the same thing:. ```; 23R1F : has null = False; 23R2F : has null = False; 23R3F : has null = False; 23R4F : has null = False; R1ST1 : has null = False; R2ST1 : has null = False; R3ST1 : has null = False; R4ST1 : has null = False; ```. I am thinking that perhaps the error is popping up somewhere _downstream_ of salmon. I presume you are using the `salmon` -> `wasabi` -> `sleuth` pipeline, is that correct? If so, I can try and see if I get the same thing importing in R. The tests above were using the python importer from [here](https://github.com/COMBINE-lab/pluribus). **Update**: Ok, that, too, has failed. I converted all of the quantifications to hdf5 files using wasabi, and then checked for nans in the converted files:. ```python; import h5py; import numpy as np; def get_num_nan(x):; nbs = int(x['aux']['num_bootstrap'].value[0]); s = 0; for i in range(nbs):; s += np.isnan(x['bootstrap']['bs{}'.format(i)].value).sum(); return s. samps = ['23R1F', '23R2F', '23R3F', '23R4F', 'R1ST1', 'R2ST1', 'R3ST1', 'R4ST1']; for s in samps:; d = h5py.File('quant/{}/abundance.h5'.format(s)) # abundance.h5 created by wasabi; null_count = get_num_nan(d); print(""{} : null count = {}"".format(s, null_count)); d.close(); ```. The output, as above, is : . ```; 23R1F : null count = 0; 23R2F : null count = 0; 23R3F : null count = 0; 23R4F : null count = 0; R1ST1 : null count = 0; R2ST1 : null count = 0; R3ST1 : null count = 0; R4ST1 : null count = 0; ```",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638553711
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638566265:99,Availability,error,error,99,"Hi @rob-p,. Thanks for your reply. Yes I am using the salmon -> wasabi -> sleuth pipeline. The NaN error showed up at this step. `so <- sleuth_prep(s2c, extra_bootstrap_summary = TRUE, read_bootstrap_tpm = TRUE, num_cores = 1, filter_fun = my_filter)`. Sleuth complained that there are NAs in bootstrap values and then stopped. Therefore, I removed the two options that are related to processing bootstrap and it worked. `so<- sleuth_prep(s2c, num_cores = 1, filter_fun = my_filter)` . However, the following sleuth_fit step showed the same error. `so <- sleuth_fit(so, ~condition, 'full', which_var = 'obs_counts') `",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638566265
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638566265:541,Availability,error,error,541,"Hi @rob-p,. Thanks for your reply. Yes I am using the salmon -> wasabi -> sleuth pipeline. The NaN error showed up at this step. `so <- sleuth_prep(s2c, extra_bootstrap_summary = TRUE, read_bootstrap_tpm = TRUE, num_cores = 1, filter_fun = my_filter)`. Sleuth complained that there are NAs in bootstrap values and then stopped. Therefore, I removed the two options that are related to processing bootstrap and it worked. `so<- sleuth_prep(s2c, num_cores = 1, filter_fun = my_filter)` . However, the following sleuth_fit step showed the same error. `so <- sleuth_fit(so, ~condition, 'full', which_var = 'obs_counts') `",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638566265
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638566265:81,Deployability,pipeline,pipeline,81,"Hi @rob-p,. Thanks for your reply. Yes I am using the salmon -> wasabi -> sleuth pipeline. The NaN error showed up at this step. `so <- sleuth_prep(s2c, extra_bootstrap_summary = TRUE, read_bootstrap_tpm = TRUE, num_cores = 1, filter_fun = my_filter)`. Sleuth complained that there are NAs in bootstrap values and then stopped. Therefore, I removed the two options that are related to processing bootstrap and it worked. `so<- sleuth_prep(s2c, num_cores = 1, filter_fun = my_filter)` . However, the following sleuth_fit step showed the same error. `so <- sleuth_fit(so, ~condition, 'full', which_var = 'obs_counts') `",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638566265
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638568102:132,Availability,error,error,132,"Hi @litongda007,. If you can share the sample table and code you are using to process up to this point, I can see if I get the same error. I'm guessing the result is something happening _after_ the conversion to h5. Some subsequent processing is likely leading to the error, but the bootstraps themselves contain no NA/NAN values.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638568102
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638568102:268,Availability,error,error,268,"Hi @litongda007,. If you can share the sample table and code you are using to process up to this point, I can see if I get the same error. I'm guessing the result is something happening _after_ the conversion to h5. Some subsequent processing is likely leading to the error, but the bootstraps themselves contain no NA/NAN values.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638568102
https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638672708:123,Availability,down,download,123,"Hi @rob-p,. Sure. Here are the metadata file and the R script file. Thanks again for your help. . https://send.firefox.com/download/2fbeef50b5ce0919/#Q8UXl4L0kXyjfGz03vyjsQ",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/529#issuecomment-638672708
https://github.com/COMBINE-lab/salmon/issues/530#issuecomment-638453196:746,Availability,down,downstream,746,"Could you please share one of the output directories? It's not immediately obvious what the problem might be, since the log ends with . ```; [2020-06-03 23:47:15.955] [jointLog] [info] Computing gene-level abundance estimates; ```. which suggests the function to aggregate abundances to the gene level should be activated. On a related note, though we are definitely interesting in figuring out what might being going awry here, the recommended way to aggregate transcript-level abundances from salmon to the gene level is to use [tximport](https://bioconductor.org/packages/release/bioc/html/tximport.html), as it accounts for across-sample variability in expressed gene length, and makes it trivial to get your corresponding gene counts into a downstream DE tool like DESeq2, EdgeR, etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/530#issuecomment-638453196
https://github.com/COMBINE-lab/salmon/issues/530#issuecomment-638453196:575,Deployability,release,release,575,"Could you please share one of the output directories? It's not immediately obvious what the problem might be, since the log ends with . ```; [2020-06-03 23:47:15.955] [jointLog] [info] Computing gene-level abundance estimates; ```. which suggests the function to aggregate abundances to the gene level should be activated. On a related note, though we are definitely interesting in figuring out what might being going awry here, the recommended way to aggregate transcript-level abundances from salmon to the gene level is to use [tximport](https://bioconductor.org/packages/release/bioc/html/tximport.html), as it accounts for across-sample variability in expressed gene length, and makes it trivial to get your corresponding gene counts into a downstream DE tool like DESeq2, EdgeR, etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/530#issuecomment-638453196
https://github.com/COMBINE-lab/salmon/issues/530#issuecomment-638453196:642,Modifiability,variab,variability,642,"Could you please share one of the output directories? It's not immediately obvious what the problem might be, since the log ends with . ```; [2020-06-03 23:47:15.955] [jointLog] [info] Computing gene-level abundance estimates; ```. which suggests the function to aggregate abundances to the gene level should be activated. On a related note, though we are definitely interesting in figuring out what might being going awry here, the recommended way to aggregate transcript-level abundances from salmon to the gene level is to use [tximport](https://bioconductor.org/packages/release/bioc/html/tximport.html), as it accounts for across-sample variability in expressed gene length, and makes it trivial to get your corresponding gene counts into a downstream DE tool like DESeq2, EdgeR, etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/530#issuecomment-638453196
https://github.com/COMBINE-lab/salmon/issues/530#issuecomment-638453196:120,Testability,log,log,120,"Could you please share one of the output directories? It's not immediately obvious what the problem might be, since the log ends with . ```; [2020-06-03 23:47:15.955] [jointLog] [info] Computing gene-level abundance estimates; ```. which suggests the function to aggregate abundances to the gene level should be activated. On a related note, though we are definitely interesting in figuring out what might being going awry here, the recommended way to aggregate transcript-level abundances from salmon to the gene level is to use [tximport](https://bioconductor.org/packages/release/bioc/html/tximport.html), as it accounts for across-sample variability in expressed gene length, and makes it trivial to get your corresponding gene counts into a downstream DE tool like DESeq2, EdgeR, etc.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/530#issuecomment-638453196
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638385340:262,Integrability,protocol,protocol,262,"Hi @rfarouni ,. Thanks a lot for raising the issue.; It looks like a corner case with the custom barcode length and I'd have to push a hot-fix for it. Basically, it's failing in the initial sanity check stage where it assumes we can provide only one single-cell protocol type. Give me like half an hour to make the changes and I'll push the fix to the develop. If you can compile salmon from source that's great, otherwise I can also forward a linux portable binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638385340
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638385340:450,Modifiability,portab,portable,450,"Hi @rfarouni ,. Thanks a lot for raising the issue.; It looks like a corner case with the custom barcode length and I'd have to push a hot-fix for it. Basically, it's failing in the initial sanity check stage where it assumes we can provide only one single-cell protocol type. Give me like half an hour to make the changes and I'll push the fix to the develop. If you can compile salmon from source that's great, otherwise I can also forward a linux portable binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638385340
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638385340:190,Safety,sanity check,sanity check,190,"Hi @rfarouni ,. Thanks a lot for raising the issue.; It looks like a corner case with the custom barcode length and I'd have to push a hot-fix for it. Basically, it's failing in the initial sanity check stage where it assumes we can provide only one single-cell protocol type. Give me like half an hour to make the changes and I'll push the fix to the develop. If you can compile salmon from source that's great, otherwise I can also forward a linux portable binary.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638385340
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638388616:103,Testability,test,testing,103,"Oh one more thing, is it possible to share a few hundred reads for your experiment, just for some unit testing on my side?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638388616
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905:266,Integrability,protocol,protocol,266,"Seems to work on some of the test at my end, let me know if its still a problem. The command to use would be; ```; salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21; ```. One thing to note, since it's a 5' protocol, you might have to change `-lISR` to `-lISF` since the 5` protocol expects the single-cell reads from the forward strand, unlike 3' where we expect the reads from reverse. It should not be a problem for the guide/feature barcodes though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905:333,Integrability,protocol,protocol,333,"Seems to work on some of the test at my end, let me know if its still a problem. The command to use would be; ```; salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21; ```. One thing to note, since it's a 5' protocol, you might have to change `-lISR` to `-lISF` since the 5` protocol expects the single-cell reads from the forward strand, unlike 3' where we expect the reads from reverse. It should not be a problem for the guide/feature barcodes though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905:29,Testability,test,test,29,"Seems to work on some of the test at my end, let me know if its still a problem. The command to use would be; ```; salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21; ```. One thing to note, since it's a 5' protocol, you might have to change `-lISR` to `-lISF` since the 5` protocol expects the single-cell reads from the forward strand, unlike 3' where we expect the reads from reverse. It should not be a problem for the guide/feature barcodes though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905:482,Usability,guid,guide,482,"Seems to work on some of the test at my end, let me know if its still a problem. The command to use would be; ```; salmon alevin -l ISR --citeseq --barcodeLength 16 --umiLength 10 --end 5 --featureStart 19 --featureLength 21; ```. One thing to note, since it's a 5' protocol, you might have to change `-lISR` to `-lISF` since the 5` protocol expects the single-cell reads from the forward strand, unlike 3' where we expect the reads from reverse. It should not be a problem for the guide/feature barcodes though.",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638444905
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638487530:46,Modifiability,portab,portable,46,"Thanks @k3yavi !If you can forward me a Linux portable binary that would be great. Whenever I try to compile something on my computer, I fail half of the time . I have Ubuntu 18.04. I will ask permission to share with you part of the data and get back to you. Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes? And do you recommend using the `--naiveEqclass`; option when there are only 64 guide sequences as features?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638487530
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638487530:420,Usability,guid,guide,420,"Thanks @k3yavi !If you can forward me a Linux portable binary that would be great. Whenever I try to compile something on my computer, I fail half of the time . I have Ubuntu 18.04. I will ask permission to share with you part of the data and get back to you. Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes? And do you recommend using the `--naiveEqclass`; option when there are only 64 guide sequences as features?",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638487530
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:126,Availability,down,download,126,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:178,Availability,avail,available,178,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:1337,Availability,error,error,1337,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983:213,Deployability,release,release,213,"Hi @rfarouni ,. Please use [this](https://drive.google.com/file/d/11lav7dOQkn5VuSNZwC2CZUgx_eeXpBmx/view?usp=sharing) link to download a linux compatible binary, the fix will be available by default with the next release . > Also, does Alevin use 10x cell barcode whitelist internally to correct barcodes?. In our experiments, we find that, in expectation, the 10x generated experiments are clean enough that we don't need the 10x whitelisted barcode to be explicitly specified or used. > And do you recommend using the `--naiveEqclass` only 64 guide sequences as features ?. That's a very good question. Basically the answer lies in how complicated the UMI graph network is. Experiment with the antibody derived barcodes (ADT) with 20 protein panel, generally, doesn't need the `--naiveEqclass` mode UMI deduplication, unless the experiment is super deeply sequenced. However, for super low diversity like 4-8 barcodes e.g. for HTO like sample barcodes, the graphical network becomes exponentially hard to solve and significantly increases the running time for alevin. . In general, I'd recommend if you expect very low diversity in the number of barcodes in your experiment, use `--naiveEqclass` otherwise prefer avoiding it. Generally, the experiment with low diversity barcodes results in such a highly dense count matrix that a few error in UMI deduplication won't matter and you can tradeoff extra long running time with reasonable under/over UMI deduplicated counts. . _In short_, 64 guide sequences are relatively high diversity and I'd advise skipping `--naiveEqclass` in your command line argument. Hope it helps !",MatchSource.ISSUE_COMMENT,COMBINE-lab,salmon,v1.10.1,https://combine-lab.github.io/salmon,https://github.com/COMBINE-lab/salmon/issues/531#issuecomment-638576983
