id,quality_attribute,keyword,matched_word,match_idx,sentence,source,author,repo,version,wiki,url
https://github.com/broadinstitute/gatk/issues/5572:328,Deployability,update,updated,328,"We have two methods that compare variants for our tests:. `VariantContextTestUtils::assertEqualVariants` is doing string conversions to compare variants. This is not so good. `VariantContextTestUtils::assertVariantContextsAreEqual` may not do a complete comparison. Either way, these should be combined into a single method and updated to be a complete comparison.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5572
https://github.com/broadinstitute/gatk/issues/5572:50,Testability,test,tests,50,"We have two methods that compare variants for our tests:. `VariantContextTestUtils::assertEqualVariants` is doing string conversions to compare variants. This is not so good. `VariantContextTestUtils::assertVariantContextsAreEqual` may not do a complete comparison. Either way, these should be combined into a single method and updated to be a complete comparison.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5572
https://github.com/broadinstitute/gatk/issues/5572:84,Testability,assert,assertEqualVariants,84,"We have two methods that compare variants for our tests:. `VariantContextTestUtils::assertEqualVariants` is doing string conversions to compare variants. This is not so good. `VariantContextTestUtils::assertVariantContextsAreEqual` may not do a complete comparison. Either way, these should be combined into a single method and updated to be a complete comparison.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5572
https://github.com/broadinstitute/gatk/issues/5572:201,Testability,assert,assertVariantContextsAreEqual,201,"We have two methods that compare variants for our tests:. `VariantContextTestUtils::assertEqualVariants` is doing string conversions to compare variants. This is not so good. `VariantContextTestUtils::assertVariantContextsAreEqual` may not do a complete comparison. Either way, these should be combined into a single method and updated to be a complete comparison.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5572
https://github.com/broadinstitute/gatk/pull/5574:58,Deployability,release,release,58,Like #5485 but for tabix. (Build will fail until new Disq release is made.),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5574
https://github.com/broadinstitute/gatk/pull/5575:243,Availability,down,down,243,"Naive subsampling of the data in the ModelSegments MCMC (i.e., limiting per-segment and global quantities to 1000 points randomly sampled each MCMC iteration and rescaling likelihoods accordingly) was implemented in #3913 to bring WGS runtime down to reasonable levels. However, this sort of naive subsampling does not accurately preserve the posterior, which leads to some artifacts in posterior estimation. @MartonKN suspected that this negatively affected downstream performance in his caller, since weights of larger segments were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:459,Availability,down,downstream,459,"Naive subsampling of the data in the ModelSegments MCMC (i.e., limiting per-segment and global quantities to 1000 points randomly sampled each MCMC iteration and rescaling likelihoods accordingly) was implemented in #3913 to bring WGS runtime down to reasonable levels. However, this sort of naive subsampling does not accurately preserve the posterior, which leads to some artifacts in posterior estimation. @MartonKN suspected that this negatively affected downstream performance in his caller, since weights of larger segments were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:470,Performance,perform,performance,470,"Naive subsampling of the data in the ModelSegments MCMC (i.e., limiting per-segment and global quantities to 1000 points randomly sampled each MCMC iteration and rescaling likelihoods accordingly) was implemented in #3913 to bring WGS runtime down to reasonable levels. However, this sort of naive subsampling does not accurately preserve the posterior, which leads to some artifacts in posterior estimation. @MartonKN suspected that this negatively affected downstream performance in his caller, since weights of larger segments were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above referen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:1089,Performance,perform,perform,1089,"global quantities to 1000 points randomly sampled each MCMC iteration and rescaling likelihoods accordingly) was implemented in #3913 to bring WGS runtime down to reasonable levels. However, this sort of naive subsampling does not accurately preserve the posterior, which leads to some artifacts in posterior estimation. @MartonKN suspected that this negatively affected downstream performance in his caller, since weights of larger segments were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above reference, and 3) adding some hash-based caching functionality to both the batch/minibatch imp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:2549,Performance,perform,performance,2549,"ds an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above reference, and 3) adding some hash-based caching functionality to both the batch/minibatch implementations, as well as to the allele-fraction likelihood calculations (see related discussion in #2860). I also made a few miscellaneous improvements to code style, etc. This is a relatively sizable change and can rather dramatically change the number of segments remaining after smoothing, etc. (although primarily on small scales and probably well within the noise). I will rerun the TCGA SNP array evaluations to make sure there are no negative effects on performance from this change or those introduced in #5556. @LeeTL1220 should also run some tests. The branch might require some further tweaking based on the results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:1507,Security,expose,exposed,1507,"nts were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above reference, and 3) adding some hash-based caching functionality to both the batch/minibatch implementations, as well as to the allele-fraction likelihood calculations (see related discussion in #2860). I also made a few miscellaneous improvements to code style, etc. This is a relatively sizable change and can rather dramatically change the number of segments remaining after smoothing, etc. (although primarily on small scales and probably well within the noise). I will rerun the TCGA SNP array evaluations to make sure there are ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:1543,Security,expose,exposed,1543,"nts were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above reference, and 3) adding some hash-based caching functionality to both the batch/minibatch implementations, as well as to the allele-fraction likelihood calculations (see related discussion in #2860). I also made a few miscellaneous improvements to code style, etc. This is a relatively sizable change and can rather dramatically change the number of segments remaining after smoothing, etc. (although primarily on small scales and probably well within the noise). I will rerun the TCGA SNP array evaluations to make sure there are ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:2024,Security,hash,hash-based,2024,"ds an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above reference, and 3) adding some hash-based caching functionality to both the batch/minibatch implementations, as well as to the allele-fraction likelihood calculations (see related discussion in #2860). I also made a few miscellaneous improvements to code style, etc. This is a relatively sizable change and can rather dramatically change the number of segments remaining after smoothing, etc. (although primarily on small scales and probably well within the noise). I will rerun the TCGA SNP array evaluations to make sure there are no negative effects on performance from this change or those introduced in #5556. @LeeTL1220 should also run some tests. The branch might require some further tweaking based on the results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:1081,Testability,test,test,1081,"global quantities to 1000 points randomly sampled each MCMC iteration and rescaling likelihoods accordingly) was implemented in #3913 to bring WGS runtime down to reasonable levels. However, this sort of naive subsampling does not accurately preserve the posterior, which leads to some artifacts in posterior estimation. @MartonKN suspected that this negatively affected downstream performance in his caller, since weights of larger segments were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above reference, and 3) adding some hash-based caching functionality to both the batch/minibatch imp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:2640,Testability,test,tests,2640,"ds an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above reference, and 3) adding some hash-based caching functionality to both the batch/minibatch implementations, as well as to the allele-fraction likelihood calculations (see related discussion in #2860). I also made a few miscellaneous improvements to code style, etc. This is a relatively sizable change and can rather dramatically change the number of segments remaining after smoothing, etc. (although primarily on small scales and probably well within the noise). I will rerun the TCGA SNP array evaluations to make sure there are no negative effects on performance from this change or those introduced in #5556. @LeeTL1220 should also run some tests. The branch might require some further tweaking based on the results.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5575:1062,Usability,simpl,simple,1062,"global quantities to 1000 points randomly sampled each MCMC iteration and rescaling likelihoods accordingly) was implemented in #3913 to bring WGS runtime down to reasonable levels. However, this sort of naive subsampling does not accurately preserve the posterior, which leads to some artifacts in posterior estimation. @MartonKN suspected that this negatively affected downstream performance in his caller, since weights of larger segments were underestimated. . For example, the copy-ratio posterior widths should scale with the inverse square root of the number of copy-ratio bins in each segment. However, subsampling yields an artificial break at 1000 bins and screws up the scaling:. ![cr-ss](https://user-images.githubusercontent.com/11076296/51122629-417be180-17e8-11e9-9a8f-e17a5d0563f5.png). To fix this, I implemented minibatch slice sampling as described in http://proceedings.mlr.press/v33/dubois14.pdf. This uses early stopping of sampling as determined by a simple statistical test to perform approximate sampling of the posterior in a way that is more well behaved:. ![cr-mb](https://user-images.githubusercontent.com/11076296/51122680-61aba080-17e8-11e9-992a-f756a267d0ce.png). Note that the scaling levels off for larger segments, but the approximation can be made exact by taking the appropriate parameter to zero (here, this parameter is set to 0.1). However, since subsampling parameters were not exposed in the old code, I have not exposed the parameters for the approximation here. We can do this in a future PR if desired. Changing these parameters can affect runtime and results, but I've set them to reasonable values for now. The implementation involved 1) creating an abstract class to extract some common functionality shared with the old batch SliceSampler (which is now no longer used in production code), 2) implementing the MinibatchSliceSampler as described in the above reference, and 3) adding some hash-based caching functionality to both the batch/minibatch imp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5575
https://github.com/broadinstitute/gatk/pull/5576:58,Testability,Test,Tested,58,"Fixes https://github.com/broadinstitute/gatk/issues/4387. Tested manually. Before (no default read filters in doc):. <img width=""714"" alt=""screen shot 2019-01-14 at 10 38 26 am"" src=""https://user-images.githubusercontent.com/10062863/51122884-e26a9c80-17e8-11e9-82ef-e9105d2fb808.png"">. After (read filters included):. <img width=""682"" alt=""screen shot 2019-01-14 at 10 41 05 am"" src=""https://user-images.githubusercontent.com/10062863/51123000-2493de00-17e9-11e9-86e0-1d7fefa14ab4.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5576
https://github.com/broadinstitute/gatk/issues/5577:248,Deployability,update,update,248,The generated online doc currently doesn't include the names of the default annotations or annotation groups used by various tools. The values are already propagated from the annotation plugin to the freemarker map by Barclay; it should be easy to update the freemarker template to display these similar to the way we display default read filters.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5577
https://github.com/broadinstitute/gatk/issues/5577:186,Modifiability,plugin,plugin,186,The generated online doc currently doesn't include the names of the default annotations or annotation groups used by various tools. The values are already propagated from the annotation plugin to the freemarker map by Barclay; it should be easy to update the freemarker template to display these similar to the way we display default read filters.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5577
https://github.com/broadinstitute/gatk/pull/5578:60,Modifiability,refactor,refactor,60,@takutosato Here's another bug fix. After the big filtering refactor M2 filters will be unit-testable and these things will be easier to prevent. This fixes the last edge case mentioned by @byoo in the discussion of #5563.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5578
https://github.com/broadinstitute/gatk/pull/5578:93,Testability,test,testable,93,@takutosato Here's another bug fix. After the big filtering refactor M2 filters will be unit-testable and these things will be easier to prevent. This fixes the last edge case mentioned by @byoo in the discussion of #5563.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5578
https://github.com/broadinstitute/gatk/pull/5580:87,Testability,test,tested,87,@takutosato Unfortunately we still have to maintain two wdls. Just a few weeks more. I tested this on FC.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5580
https://github.com/broadinstitute/gatk/pull/5581:235,Deployability,update,updated,235,This is for [Issue 5397](https://github.com/broadinstitute/gatk/pull/5397). Got the tests to pass by generating new expected results for the following failing tests and use the GenomicsDBImports folder for expected results as that was updated as part of [PR 5170](https://github.com/broadinstitute/gatk/pull/5471). * testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData; * testGenomicsDBImportFileInputs_newMQ. Note that this PR also has merged with master.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5581
https://github.com/broadinstitute/gatk/pull/5581:84,Testability,test,tests,84,This is for [Issue 5397](https://github.com/broadinstitute/gatk/pull/5397). Got the tests to pass by generating new expected results for the following failing tests and use the GenomicsDBImports folder for expected results as that was updated as part of [PR 5170](https://github.com/broadinstitute/gatk/pull/5471). * testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData; * testGenomicsDBImportFileInputs_newMQ. Note that this PR also has merged with master.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5581
https://github.com/broadinstitute/gatk/pull/5581:159,Testability,test,tests,159,This is for [Issue 5397](https://github.com/broadinstitute/gatk/pull/5397). Got the tests to pass by generating new expected results for the following failing tests and use the GenomicsDBImports folder for expected results as that was updated as part of [PR 5170](https://github.com/broadinstitute/gatk/pull/5471). * testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData; * testGenomicsDBImportFileInputs_newMQ. Note that this PR also has merged with master.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5581
https://github.com/broadinstitute/gatk/pull/5581:317,Testability,test,testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData,317,This is for [Issue 5397](https://github.com/broadinstitute/gatk/pull/5397). Got the tests to pass by generating new expected results for the following failing tests and use the GenomicsDBImports folder for expected results as that was updated as part of [PR 5170](https://github.com/broadinstitute/gatk/pull/5471). * testGenomicsDBImportFileInputsAgainstCombineGVCFWithNonDiploidData; * testGenomicsDBImportFileInputs_newMQ. Note that this PR also has merged with master.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5581
https://github.com/broadinstitute/gatk/issues/5582:157,Availability,error,error,157,"## Feature request. ### Tool(s) or class(es) involved; VariantFiltration. ### Description; While running variantFiltration, I have encountered the following error:; ```; java.lang.NumberFormatException: For input string: ""nan""; at sun.misc.FloatingDecimal.readJavaFormatString(FloatingDecimal.java:2043); at sun.misc.FloatingDecimal.parseDouble(FloatingDecimal.java:110); at java.lang.Double.parseDouble(Double.java:538); at org.apache.commons.jexl2.JexlArithmetic.toDouble(JexlArithmetic.java:1016); at org.apache.commons.jexl2.JexlArithmetic.compare(JexlArithmetic.java:699); at org.apache.commons.jexl2.JexlArithmetic.lessThan(JexlArithmetic.java:774); at org.apache.commons.jexl2.Interpreter.visit(Interpreter.java:967); at org.apache.commons.jexl2.parser.ASTLTNode.jjtAccept(ASTLTNode.java:18); at org.apache.commons.jexl2.Interpreter.interpret(Interpreter.java:232); at org.apache.commons.jexl2.ExpressionImpl.evaluate(ExpressionImpl.java:65); at htsjdk.variant.variantcontext.JEXLMap.evaluateExpression(JEXLMap.java:186); at htsjdk.variant.variantcontext.JEXLMap.get(JEXLMap.java:95); at htsjdk.variant.variantcontext.JEXLMap.get(JEXLMap.java:15); at htsjdk.variant.variantcontext.VariantContextUtils.match(VariantContextUtils.java:338); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.matchesFilter(VariantFiltration.java:379); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.filter(VariantFiltration.java:338); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.apply(VariantFiltration.java:298); at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:153); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5582
https://github.com/broadinstitute/gatk/issues/5582:2251,Integrability,wrap,wrapAndCopyInto,2251,hellbender.tools.walkers.filters.VariantFiltration.matchesFilter(VariantFiltration.java:379); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.filter(VariantFiltration.java:338); at org.broadinstitute.hellbender.tools.walkers.filters.VariantFiltration.apply(VariantFiltration.java:298); at org.broadinstitute.hellbender.engine.VariantWalkerBase.lambda$traverse$0(VariantWalkerBase.java:153); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:151); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hel,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5582
https://github.com/broadinstitute/gatk/issues/5582:3406,Modifiability,variab,variable,3406,"$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalkerBase.traverse(VariantWalkerBase.java:151); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:192); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289); Using GATK jar /opt/conda/share/gatk4-4.0.11.0-0/gatk-package-4.0.11.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=8 -Xms4G -Xmx6G -Djava.io.tmpdir=. -jar /opt/conda/share/gatk4-4.0.11.0-0/gatk-package-4.0.11.0-local.jar VariantFiltration -V indels.vcf.gz -O indels.flt.vcf.gz --filter-expression QD < 2.0 --filter-name lowQD --filter-expression MQ < 20.0 --filter-name lowMQ --filter-expression FS > 200.0 --filter-name highFS --filter-expression SOR > 10.0 --filter-name highSOR --filter-expression ReadPosRankSum < -20.0 --filter-name lowReadPosRankSum --filter-expression InbreedingCoeff < -0.8 --filter-name lowInbreedingCoeff; ```; In an upstream stage I have passed my VCF through bcftools. Some digging led me to find that bcftools (view, norm, etc) will apparently convert fields such as ""MQ=NaN"" to ""MQ=nan"" when writing VCF. This issue has been reported to the bcftools team but has been closed as ""wontfix"", [see here](ht",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5582
https://github.com/broadinstitute/gatk/pull/5585:316,Deployability,update,updated,316,"* Updating htsjdk 2.18.1 -> 2.18.2; * Remove deprecated method use; * Changing IntervalUtilsUnitTest due to changes in IntervalList; * IntervalList now rejects certain invalid intervals that it previously didn't and throw IllegalArgumentException.; * This ends up changing which exceptions are thrown in some cases, updated some tests to accept both MalformedFile and MalforedGenomeLoc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5585
https://github.com/broadinstitute/gatk/pull/5585:329,Testability,test,tests,329,"* Updating htsjdk 2.18.1 -> 2.18.2; * Remove deprecated method use; * Changing IntervalUtilsUnitTest due to changes in IntervalList; * IntervalList now rejects certain invalid intervals that it previously didn't and throw IllegalArgumentException.; * This ends up changing which exceptions are thrown in some cases, updated some tests to accept both MalformedFile and MalforedGenomeLoc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5585
https://github.com/broadinstitute/gatk/pull/5587:577,Availability,error,error,577,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587
https://github.com/broadinstitute/gatk/pull/5587:299,Deployability,release,release,299,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587
https://github.com/broadinstitute/gatk/pull/5587:671,Deployability,release,release,671,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587
https://github.com/broadinstitute/gatk/pull/5587:160,Modifiability,refactor,refactoring,160,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587
https://github.com/broadinstitute/gatk/pull/5587:440,Modifiability,refactor,refactoring,440,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587
https://github.com/broadinstitute/gatk/pull/5587:246,Performance,perform,performance,246,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587
https://github.com/broadinstitute/gatk/pull/5587:492,Testability,log,logging,492,See [Issue 5277 - Migrate to org.genomicsdb fork](https://github.com/broadinstitute/gatk/issues/5277). . The first genomicsdb 1.0.0.beta jar consists of only a refactoring of all the packages to org.genomicsdb. Note that this pass should have no performance implications compared to the last [Intel release](https://mvnrepository.com/artifact/com.intel/genomicsdb/0.10.2-proto-3.0.0-beta-1+90dad1af8ce0e4d) as there is no change other than refactoring. Issues [5568-buffer resizing excessive logging](https://github.com/broadinstitute/gatk/issues/5568) and [5342-file synching error](https://github.com/broadinstitute/gatk/issues/5342) will both be addressed in the next release.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5587
https://github.com/broadinstitute/gatk/pull/5589:0,Deployability,Upgrade,Upgraded,0,Upgraded the Funcotator WDL to WDL 1.0 and implemented new features:; * Automatic disk size calculation; * More readable default parameter settings; * Use of WDL 1.0 features such as defaults and true/false operators; * Using GATK to pull the default datasource rather than wget,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5589
https://github.com/broadinstitute/gatk/pull/5594:400,Availability,down,downsampled,400,"Fixes https://github.com/broadinstitute/gatk/issues/4768. The ReservoirDownsampler currently declares reads to be finalized immediately after they're submitted, but in order to guaranty that every read has equal probability of being discarded, it should consume the entire stream of input items before declaring any item to be finalized. When used with a ReadsDownsamplingIterator, no reads are ever downsampled because the iterator populates it's internal cache by eagerly consuming finalized reads as soon as they become available. Also, PositionalDownsampler doesn't reset it's internal ReservoirDownsampler's state correctly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594
https://github.com/broadinstitute/gatk/pull/5594:523,Availability,avail,available,523,"Fixes https://github.com/broadinstitute/gatk/issues/4768. The ReservoirDownsampler currently declares reads to be finalized immediately after they're submitted, but in order to guaranty that every read has equal probability of being discarded, it should consume the entire stream of input items before declaring any item to be finalized. When used with a ReadsDownsamplingIterator, no reads are ever downsampled because the iterator populates it's internal cache by eagerly consuming finalized reads as soon as they become available. Also, PositionalDownsampler doesn't reset it's internal ReservoirDownsampler's state correctly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594
https://github.com/broadinstitute/gatk/pull/5594:457,Performance,cache,cache,457,"Fixes https://github.com/broadinstitute/gatk/issues/4768. The ReservoirDownsampler currently declares reads to be finalized immediately after they're submitted, but in order to guaranty that every read has equal probability of being discarded, it should consume the entire stream of input items before declaring any item to be finalized. When used with a ReadsDownsamplingIterator, no reads are ever downsampled because the iterator populates it's internal cache by eagerly consuming finalized reads as soon as they become available. Also, PositionalDownsampler doesn't reset it's internal ReservoirDownsampler's state correctly.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5594
https://github.com/broadinstitute/gatk/pull/5595:122,Modifiability,refactor,refactoring,122,@LeeTL1220 Can you review? This fixes your bug. I'm not putting in a regression test yet because i the upcoming filtering refactoring it will become much easier to unit test for bugs like this. @madduran Yours too.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5595
https://github.com/broadinstitute/gatk/pull/5595:80,Testability,test,test,80,@LeeTL1220 Can you review? This fixes your bug. I'm not putting in a regression test yet because i the upcoming filtering refactoring it will become much easier to unit test for bugs like this. @madduran Yours too.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5595
https://github.com/broadinstitute/gatk/pull/5595:169,Testability,test,test,169,@LeeTL1220 Can you review? This fixes your bug. I'm not putting in a regression test yet because i the upcoming filtering refactoring it will become much easier to unit test for bugs like this. @madduran Yours too.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5595
https://github.com/broadinstitute/gatk/pull/5597:4,Deployability,update,updated,4,Has updated optical duplicate finding code for MarkDuplicatesSpark.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5597
https://github.com/broadinstitute/gatk/issues/5600:723,Deployability,toggle,toggle,723,"Currently there are number of boolean arguments in GATKTool that are set by default to `true`. Unfortunately, the current syntax for changing that value on the command line is to write ""--argument-name false"" (eg. `--create-output-bam-index false`) which is confusing an counterintuitive. This format made more sense for arguments when the syntax in picard/gatk3 used to be ""ARGUMENT=FALSE"". I propose that these arguments have their statements inverted wherever possible so that the user should always be negating some option that was perviously true (eg. `--disable-output-bam-index-creation` or some other alternative). Given the new argument input format this makes more intuitive sense and helps differentiate between toggle arguments and arguments with inputs on the command line without having to remember what the default value is. . A change like this would affect a number of old/bedrock arguments in the engine like, `--create-output-variant-index`, `--add-output-sam-program-record`, and `--add-output-vcf-command-line` to name a few. Since there are many arguments following this pattern perhaps the overhead from making a change like this isn't worth it. . (Also applies to changes in GATKSparkTool #5574)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5600
https://github.com/broadinstitute/gatk/issues/5600:675,Usability,intuit,intuitive,675,"Currently there are number of boolean arguments in GATKTool that are set by default to `true`. Unfortunately, the current syntax for changing that value on the command line is to write ""--argument-name false"" (eg. `--create-output-bam-index false`) which is confusing an counterintuitive. This format made more sense for arguments when the syntax in picard/gatk3 used to be ""ARGUMENT=FALSE"". I propose that these arguments have their statements inverted wherever possible so that the user should always be negating some option that was perviously true (eg. `--disable-output-bam-index-creation` or some other alternative). Given the new argument input format this makes more intuitive sense and helps differentiate between toggle arguments and arguments with inputs on the command line without having to remember what the default value is. . A change like this would affect a number of old/bedrock arguments in the engine like, `--create-output-variant-index`, `--add-output-sam-program-record`, and `--add-output-vcf-command-line` to name a few. Since there are many arguments following this pattern perhaps the overhead from making a change like this isn't worth it. . (Also applies to changes in GATKSparkTool #5574)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5600
https://github.com/broadinstitute/gatk/pull/5601:33,Deployability,Update,Update,33,"- Clarify tool documentation:; - Update `joint posteriors (JL)` to `joint posteriors (JP)`; - Remove statistical notes and provide link to GATK Article#11074 for background and math; - Consolidate Notes and Caveats sections; - Clarify at top the three different sources of priors and tool behavior regarding these; - Clarify for family priors the tool only considers trio groups; - Add Laura's comment that recent updates allow the tool to appropriately apply priors to indels; - Change logger.info to logger.warn for situation where trio pedigree file is incomplete; - Note that in this situation, in the absence of other refinement, the results are identical to the input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601
https://github.com/broadinstitute/gatk/pull/5601:414,Deployability,update,updates,414,"- Clarify tool documentation:; - Update `joint posteriors (JL)` to `joint posteriors (JP)`; - Remove statistical notes and provide link to GATK Article#11074 for background and math; - Consolidate Notes and Caveats sections; - Clarify at top the three different sources of priors and tool behavior regarding these; - Clarify for family priors the tool only considers trio groups; - Add Laura's comment that recent updates allow the tool to appropriately apply priors to indels; - Change logger.info to logger.warn for situation where trio pedigree file is incomplete; - Note that in this situation, in the absence of other refinement, the results are identical to the input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601
https://github.com/broadinstitute/gatk/pull/5601:487,Testability,log,logger,487,"- Clarify tool documentation:; - Update `joint posteriors (JL)` to `joint posteriors (JP)`; - Remove statistical notes and provide link to GATK Article#11074 for background and math; - Consolidate Notes and Caveats sections; - Clarify at top the three different sources of priors and tool behavior regarding these; - Clarify for family priors the tool only considers trio groups; - Add Laura's comment that recent updates allow the tool to appropriately apply priors to indels; - Change logger.info to logger.warn for situation where trio pedigree file is incomplete; - Note that in this situation, in the absence of other refinement, the results are identical to the input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601
https://github.com/broadinstitute/gatk/pull/5601:502,Testability,log,logger,502,"- Clarify tool documentation:; - Update `joint posteriors (JL)` to `joint posteriors (JP)`; - Remove statistical notes and provide link to GATK Article#11074 for background and math; - Consolidate Notes and Caveats sections; - Clarify at top the three different sources of priors and tool behavior regarding these; - Clarify for family priors the tool only considers trio groups; - Add Laura's comment that recent updates allow the tool to appropriately apply priors to indels; - Change logger.info to logger.warn for situation where trio pedigree file is incomplete; - Note that in this situation, in the absence of other refinement, the results are identical to the input",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5601
https://github.com/broadinstitute/gatk/pull/5602:49,Energy Efficiency,reduce,reduced,49,"Some travis jobs are still failing even with the reduced set of CRAN mirrors. This is a possible alternative solution that restores the previous set of fallback repos, but relaxes the treatment of remote warnings. Might be overkill but it seems to work. See `R_REMOTES_NO_ERRORS_FROM_WARNINGS ` under https://github.com/r-lib/remotes#environment-variables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5602
https://github.com/broadinstitute/gatk/pull/5602:346,Modifiability,variab,variables,346,"Some travis jobs are still failing even with the reduced set of CRAN mirrors. This is a possible alternative solution that restores the previous set of fallback repos, but relaxes the treatment of remote warnings. Might be overkill but it seems to work. See `R_REMOTES_NO_ERRORS_FROM_WARNINGS ` under https://github.com/r-lib/remotes#environment-variables.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5602
https://github.com/broadinstitute/gatk/pull/5603:44,Deployability,release,release,44,Removing the beta tag in advance of the 4.1 release. . Resolves #4675,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5603
https://github.com/broadinstitute/gatk/pull/5604:207,Deployability,update,updates,207,Added WDLs to subset the INFO field annotations to only the allele frequency fields. Also modified the `mutect2` wdl to produce a gnomAD with only AF fields. @davidbenjamin - you may want to double-check my updates to the M2 support wdl.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5604
https://github.com/broadinstitute/gatk/issues/5605:820,Performance,perform,performs,820,"Funcotator needs a way to read in a locatable XSV but also have it match alleles before adding an annotation (in the same way that VCF data sources require matching alleles). This would allow transformation of large VCF data sources (such as gnomAD) into more easily compressed and read XSV files. . After implementation this needs to be benchmarked for gnomAD 2.1 Alelle Frequency VCFs and if the XSVs are faster, the VCFs should be transformed into XSV files and uploaded to the Funcotator data source bucket. The current speed of gnomAD annotation on the canonical hg19 file is ~2500 variants / minute after the first couple of minutes. Full stats (as pulled from the `ProgressMeter` output):; MIN: 1674.8; MAX: 2996.8; MEAN: 2459.4; MEDIAN: 2344.3; STDEV: 217.057; NUM DATA POINTS: 573. For the same file, Oncotator performs at ~12000 variants/minute with all the same data sources except gnomAD. . Even with gnomAD there is no reason Funcotator should be slower than Oncotator.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5605
https://github.com/broadinstitute/gatk/issues/5605:338,Testability,benchmark,benchmarked,338,"Funcotator needs a way to read in a locatable XSV but also have it match alleles before adding an annotation (in the same way that VCF data sources require matching alleles). This would allow transformation of large VCF data sources (such as gnomAD) into more easily compressed and read XSV files. . After implementation this needs to be benchmarked for gnomAD 2.1 Alelle Frequency VCFs and if the XSVs are faster, the VCFs should be transformed into XSV files and uploaded to the Funcotator data source bucket. The current speed of gnomAD annotation on the canonical hg19 file is ~2500 variants / minute after the first couple of minutes. Full stats (as pulled from the `ProgressMeter` output):; MIN: 1674.8; MAX: 2996.8; MEAN: 2459.4; MEDIAN: 2344.3; STDEV: 217.057; NUM DATA POINTS: 573. For the same file, Oncotator performs at ~12000 variants/minute with all the same data sources except gnomAD. . Even with gnomAD there is no reason Funcotator should be slower than Oncotator.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5605
https://github.com/broadinstitute/gatk/pull/5607:821,Modifiability,refactor,refactored,821,"More concrete runtime numbers are forthcoming but the profiler shows the following. Note that these numbers are generated on this branch hanging off of master ca. November and does not include the other optimizations to this part of the code that have been made recently.; Before:; <img width=""1068"" alt=""screen shot 2019-01-25 at 2 59 53 pm"" src=""https://user-images.githubusercontent.com/16102845/51772199-6c541880-20b9-11e9-8823-7249e7f4d874.png"">; ; After: ; <img width=""1092"" alt=""screen shot 2019-01-25 at 3 00 05 pm"" src=""https://user-images.githubusercontent.com/16102845/51772174-57778500-20b9-11e9-9d74-9f93d76358a0.png"">. Beyond the tests that I have written explicitly to illuminate discrepancies, I have run HaplotypeCaller in GVCF mode on the input bams in large and explicitly checked for places where the refactored method mismatches from the previous code and it appears to be matching over a wide range of cases, there probably could be more. . Resolves #5488",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5607
https://github.com/broadinstitute/gatk/pull/5607:203,Performance,optimiz,optimizations,203,"More concrete runtime numbers are forthcoming but the profiler shows the following. Note that these numbers are generated on this branch hanging off of master ca. November and does not include the other optimizations to this part of the code that have been made recently.; Before:; <img width=""1068"" alt=""screen shot 2019-01-25 at 2 59 53 pm"" src=""https://user-images.githubusercontent.com/16102845/51772199-6c541880-20b9-11e9-8823-7249e7f4d874.png"">; ; After: ; <img width=""1092"" alt=""screen shot 2019-01-25 at 3 00 05 pm"" src=""https://user-images.githubusercontent.com/16102845/51772174-57778500-20b9-11e9-9d74-9f93d76358a0.png"">. Beyond the tests that I have written explicitly to illuminate discrepancies, I have run HaplotypeCaller in GVCF mode on the input bams in large and explicitly checked for places where the refactored method mismatches from the previous code and it appears to be matching over a wide range of cases, there probably could be more. . Resolves #5488",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5607
https://github.com/broadinstitute/gatk/pull/5607:644,Testability,test,tests,644,"More concrete runtime numbers are forthcoming but the profiler shows the following. Note that these numbers are generated on this branch hanging off of master ca. November and does not include the other optimizations to this part of the code that have been made recently.; Before:; <img width=""1068"" alt=""screen shot 2019-01-25 at 2 59 53 pm"" src=""https://user-images.githubusercontent.com/16102845/51772199-6c541880-20b9-11e9-8823-7249e7f4d874.png"">; ; After: ; <img width=""1092"" alt=""screen shot 2019-01-25 at 3 00 05 pm"" src=""https://user-images.githubusercontent.com/16102845/51772174-57778500-20b9-11e9-9d74-9f93d76358a0.png"">. Beyond the tests that I have written explicitly to illuminate discrepancies, I have run HaplotypeCaller in GVCF mode on the input bams in large and explicitly checked for places where the refactored method mismatches from the previous code and it appears to be matching over a wide range of cases, there probably could be more. . Resolves #5488",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5607
https://github.com/broadinstitute/gatk/pull/5608:139,Availability,error,error,139,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608
https://github.com/broadinstitute/gatk/pull/5608:198,Availability,error,error,198,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608
https://github.com/broadinstitute/gatk/pull/5608:362,Availability,error,error,362,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608
https://github.com/broadinstitute/gatk/pull/5608:417,Availability,error,error,417,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608
https://github.com/broadinstitute/gatk/pull/5608:60,Integrability,message,messages,60,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608
https://github.com/broadinstitute/gatk/pull/5608:390,Integrability,message,messages,390,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608
https://github.com/broadinstitute/gatk/pull/5608:260,Modifiability,variab,variable,260,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608
https://github.com/broadinstitute/gatk/pull/5608:378,Testability,log,log,378,"This addresses issues #5568 and #5342.; #5568 Buffer resize messages are now turned on only for Debug builds.; #5342: Added better general error reporting for system commands. For the file synching error in question, implemented a workaround. With environment variable - TILEDB_DISABLE_FILE_LOCKING - set to true or 1, there is no file locking and file synching error will only log warning messages and not return an error. Hopefully, this will mitigate the issues on NFS and CIFS.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5608
https://github.com/broadinstitute/gatk/issues/5610:74,Testability,test,tests,74,"Now that MarkDuplicatesSpark matches Picard MarkDuplicates, we should add tests to ensure that it stays that way.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5610
https://github.com/broadinstitute/gatk/issues/5611:0,Deployability,Update,Update,0,Update docs for Funcotator. Tool documentation and GencodeFuncotationFactory / GencodeFuncotation documents (to create a proper spec. for the gencode annotations).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5611
https://github.com/broadinstitute/gatk/pull/5612:237,Deployability,release,release,237,"Ops fixed the bad blacklist in broad-references, but this means the old file is no longer there. @ldgauthier or @jsotobroad Can you please do a quick review? @droazen It would be really awesome if this (tiny!) commit could end up in the release.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5612
https://github.com/broadinstitute/gatk/pull/5614:131,Availability,down,download,131,"- Released new data sources to google bucket and FTP site for both somatic and germline (clinical pipeline); - Updated data source download URL to point to the bucket for v1.6.20190124; - Updated minimum version of data sources to v1.6.20190124. With the release and these changes, the following issues are addressed:. Fixes #5259 ; Fixes #5428 ; Fixes #5429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5614
https://github.com/broadinstitute/gatk/pull/5614:2,Deployability,Release,Released,2,"- Released new data sources to google bucket and FTP site for both somatic and germline (clinical pipeline); - Updated data source download URL to point to the bucket for v1.6.20190124; - Updated minimum version of data sources to v1.6.20190124. With the release and these changes, the following issues are addressed:. Fixes #5259 ; Fixes #5428 ; Fixes #5429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5614
https://github.com/broadinstitute/gatk/pull/5614:98,Deployability,pipeline,pipeline,98,"- Released new data sources to google bucket and FTP site for both somatic and germline (clinical pipeline); - Updated data source download URL to point to the bucket for v1.6.20190124; - Updated minimum version of data sources to v1.6.20190124. With the release and these changes, the following issues are addressed:. Fixes #5259 ; Fixes #5428 ; Fixes #5429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5614
https://github.com/broadinstitute/gatk/pull/5614:111,Deployability,Update,Updated,111,"- Released new data sources to google bucket and FTP site for both somatic and germline (clinical pipeline); - Updated data source download URL to point to the bucket for v1.6.20190124; - Updated minimum version of data sources to v1.6.20190124. With the release and these changes, the following issues are addressed:. Fixes #5259 ; Fixes #5428 ; Fixes #5429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5614
https://github.com/broadinstitute/gatk/pull/5614:188,Deployability,Update,Updated,188,"- Released new data sources to google bucket and FTP site for both somatic and germline (clinical pipeline); - Updated data source download URL to point to the bucket for v1.6.20190124; - Updated minimum version of data sources to v1.6.20190124. With the release and these changes, the following issues are addressed:. Fixes #5259 ; Fixes #5428 ; Fixes #5429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5614
https://github.com/broadinstitute/gatk/pull/5614:255,Deployability,release,release,255,"- Released new data sources to google bucket and FTP site for both somatic and germline (clinical pipeline); - Updated data source download URL to point to the bucket for v1.6.20190124; - Updated minimum version of data sources to v1.6.20190124. With the release and these changes, the following issues are addressed:. Fixes #5259 ; Fixes #5428 ; Fixes #5429",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5614
https://github.com/broadinstitute/gatk/pull/5616:684,Integrability,Depend,Depends,684,"These methods were showing up as time syncs on the profile for #5607 ; <img width=""982"" alt=""screen shot 2019-01-28 at 3 44 55 pm"" src=""https://user-images.githubusercontent.com/16102845/51868464-1e8b1a80-231c-11e9-8221-5424f31cbdde.png"">; <img width=""980"" alt=""screen shot 2019-01-28 at 3 44 41 pm"" src=""https://user-images.githubusercontent.com/16102845/51868479-2480fb80-231c-11e9-9e51-8f66d11e2244.png"">. I ran these a few times vs. its root branch, its probably a small improvement in runtime over this chr15 snippet of an exome on my local machine:; #5607:; ```; 2m57.516s, 2m56.677s, 3m2.531s, 3m4.116s; ```; This branch:; ```; 2m50.112s, 2m46.981s, 2m43.764s, 2m40.806s; ```. Depends On #5607",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5616
https://github.com/broadinstitute/gatk/pull/5617:13,Deployability,update,update,13,We forgot to update this when travis migrated. We're seeing old output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5617
https://github.com/broadinstitute/gatk/issues/5618:664,Energy Efficiency,reduce,reduced,664,"In doing continued profiling of the HaplotypeCaller GVCF mode I have observed that somewhere in the range of 12% of our overall runtime (after i've made my other optimizations) is spent in `VariantContextBuilder.make()` upon further investigation I have noticed that we are currently building a VariantContext object for each pileup in `ReferenceConfidenceModel.calculateReferenceConfidence()`. This means that we are building a unique VariantContext object for essentially every spot on the genome. VariantContext object building represents a significant overhead in terms of validation and construction and memory usage. I suspect that if we were to create some reduced object without as much overhead we could save ourselves a lot of trouble time and memory merging these things. Unfortunately I think the merging of these context objects happens in the GVCF writer which means it won't be a trivial change to make to the engine. Perhaps it is worth investigating what can be done to this code, as it represents another size-able chunk of speedup if we can squash it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5618
https://github.com/broadinstitute/gatk/issues/5618:162,Performance,optimiz,optimizations,162,"In doing continued profiling of the HaplotypeCaller GVCF mode I have observed that somewhere in the range of 12% of our overall runtime (after i've made my other optimizations) is spent in `VariantContextBuilder.make()` upon further investigation I have noticed that we are currently building a VariantContext object for each pileup in `ReferenceConfidenceModel.calculateReferenceConfidence()`. This means that we are building a unique VariantContext object for essentially every spot on the genome. VariantContext object building represents a significant overhead in terms of validation and construction and memory usage. I suspect that if we were to create some reduced object without as much overhead we could save ourselves a lot of trouble time and memory merging these things. Unfortunately I think the merging of these context objects happens in the GVCF writer which means it won't be a trivial change to make to the engine. Perhaps it is worth investigating what can be done to this code, as it represents another size-able chunk of speedup if we can squash it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5618
https://github.com/broadinstitute/gatk/issues/5618:577,Security,validat,validation,577,"In doing continued profiling of the HaplotypeCaller GVCF mode I have observed that somewhere in the range of 12% of our overall runtime (after i've made my other optimizations) is spent in `VariantContextBuilder.make()` upon further investigation I have noticed that we are currently building a VariantContext object for each pileup in `ReferenceConfidenceModel.calculateReferenceConfidence()`. This means that we are building a unique VariantContext object for essentially every spot on the genome. VariantContext object building represents a significant overhead in terms of validation and construction and memory usage. I suspect that if we were to create some reduced object without as much overhead we could save ourselves a lot of trouble time and memory merging these things. Unfortunately I think the merging of these context objects happens in the GVCF writer which means it won't be a trivial change to make to the engine. Perhaps it is worth investigating what can be done to this code, as it represents another size-able chunk of speedup if we can squash it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5618
https://github.com/broadinstitute/gatk/issues/5625:107,Modifiability,refactor,refactoring,107,"There is some duplication of code to handle CR-only, AF-only, and CR+AF that could be eliminated with some refactoring.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5625
https://github.com/broadinstitute/gatk/issues/5626:205,Performance,perform,perform,205,"There are a few heuristics in allele-fraction/joint segmentation, namely: 1) only the first het in each copy-ratio bin is used, and 2) copy-ratio bins with no hets are assigned AAF = 0.5. We could instead perform a simple estimation of MAF in each bin using all of the hets, or something along those lines. Alternatively, although kernel segmentation doesn't require it, we could also investigate whether simply folding the AAF suffices. Probably OK to ignore reference bias in both of these methods. Hopefully any AF concordance added in #4122 will be able to discern the level of improvement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5626
https://github.com/broadinstitute/gatk/issues/5626:215,Usability,simpl,simple,215,"There are a few heuristics in allele-fraction/joint segmentation, namely: 1) only the first het in each copy-ratio bin is used, and 2) copy-ratio bins with no hets are assigned AAF = 0.5. We could instead perform a simple estimation of MAF in each bin using all of the hets, or something along those lines. Alternatively, although kernel segmentation doesn't require it, we could also investigate whether simply folding the AAF suffices. Probably OK to ignore reference bias in both of these methods. Hopefully any AF concordance added in #4122 will be able to discern the level of improvement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5626
https://github.com/broadinstitute/gatk/issues/5626:405,Usability,simpl,simply,405,"There are a few heuristics in allele-fraction/joint segmentation, namely: 1) only the first het in each copy-ratio bin is used, and 2) copy-ratio bins with no hets are assigned AAF = 0.5. We could instead perform a simple estimation of MAF in each bin using all of the hets, or something along those lines. Alternatively, although kernel segmentation doesn't require it, we could also investigate whether simply folding the AAF suffices. Probably OK to ignore reference bias in both of these methods. Hopefully any AF concordance added in #4122 will be able to discern the level of improvement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5626
https://github.com/broadinstitute/gatk/issues/5629:429,Deployability,update,update,429,"## Feature request. ### Tool(s) or class(es) involved; M2 PoN Creation. ### Description; There is no progress meter when running `CreateSomaticPanelOfNormals`. This makes debugging harder and the tool could be accidentally identified as frozen. ### Proposed solution; `final Consumer<Locatable> progressUpdater,` as a parameter to the backend class.; The CLI ( `CreateSomaticPanelOfNormals`) can just pass in `l -> progressMeter.update(l)` as long as the CLI extends GATKTool.; When you want to disable the progress meter, you can simply pass in: `l -> {}`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5629
https://github.com/broadinstitute/gatk/issues/5629:110,Energy Efficiency,meter,meter,110,"## Feature request. ### Tool(s) or class(es) involved; M2 PoN Creation. ### Description; There is no progress meter when running `CreateSomaticPanelOfNormals`. This makes debugging harder and the tool could be accidentally identified as frozen. ### Proposed solution; `final Consumer<Locatable> progressUpdater,` as a parameter to the backend class.; The CLI ( `CreateSomaticPanelOfNormals`) can just pass in `l -> progressMeter.update(l)` as long as the CLI extends GATKTool.; When you want to disable the progress meter, you can simply pass in: `l -> {}`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5629
https://github.com/broadinstitute/gatk/issues/5629:516,Energy Efficiency,meter,meter,516,"## Feature request. ### Tool(s) or class(es) involved; M2 PoN Creation. ### Description; There is no progress meter when running `CreateSomaticPanelOfNormals`. This makes debugging harder and the tool could be accidentally identified as frozen. ### Proposed solution; `final Consumer<Locatable> progressUpdater,` as a parameter to the backend class.; The CLI ( `CreateSomaticPanelOfNormals`) can just pass in `l -> progressMeter.update(l)` as long as the CLI extends GATKTool.; When you want to disable the progress meter, you can simply pass in: `l -> {}`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5629
https://github.com/broadinstitute/gatk/issues/5629:459,Modifiability,extend,extends,459,"## Feature request. ### Tool(s) or class(es) involved; M2 PoN Creation. ### Description; There is no progress meter when running `CreateSomaticPanelOfNormals`. This makes debugging harder and the tool could be accidentally identified as frozen. ### Proposed solution; `final Consumer<Locatable> progressUpdater,` as a parameter to the backend class.; The CLI ( `CreateSomaticPanelOfNormals`) can just pass in `l -> progressMeter.update(l)` as long as the CLI extends GATKTool.; When you want to disable the progress meter, you can simply pass in: `l -> {}`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5629
https://github.com/broadinstitute/gatk/issues/5629:531,Usability,simpl,simply,531,"## Feature request. ### Tool(s) or class(es) involved; M2 PoN Creation. ### Description; There is no progress meter when running `CreateSomaticPanelOfNormals`. This makes debugging harder and the tool could be accidentally identified as frozen. ### Proposed solution; `final Consumer<Locatable> progressUpdater,` as a parameter to the backend class.; The CLI ( `CreateSomaticPanelOfNormals`) can just pass in `l -> progressMeter.update(l)` as long as the CLI extends GATKTool.; When you want to disable the progress meter, you can simply pass in: `l -> {}`",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5629
https://github.com/broadinstitute/gatk/issues/5631:1254,Availability,avail,available,1254, runs of these tools) when using NIO:. > java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:318); at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:571); at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:560); at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); at java.io.DataInputStream.read(DataInputStream.java:149); at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:421); at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:394); at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:268); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); at htsjdk.samtools.BAMF,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:7891,Availability,down,download,7891,"d.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused by: java.io.IOException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.throwIfFalseEOF(NetHttpResponse.java:202); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:171); at java.io.FilterInputStream.read(FilterInputStream.java:107); at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:638); ... 13 more. Subsequent reruns on 4.0.4.0 yielded only 1 failure out of 3 walkers x 59 samples; no failures were observed on 4.0.9.0 or 4.0.11.0. So the problem is not unique to 4.0.12.0, but the rate of failure is much higher. Additional reruns on 4.0.12.0 suggest that the original failures were not intermittent; one run showed 6/59 samples failing, with many of those having more than one of the 3 walkers fail. Additional reruns with a branch of 4.0.12.0 that reverted to the htsjdk version used in 4.0.11.0 still showed a high rate of failure. There were at least a couple of instances where the same BAM appeared to fail in roughly the same spot as in the 4.0.12.0 runs, and other instances where the same BAM failed in roughly the same spot, only in two different walkers. However, the set of BAMs that failed was not consistent across all runs. Leaving to @droazen to delegate. The FC CNV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:8062,Availability,failure,failure,8062,"nnel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused by: java.io.IOException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.throwIfFalseEOF(NetHttpResponse.java:202); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:171); at java.io.FilterInputStream.read(FilterInputStream.java:107); at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:638); ... 13 more. Subsequent reruns on 4.0.4.0 yielded only 1 failure out of 3 walkers x 59 samples; no failures were observed on 4.0.9.0 or 4.0.11.0. So the problem is not unique to 4.0.12.0, but the rate of failure is much higher. Additional reruns on 4.0.12.0 suggest that the original failures were not intermittent; one run showed 6/59 samples failing, with many of those having more than one of the 3 walkers fail. Additional reruns with a branch of 4.0.12.0 that reverted to the htsjdk version used in 4.0.11.0 still showed a high rate of failure. There were at least a couple of instances where the same BAM appeared to fail in roughly the same spot as in the 4.0.12.0 runs, and other instances where the same BAM failed in roughly the same spot, only in two different walkers. However, the set of BAMs that failed was not consistent across all runs. Leaving to @droazen to delegate. The FC CNV featured WDL doesn't use NIO yet, but I'm surprised this hasn't cropped up in other WDLs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:8104,Availability,failure,failures,8104,"nnel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused by: java.io.IOException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.throwIfFalseEOF(NetHttpResponse.java:202); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:171); at java.io.FilterInputStream.read(FilterInputStream.java:107); at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:638); ... 13 more. Subsequent reruns on 4.0.4.0 yielded only 1 failure out of 3 walkers x 59 samples; no failures were observed on 4.0.9.0 or 4.0.11.0. So the problem is not unique to 4.0.12.0, but the rate of failure is much higher. Additional reruns on 4.0.12.0 suggest that the original failures were not intermittent; one run showed 6/59 samples failing, with many of those having more than one of the 3 walkers fail. Additional reruns with a branch of 4.0.12.0 that reverted to the htsjdk version used in 4.0.11.0 still showed a high rate of failure. There were at least a couple of instances where the same BAM appeared to fail in roughly the same spot as in the 4.0.12.0 runs, and other instances where the same BAM failed in roughly the same spot, only in two different walkers. However, the set of BAMs that failed was not consistent across all runs. Leaving to @droazen to delegate. The FC CNV featured WDL doesn't use NIO yet, but I'm surprised this hasn't cropped up in other WDLs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:8209,Availability,failure,failure,8209,"nnel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused by: java.io.IOException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.throwIfFalseEOF(NetHttpResponse.java:202); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:171); at java.io.FilterInputStream.read(FilterInputStream.java:107); at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:638); ... 13 more. Subsequent reruns on 4.0.4.0 yielded only 1 failure out of 3 walkers x 59 samples; no failures were observed on 4.0.9.0 or 4.0.11.0. So the problem is not unique to 4.0.12.0, but the rate of failure is much higher. Additional reruns on 4.0.12.0 suggest that the original failures were not intermittent; one run showed 6/59 samples failing, with many of those having more than one of the 3 walkers fail. Additional reruns with a branch of 4.0.12.0 that reverted to the htsjdk version used in 4.0.11.0 still showed a high rate of failure. There were at least a couple of instances where the same BAM appeared to fail in roughly the same spot as in the 4.0.12.0 runs, and other instances where the same BAM failed in roughly the same spot, only in two different walkers. However, the set of BAMs that failed was not consistent across all runs. Leaving to @droazen to delegate. The FC CNV featured WDL doesn't use NIO yet, but I'm surprised this hasn't cropped up in other WDLs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:8289,Availability,failure,failures,8289,"nnel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused by: java.io.IOException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.throwIfFalseEOF(NetHttpResponse.java:202); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:171); at java.io.FilterInputStream.read(FilterInputStream.java:107); at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:638); ... 13 more. Subsequent reruns on 4.0.4.0 yielded only 1 failure out of 3 walkers x 59 samples; no failures were observed on 4.0.9.0 or 4.0.11.0. So the problem is not unique to 4.0.12.0, but the rate of failure is much higher. Additional reruns on 4.0.12.0 suggest that the original failures were not intermittent; one run showed 6/59 samples failing, with many of those having more than one of the 3 walkers fail. Additional reruns with a branch of 4.0.12.0 that reverted to the htsjdk version used in 4.0.11.0 still showed a high rate of failure. There were at least a couple of instances where the same BAM appeared to fail in roughly the same spot as in the 4.0.12.0 runs, and other instances where the same BAM failed in roughly the same spot, only in two different walkers. However, the set of BAMs that failed was not consistent across all runs. Leaving to @droazen to delegate. The FC CNV featured WDL doesn't use NIO yet, but I'm surprised this hasn't cropped up in other WDLs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:8546,Availability,failure,failure,8546,"nnel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused by: java.io.IOException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.throwIfFalseEOF(NetHttpResponse.java:202); at shaded.cloud_nio.com.google.api.client.http.javanet.NetHttpResponse$SizeValidatingInputStream.read(NetHttpResponse.java:171); at java.io.FilterInputStream.read(FilterInputStream.java:107); at shaded.cloud_nio.com.google.api.client.util.ByteStreams.copy(ByteStreams.java:51); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:94); at shaded.cloud_nio.com.google.api.client.util.IOUtils.copy(IOUtils.java:63); at shaded.cloud_nio.com.google.api.client.http.HttpResponse.download(HttpResponse.java:421); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:638); ... 13 more. Subsequent reruns on 4.0.4.0 yielded only 1 failure out of 3 walkers x 59 samples; no failures were observed on 4.0.9.0 or 4.0.11.0. So the problem is not unique to 4.0.12.0, but the rate of failure is much higher. Additional reruns on 4.0.12.0 suggest that the original failures were not intermittent; one run showed 6/59 samples failing, with many of those having more than one of the 3 walkers fail. Additional reruns with a branch of 4.0.12.0 that reverted to the htsjdk version used in 4.0.11.0 still showed a high rate of failure. There were at least a couple of instances where the same BAM appeared to fail in roughly the same spot as in the 4.0.12.0 runs, and other instances where the same BAM failed in roughly the same spot, only in two different walkers. However, the set of BAMs that failed was not consistent across all runs. Leaving to @droazen to delegate. The FC CNV featured WDL doesn't use NIO yet, but I'm surprised this hasn't cropped up in other WDLs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:3417,Integrability,wrap,wrapAndCopyInto,3417,tsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:89); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.ma,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:351,Performance,concurren,concurrent,351,The problem may not be specific to these CNV walkers. A ModelSegments FC evaluation of a branch of 4.0.12.0 yielded several instances of the following exception in the CollectReadCounts and CollectAllelicCounts tools (multiple samples failing on one or more out of 3 total runs of these tools) when using NIO:. > java.lang.RuntimeException: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:318); at htsjdk.samtools.seekablestream.SeekablePathStream.read(SeekablePathStream.java:86); at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:571); at htsjdk.samtools.util.BlockCompressedInputStream.readBytes(BlockCompressedInputStream.java:560); at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:525); at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); at htsjdk.samtools.util.BlockCompressedInputStream.readBlock(BlockCompressedInputStream.java:458); at htsjdk.samtools.util.BlockCompressedInputStream.available(BlockCompressedInputStream.java:196); at htsjdk.samtools.util.BlockCompressedInputStream.read(BlockCompressedInputStream.java:331); at java.io.DataInputStream.read(DataInputStream.java:149); at htsjdk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:421); at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:394); at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:268); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:2650,Performance,load,loadNextRecord,2650,t htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:268); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.eva,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:4474,Performance,concurren,concurrent,4474,m.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:89); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:140); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:264); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); ... 44 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:123); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStora,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:4631,Performance,concurren,concurrent,4631,va.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:89); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:140); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:264); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); ... 44 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:123); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:93); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.handleStorageException(CloudStorageReadChannel.java:242); at com.goo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:4695,Performance,concurren,concurrent,4695,34); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:89); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.util.concurrent.ExecutionException: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at java.util.concurrent.FutureTask.report(FutureTask.java:122); at java.util.concurrent.FutureTask.get(FutureTask.java:192); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.getBuf(SeekableByteChannelPrefetcher.java:140); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.fetch(SeekableByteChannelPrefetcher.java:264); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); ... 44 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:123); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:93); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.handleStorageException(CloudStorageReadChannel.java:242); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(Cloud,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:6000,Performance,concurren,concurrent,6000,"institute.hellbender.utils.nio.SeekableByteChannelPrefetcher.read(SeekableByteChannelPrefetcher.java:309); ... 44 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:123); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:93); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.handleStorageException(CloudStorageReadChannel.java:242); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:145); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:135); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:108); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: com.google.cloud.storage.StorageException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:644); at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.BlobReadChannel.read(Blob",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:6061,Performance,concurren,concurrent,6061,"read(SeekableByteChannelPrefetcher.java:309); ... 44 more; Caused by: com.google.cloud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:123); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:93); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.handleStorageException(CloudStorageReadChannel.java:242); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:145); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:135); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:108); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: com.google.cloud.storage.StorageException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:644); at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.ni",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:6145,Performance,concurren,concurrent,6145,"ud.storage.StorageException: All 20 reopens failed. Waited a total of 1918000 ms between attempts; at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleReopenForStorageException(CloudStorageRetryHandler.java:123); at com.google.cloud.storage.contrib.nio.CloudStorageRetryHandler.handleStorageException(CloudStorageRetryHandler.java:93); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.handleStorageException(CloudStorageReadChannel.java:242); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:145); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:135); at org.broadinstitute.hellbender.utils.nio.SeekableByteChannelPrefetcher$WorkUnit.call(SeekableByteChannelPrefetcher.java:108); at java.util.concurrent.FutureTask.run(FutureTask.java:266); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); at java.lang.Thread.run(Thread.java:748); Caused by: com.google.cloud.storage.StorageException: Connection closed prematurely: bytesRead = 16777216, Content-Length = 41943040; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:220); at com.google.cloud.storage.spi.v1.HttpStorageRpc.read(HttpStorageRpc.java:644); at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:127); at com.google.cloud.storage.BlobReadChannel$1.call(BlobReadChannel.java:124); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:89); at com.google.cloud.RetryHelper.run(RetryHelper.java:74); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:51); at com.google.cloud.storage.BlobReadChannel.read(BlobReadChannel.java:124); at com.google.cloud.storage.contrib.nio.CloudStorageReadChannel.read(CloudStorageReadChannel.java:141); ... 6 more; Caused",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:2458,Testability,Assert,AssertingIterator,2458,dk.samtools.util.BinaryCodec.readBytesOrFewer(BinaryCodec.java:421); at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:394); at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:268); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5631:2531,Testability,Assert,AssertingIterator,2531,tsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:394); at htsjdk.samtools.util.BinaryCodec.readBytes(BinaryCodec.java:380); at htsjdk.samtools.BAMRecordCodec.decode(BAMRecordCodec.java:268); at htsjdk.samtools.BAMFileReader$BAMFileIterator.getNextRecord(BAMFileReader.java:829); at htsjdk.samtools.BAMFileReader$BAMFileIndexIterator.getNextRecord(BAMFileReader.java:981); at htsjdk.samtools.BAMFileReader$BAMFileIterator.advance(BAMFileReader.java:803); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:797); at htsjdk.samtools.BAMFileReader$BAMFileIterator.next(BAMFileReader.java:765); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.advance(BAMFileReader.java:1034); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:1024); at htsjdk.samtools.BAMFileReader$BAMQueryFilteringIterator.next(BAMFileReader.java:988); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:569); at htsjdk.samtools.SamReader$AssertingIterator.next(SamReader.java:548); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextRecord(SamReaderQueryingIterator.java:114); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:151); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.next(SamReaderQueryingIterator.java:29); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:27); at org.broadinstitute.hellbender.utils.iterators.SAMRecordToReadIterator.next(SAMRecordToReadIterator.java:13); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5631
https://github.com/broadinstitute/gatk/issues/5644:3034,Availability,error,error,3034,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:3093,Availability,error,error,3093,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:115,Deployability,release,release,115,## Bug Report. ### Affected tool(s) or class(es); ReadsPipelineSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinsti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:636,Deployability,pipeline,pipelines,636,## Bug Report. ### Affected tool(s) or class(es); ReadsPipelineSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinsti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:1309,Deployability,pipeline,pipelines,1309,the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delega,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:2422,Deployability,deploy,deploy,2422,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:2459,Deployability,deploy,deploy,2459,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:2532,Deployability,deploy,deploy,2532,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:2609,Deployability,deploy,deploy,2609,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:2681,Deployability,deploy,deploy,2681,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:2751,Deployability,deploy,deploy,2751,"stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:775); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); ```. #### Steps to reproduce; In the scripts/spark_eval directory, run; ```; NUM_WORKERS=20 nohup ./run_gcs_cluster.sh copy_genome_to_hdfs_on_gcs.sh genome_reads-pipeline_hdfs.sh &; ```. #### Expected behavior; The tool should run without error. #### Actual behavior; The tool exits with the above error about 30 mins into the run.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:1053,Energy Efficiency,Reduce,ReduceOps,1053,eSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:1063,Energy Efficiency,Reduce,ReduceOp,1063,eSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:1091,Energy Efficiency,Reduce,ReduceOps,1091,sion(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:988,Integrability,wrap,wrapAndCopyInto,988,ug Report. ### Affected tool(s) or class(es); ReadsPipelineSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:385,Security,validat,validateArg,385,## Bug Report. ### Affected tool(s) or class(es); ReadsPipelineSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinsti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5644:182,Testability,test,test,182,## Bug Report. ### Affected tool(s) or class(es); ReadsPipelineSpark. ### Affected version(s); - [x] Latest public release version 4.1.0.0; - [ ] Latest master branch as of [date of test?]. ### Description . ```; java.lang.IllegalArgumentException: Interval NC_007605:1-171823 not within the bounds of a contig in the provided dictionary; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:87); 	at org.broadinstitute.hellbender.engine.Shard.divideIntervalIntoShards(Shard.java:66); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.lambda$runTool$0(ReadsPipelineSpark.java:221); 	at java.util.stream.ReferencePipeline$7$1.accept(ReferencePipeline.java:267); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:222); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinsti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5644
https://github.com/broadinstitute/gatk/issues/5646:1764,Modifiability,refactor,refactor,1764,"While working on validating #5607 I noticed that at the top of the method `isReadInformativeOfIndelOfSize()` that there is the following breakout condition:; ```; if( read.getLength() - readStart < maxIndelSize || refBases.length - refStart < maxIndelSize ) {; return false;; }; ```; This says that if the readStart is too close to the read.getLenght() then it will break out and not calculate the informativeness of a read. Unfortunately readStart isn't the readbase indexed readStart, its actually the ""IGV view"" offset for the read generated by the pileup for a particular reference position. The actual length that matters to us is: `AlignmentUtils.getBasesAlignedOneToOne(read).length` which is computed later when we realign the read bases to the reference. What this means is that if a read happens to have a long deletion in it then we will end up prematurely marking the read bases as being non-informative despite there being more than enough bases to work with when doing computations. Furthermore, since we realign the read bases later in the codepath, these bases in the gap between the realigned length and `read.getLength()` are still used to compute mismatch likelihood for bases before that point in the read. An example of this issue: I have a read with the cigar ""77M10D24M"", at position 92 of the read (the igv offset so in reality the 5th base into the last element of the cigar) the code returns false due to this condition. In reality `AlignmentUtils.getBasesAlignedOneToOne(read).length - readStart` value is 19, and thus comparable since there are >10 bases left in the read to test. . I have duplicated this behavior in #5607, perhaps it would be easiest to get that branch in first before tackling this issue just so validation for that refactor is easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5646
https://github.com/broadinstitute/gatk/issues/5646:17,Security,validat,validating,17,"While working on validating #5607 I noticed that at the top of the method `isReadInformativeOfIndelOfSize()` that there is the following breakout condition:; ```; if( read.getLength() - readStart < maxIndelSize || refBases.length - refStart < maxIndelSize ) {; return false;; }; ```; This says that if the readStart is too close to the read.getLenght() then it will break out and not calculate the informativeness of a read. Unfortunately readStart isn't the readbase indexed readStart, its actually the ""IGV view"" offset for the read generated by the pileup for a particular reference position. The actual length that matters to us is: `AlignmentUtils.getBasesAlignedOneToOne(read).length` which is computed later when we realign the read bases to the reference. What this means is that if a read happens to have a long deletion in it then we will end up prematurely marking the read bases as being non-informative despite there being more than enough bases to work with when doing computations. Furthermore, since we realign the read bases later in the codepath, these bases in the gap between the realigned length and `read.getLength()` are still used to compute mismatch likelihood for bases before that point in the read. An example of this issue: I have a read with the cigar ""77M10D24M"", at position 92 of the read (the igv offset so in reality the 5th base into the last element of the cigar) the code returns false due to this condition. In reality `AlignmentUtils.getBasesAlignedOneToOne(read).length - readStart` value is 19, and thus comparable since there are >10 bases left in the read to test. . I have duplicated this behavior in #5607, perhaps it would be easiest to get that branch in first before tackling this issue just so validation for that refactor is easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5646
https://github.com/broadinstitute/gatk/issues/5646:1744,Security,validat,validation,1744,"While working on validating #5607 I noticed that at the top of the method `isReadInformativeOfIndelOfSize()` that there is the following breakout condition:; ```; if( read.getLength() - readStart < maxIndelSize || refBases.length - refStart < maxIndelSize ) {; return false;; }; ```; This says that if the readStart is too close to the read.getLenght() then it will break out and not calculate the informativeness of a read. Unfortunately readStart isn't the readbase indexed readStart, its actually the ""IGV view"" offset for the read generated by the pileup for a particular reference position. The actual length that matters to us is: `AlignmentUtils.getBasesAlignedOneToOne(read).length` which is computed later when we realign the read bases to the reference. What this means is that if a read happens to have a long deletion in it then we will end up prematurely marking the read bases as being non-informative despite there being more than enough bases to work with when doing computations. Furthermore, since we realign the read bases later in the codepath, these bases in the gap between the realigned length and `read.getLength()` are still used to compute mismatch likelihood for bases before that point in the read. An example of this issue: I have a read with the cigar ""77M10D24M"", at position 92 of the read (the igv offset so in reality the 5th base into the last element of the cigar) the code returns false due to this condition. In reality `AlignmentUtils.getBasesAlignedOneToOne(read).length - readStart` value is 19, and thus comparable since there are >10 bases left in the read to test. . I have duplicated this behavior in #5607, perhaps it would be easiest to get that branch in first before tackling this issue just so validation for that refactor is easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5646
https://github.com/broadinstitute/gatk/issues/5646:1603,Testability,test,test,1603,"While working on validating #5607 I noticed that at the top of the method `isReadInformativeOfIndelOfSize()` that there is the following breakout condition:; ```; if( read.getLength() - readStart < maxIndelSize || refBases.length - refStart < maxIndelSize ) {; return false;; }; ```; This says that if the readStart is too close to the read.getLenght() then it will break out and not calculate the informativeness of a read. Unfortunately readStart isn't the readbase indexed readStart, its actually the ""IGV view"" offset for the read generated by the pileup for a particular reference position. The actual length that matters to us is: `AlignmentUtils.getBasesAlignedOneToOne(read).length` which is computed later when we realign the read bases to the reference. What this means is that if a read happens to have a long deletion in it then we will end up prematurely marking the read bases as being non-informative despite there being more than enough bases to work with when doing computations. Furthermore, since we realign the read bases later in the codepath, these bases in the gap between the realigned length and `read.getLength()` are still used to compute mismatch likelihood for bases before that point in the read. An example of this issue: I have a read with the cigar ""77M10D24M"", at position 92 of the read (the igv offset so in reality the 5th base into the last element of the cigar) the code returns false due to this condition. In reality `AlignmentUtils.getBasesAlignedOneToOne(read).length - readStart` value is 19, and thus comparable since there are >10 bases left in the read to test. . I have duplicated this behavior in #5607, perhaps it would be easiest to get that branch in first before tackling this issue just so validation for that refactor is easier.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5646
https://github.com/broadinstitute/gatk/issues/5647:494,Availability,Error,Error,494,"Have been having the following issue running StructuralVariationDiscoveryPipeline on previous and the newest release of GATK(4.1.0.0). Currently attempting to use on a computing cluster without spark enabled. . Command line used:; gatk/gatk-4.1.0.0/gatk StructuralVariationDiscoveryPipelineSpark \; -I $CRAM \; -R $Hg38 \; --aligner-index-image reference.fasta.Hg38.img \; --kmers-to-ignore kmers_to_ignore_hg38.txt \; --contig-sam-file aligned_contigs.sam \; -O ${base}_GATK_SV_output.vcf . **Error Log**:; 19/02/01 21:28:27 INFO TaskSetManager: Starting task 700.0 in stage 5.0 (TID 4405, localhost, executor driver, partition 700, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:27 INFO Executor: Running task 700.0 in stage 5.0 (TID 4405); 19/02/01 21:28:27 INFO TaskSetManager: Finished task 668.0 in stage 5.0 (TID 4373) in 37331 ms on localhost (executor driver) (669/741); 19/02/01 21:28:27 INFO BlockManagerInfo: Removed taskresult_4373 on 10.120.16.54:34926 in memory (size: 1645.1 KB, free: 15.8 GB); 19/02/01 21:28:27 INFO NewHadoopRDD: Input split: file: /cram8/1-00004__CG0000-1789.GMKF2.cram:23488102400+33554432; 19/02/01 21:28:28 ERROR Executor: Exception in task 698.0 in stage 5.0 (TID 4403); **java.lang.IllegalArgumentException: provided start is negative: -24**; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.ha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:1142,Availability,ERROR,ERROR,1142,"rently attempting to use on a computing cluster without spark enabled. . Command line used:; gatk/gatk-4.1.0.0/gatk StructuralVariationDiscoveryPipelineSpark \; -I $CRAM \; -R $Hg38 \; --aligner-index-image reference.fasta.Hg38.img \; --kmers-to-ignore kmers_to_ignore_hg38.txt \; --contig-sam-file aligned_contigs.sam \; -O ${base}_GATK_SV_output.vcf . **Error Log**:; 19/02/01 21:28:27 INFO TaskSetManager: Starting task 700.0 in stage 5.0 (TID 4405, localhost, executor driver, partition 700, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:27 INFO Executor: Running task 700.0 in stage 5.0 (TID 4405); 19/02/01 21:28:27 INFO TaskSetManager: Finished task 668.0 in stage 5.0 (TID 4373) in 37331 ms on localhost (executor driver) (669/741); 19/02/01 21:28:27 INFO BlockManagerInfo: Removed taskresult_4373 on 10.120.16.54:34926 in memory (size: 1645.1 KB, free: 15.8 GB); 19/02/01 21:28:27 INFO NewHadoopRDD: Input split: file: /cram8/1-00004__CG0000-1789.GMKF2.cram:23488102400+33554432; 19/02/01 21:28:28 ERROR Executor: Exception in task 698.0 in stage 5.0 (TID 4403); **java.lang.IllegalArgumentException: provided start is negative: -24**; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterato",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:109,Deployability,release,release,109,"Have been having the following issue running StructuralVariationDiscoveryPipeline on previous and the newest release of GATK(4.1.0.0). Currently attempting to use on a computing cluster without spark enabled. . Command line used:; gatk/gatk-4.1.0.0/gatk StructuralVariationDiscoveryPipelineSpark \; -I $CRAM \; -R $Hg38 \; --aligner-index-image reference.fasta.Hg38.img \; --kmers-to-ignore kmers_to_ignore_hg38.txt \; --contig-sam-file aligned_contigs.sam \; -O ${base}_GATK_SV_output.vcf . **Error Log**:; 19/02/01 21:28:27 INFO TaskSetManager: Starting task 700.0 in stage 5.0 (TID 4405, localhost, executor driver, partition 700, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:27 INFO Executor: Running task 700.0 in stage 5.0 (TID 4405); 19/02/01 21:28:27 INFO TaskSetManager: Finished task 668.0 in stage 5.0 (TID 4373) in 37331 ms on localhost (executor driver) (669/741); 19/02/01 21:28:27 INFO BlockManagerInfo: Removed taskresult_4373 on 10.120.16.54:34926 in memory (size: 1645.1 KB, free: 15.8 GB); 19/02/01 21:28:27 INFO NewHadoopRDD: Input split: file: /cram8/1-00004__CG0000-1789.GMKF2.cram:23488102400+33554432; 19/02/01 21:28:28 ERROR Executor: Exception in task 698.0 in stage 5.0 (TID 4403); **java.lang.IllegalArgumentException: provided start is negative: -24**; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.ha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:3250,Energy Efficiency,schedul,scheduler,3250,".tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 19/02/01 21:28:28 INFO TaskSetManager: Starting task 701.0 in stage 5.0 (TID 4406, localhost, executor driver, partition 701, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:28 INFO Executor: Running task 701.0 in stage 5.0 (TID 4406)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:3321,Energy Efficiency,schedul,scheduler,3321,".tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 19/02/01 21:28:28 INFO TaskSetManager: Starting task 701.0 in stage 5.0 (TID 4406, localhost, executor driver, partition 701, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:28 INFO Executor: Running task 701.0 in stage 5.0 (TID 4406)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:2058,Integrability,Wrap,Wrappers,2058,-00004__CG0000-1789.GMKF2.cram:23488102400+33554432; 19/02/01 21:28:28 ERROR Executor: Exception in task 698.0 in stage 5.0 (TID 4403); **java.lang.IllegalArgumentException: provided start is negative: -24**; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:2092,Integrability,Wrap,Wrappers,2092,MKF2.cram:23488102400+33554432; 19/02/01 21:28:28 ERROR Executor: Exception in task 698.0 in stage 5.0 (TID 4403); **java.lang.IllegalArgumentException: provided start is negative: -24**; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkCont,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:3444,Performance,concurren,concurrent,3444,".tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 19/02/01 21:28:28 INFO TaskSetManager: Starting task 701.0 in stage 5.0 (TID 4406, localhost, executor driver, partition 701, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:28 INFO Executor: Running task 701.0 in stage 5.0 (TID 4406)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:3528,Performance,concurren,concurrent,3528,".tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:936); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2062); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:108); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 19/02/01 21:28:28 INFO TaskSetManager: Starting task 701.0 in stage 5.0 (TID 4406, localhost, executor driver, partition 701, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:28 INFO Executor: Running task 701.0 in stage 5.0 (TID 4406)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5647:500,Testability,Log,Log,500,"Have been having the following issue running StructuralVariationDiscoveryPipeline on previous and the newest release of GATK(4.1.0.0). Currently attempting to use on a computing cluster without spark enabled. . Command line used:; gatk/gatk-4.1.0.0/gatk StructuralVariationDiscoveryPipelineSpark \; -I $CRAM \; -R $Hg38 \; --aligner-index-image reference.fasta.Hg38.img \; --kmers-to-ignore kmers_to_ignore_hg38.txt \; --contig-sam-file aligned_contigs.sam \; -O ${base}_GATK_SV_output.vcf . **Error Log**:; 19/02/01 21:28:27 INFO TaskSetManager: Starting task 700.0 in stage 5.0 (TID 4405, localhost, executor driver, partition 700, PROCESS_LOCAL, 4940 bytes); 19/02/01 21:28:27 INFO Executor: Running task 700.0 in stage 5.0 (TID 4405); 19/02/01 21:28:27 INFO TaskSetManager: Finished task 668.0 in stage 5.0 (TID 4373) in 37331 ms on localhost (executor driver) (669/741); 19/02/01 21:28:27 INFO BlockManagerInfo: Removed taskresult_4373 on 10.120.16.54:34926 in memory (size: 1645.1 KB, free: 15.8 GB); 19/02/01 21:28:27 INFO NewHadoopRDD: Input split: file: /cram8/1-00004__CG0000-1789.GMKF2.cram:23488102400+33554432; 19/02/01 21:28:28 ERROR Executor: Exception in task 698.0 in stage 5.0 (TID 4403); **java.lang.IllegalArgumentException: provided start is negative: -24**; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.ha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5647
https://github.com/broadinstitute/gatk/issues/5648:140,Testability,test,tests,140,These methods `getBasesAlignedOneToOne` `getBaseQualsAlignedOneToOne` and `getSequenceAlignedOneToOne` should have javadoc and ideally unit-tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5648
https://github.com/broadinstitute/gatk/issues/5649:168,Availability,down,downstream,168,"## Feature request. ### Tool(s) or class(es) involved. CreateSomaticPanelOfNormals. ### Description. Currently, CreateSomaticPanelOfNormals emits sites-only VCFs. Some downstream tools require full VCFs, as could be created previously in the PON CombineVariants workflow. Perhaps this feature will be covered when CombineVariants becomes available, but I believe it may still be desirable if CreateSomaticPanelOfNormals could pass `--sites-only-vcf-output=false` to allow full VCFs to be returned. This would permit calculation of mapping bias using allele frequencies of the normal samples. Thank you for your tremendous service developing this tool. Sincerely,. Andrew",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5649
https://github.com/broadinstitute/gatk/issues/5649:338,Availability,avail,available,338,"## Feature request. ### Tool(s) or class(es) involved. CreateSomaticPanelOfNormals. ### Description. Currently, CreateSomaticPanelOfNormals emits sites-only VCFs. Some downstream tools require full VCFs, as could be created previously in the PON CombineVariants workflow. Perhaps this feature will be covered when CombineVariants becomes available, but I believe it may still be desirable if CreateSomaticPanelOfNormals could pass `--sites-only-vcf-output=false` to allow full VCFs to be returned. This would permit calculation of mapping bias using allele frequencies of the normal samples. Thank you for your tremendous service developing this tool. Sincerely,. Andrew",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5649
https://github.com/broadinstitute/gatk/issues/5650:403,Availability,down,downstream,403,"## Bug Report. ### Affected tool(s) or class(es); GATK Haplotype caller. ### Affected version(s); - [x] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. The Genome Analysis Toolkit (GATK) v4.1.0.0; HTSJDK Version: 2.18.2; Picard Version: 2.18.25. ### Description ; HaplotypeCaller is outputting variants which have a no-call as the ALT, which breaks a bunch of downstream tools, this is new behavior in 4.1, AFAICT. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	s1564	s1741	s1851	s1852	s1862	s1901	s1912	s1971	s2017	s2021	s2026	s2056	s2100	s2102	s2104	s2122	s2124	s2151	s2157; 1	937796	.	T	.	179.65	.	AN=38;DP=31;MMQ=60;MQ=60.00	GT:AD:DP	0/0:0:0	0/0:0:0	0/0:4:4	0/0:0:0	0/0:1:1	0/0:1:1	0/0:0:0	0/0:0:0	0/0:2:2	0/0:0:0	0/0:1:1	0/0:3:3	0/0:1:1	0/0:8:8	0/0:1:1	0/0:0:0	0/0:2:2	0/0:7:7	0/0:0:0; ```. #### Steps to reproduce; I'm not doing anything special, so I suspect these variants should exist in other projects as well. I'm doing batch calling on several samples simultaneously; an example:. ```; unset JAVA_HOME && export PATH=/home/rdk4/local/share/bcbio/anaconda/bin:$PATH && gatk --java-options '-Xms4g -Xmx5000m -XX:+UseSerialGC -Djava.io.tmpdir=/n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/bcbiotx/tmpTSg0hJ' HaplotypeCaller -R /n/app/bcbio/dev/genomes/Hsapiens/GRCh37/seq/GRCh37.fa --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation BaseQualityRankSumTest --annotation FisherStrand --annotation MappingQuality --annotation DepthPerAlleleBySample --annotation Coverage -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2017/s2017-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2056/s2056-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2122/s2122-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5650
https://github.com/broadinstitute/gatk/issues/5650:118,Deployability,release,release,118,"## Bug Report. ### Affected tool(s) or class(es); GATK Haplotype caller. ### Affected version(s); - [x] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. The Genome Analysis Toolkit (GATK) v4.1.0.0; HTSJDK Version: 2.18.2; Picard Version: 2.18.25. ### Description ; HaplotypeCaller is outputting variants which have a no-call as the ALT, which breaks a bunch of downstream tools, this is new behavior in 4.1, AFAICT. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	s1564	s1741	s1851	s1852	s1862	s1901	s1912	s1971	s2017	s2021	s2026	s2056	s2100	s2102	s2104	s2122	s2124	s2151	s2157; 1	937796	.	T	.	179.65	.	AN=38;DP=31;MMQ=60;MQ=60.00	GT:AD:DP	0/0:0:0	0/0:0:0	0/0:4:4	0/0:0:0	0/0:1:1	0/0:1:1	0/0:0:0	0/0:0:0	0/0:2:2	0/0:0:0	0/0:1:1	0/0:3:3	0/0:1:1	0/0:8:8	0/0:1:1	0/0:0:0	0/0:2:2	0/0:7:7	0/0:0:0; ```. #### Steps to reproduce; I'm not doing anything special, so I suspect these variants should exist in other projects as well. I'm doing batch calling on several samples simultaneously; an example:. ```; unset JAVA_HOME && export PATH=/home/rdk4/local/share/bcbio/anaconda/bin:$PATH && gatk --java-options '-Xms4g -Xmx5000m -XX:+UseSerialGC -Djava.io.tmpdir=/n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/bcbiotx/tmpTSg0hJ' HaplotypeCaller -R /n/app/bcbio/dev/genomes/Hsapiens/GRCh37/seq/GRCh37.fa --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation BaseQualityRankSumTest --annotation FisherStrand --annotation MappingQuality --annotation DepthPerAlleleBySample --annotation Coverage -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2017/s2017-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2056/s2056-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2122/s2122-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5650
https://github.com/broadinstitute/gatk/issues/5650:187,Testability,test,test,187,"## Bug Report. ### Affected tool(s) or class(es); GATK Haplotype caller. ### Affected version(s); - [x] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. The Genome Analysis Toolkit (GATK) v4.1.0.0; HTSJDK Version: 2.18.2; Picard Version: 2.18.25. ### Description ; HaplotypeCaller is outputting variants which have a no-call as the ALT, which breaks a bunch of downstream tools, this is new behavior in 4.1, AFAICT. ```; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	s1564	s1741	s1851	s1852	s1862	s1901	s1912	s1971	s2017	s2021	s2026	s2056	s2100	s2102	s2104	s2122	s2124	s2151	s2157; 1	937796	.	T	.	179.65	.	AN=38;DP=31;MMQ=60;MQ=60.00	GT:AD:DP	0/0:0:0	0/0:0:0	0/0:4:4	0/0:0:0	0/0:1:1	0/0:1:1	0/0:0:0	0/0:0:0	0/0:2:2	0/0:0:0	0/0:1:1	0/0:3:3	0/0:1:1	0/0:8:8	0/0:1:1	0/0:0:0	0/0:2:2	0/0:7:7	0/0:0:0; ```. #### Steps to reproduce; I'm not doing anything special, so I suspect these variants should exist in other projects as well. I'm doing batch calling on several samples simultaneously; an example:. ```; unset JAVA_HOME && export PATH=/home/rdk4/local/share/bcbio/anaconda/bin:$PATH && gatk --java-options '-Xms4g -Xmx5000m -XX:+UseSerialGC -Djava.io.tmpdir=/n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/bcbiotx/tmpTSg0hJ' HaplotypeCaller -R /n/app/bcbio/dev/genomes/Hsapiens/GRCh37/seq/GRCh37.fa --annotation MappingQualityRankSumTest --annotation MappingQualityZero --annotation QualByDepth --annotation ReadPosRankSumTest --annotation RMSMappingQuality --annotation BaseQualityRankSumTest --annotation FisherStrand --annotation MappingQuality --annotation DepthPerAlleleBySample --annotation Coverage -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2017/s2017-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2056/s2056-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-variants/work/align/s2122/s2122-sort.bam -I /n/data1/cores/bcbio/PIs/rudy_tanzi/tau-exome/tau-var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5650
https://github.com/broadinstitute/gatk/issues/5651:551,Availability,down,down,551,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1 with -ERC GVCF. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; It would appear that variants covered by a spanning deletion are not output with phasing information even when surrounded by phased variants on either side. Since one of the alleles is covered by an upstream deletion phase is known, but the genotype itself is not phased and no phase set is attached. The following is a cut-down example from a gVCF:. ```; chr6 51618169 . GT G,<NON_REF> 948.60 . DP=94 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:32,39,0:71:3,4,0:29,35,0:99:0|1:51618169_GT_G:956,0,808,1054,926,1980:51618169:3,29,4,35; chr6 51618170 . T *,G,<NON_REF> 776.01 . DP=92 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:2,39,30,0:71:1,4,2,0:1,35,28,0:99:3533,786,723,1141,0,956,2837,916,1206,2757:1,1,6,63; chr6 51618171 . G <NON_REF> . . END=51618173 GT:DP:GQ:MIN_DP:PL 0/0:90:99:90:0,120,1800; chr6 51618174 . A G,<NON_REF> 1001.60 . DP=89 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:33,41,0:74:3,4,0:30,37,0:99:0|1:51618169_GT_G:1009,0,803,1108,926,2034:51618169:3,30,4,37; ```. You can see that the SNP at 51618170 is flanked by phased variants at 51618169 and 51618174, but is output with unphased genotype and no `PS` (or `PID/PGT`). I'm not entirely sure if this is on purpose for some reason I don't understand, or simply an edge case in the phasing code that's handled incorrectly. #### Steps to reproduce; Run HC on reads with three variants, starting with a deletion, a variant spanned by the deletion and a variant just beyond the deletion. FWIW I've requested permission to share an example case from real data and am awaiting an answer. #### Expected behavior; I think the spanned variant should be output with phasing information, e.g. in the above case I would expect (abbreviated):. ```; chr6 51618169 . GT G,<NON_REF> ... GT:DP:PS 0|1:71:51618169; chr6 51618170 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5651
https://github.com/broadinstitute/gatk/issues/5651:131,Deployability,release,release,131,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1 with -ERC GVCF. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; It would appear that variants covered by a spanning deletion are not output with phasing information even when surrounded by phased variants on either side. Since one of the alleles is covered by an upstream deletion phase is known, but the genotype itself is not phased and no phase set is attached. The following is a cut-down example from a gVCF:. ```; chr6 51618169 . GT G,<NON_REF> 948.60 . DP=94 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:32,39,0:71:3,4,0:29,35,0:99:0|1:51618169_GT_G:956,0,808,1054,926,1980:51618169:3,29,4,35; chr6 51618170 . T *,G,<NON_REF> 776.01 . DP=92 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:2,39,30,0:71:1,4,2,0:1,35,28,0:99:3533,786,723,1141,0,956,2837,916,1206,2757:1,1,6,63; chr6 51618171 . G <NON_REF> . . END=51618173 GT:DP:GQ:MIN_DP:PL 0/0:90:99:90:0,120,1800; chr6 51618174 . A G,<NON_REF> 1001.60 . DP=89 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:33,41,0:74:3,4,0:30,37,0:99:0|1:51618169_GT_G:1009,0,803,1108,926,2034:51618169:3,30,4,37; ```. You can see that the SNP at 51618170 is flanked by phased variants at 51618169 and 51618174, but is output with unphased genotype and no `PS` (or `PID/PGT`). I'm not entirely sure if this is on purpose for some reason I don't understand, or simply an edge case in the phasing code that's handled incorrectly. #### Steps to reproduce; Run HC on reads with three variants, starting with a deletion, a variant spanned by the deletion and a variant just beyond the deletion. FWIW I've requested permission to share an example case from real data and am awaiting an answer. #### Expected behavior; I think the spanned variant should be output with phasing information, e.g. in the above case I would expect (abbreviated):. ```; chr6 51618169 . GT G,<NON_REF> ... GT:DP:PS 0|1:71:51618169; chr6 51618170 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5651
https://github.com/broadinstitute/gatk/issues/5651:201,Testability,test,test,201,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1 with -ERC GVCF. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; It would appear that variants covered by a spanning deletion are not output with phasing information even when surrounded by phased variants on either side. Since one of the alleles is covered by an upstream deletion phase is known, but the genotype itself is not phased and no phase set is attached. The following is a cut-down example from a gVCF:. ```; chr6 51618169 . GT G,<NON_REF> 948.60 . DP=94 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:32,39,0:71:3,4,0:29,35,0:99:0|1:51618169_GT_G:956,0,808,1054,926,1980:51618169:3,29,4,35; chr6 51618170 . T *,G,<NON_REF> 776.01 . DP=92 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:2,39,30,0:71:1,4,2,0:1,35,28,0:99:3533,786,723,1141,0,956,2837,916,1206,2757:1,1,6,63; chr6 51618171 . G <NON_REF> . . END=51618173 GT:DP:GQ:MIN_DP:PL 0/0:90:99:90:0,120,1800; chr6 51618174 . A G,<NON_REF> 1001.60 . DP=89 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:33,41,0:74:3,4,0:30,37,0:99:0|1:51618169_GT_G:1009,0,803,1108,926,2034:51618169:3,30,4,37; ```. You can see that the SNP at 51618170 is flanked by phased variants at 51618169 and 51618174, but is output with unphased genotype and no `PS` (or `PID/PGT`). I'm not entirely sure if this is on purpose for some reason I don't understand, or simply an edge case in the phasing code that's handled incorrectly. #### Steps to reproduce; Run HC on reads with three variants, starting with a deletion, a variant spanned by the deletion and a variant just beyond the deletion. FWIW I've requested permission to share an example case from real data and am awaiting an answer. #### Expected behavior; I think the spanned variant should be output with phasing information, e.g. in the above case I would expect (abbreviated):. ```; chr6 51618169 . GT G,<NON_REF> ... GT:DP:PS 0|1:71:51618169; chr6 51618170 .",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5651
https://github.com/broadinstitute/gatk/issues/5651:1443,Usability,simpl,simply,1443," deletion phase is known, but the genotype itself is not phased and no phase set is attached. The following is a cut-down example from a gVCF:. ```; chr6 51618169 . GT G,<NON_REF> 948.60 . DP=94 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:32,39,0:71:3,4,0:29,35,0:99:0|1:51618169_GT_G:956,0,808,1054,926,1980:51618169:3,29,4,35; chr6 51618170 . T *,G,<NON_REF> 776.01 . DP=92 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:2,39,30,0:71:1,4,2,0:1,35,28,0:99:3533,786,723,1141,0,956,2837,916,1206,2757:1,1,6,63; chr6 51618171 . G <NON_REF> . . END=51618173 GT:DP:GQ:MIN_DP:PL 0/0:90:99:90:0,120,1800; chr6 51618174 . A G,<NON_REF> 1001.60 . DP=89 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:33,41,0:74:3,4,0:30,37,0:99:0|1:51618169_GT_G:1009,0,803,1108,926,2034:51618169:3,30,4,37; ```. You can see that the SNP at 51618170 is flanked by phased variants at 51618169 and 51618174, but is output with unphased genotype and no `PS` (or `PID/PGT`). I'm not entirely sure if this is on purpose for some reason I don't understand, or simply an edge case in the phasing code that's handled incorrectly. #### Steps to reproduce; Run HC on reads with three variants, starting with a deletion, a variant spanned by the deletion and a variant just beyond the deletion. FWIW I've requested permission to share an example case from real data and am awaiting an answer. #### Expected behavior; I think the spanned variant should be output with phasing information, e.g. in the above case I would expect (abbreviated):. ```; chr6 51618169 . GT G,<NON_REF> ... GT:DP:PS 0|1:71:51618169; chr6 51618170 . T *,G,<NON_REF> ... GT:DP:PS 2|1:71:51618169; chr6 51618171 . G <NON_REF> ... GT:DP 0/0:90; chr6 51618174 . A G,<NON_REF> ... GT:DP:PS 0|1:74:51618169; ```. #### Actual behavior; The actual output (abbreviated):. ```; chr6 51618169 . GT G,<NON_REF> ... GT:DP:PS 0|1:71:51618169; chr6 51618170 . T *,G,<NON_REF> ... GT:DP 1/2:71:51618169; chr6 51618171 . G <NON_REF> ... GT:DP 0/0:90; chr6 51618174 . A G,<NON_REF> ... GT:DP:PS 0|1:74",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5651
https://github.com/broadinstitute/gatk/pull/5652:135,Energy Efficiency,efficient,efficient,135,"This was showing up in the profile, as with a few other places this was copying the cigar for each read over each pileup, which is not efficient.; Before:; <img width=""912"" alt=""screen shot 2019-02-07 at 11 55 24 am"" src=""https://user-images.githubusercontent.com/16102845/52429724-a4ecec80-2ad2-11e9-9e63-e79c62767215.png"">; After:; <img width=""951"" alt=""screen shot 2019-02-07 at 12 20 16 pm"" src=""https://user-images.githubusercontent.com/16102845/52429818-cf3eaa00-2ad2-11e9-84fb-b8cbcf7617b2.png"">",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5652
https://github.com/broadinstitute/gatk/issues/5653:125,Performance,cache,cache,125,"`SAMRecord` has functionality for transient attribute fields. Specifically they are attributes on a read that can be used to cache expensive/useful information for later stages of a tool that we want to avoid writing into the output. Currently `MarkDuplicatesSpark` uses this functionality by casting its `GATKRead` objects to `SAMRecord` objects to get at the field, which is less than ideal for a number of reasons. This would also be useful for the HaplotypeCaller engine, specifically the changes made in #5607.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5653
https://github.com/broadinstitute/gatk/issues/5653:203,Safety,avoid,avoid,203,"`SAMRecord` has functionality for transient attribute fields. Specifically they are attributes on a read that can be used to cache expensive/useful information for later stages of a tool that we want to avoid writing into the output. Currently `MarkDuplicatesSpark` uses this functionality by casting its `GATKRead` objects to `SAMRecord` objects to get at the field, which is less than ideal for a number of reasons. This would also be useful for the HaplotypeCaller engine, specifically the changes made in #5607.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5653
https://github.com/broadinstitute/gatk/pull/5655:41,Security,access,access,41,I took over #5367 but since I don't have access to your fork @magicDGS I have created a second branch with my changes. Closes #4860,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5655
https://github.com/broadinstitute/gatk/pull/5656:41,Security,access,access,41,I took over #5367 but since I don't have access to your fork @magicDGS I have created a second branch with my changes. Closes #4860,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5656
https://github.com/broadinstitute/gatk/issues/5657:741,Availability,error,error-in-the-documentation-regarding-genotyping-mode,741,"Requested on the forum from a researcher.; ___; Hi GATK team,. I just wanted to let you know, that there is a minor mistake in the documentation here:; https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_haplotypecaller_HaplotypeCaller.php#--pcr-indel-model. The file in which you want to replace --genotyping_mode with --genotyping-mode is this one:; gatk/src/main/java/org/broadinstitute/hellbender/tools/walkers/genotyper/StandardCallerArgumentCollection.java. I don't seem to be able to create a pull request, so I'll leave it to you. Thanks,; Tommy. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23442/minor-error-in-the-documentation-regarding-genotyping-mode/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5657
https://github.com/broadinstitute/gatk/pull/5658:14,Deployability,update,update,14,Documentation update,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5658
https://github.com/broadinstitute/gatk/issues/5660:80,Availability,error,error,80,"If the version of the data sources is not up to date, Funcotator should give an error to that effect and halt.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5660
https://github.com/broadinstitute/gatk/pull/5661:198,Testability,log,logging,198,"With the new qual model as the default for HC as of 4.1, occasionally sites get output with a . ALT when the * allele gets dropped. Expected behavior is no output at that site in the VC. Also limit logging for DepthPerSampleHC annotation. Note that the expected behavior in one of the existing tests was also wrong and had to be fixed. Fixes #5650",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5661
https://github.com/broadinstitute/gatk/pull/5661:294,Testability,test,tests,294,"With the new qual model as the default for HC as of 4.1, occasionally sites get output with a . ALT when the * allele gets dropped. Expected behavior is no output at that site in the VC. Also limit logging for DepthPerSampleHC annotation. Note that the expected behavior in one of the existing tests was also wrong and had to be fixed. Fixes #5650",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5661
https://github.com/broadinstitute/gatk/pull/5665:331,Availability,error,error,331,"This was a bug I uncovered while finalizing #5607, fixing it seems to change the GVCF blocking by enough that I would like somebody to take a look at it @ldgauthier. Specifically look at the last commit on this branch as that is what actually changes the test/code to no longer duplicate the old behavior. For a description of the error this fixes, see #5646. . Fixes #5646 . Blocked by #5607",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5665
https://github.com/broadinstitute/gatk/pull/5665:255,Testability,test,test,255,"This was a bug I uncovered while finalizing #5607, fixing it seems to change the GVCF blocking by enough that I would like somebody to take a look at it @ldgauthier. Specifically look at the last commit on this branch as that is what actually changes the test/code to no longer duplicate the old behavior. For a description of the error this fixes, see #5646. . Fixes #5646 . Blocked by #5607",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5665
https://github.com/broadinstitute/gatk/pull/5669:32,Energy Efficiency,adapt,adaptive,32,This uses the new defaults with adaptive pruning in version 4.1.0.0 in Mutect and removes the old ad hoc pruning argument. @ldgauthier can you please take a look when you get a chance?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5669
https://github.com/broadinstitute/gatk/pull/5669:32,Modifiability,adapt,adaptive,32,This uses the new defaults with adaptive pruning in version 4.1.0.0 in Mutect and removes the old ad hoc pruning argument. @ldgauthier can you please take a look when you get a chance?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5669
https://github.com/broadinstitute/gatk/issues/5670:541,Safety,avoid,avoid,541,"For @jamesemery, per their request, the researcher has submitted test data that I will slack you the location of. They have also provided additional information that will interest you in the forum thread that is not represented below. ---; @shlee ; I uploaded the file to a fileshare folder of the University Mainz and send you a PM with the link and login data. ; One other small thing I found out is, that MDSpark does not take comma in filenames although I quoted the filepath. Is there any way to allow comma in filepath or do I have to avoid using them?. @mack812 ; I tested MDSpark with an unsorted SAM file as input and you are right, it works and the output is a sorted BAM file. So I can skip the SortSam tool wich will save me about 15-20 min in processing time. So MDSpark in total is faster than SortSam+MD +Indexing. ; Thanks for that hint!. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/56337#Comment_56337",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5670
https://github.com/broadinstitute/gatk/issues/5670:65,Testability,test,test,65,"For @jamesemery, per their request, the researcher has submitted test data that I will slack you the location of. They have also provided additional information that will interest you in the forum thread that is not represented below. ---; @shlee ; I uploaded the file to a fileshare folder of the University Mainz and send you a PM with the link and login data. ; One other small thing I found out is, that MDSpark does not take comma in filenames although I quoted the filepath. Is there any way to allow comma in filepath or do I have to avoid using them?. @mack812 ; I tested MDSpark with an unsorted SAM file as input and you are right, it works and the output is a sorted BAM file. So I can skip the SortSam tool wich will save me about 15-20 min in processing time. So MDSpark in total is faster than SortSam+MD +Indexing. ; Thanks for that hint!. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/56337#Comment_56337",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5670
https://github.com/broadinstitute/gatk/issues/5670:351,Testability,log,login,351,"For @jamesemery, per their request, the researcher has submitted test data that I will slack you the location of. They have also provided additional information that will interest you in the forum thread that is not represented below. ---; @shlee ; I uploaded the file to a fileshare folder of the University Mainz and send you a PM with the link and login data. ; One other small thing I found out is, that MDSpark does not take comma in filenames although I quoted the filepath. Is there any way to allow comma in filepath or do I have to avoid using them?. @mack812 ; I tested MDSpark with an unsorted SAM file as input and you are right, it works and the output is a sorted BAM file. So I can skip the SortSam tool wich will save me about 15-20 min in processing time. So MDSpark in total is faster than SortSam+MD +Indexing. ; Thanks for that hint!. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/56337#Comment_56337",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5670
https://github.com/broadinstitute/gatk/issues/5670:573,Testability,test,tested,573,"For @jamesemery, per their request, the researcher has submitted test data that I will slack you the location of. They have also provided additional information that will interest you in the forum thread that is not represented below. ---; @shlee ; I uploaded the file to a fileshare folder of the University Mainz and send you a PM with the link and login data. ; One other small thing I found out is, that MDSpark does not take comma in filenames although I quoted the filepath. Is there any way to allow comma in filepath or do I have to avoid using them?. @mack812 ; I tested MDSpark with an unsorted SAM file as input and you are right, it works and the output is a sorted BAM file. So I can skip the SortSam tool wich will save me about 15-20 min in processing time. So MDSpark in total is faster than SortSam+MD +Indexing. ; Thanks for that hint!. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/56337#Comment_56337",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5670
https://github.com/broadinstitute/gatk/issues/5671:211,Availability,error,error,211,"~~The `ALL_TRANSCRIPTS` output from Funcotator is not properly parsed by the built-in parsing methods for the funcotations.~~. ~~This should be fixed so that these parsing methods will work without producing an error.~~. -----------. It turns out that at least for `ClinVar_VCF_CLNVI`, hashes aren't being properly cleaned (i.e. URL encoded) before writing to the VCF fields. This is bad, because hash is our delimiter for `ALL_TRANSCRIPTS` mode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5671
https://github.com/broadinstitute/gatk/issues/5671:286,Security,hash,hashes,286,"~~The `ALL_TRANSCRIPTS` output from Funcotator is not properly parsed by the built-in parsing methods for the funcotations.~~. ~~This should be fixed so that these parsing methods will work without producing an error.~~. -----------. It turns out that at least for `ClinVar_VCF_CLNVI`, hashes aren't being properly cleaned (i.e. URL encoded) before writing to the VCF fields. This is bad, because hash is our delimiter for `ALL_TRANSCRIPTS` mode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5671
https://github.com/broadinstitute/gatk/issues/5671:397,Security,hash,hash,397,"~~The `ALL_TRANSCRIPTS` output from Funcotator is not properly parsed by the built-in parsing methods for the funcotations.~~. ~~This should be fixed so that these parsing methods will work without producing an error.~~. -----------. It turns out that at least for `ClinVar_VCF_CLNVI`, hashes aren't being properly cleaned (i.e. URL encoded) before writing to the VCF fields. This is bad, because hash is our delimiter for `ALL_TRANSCRIPTS` mode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5671
https://github.com/broadinstitute/gatk/issues/5674:255,Availability,error,error,255,"## Bug Report. ### Affected tool(s) or class(es); _VariantEval, -O, --output_. ### Affected version(s); - [X] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Program starts then terminates with error:; `***********************************************************************`; `A USER ERROR has occurred: Couldn't read file file:///[.....]/X034.eval.grp. Error was: It doesn't exist.`; `***********************************************************************`. Note: The error is not thrown, and VariantEval completes successfully, if a zero-byte file with the name passed with the `-o` or `--output` arguments is created before executing VariantEval; For this example: `touch X034.eval.grp`. #### Steps to reproduce; Command line:; `gatk VariantEval -R $ref -L autosomes.list --eval W034.raw.annotated.vcf.gz --dbsnp dbsnp.vcf.gz -O X034.eval.grp`. #### Expected behavior; Expect the program to create its own output file (as other GATK tools do). #### Actual behavior; Terminates with error (above). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674
https://github.com/broadinstitute/gatk/issues/5674:346,Availability,ERROR,ERROR,346,"## Bug Report. ### Affected tool(s) or class(es); _VariantEval, -O, --output_. ### Affected version(s); - [X] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Program starts then terminates with error:; `***********************************************************************`; `A USER ERROR has occurred: Couldn't read file file:///[.....]/X034.eval.grp. Error was: It doesn't exist.`; `***********************************************************************`. Note: The error is not thrown, and VariantEval completes successfully, if a zero-byte file with the name passed with the `-o` or `--output` arguments is created before executing VariantEval; For this example: `touch X034.eval.grp`. #### Steps to reproduce; Command line:; `gatk VariantEval -R $ref -L autosomes.list --eval W034.raw.annotated.vcf.gz --dbsnp dbsnp.vcf.gz -O X034.eval.grp`. #### Expected behavior; Expect the program to create its own output file (as other GATK tools do). #### Actual behavior; Terminates with error (above). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674
https://github.com/broadinstitute/gatk/issues/5674:416,Availability,Error,Error,416,"## Bug Report. ### Affected tool(s) or class(es); _VariantEval, -O, --output_. ### Affected version(s); - [X] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Program starts then terminates with error:; `***********************************************************************`; `A USER ERROR has occurred: Couldn't read file file:///[.....]/X034.eval.grp. Error was: It doesn't exist.`; `***********************************************************************`. Note: The error is not thrown, and VariantEval completes successfully, if a zero-byte file with the name passed with the `-o` or `--output` arguments is created before executing VariantEval; For this example: `touch X034.eval.grp`. #### Steps to reproduce; Command line:; `gatk VariantEval -R $ref -L autosomes.list --eval W034.raw.annotated.vcf.gz --dbsnp dbsnp.vcf.gz -O X034.eval.grp`. #### Expected behavior; Expect the program to create its own output file (as other GATK tools do). #### Actual behavior; Terminates with error (above). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674
https://github.com/broadinstitute/gatk/issues/5674:532,Availability,error,error,532,"## Bug Report. ### Affected tool(s) or class(es); _VariantEval, -O, --output_. ### Affected version(s); - [X] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Program starts then terminates with error:; `***********************************************************************`; `A USER ERROR has occurred: Couldn't read file file:///[.....]/X034.eval.grp. Error was: It doesn't exist.`; `***********************************************************************`. Note: The error is not thrown, and VariantEval completes successfully, if a zero-byte file with the name passed with the `-o` or `--output` arguments is created before executing VariantEval; For this example: `touch X034.eval.grp`. #### Steps to reproduce; Command line:; `gatk VariantEval -R $ref -L autosomes.list --eval W034.raw.annotated.vcf.gz --dbsnp dbsnp.vcf.gz -O X034.eval.grp`. #### Expected behavior; Expect the program to create its own output file (as other GATK tools do). #### Actual behavior; Terminates with error (above). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674
https://github.com/broadinstitute/gatk/issues/5674:1048,Availability,error,error,1048,"## Bug Report. ### Affected tool(s) or class(es); _VariantEval, -O, --output_. ### Affected version(s); - [X] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Program starts then terminates with error:; `***********************************************************************`; `A USER ERROR has occurred: Couldn't read file file:///[.....]/X034.eval.grp. Error was: It doesn't exist.`; `***********************************************************************`. Note: The error is not thrown, and VariantEval completes successfully, if a zero-byte file with the name passed with the `-o` or `--output` arguments is created before executing VariantEval; For this example: `touch X034.eval.grp`. #### Steps to reproduce; Command line:; `gatk VariantEval -R $ref -L autosomes.list --eval W034.raw.annotated.vcf.gz --dbsnp dbsnp.vcf.gz -O X034.eval.grp`. #### Expected behavior; Expect the program to create its own output file (as other GATK tools do). #### Actual behavior; Terminates with error (above). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674
https://github.com/broadinstitute/gatk/issues/5674:124,Deployability,release,release,124,"## Bug Report. ### Affected tool(s) or class(es); _VariantEval, -O, --output_. ### Affected version(s); - [X] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Program starts then terminates with error:; `***********************************************************************`; `A USER ERROR has occurred: Couldn't read file file:///[.....]/X034.eval.grp. Error was: It doesn't exist.`; `***********************************************************************`. Note: The error is not thrown, and VariantEval completes successfully, if a zero-byte file with the name passed with the `-o` or `--output` arguments is created before executing VariantEval; For this example: `touch X034.eval.grp`. #### Steps to reproduce; Command line:; `gatk VariantEval -R $ref -L autosomes.list --eval W034.raw.annotated.vcf.gz --dbsnp dbsnp.vcf.gz -O X034.eval.grp`. #### Expected behavior; Expect the program to create its own output file (as other GATK tools do). #### Actual behavior; Terminates with error (above). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674
https://github.com/broadinstitute/gatk/issues/5674:193,Testability,test,test,193,"## Bug Report. ### Affected tool(s) or class(es); _VariantEval, -O, --output_. ### Affected version(s); - [X] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; Program starts then terminates with error:; `***********************************************************************`; `A USER ERROR has occurred: Couldn't read file file:///[.....]/X034.eval.grp. Error was: It doesn't exist.`; `***********************************************************************`. Note: The error is not thrown, and VariantEval completes successfully, if a zero-byte file with the name passed with the `-o` or `--output` arguments is created before executing VariantEval; For this example: `touch X034.eval.grp`. #### Steps to reproduce; Command line:; `gatk VariantEval -R $ref -L autosomes.list --eval W034.raw.annotated.vcf.gz --dbsnp dbsnp.vcf.gz -O X034.eval.grp`. #### Expected behavior; Expect the program to create its own output file (as other GATK tools do). #### Actual behavior; Terminates with error (above). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5674
https://github.com/broadinstitute/gatk/issues/5676:543,Modifiability,extend,extendedSpan,543,"The HaplotypeCaller has slightly different behavior between VCF and GVCF output in some cases, which means that the variants at the edges of the active region may not be called in both. This is due to the following line:; https://github.com/broadinstitute/gatk/blob/89ea9e01225db5c9bbe262c888a0abb74509f94c/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyRegionTrimmer.java#L318. The behavior for VCF mode should be made to conform to GVCF mode by defining `callableRegion = originalRegion.trim(callableSpan, extendedSpan);`. Super easy fix, but will break a bunch of tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5676
https://github.com/broadinstitute/gatk/issues/5676:602,Testability,test,tests,602,"The HaplotypeCaller has slightly different behavior between VCF and GVCF output in some cases, which means that the variants at the edges of the active region may not be called in both. This is due to the following line:; https://github.com/broadinstitute/gatk/blob/89ea9e01225db5c9bbe262c888a0abb74509f94c/src/main/java/org/broadinstitute/hellbender/tools/walkers/haplotypecaller/AssemblyRegionTrimmer.java#L318. The behavior for VCF mode should be made to conform to GVCF mode by defining `callableRegion = originalRegion.trim(callableSpan, extendedSpan);`. Super easy fix, but will break a bunch of tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5676
https://github.com/broadinstitute/gatk/issues/5678:145,Deployability,integrat,integration,145,"This `DS` annotation has been kicking around in the header for a long time, but I've never seen it in the wild. It doesn't show up in any of our integration tests. We only have the ability to add this annotation if the GenotypingEngine gets a non-null `Map<String, AlignmentContext> stratifiedContexts`, but that doesn't seem to be the case in *_any_* of our tests. Maybe it's a holdover from UnifiedGenotyper?. @davidbenjamin have you seen cases where we `calculateGenotypes` with stratifiedContexts (or refContext or rawContext or likelihoods)? Given that there's zero test coverage, how would you feel about ripping it out and seeing if anyone complains?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678
https://github.com/broadinstitute/gatk/issues/5678:145,Integrability,integrat,integration,145,"This `DS` annotation has been kicking around in the header for a long time, but I've never seen it in the wild. It doesn't show up in any of our integration tests. We only have the ability to add this annotation if the GenotypingEngine gets a non-null `Map<String, AlignmentContext> stratifiedContexts`, but that doesn't seem to be the case in *_any_* of our tests. Maybe it's a holdover from UnifiedGenotyper?. @davidbenjamin have you seen cases where we `calculateGenotypes` with stratifiedContexts (or refContext or rawContext or likelihoods)? Given that there's zero test coverage, how would you feel about ripping it out and seeing if anyone complains?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678
https://github.com/broadinstitute/gatk/issues/5678:157,Testability,test,tests,157,"This `DS` annotation has been kicking around in the header for a long time, but I've never seen it in the wild. It doesn't show up in any of our integration tests. We only have the ability to add this annotation if the GenotypingEngine gets a non-null `Map<String, AlignmentContext> stratifiedContexts`, but that doesn't seem to be the case in *_any_* of our tests. Maybe it's a holdover from UnifiedGenotyper?. @davidbenjamin have you seen cases where we `calculateGenotypes` with stratifiedContexts (or refContext or rawContext or likelihoods)? Given that there's zero test coverage, how would you feel about ripping it out and seeing if anyone complains?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678
https://github.com/broadinstitute/gatk/issues/5678:359,Testability,test,tests,359,"This `DS` annotation has been kicking around in the header for a long time, but I've never seen it in the wild. It doesn't show up in any of our integration tests. We only have the ability to add this annotation if the GenotypingEngine gets a non-null `Map<String, AlignmentContext> stratifiedContexts`, but that doesn't seem to be the case in *_any_* of our tests. Maybe it's a holdover from UnifiedGenotyper?. @davidbenjamin have you seen cases where we `calculateGenotypes` with stratifiedContexts (or refContext or rawContext or likelihoods)? Given that there's zero test coverage, how would you feel about ripping it out and seeing if anyone complains?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678
https://github.com/broadinstitute/gatk/issues/5678:571,Testability,test,test,571,"This `DS` annotation has been kicking around in the header for a long time, but I've never seen it in the wild. It doesn't show up in any of our integration tests. We only have the ability to add this annotation if the GenotypingEngine gets a non-null `Map<String, AlignmentContext> stratifiedContexts`, but that doesn't seem to be the case in *_any_* of our tests. Maybe it's a holdover from UnifiedGenotyper?. @davidbenjamin have you seen cases where we `calculateGenotypes` with stratifiedContexts (or refContext or rawContext or likelihoods)? Given that there's zero test coverage, how would you feel about ripping it out and seeing if anyone complains?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5678
https://github.com/broadinstitute/gatk/issues/5680:827,Deployability,Configurat,Configuration,827,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:6737,Deployability,pipeline,pipelines,6737,ger.scala:1029); 	at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:792); 	at org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:1350); 	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:122); 	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88); 	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); 	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:56); 	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1488); 	at org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:650); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:191); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:316); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:7698,Deployability,pipeline,pipelines,7698,dinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:10951,Integrability,Message,MessageHub,10951,ionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.ConcurrentModificationException; 	at java.util.Vector$Itr.checkForComodification(Vector.java:1184); 	at java.util.Vector$Itr.next(Vector.java:1137); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 120 more; ```. @jamesemery @t,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:10974,Integrability,Message,MessageHub,10974,35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.ConcurrentModificationException; 	at java.util.Vector$Itr.checkForComodification(Vector.java:1184); 	at java.util.Vector$Itr.next(Vector.java:1137); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 120 more; ```. @jamesemery @tomwhite I've seen,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:827,Modifiability,Config,Configuration,827,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:66,Performance,Concurren,ConcurrentModificationException,66,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:204,Performance,race condition,race condition,204,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:692,Performance,Concurren,ConcurrentModificationException,692,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:11020,Performance,concurren,concurrent,11020,"Dispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.ConcurrentModificationException; 	at java.util.Vector$Itr.checkForComodification(Vector.java:1184); 	at java.util.Vector$Itr.next(Vector.java:1137); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 120 more; ```. @jamesemery @tomwhite I've seen this once, so it may be a super rare one that w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:11128,Performance,concurren,concurrent,11128,"ispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.ConcurrentModificationException; 	at java.util.Vector$Itr.checkForComodification(Vector.java:1184); 	at java.util.Vector$Itr.next(Vector.java:1137); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 120 more; ```. @jamesemery @tomwhite I've seen this once, so it may be a super rare one that we're just hitting now, or something newly introduced. Not sure there's anything to do until we see it more o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:11213,Performance,concurren,concurrent,11213,"ternal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.ConcurrentModificationException; 	at java.util.Vector$Itr.checkForComodification(Vector.java:1184); 	at java.util.Vector$Itr.next(Vector.java:1137); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 120 more; ```. @jamesemery @tomwhite I've seen this once, so it may be a super rare one that we're just hitting now, or something newly introduced. Not sure there's anything to do until we see it more often, but thought I'd record it in case it keeps coming back.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:11298,Performance,concurren,concurrent,11298,"ternal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.ConcurrentModificationException; 	at java.util.Vector$Itr.checkForComodification(Vector.java:1184); 	at java.util.Vector$Itr.next(Vector.java:1137); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 120 more; ```. @jamesemery @tomwhite I've seen this once, so it may be a super rare one that we're just hitting now, or something newly introduced. Not sure there's anything to do until we see it more often, but thought I'd record it in case it keeps coming back.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:11433,Performance,Concurren,ConcurrentModificationException,11433,"ternal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: java.util.ConcurrentModificationException; 	at java.util.Vector$Itr.checkForComodification(Vector.java:1184); 	at java.util.Vector$Itr.next(Vector.java:1137); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:92); 	at com.esotericsoftware.kryo.serializers.CollectionSerializer.write(CollectionSerializer.java:40); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 120 more; ```. @jamesemery @tomwhite I've seen this once, so it may be a super rare one that we're just hitting now, or something newly introduced. Not sure there's anything to do until we see it more often, but thought I'd record it in case it keeps coming back.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:151,Testability,test,testReadsPipelineSpark,151,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:235,Testability,test,testReadsPipelineSpark,235,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:323,Testability,test,test,323,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:480,Testability,test,test,480,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:549,Testability,test,test,549,"We've seen at least 1 non-deterministically occurring instance of ConcurrentModificationException while running the `ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark[5]`. It seems like there is a race condition somewhere. ```; testReadsPipelineSpark[5](ReadsPipeline(bam='/home/travis/build/broadinstitute/gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.tiny.unaligned.bam', args='--align --bwa-mem-index-image /home/travis/build/broadinstitute/gatk/src/test/resources/large/human_g1k_v37.20.21.fasta.img --known-sites src/test/resources/org/broadinstitute/hellbender/tools/BQSR/dbsnp_138.b37.20.10m-10m100.vcf')); com.esotericsoftware.kryo.KryoException: java.util.ConcurrentModificationException; Serialization trace:; classes (sun.misc.Launcher$AppClassLoader); classLoader (org.apache.hadoop.conf.Configuration); conf (org.apache.hadoop.hdfs.DistributedFileSystem); fs (hdfs.jsr203.HadoopFileSystem); hdfs (hdfs.jsr203.HadoopPath); path (htsjdk.samtools.seekablestream.SeekablePathStream); seekableStream (htsjdk.tribble.TribbleIndexedFeatureReader); featureReader (org.broadinstitute.hellbender.engine.FeatureDataSource); featureSources (org.broadinstitute.hellbender.engine.FeatureManager); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:101); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:518); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectFie",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:7742,Testability,test,testReadsPipelineSpark,7742,ool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8111,Testability,test,testng,8111,at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8205,Testability,test,testng,8205,ineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8269,Testability,test,testng,8269,CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.grad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8337,Testability,test,testng,8337,g.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8407,Testability,test,testng,8407, 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tas,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8423,Testability,Test,TestMethodWorker,8423,te.hellbender.Main.instanceMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.Tes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8458,Testability,Test,TestMethodWorker,8458,eMain(Main.java:148); 	at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8494,Testability,test,testng,8494,tute.hellbender.Main.instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8510,Testability,Test,TestMethodWorker,8510,instanceMain(Main.java:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.intern,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8531,Testability,Test,TestMethodWorker,8531,ava:189); 	at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.Su,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8567,Testability,test,testng,8567,hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteT,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8574,Testability,Test,TestRunner,8574,r.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8596,Testability,Test,TestRunner,8596,ramTest.runCommandLine(CommandLineProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8626,Testability,test,testng,8626,eProgramTest.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAcc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8633,Testability,Test,TestRunner,8633,est.java:27); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8648,Testability,Test,TestRunner,8648, 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Nati,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8678,Testability,test,testng,8678,nder.tools.spark.pipelines.ReadsPipelineSparkIntegrationTest.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.Na,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8736,Testability,test,testng,8736,st.testReadsPipelineSpark(ReadsPipelineSparkIntegrationTest.java:125); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8802,Testability,test,testng,8802,25); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delegat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8863,Testability,test,testng,8863,Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8917,Testability,test,testng,8917,ke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:8987,Testability,test,testng,8987,AccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.g,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9052,Testability,test,testng,9052,va.lang.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9059,Testability,Test,TestNG,9059,.reflect.Method.invoke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9088,Testability,Test,TestNG,9088,voke(Method.java:498); 	at org.testng.internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.g,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9115,Testability,test,testng,9115,internal.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9122,Testability,Test,TestNG,9122,l.MethodInvocationHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9146,Testability,Test,TestNG,9146,onHelper.invokeMethod(MethodInvocationHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9173,Testability,test,testng,9173,ionHelper.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32);,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9180,Testability,Test,TestNG,9180,er.java:108); 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at or,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9197,Testability,Test,TestNG,9197,; 	at org.testng.internal.Invoker.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.int,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9224,Testability,test,testng,9224,r.invokeMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9231,Testability,Test,TestNG,9231,eMethod(Invoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$Dis,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9242,Testability,Test,TestNG,9242,nvoker.java:661); 	at org.testng.internal.Invoker.invokeTestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingI,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9295,Testability,test,testing,9295,TestMethod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9303,Testability,test,testng,9303,hod(Invoker.java:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9310,Testability,Test,TestNGTestClassProcessor,9310,:869); 	at org.testng.internal.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9344,Testability,Test,TestNGTestClassProcessor,9344,.Invoker.invokeTestMethods(Invoker.java:1193); 	at org.testng.internal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9414,Testability,test,testing,9414,rnal.TestMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9422,Testability,test,testng,9422,stMethodWorker.invokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9429,Testability,Test,TestNGTestClassProcessor,9429,nvokeTestMethods(TestMethodWorker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9459,Testability,Test,TestNGTestClassProcessor,9459,orker.java:126); 	at org.testng.internal.TestMethodWorker.run(TestMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccesso,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:9528,Testability,test,testing,9528,estMethodWorker.java:109); 	at org.testng.TestRunner.privateRun(TestRunner.java:744); 	at org.testng.TestRunner.run(TestRunner.java:602); 	at org.testng.SuiteRunner.runTest(SuiteRunner.java:380); 	at org.testng.SuiteRunner.runSequentially(SuiteRunner.java:375); 	at org.testng.SuiteRunner.privateRun(SuiteRunner.java:340); 	at org.testng.SuiteRunner.run(SuiteRunner.java:289); 	at org.testng.SuiteRunnerWorker.runSuite(SuiteRunnerWorker.java:52); 	at org.testng.SuiteRunnerWorker.run(SuiteRunnerWorker.java:86); 	at org.testng.TestNG.runSuitesSequentially(TestNG.java:1301); 	at org.testng.TestNG.runSuitesLocally(TestNG.java:1226); 	at org.testng.TestNG.runSuites(TestNG.java:1144); 	at org.testng.TestNG.run(TestNG.java:1115); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.runTests(TestNGTestClassProcessor.java:129); 	at org.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAcces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:10381,Testability,test,testing,10381,rg.gradle.api.internal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Th,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:10396,Testability,Test,TestWorker,10396,ternal.tasks.testing.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/issues/5680:10412,Testability,Test,TestWorker,10412,esting.testng.TestNGTestClassProcessor.stop(TestNGTestClassProcessor.java:88); 	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:61); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:32); 	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:93); 	at com.sun.proxy.$Proxy2.stop(Unknown Source); 	at org.gradle.api.internal.tasks.testing.worker.TestWorker.stop(TestWorker.java:120); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:35); 	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24); 	at org.gradle.internal.remote.internal.hub.MessageHub$Handler.run(MessageHub.java:377); 	at org.gradle.internal.concurrent.ExecutorPolicy$CatchAndRecordFailures.onExecute(ExecutorPolicy.java:54); 	at org.gradle.internal.concurrent.StoppableExecutorImpl$1.run(StoppableExecutorImpl.java:40); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Cau,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5680
https://github.com/broadinstitute/gatk/pull/5681:91,Deployability,Integrat,IntegrationTestSpec,91,Fixes https://github.com/broadinstitute/gatk/issues/5674. All the tests are written using `IntegrationTestSpec` which creates the output files before running the test. Verified manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5681
https://github.com/broadinstitute/gatk/pull/5681:91,Integrability,Integrat,IntegrationTestSpec,91,Fixes https://github.com/broadinstitute/gatk/issues/5674. All the tests are written using `IntegrationTestSpec` which creates the output files before running the test. Verified manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5681
https://github.com/broadinstitute/gatk/pull/5681:66,Testability,test,tests,66,Fixes https://github.com/broadinstitute/gatk/issues/5674. All the tests are written using `IntegrationTestSpec` which creates the output files before running the test. Verified manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5681
https://github.com/broadinstitute/gatk/pull/5681:162,Testability,test,test,162,Fixes https://github.com/broadinstitute/gatk/issues/5674. All the tests are written using `IntegrationTestSpec` which creates the output files before running the test. Verified manually.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5681
https://github.com/broadinstitute/gatk/issues/5683:761,Availability,down,down,761,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description ; Funcotator returns a `NullPointerException` when trying to output compressed VCF:. ```; 15:35:26.085 INFO Funcotator - Creating a VCF file for output: XXXX; 15:35:26.125 INFO ProgressMeter - Starting traversal; 15:35:26.125 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.vcf.VcfFuncotationFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 15:35:26.328 INFO Funcotator - Shutting down engine; [February 15, 2019 3:35:26 PM EST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=3391094784; java.lang.NullPointerException; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:231); at org.broadinstitute.hellbender.tools.funcotator.vcfOutput.VcfOutputRenderer.close(VcfOutputRenderer.java:137); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:883); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:970); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5683
https://github.com/broadinstitute/gatk/issues/5683:2755,Availability,error,error,2755,"mory()=3391094784; java.lang.NullPointerException; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:231); at org.broadinstitute.hellbender.tools.funcotator.vcfOutput.VcfOutputRenderer.close(VcfOutputRenderer.java:137); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:883); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:970); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. #### Steps to reproduce. Does not work; ```; gatk Funcotator --variant input.vcf.gz --reference /path/to/human_g1k_v37_decoy.fasta --ref-version hg19 --data-sources-path /path/to/funcotator_dataSources.v1.6.20190124s/ --output output.vcf.gz --output-file-format VCF; ```. Works; ```; gatk Funcotator --variant input.vcf.gz --reference /path/to/human_g1k_v37_decoy.fasta --ref-version hg19 --data-sources-path /path/to/funcotator_dataSources.v1.6.20190124s/ --output output.vcf --output-file-format VCF; ```. (note that the `--output` parameter is different). #### Expected behavior; It should either give an error/warning saying outputting compressed VCF output is not supported, or output a compressed VCF like other GATK tools. #### Actual behavior; See examples above",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5683
https://github.com/broadinstitute/gatk/issues/5683:102,Deployability,release,release,102,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description ; Funcotator returns a `NullPointerException` when trying to output compressed VCF:. ```; 15:35:26.085 INFO Funcotator - Creating a VCF file for output: XXXX; 15:35:26.125 INFO ProgressMeter - Starting traversal; 15:35:26.125 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.vcf.VcfFuncotationFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 15:35:26.328 INFO Funcotator - Shutting down engine; [February 15, 2019 3:35:26 PM EST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=3391094784; java.lang.NullPointerException; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:231); at org.broadinstitute.hellbender.tools.funcotator.vcfOutput.VcfOutputRenderer.close(VcfOutputRenderer.java:137); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:883); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:970); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5683
https://github.com/broadinstitute/gatk/issues/5683:483,Testability,log,logger,483,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description ; Funcotator returns a `NullPointerException` when trying to output compressed VCF:. ```; 15:35:26.085 INFO Funcotator - Creating a VCF file for output: XXXX; 15:35:26.125 INFO ProgressMeter - Starting traversal; 15:35:26.125 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.vcf.VcfFuncotationFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 15:35:26.328 INFO Funcotator - Shutting down engine; [February 15, 2019 3:35:26 PM EST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=3391094784; java.lang.NullPointerException; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:231); at org.broadinstitute.hellbender.tools.funcotator.vcfOutput.VcfOutputRenderer.close(VcfOutputRenderer.java:137); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:883); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:970); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5683
https://github.com/broadinstitute/gatk/issues/5683:658,Testability,log,logging,658,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description ; Funcotator returns a `NullPointerException` when trying to output compressed VCF:. ```; 15:35:26.085 INFO Funcotator - Creating a VCF file for output: XXXX; 15:35:26.125 INFO ProgressMeter - Starting traversal; 15:35:26.125 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; log4j:WARN No appenders could be found for logger (org.broadinstitute.hellbender.tools.funcotator.dataSources.vcf.VcfFuncotationFactory).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 15:35:26.328 INFO Funcotator - Shutting down engine; [February 15, 2019 3:35:26 PM EST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 0.18 minutes.; Runtime.totalMemory()=3391094784; java.lang.NullPointerException; at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeFeature(TabixIndexCreator.java:106); at htsjdk.tribble.index.tabix.TabixIndexCreator.finalizeIndex(TabixIndexCreator.java:129); at htsjdk.variant.variantcontext.writer.IndexingVariantContextWriter.close(IndexingVariantContextWriter.java:177); at htsjdk.variant.variantcontext.writer.VCFWriter.close(VCFWriter.java:231); at org.broadinstitute.hellbender.tools.funcotator.vcfOutput.VcfOutputRenderer.close(VcfOutputRenderer.java:137); at org.broadinstitute.hellbender.tools.funcotator.Funcotator.closeTool(Funcotator.java:883); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:970); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgra",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5683
https://github.com/broadinstitute/gatk/issues/5684:396,Availability,error,error,396,"## Bug Report. ### Affected tool(s) or class(es); _FilterMutectCalls_. ### Affected version(s); - gatk-4.1.0.0 (_latest_). ### Description . Hi,. I am using _Mutect2_ (v4.1.0.0) and similar to a previous bug reported on `AF=.`, _FilterMutectCalls_ seems to complain about MPOS fields having a value of `.`. No intermediate processing was done between _Mutect2_ and _FilterMutectCalls_. Below the error stack trace :. ```; 17:13:28.491 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data-ddn/home/anthony/sbx/mutect2/work/conda/gatk4-mutect2-nf-bcf605d6af4c0524a368d3d105898641/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:13:30.503 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.503 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.0.0; 17:13:30.504 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:13:30.504 INFO FilterMutectCalls - Executing as anthony@node063 on Linux v2.6.32-220.el6.x86_64 amd64; 17:13:30.504 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 17:13:30.504 INFO FilterMutectCalls - Start Date/Time: February 17, 2019 5:13:28 PM CET; 17:13:30.504 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Version: 2.18.2; 17:13:30.505 INFO FilterMutectCalls - Picard Version: 2.18.25; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:13:30.506 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:13:30.506 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5684:2824,Availability,down,down,2824,"s - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:13:30.506 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:13:30.506 INFO FilterMutectCalls - Deflater: IntelDeflater; 17:13:30.506 INFO FilterMutectCalls - Inflater: IntelInflater; 17:13:30.506 INFO FilterMutectCalls - GCS max retries/reopens: 20; 17:13:30.506 INFO FilterMutectCalls - Requester pays: disabled; 17:13:30.506 INFO FilterMutectCalls - Initializing engine; 17:13:30.997 INFO FeatureManager - Using codec VCFCodec to read file file:///data-ddn/home/anthony/sbx/mutect2/work/ea/18e314102728d4b34b636b04f1f897/her2-crefix-unfiltered.vcf; 17:13:31.136 INFO FilterMutectCalls - Done initializing engine; 17:13:31.285 INFO ProgressMeter - Starting traversal; 17:13:31.286 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 17:13:31.286 INFO FilterMutectCalls - Starting first pass through the variants; 17:13:31.570 INFO FilterMutectCalls - Shutting down engine; [February 17, 2019 5:13:31 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=845676544; java.lang.NumberFormatException: For input string: "".""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Integer.parseInt(Integer.java:569); 	at java.lang.Integer.valueOf(Integer.java:766); 	at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:287); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Collections$2.tryAdvance(Collections.java:4717); 	at java.util.Collections$2.forEachRemaining(Collections.java:4725); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5684:1194,Deployability,release,release-,1194,"bug reported on `AF=.`, _FilterMutectCalls_ seems to complain about MPOS fields having a value of `.`. No intermediate processing was done between _Mutect2_ and _FilterMutectCalls_. Below the error stack trace :. ```; 17:13:28.491 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data-ddn/home/anthony/sbx/mutect2/work/conda/gatk4-mutect2-nf-bcf605d6af4c0524a368d3d105898641/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:13:30.503 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.503 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.0.0; 17:13:30.504 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:13:30.504 INFO FilterMutectCalls - Executing as anthony@node063 on Linux v2.6.32-220.el6.x86_64 amd64; 17:13:30.504 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 17:13:30.504 INFO FilterMutectCalls - Start Date/Time: February 17, 2019 5:13:28 PM CET; 17:13:30.504 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Version: 2.18.2; 17:13:30.505 INFO FilterMutectCalls - Picard Version: 2.18.25; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:13:30.506 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:13:30.506 INFO FilterMutectCalls - Deflater: IntelDeflater; 17:13:30.506 INFO FilterMutectCalls - Inflater: IntelInflater; 17:13:30.506 INFO FilterMutectCalls - GCS max retries/reopens: 20; 17:13:30.506 INFO FilterMut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5684:3734,Energy Efficiency,Reduce,ReduceOps,3734," first pass through the variants; 17:13:31.570 INFO FilterMutectCalls - Shutting down engine; [February 17, 2019 5:13:31 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=845676544; java.lang.NumberFormatException: For input string: "".""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Integer.parseInt(Integer.java:569); 	at java.lang.Integer.valueOf(Integer.java:766); 	at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:287); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Collections$2.tryAdvance(Collections.java:4717); 	at java.util.Collections$2.forEachRemaining(Collections.java:4725); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:273); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:281); 	at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:738); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyReadPositionFilter(Mutect2FilteringEngine.java:223); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:529); 	at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); 	at java.util.stream.ForEac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5684:3744,Energy Efficiency,Reduce,ReduceOp,3744," first pass through the variants; 17:13:31.570 INFO FilterMutectCalls - Shutting down engine; [February 17, 2019 5:13:31 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=845676544; java.lang.NumberFormatException: For input string: "".""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Integer.parseInt(Integer.java:569); 	at java.lang.Integer.valueOf(Integer.java:766); 	at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:287); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Collections$2.tryAdvance(Collections.java:4717); 	at java.util.Collections$2.forEachRemaining(Collections.java:4725); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:273); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:281); 	at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:738); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyReadPositionFilter(Mutect2FilteringEngine.java:223); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:529); 	at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); 	at java.util.stream.ForEac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5684:3772,Energy Efficiency,Reduce,ReduceOps,3772,"variants; 17:13:31.570 INFO FilterMutectCalls - Shutting down engine; [February 17, 2019 5:13:31 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=845676544; java.lang.NumberFormatException: For input string: "".""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Integer.parseInt(Integer.java:569); 	at java.lang.Integer.valueOf(Integer.java:766); 	at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:287); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Collections$2.tryAdvance(Collections.java:4717); 	at java.util.Collections$2.forEachRemaining(Collections.java:4725); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:273); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:281); 	at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:738); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyReadPositionFilter(Mutect2FilteringEngine.java:223); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:529); 	at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.acc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5684:3669,Integrability,wrap,wrapAndCopyInto,3669,"nts/Minute; 17:13:31.286 INFO FilterMutectCalls - Starting first pass through the variants; 17:13:31.570 INFO FilterMutectCalls - Shutting down engine; [February 17, 2019 5:13:31 PM CET] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=845676544; java.lang.NumberFormatException: For input string: "".""; 	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); 	at java.lang.Integer.parseInt(Integer.java:569); 	at java.lang.Integer.valueOf(Integer.java:766); 	at htsjdk.variant.variantcontext.CommonInfo.lambda$getAttributeAsIntList$1(CommonInfo.java:287); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Collections$2.tryAdvance(Collections.java:4717); 	at java.util.Collections$2.forEachRemaining(Collections.java:4725); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsList(CommonInfo.java:273); 	at htsjdk.variant.variantcontext.CommonInfo.getAttributeAsIntList(CommonInfo.java:281); 	at htsjdk.variant.variantcontext.VariantContext.getAttributeAsIntList(VariantContext.java:738); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyReadPositionFilter(Mutect2FilteringEngine.java:223); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:529); 	at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5684:5135,Integrability,wrap,wrapAndCopyInto,5135,t.VariantContext.getAttributeAsIntList(VariantContext.java:738); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyReadPositionFilter(Mutect2FilteringEngine.java:223); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:529); 	at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); 	at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.ru,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5684:462,Performance,Load,Loading,462,"## Bug Report. ### Affected tool(s) or class(es); _FilterMutectCalls_. ### Affected version(s); - gatk-4.1.0.0 (_latest_). ### Description . Hi,. I am using _Mutect2_ (v4.1.0.0) and similar to a previous bug reported on `AF=.`, _FilterMutectCalls_ seems to complain about MPOS fields having a value of `.`. No intermediate processing was done between _Mutect2_ and _FilterMutectCalls_. Below the error stack trace :. ```; 17:13:28.491 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data-ddn/home/anthony/sbx/mutect2/work/conda/gatk4-mutect2-nf-bcf605d6af4c0524a368d3d105898641/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 17:13:30.503 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.503 INFO FilterMutectCalls - The Genome Analysis Toolkit (GATK) v4.1.0.0; 17:13:30.504 INFO FilterMutectCalls - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:13:30.504 INFO FilterMutectCalls - Executing as anthony@node063 on Linux v2.6.32-220.el6.x86_64 amd64; 17:13:30.504 INFO FilterMutectCalls - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_152-release-1056-b12; 17:13:30.504 INFO FilterMutectCalls - Start Date/Time: February 17, 2019 5:13:28 PM CET; 17:13:30.504 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - ------------------------------------------------------------; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Version: 2.18.2; 17:13:30.505 INFO FilterMutectCalls - Picard Version: 2.18.25; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 17:13:30.505 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 17:13:30.506 INFO FilterMutectCalls - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 17:13:30.506 INF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5684
https://github.com/broadinstitute/gatk/issues/5685:1048,Availability,heartbeat,heartbeatInterval,1048,"lVariationDiscoveryPipelineSpark. ### Affected version(s); GATK 4.1.0.0. ### Description . Running SV program generates a Java exception...; java.lang.IllegalArgumentException: provided start is negative: -1. #### Steps to reproduce; ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv//$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 80G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:2339,Availability,heartbeat,heartbeatInterval,2339,"00 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr --files file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa.img,file:///restricted/projectnb/casa/ref/GRCh38_ignored_kmers.txt --conf spark.yarn.executor.memoryOverhead=5000 --conf spark.network.timeout=600 --conf spark.executor.heartbeatInterval=120 /share/pkg/gatk/4.1.0.0/install/bin/gatk-package-4.1.0.0-spark.jar StructuralVariationDiscoveryPipelineSpark -R file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img --kmers-to-ignore GRCh38_ignored_kmers.txt --contig-sam-file hdfs:///project/casa/gcad/adsp.cc/sv//A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file -I hdfs:///project/casa/gcad/adsp.cc/cram/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.cram -O hdfs:///project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.sv.vcf --spark-master yarn. ```. #### Expected behavior. Run to completion with SV vcf output. #### Actual behavior. ```; 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 85.0 in stage 5.0 (TID 1031) in 28293 ms on scc-q09.scc.bu.edu (executor 30) (74/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresult_1031 on scc-q09.scc.bu.edu:40204 in memory (size: ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:38987,Availability,ERROR,ERROR,38987,"k.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:39354,Availability,failure,failure,39354,"anager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:39413,Availability,failure,failure,39413,"q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:42422,Availability,down,down,42422,"unTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:42541,Availability,down,down,42541,"unTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:42920,Availability,ERROR,ERROR,42920," 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:42956,Availability,Error,Error,42956," 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:46886,Availability,down,down,46886,"Optimized(NioEventLoop.java:580); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QName",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:47148,Availability,failure,failure,47148,"utor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:47207,Availability,failure,failure,47207,"ory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:733,Deployability,deploy,deploy-mode,733,"## Bug Report. ### Affected tool(s) or class(es); StructuralVariationDiscoveryPipelineSpark. ### Affected version(s); GATK 4.1.0.0. ### Description . Running SV program generates a Java exception...; java.lang.IllegalArgumentException: provided start is negative: -1. #### Steps to reproduce; ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv//$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 80G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:1114,Deployability,install,install,1114,"cription . Running SV program generates a Java exception...; java.lang.IllegalArgumentException: provided start is negative: -1. #### Steps to reproduce; ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv//$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 80G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr --files file:///restricted/projectnb/casa/ref/GRCh38_full_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:1876,Deployability,deploy,deploy-mode,1876,"F.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr --files file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa.img,file:///restricted/projectnb/casa/ref/GRCh38_ignored_kmers.txt --conf spark.yarn.executor.memoryOverhead=5000 --conf spark.network.timeout=600 --conf spark.executor.heartbeatInterval=120 /share/pkg/gatk/4.1.0.0/install/bin/gatk-package-4.1.0.0-spark.jar StructuralVariationDiscoveryPipelineSpark -R file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img --kmers-to-ignore GRCh38_ignored_kmers.txt --contig-sam-file hdfs:///project/casa/gcad/adsp.cc/sv//A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file -I hdfs:///project/casa/gcad/adsp.cc/cram/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.cram -O hdfs:///project/casa/gcad/adsp.cc/sv/A",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:2385,Deployability,install,install,2385,"s=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr --files file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa.img,file:///restricted/projectnb/casa/ref/GRCh38_ignored_kmers.txt --conf spark.yarn.executor.memoryOverhead=5000 --conf spark.network.timeout=600 --conf spark.executor.heartbeatInterval=120 /share/pkg/gatk/4.1.0.0/install/bin/gatk-package-4.1.0.0-spark.jar StructuralVariationDiscoveryPipelineSpark -R file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img --kmers-to-ignore GRCh38_ignored_kmers.txt --contig-sam-file hdfs:///project/casa/gcad/adsp.cc/sv//A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file -I hdfs:///project/casa/gcad/adsp.cc/cram/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.cram -O hdfs:///project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.sv.vcf --spark-master yarn. ```. #### Expected behavior. Run to completion with SV vcf output. #### Actual behavior. ```; 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 85.0 in stage 5.0 (TID 1031) in 28293 ms on scc-q09.scc.bu.edu (executor 30) (74/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresult_1031 on scc-q09.scc.bu.edu:40204 in memory (size: 5.4 MB, free: 42.5 GB); 2019-02-17 16:2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:53461,Deployability,deploy,deploy,53461,ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:53542,Deployability,deploy,deploy,53542,e.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:53579,Deployability,deploy,deploy,53579,gram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:53651,Deployability,deploy,deploy,53651,ute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.bro,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:53727,Deployability,deploy,deploy,53727,8); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.j,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:53798,Deployability,deploy,deploy,53798,eMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:53867,Deployability,deploy,deploy,53867,e.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:6268,Energy Efficiency,schedul,scheduler,6268,"lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1024 in memory on scc-q02.scc.bu.edu:34157 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 181.1 in stage 5.0 (TID 1122, scc-q02.scc.bu.edu, executor 24, partition 181, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1013 in memory on scc-q08.scc.bu.edu:45340 (size: 4.6 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 173.0 in stage 5.0 (TID 1123, scc-q08.scc.bu.edu, executor 18, partition 173, NODE_LOCAL, 7996 bytes); 2019-02-17 16:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:6339,Energy Efficiency,schedul,scheduler,6339,"la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1024 in memory on scc-q02.scc.bu.edu:34157 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 181.1 in stage 5.0 (TID 1122, scc-q02.scc.bu.edu, executor 24, partition 181, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1013 in memory on scc-q08.scc.bu.edu:45340 (size: 4.6 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 173.0 in stage 5.0 (TID 1123, scc-q08.scc.bu.edu, executor 18, partition 173, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 71.0 in stage 5.0 (TID 102",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:12627,Energy Efficiency,schedul,scheduler,12627,"lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 179.1 in stage 5.0 (TID 1128, scc-q03.scc.bu.edu, executor 9, partition 179, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1008 in memory on scc-q15.scc.bu.edu:35739 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 182.0 in stage 5.0 (TID 1129, scc-q15.scc.bu.edu, executor 15, partition 182, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 6.0 in stage 5.0 (TID 948) in 28894 ms on scc-q10.scc.bu.edu (executor 14) (83/189); 2019-02-1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:12698,Energy Efficiency,schedul,scheduler,12698,"la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 179.1 in stage 5.0 (TID 1128, scc-q03.scc.bu.edu, executor 9, partition 179, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1008 in memory on scc-q15.scc.bu.edu:35739 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 182.0 in stage 5.0 (TID 1129, scc-q15.scc.bu.edu, executor 15, partition 182, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 6.0 in stage 5.0 (TID 948) in 28894 ms on scc-q10.scc.bu.edu (executor 14) (83/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresult_948 on scc-q10",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:19787,Energy Efficiency,schedul,scheduler,19787,"lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:49 INFO TaskSetManager:54 - Starting task 186.1 in stage 5.0 (TID 1133, scc-q03.scc.bu.edu, executor 26, partition 186, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:49 INFO TaskSetManager:54 - Finished task 55.0 in stage 5.0 (TID 1000) in 29228 ms on scc-q10.scc.bu.edu (executor 14) (92/189); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Removed taskresult_1000 on scc-q10.scc.bu.edu:43393 in memory (size: 4.8 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Added taskresult_997 in memory on scc-q05.scc.bu.edu:36688 (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO Bloc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:19858,Energy Efficiency,schedul,scheduler,19858,"la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:49 INFO TaskSetManager:54 - Starting task 186.1 in stage 5.0 (TID 1133, scc-q03.scc.bu.edu, executor 26, partition 186, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:49 INFO TaskSetManager:54 - Finished task 55.0 in stage 5.0 (TID 1000) in 29228 ms on scc-q10.scc.bu.edu (executor 14) (92/189); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Removed taskresult_1000 on scc-q10.scc.bu.edu:43393 in memory (size: 4.8 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Added taskresult_997 in memory on scc-q05.scc.bu.edu:36688 (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Added taskresult_992 in memory on scc-q02.scc.bu.edu:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:30485,Energy Efficiency,schedul,scheduler,30485,"lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 177.1 in stage 5.0 (TID 1140, scc-q08.scc.bu.edu, executor 29, partition 177, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_1032 in memory on scc-q05.scc.bu.edu:37576 (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_996 in memory on scc-q03.scc.bu.edu:42184 (size: 7.3 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_978 in memory on scc-q16.scc.bu.edu:34227 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:30556,Energy Efficiency,schedul,scheduler,30556,"la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 177.1 in stage 5.0 (TID 1140, scc-q08.scc.bu.edu, executor 29, partition 177, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_1032 in memory on scc-q05.scc.bu.edu:37576 (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_996 in memory on scc-q03.scc.bu.edu:42184 (size: 7.3 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_978 in memory on scc-q16.scc.bu.edu:34227 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 81.0 in stage 5.0 (TID 1032) in 30394 ms on scc-q05.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:37925,Energy Efficiency,schedul,scheduler,37925,"lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:37996,Energy Efficiency,schedul,scheduler,37996,"la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:41534,Energy Efficiency,schedul,scheduler,41534,"lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:41605,Energy Efficiency,schedul,scheduler,41605,"la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stoppi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:42340,Energy Efficiency,monitor,monitor,42340,"unTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:42572,Energy Efficiency,Schedul,SchedulerExtensionServices,42572,"unTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:42613,Energy Efficiency,Schedul,SchedulerExtensionServices,42613,"unTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apac",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49328,Energy Efficiency,schedul,scheduler,49328,lection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.schedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49399,Energy Efficiency,schedul,scheduler,49399,la.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49759,Energy Efficiency,schedul,scheduler,49759,); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49799,Energy Efficiency,schedul,scheduler,49799,cala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSchedule,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49897,Energy Efficiency,schedul,scheduler,49897,cala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onRec,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49994,Energy Efficiency,schedul,scheduler,49994,.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50245,Energy Efficiency,schedul,scheduler,50245,Context$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(Sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50325,Energy Efficiency,schedul,scheduler,50325,heduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.sca,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50430,Energy Efficiency,schedul,scheduler,50430,at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50578,Energy Efficiency,schedul,scheduler,50578,va:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50666,Energy Efficiency,schedul,scheduler,50666,617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.sc,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50763,Energy Efficiency,schedul,scheduler,50763,.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:938); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50858,Energy Efficiency,schedul,scheduler,50858,.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:938); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:51021,Energy Efficiency,schedul,scheduler,51021,abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2092); at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); at org.apache.spark.rdd.RDD.collect(RDD.scala:938); at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); at org.broadinstitute.hellbender.tools.spark.sv.evidence.FindBreakpointEvidenceSpark.getQNames(FindBreakpointEvidenceSpark.java:963); at org.broadinstitute.hellbender.tools.spark.sv.evid,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:55960,Energy Efficiency,schedul,scheduler,55960,appers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-4adbc571-167b-4e5f-af72-d3df0d3601e6; 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-cf45c696-2402-4d31-be67-f3063bec805a. real 5m25.740s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:56031,Energy Efficiency,schedul,scheduler,56031,appers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-4adbc571-167b-4e5f-af72-d3df0d3601e6; 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-cf45c696-2402-4d31-be67-f3063bec805a. real 5m25.740s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:5076,Integrability,Wrap,Wrappers,5076,"DE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 WARN TaskSetManager:66 - Lost task 181.0 in stage 5.0 (TID 1113, scc-q03.scc.bu.edu, executor 9): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:5110,Integrability,Wrap,Wrappers,5110,"; 2019-02-17 16:25:48 WARN TaskSetManager:66 - Lost task 181.0 in stage 5.0 (TID 1113, scc-q03.scc.bu.edu, executor 9): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:11435,Integrability,Wrap,Wrappers,11435,".9 MB, free: 42.5 GB); 2019-02-17 16:25:48 WARN TaskSetManager:66 - Lost task 179.0 in stage 5.0 (TID 1108, scc-q03.scc.bu.edu, executor 9): java.lang.IllegalArgumentException: provided start is negative: -2; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:11469,Integrability,Wrap,Wrappers,11469,"; 2019-02-17 16:25:48 WARN TaskSetManager:66 - Lost task 179.0 in stage 5.0 (TID 1108, scc-q03.scc.bu.edu, executor 9): java.lang.IllegalArgumentException: provided start is negative: -2; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:18595,Integrability,Wrap,Wrappers,18595,".8 MB, free: 42.5 GB); 2019-02-17 16:25:49 WARN TaskSetManager:66 - Lost task 186.0 in stage 5.0 (TID 1121, scc-q03.scc.bu.edu, executor 9): java.lang.IllegalArgumentException: provided start is negative: -5; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:18629,Integrability,Wrap,Wrappers,18629,"; 2019-02-17 16:25:49 WARN TaskSetManager:66 - Lost task 186.0 in stage 5.0 (TID 1121, scc-q03.scc.bu.edu, executor 9): java.lang.IllegalArgumentException: provided start is negative: -5; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:29293,Integrability,Wrap,Wrappers,29293," MB, free: 42.5 GB); 2019-02-17 16:25:50 WARN TaskSetManager:66 - Lost task 177.0 in stage 5.0 (TID 1127, scc-q10.scc.bu.edu, executor 14): java.lang.IllegalArgumentException: provided start is negative: -12; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:29327,Integrability,Wrap,Wrappers,29327,"2019-02-17 16:25:50 WARN TaskSetManager:66 - Lost task 177.0 in stage 5.0 (TID 1127, scc-q10.scc.bu.edu, executor 14): java.lang.IllegalArgumentException: provided start is negative: -12; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:36733,Integrability,Wrap,Wrappers,36733," MB, free: 42.5 GB); 2019-02-17 16:25:50 WARN TaskSetManager:66 - Lost task 178.0 in stage 5.0 (TID 1119, scc-q06.scc.bu.edu, executor 23): java.lang.IllegalArgumentException: provided start is negative: -26; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:36767,Integrability,Wrap,Wrappers,36767,"2019-02-17 16:25:50 WARN TaskSetManager:66 - Lost task 178.0 in stage 5.0 (TID 1119, scc-q06.scc.bu.edu, executor 23): java.lang.IllegalArgumentException: provided start is negative: -26; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:40342,Integrability,Wrap,Wrappers,40342,"ailure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:40376,Integrability,Wrap,Wrappers,40376,"tage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:43010,Integrability,message,message,43010," 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_980 in memory on scc-q04.scc.bu.edu:41981 (size: 4.9 MB, free: 42.5 GB); 2019-02-17 16:25:50 ERROR TransportRequestHandler:210 - Error while invoking RpcHandler#receive() for one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:44412,Integrability,Message,MessageToMessageDecoder,44412,tHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandler,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:44448,Integrability,Message,MessageToMessageDecoder,44448,che.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:48136,Integrability,Wrap,Wrappers,48136,"ailure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:48170,Integrability,Wrap,Wrappers,48170,"tage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkCont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:54768,Integrability,Wrap,Wrappers,54768,org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:54802,Integrability,Wrap,Wrappers,54802,oy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkCont,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:6462,Performance,concurren,concurrent,6462,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1024 in memory on scc-q02.scc.bu.edu:34157 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 181.1 in stage 5.0 (TID 1122, scc-q02.scc.bu.edu, executor 24, partition 181, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1013 in memory on scc-q08.scc.bu.edu:45340 (size: 4.6 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 173.0 in stage 5.0 (TID 1123, scc-q08.scc.bu.edu, executor 18, partition 173, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 71.0 in stage 5.0 (TID 1024) in 28667 ms on scc-q02.scc.bu.edu (executor 24) (77/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:6546,Performance,concurren,concurrent,6546,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1024 in memory on scc-q02.scc.bu.edu:34157 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 181.1 in stage 5.0 (TID 1122, scc-q02.scc.bu.edu, executor 24, partition 181, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1013 in memory on scc-q08.scc.bu.edu:45340 (size: 4.6 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 173.0 in stage 5.0 (TID 1123, scc-q08.scc.bu.edu, executor 18, partition 173, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 71.0 in stage 5.0 (TID 1024) in 28667 ms on scc-q02.scc.bu.edu (executor 24) (77/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresult_1024 on scc-q02.scc.bu.edu:34157 in memory (size: 4.5 MB, free: 42.5 GB); 2019-02",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:12821,Performance,concurren,concurrent,12821,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 179.1 in stage 5.0 (TID 1128, scc-q03.scc.bu.edu, executor 9, partition 179, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1008 in memory on scc-q15.scc.bu.edu:35739 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 182.0 in stage 5.0 (TID 1129, scc-q15.scc.bu.edu, executor 15, partition 182, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 6.0 in stage 5.0 (TID 948) in 28894 ms on scc-q10.scc.bu.edu (executor 14) (83/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresult_948 on scc-q10.scc.bu.edu:43393 in memory (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:12905,Performance,concurren,concurrent,12905,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 179.1 in stage 5.0 (TID 1128, scc-q03.scc.bu.edu, executor 9, partition 179, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_1008 in memory on scc-q15.scc.bu.edu:35739 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Starting task 182.0 in stage 5.0 (TID 1129, scc-q15.scc.bu.edu, executor 15, partition 182, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 6.0 in stage 5.0 (TID 948) in 28894 ms on scc-q10.scc.bu.edu (executor 14) (83/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresult_948 on scc-q10.scc.bu.edu:43393 in memory (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Added taskresult_999 in memory on scc-q02.scc.bu.edu:34157 (size: 5.2 MB, free: 42.5 GB); 2019-02-17 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:19981,Performance,concurren,concurrent,19981,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:49 INFO TaskSetManager:54 - Starting task 186.1 in stage 5.0 (TID 1133, scc-q03.scc.bu.edu, executor 26, partition 186, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:49 INFO TaskSetManager:54 - Finished task 55.0 in stage 5.0 (TID 1000) in 29228 ms on scc-q10.scc.bu.edu (executor 14) (92/189); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Removed taskresult_1000 on scc-q10.scc.bu.edu:43393 in memory (size: 4.8 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Added taskresult_997 in memory on scc-q05.scc.bu.edu:36688 (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Added taskresult_992 in memory on scc-q02.scc.bu.edu:38907 (size: 5.0 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO TaskSetManager:54 - Starting task 187.0 in stage 5.0 (TID 1134",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:20065,Performance,concurren,concurrent,20065,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:49 INFO TaskSetManager:54 - Starting task 186.1 in stage 5.0 (TID 1133, scc-q03.scc.bu.edu, executor 26, partition 186, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:49 INFO TaskSetManager:54 - Finished task 55.0 in stage 5.0 (TID 1000) in 29228 ms on scc-q10.scc.bu.edu (executor 14) (92/189); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Removed taskresult_1000 on scc-q10.scc.bu.edu:43393 in memory (size: 4.8 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Added taskresult_997 in memory on scc-q05.scc.bu.edu:36688 (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO BlockManagerInfo:54 - Added taskresult_992 in memory on scc-q02.scc.bu.edu:38907 (size: 5.0 MB, free: 42.5 GB); 2019-02-17 16:25:49 INFO TaskSetManager:54 - Starting task 187.0 in stage 5.0 (TID 1134, scc-q02.scc.bu.edu, executor 7, partition 187, NODE_LOCAL, 7996 bytes); 2019-02-17",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:30679,Performance,concurren,concurrent,30679,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 177.1 in stage 5.0 (TID 1140, scc-q08.scc.bu.edu, executor 29, partition 177, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_1032 in memory on scc-q05.scc.bu.edu:37576 (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_996 in memory on scc-q03.scc.bu.edu:42184 (size: 7.3 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_978 in memory on scc-q16.scc.bu.edu:34227 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 81.0 in stage 5.0 (TID 1032) in 30394 ms on scc-q05.scc.bu.edu (executor 10) (107/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_1032 on scc-q05.scc.bu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:30763,Performance,concurren,concurrent,30763,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 177.1 in stage 5.0 (TID 1140, scc-q08.scc.bu.edu, executor 29, partition 177, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_1032 in memory on scc-q05.scc.bu.edu:37576 (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_996 in memory on scc-q03.scc.bu.edu:42184 (size: 7.3 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added taskresult_978 in memory on scc-q16.scc.bu.edu:34227 (size: 4.5 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 81.0 in stage 5.0 (TID 1032) in 30394 ms on scc-q05.scc.bu.edu (executor 10) (107/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_1032 on scc-q05.scc.bu.edu:37576 in memory (size: 4.4 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO BlockMa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:38119,Performance,concurren,concurrent,38119,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:38203,Performance,concurren,concurrent,38203,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:41728,Performance,concurren,concurrent,41728,"eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSc",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:41812,Performance,concurren,concurrent,41812,"ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; 2019-02-17 16:25:50 INFO DAGScheduler:54 - Job 4 failed: collect at FindBreakpointEvidenceSpark.java:963, took 30.909355 s; 2019-02-17 16:25:50 INFO AbstractConnector:318 - Stopped Spark@7433ca19{HTTP/1.1,[http/1.1]}{0.0.0.0:4041}; 2019-02-17 16:25:50 INFO SparkUI:54 - Stopped Spark web UI at http://scc-hadoop.bu.edu:4041; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Interrupting monitor thread; 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Shutting down all executors; 2019-02-17 16:25:50 INFO YarnSchedulerBackend$YarnDriverEndpoint:54 - Asking each executor to shut down; 2019-02-17 16:25:50 INFO SchedulerExtensionServices:54 - Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 2019-02-17 16:25:50 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Added ta",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:46078,Performance,concurren,concurrent,46078,"(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=505937",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:46175,Performance,concurren,concurrent,46175,"hannelRead(AbstractChannelHandlerContext.java:340); at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 fa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49522,Performance,concurren,concurrent,49522,eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Opti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49606,Performance,concurren,concurrent,49606,ce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskS,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:56154,Performance,concurren,concurrent,56154,appers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-4adbc571-167b-4e5f-af72-d3df0d3601e6; 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-cf45c696-2402-4d31-be67-f3063bec805a. real 5m25.740s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:56238,Performance,concurren,concurrent,56238,appers$JIteratorWrapper.hasNext(Wrappers.scala:42); at scala.collection.Iterator$class.foreach(Iterator.scala:893); at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); at scala.collection.AbstractIterator.to(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-4adbc571-167b-4e5f-af72-d3df0d3601e6; 2019-02-17 16:25:50 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-cf45c696-2402-4d31-be67-f3063bec805a. real 5m25.740s; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:1011,Safety,timeout,timeout,1011,"ed tool(s) or class(es); StructuralVariationDiscoveryPipelineSpark. ### Affected version(s); GATK 4.1.0.0. ### Description . Running SV program generates a Java exception...; java.lang.IllegalArgumentException: provided start is negative: -1. #### Steps to reproduce; ```; gatk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv//$SAMPLE.contig-sam-file\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 80G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120. ```. ```; Running:; /share/pkg/spark/2.3.0/install/bin/spark-submit --master yarn --conf spark.kryoserializer.buffer.max=512m --conf spark.driver.maxResultSize=0 --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false -",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:2305,Safety,timeout,timeout,2305,"-conf spark.yarn.executor.memoryOverhead=600 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Djava.io.tmpdir=tmp --deploy-mode client --executor-memory 80G --driver-memory 30g --num-executors 40 --executor-cores 4 --conf spark.yarn.submit.waitAppCompletion=false --name A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr --files file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa.img,file:///restricted/projectnb/casa/ref/GRCh38_ignored_kmers.txt --conf spark.yarn.executor.memoryOverhead=5000 --conf spark.network.timeout=600 --conf spark.executor.heartbeatInterval=120 /share/pkg/gatk/4.1.0.0/install/bin/gatk-package-4.1.0.0-spark.jar StructuralVariationDiscoveryPipelineSpark -R file:///restricted/projectnb/casa/ref/GRCh38_full_analysis_set_plus_decoy_hla.fa --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img --kmers-to-ignore GRCh38_ignored_kmers.txt --contig-sam-file hdfs:///project/casa/gcad/adsp.cc/sv//A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file -I hdfs:///project/casa/gcad/adsp.cc/cram/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.cram -O hdfs:///project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.sv.vcf --spark-master yarn. ```. #### Expected behavior. Run to completion with SV vcf output. #### Actual behavior. ```; 2019-02-17 16:25:48 INFO TaskSetManager:54 - Finished task 85.0 in stage 5.0 (TID 1031) in 28293 ms on scc-q09.scc.bu.edu (executor 30) (74/189); 2019-02-17 16:25:48 INFO BlockManagerInfo:54 - Removed taskresult_1031 on s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:39051,Safety,abort,aborting,39051,"adPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). 2019-02-17 16:25:50 INFO TaskSetManager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.sp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:39333,Safety,abort,aborted,39333,"anager:54 - Starting task 178.1 in stage 5.0 (TID 1142, scc-q06.scc.bu.edu, executor 23, partition 178, NODE_LOCAL, 7996 bytes); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Finished task 12.0 in stage 5.0 (TID 957) in 30736 ms on scc-q15.scc.bu.edu (executor 15) (117/189); 2019-02-17 16:25:50 INFO BlockManagerInfo:54 - Removed taskresult_957 on scc-q15.scc.bu.edu:35739 in memory (size: 5.2 MB, free: 42.5 GB); 2019-02-17 16:25:50 INFO TaskSetManager:54 - Lost task 181.3 in stage 5.0 (TID 1139) on scc-q02.scc.bu.edu, executor 24: java.lang.IllegalArgumentException (provided start is negative: -1) [duplicate 3]; 2019-02-17 16:25:50 ERROR TaskSetManager:70 - Task 181 in stage 5.0 failed 4 times; aborting job; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Cancelling stage 5; 2019-02-17 16:25:50 INFO YarnScheduler:54 - Stage 5 was cancelled; 2019-02-17 16:25:50 INFO DAGScheduler:54 - ResultStage 5 (collect at FindBreakpointEvidenceSpark.java:963) failed in 30.887 s due to Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappers",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:43996,Safety,timeout,timeout,43996,r one-way message.; org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.; at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:160); at org.apache.spark.rpc.netty.Dispatcher.postOneWayMessage(Dispatcher.scala:140); at org.apache.spark.rpc.netty.NettyRpcHandler.receive(NettyRpcEnv.scala:655); at org.apache.spark.network.server.TransportRequestHandler.processOneWayMessage(TransportRequestHandler.java:208); at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:113); at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:286); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348); at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340); at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85); at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerCon,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:47127,Safety,abort,aborted,47127,"utor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$andThen$0(SVInterval.java:61); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:86); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval.<init>(SVInterval.java:51); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:48); at org.broadinstitute.hellbender.tools.spark.sv.evidence.QNameFinder.apply(QNameFinder.java:16); at org.broadinstitute.hellbender.tools.spark.utils.FlatMapGluer.hasNext(FlatMapGluer.java:44); at scala.collection.convert.Wrappe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:49929,Safety,abort,abortStage,49929,llection.AbstractIterator.toArray(Iterator.scala:1336); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50026,Safety,abort,abortStage,50026,ect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.s,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:50268,Safety,abort,abortStage,50268,y(SparkContext.scala:2067); at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); at org.apache.spark.scheduler.Task.run(Task.scala:109); at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); at java.lang.Thread.run(Thread.java:745). Driver stacktrace:; at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587); at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586); at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); at scala.Option.foreach(Option.scala:257); at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769); at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758); at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2027); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2048); at org.apache.spark.SparkContext.runJob(SparkContext.scala:2067); at org,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5685:46466,Usability,clear,cleared,46466,"ty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935); at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:138); at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645); at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:580); at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:497); at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459); at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858); at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:138); at java.lang.Thread.run(Thread.java:745); 2019-02-17 16:25:50 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-02-17 16:25:50 INFO MemoryStore:54 - MemoryStore cleared; 2019-02-17 16:25:50 INFO BlockManager:54 - BlockManager stopped; 2019-02-17 16:25:50 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-02-17 16:25:50 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-02-17 16:25:50 INFO SparkContext:54 - Successfully stopped SparkContext; 16:25:50.893 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [February 17, 2019 4:25:50 PM EST] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 5.28 minutes.; Runtime.totalMemory()=5059379200; org.apache.spark.SparkException: Job aborted due to stage failure: Task 181 in stage 5.0 failed 4 times, most recent failure: Lost task 181.3 in stage 5.0 (TID 1139, scc-q02.scc.bu.edu, executor 24): java.lang.IllegalArgumentException: provided start is negative: -1; at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SVIntervalConstructorArgsValidator.lambda$static$3(SVInterval.java:76); at org.broadinstitute.hellbender.tools.spark.sv.utils.SVInterval$SV",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5685
https://github.com/broadinstitute/gatk/issues/5686:2174,Availability,down,down,2174,"3.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostPa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:5263,Availability,error,error,5263,"rogram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: cngb-nas-f17-1: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1631); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:924); 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1504); 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:843); 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1494); 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1626); 	... 21 more; ```. #### Steps to reproduce; On a Linux machine without _Hadoop_, run `java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5` locally. #### Expected behavior; Produce *out.pon.hdf5*. #### Actual behavior; Exit with error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:126,Deployability,release,release,126,"## Bug Report. ### Affected tool(s) or class(es); CreateReadCountPanelOfNormals. ### Affected version(s); - [ ] Latest public release version [4.1.0.0]. ### Description ; When you run it on a single machine, it trys to use _hadoop_ and failed. ```; $ java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5; 12:33:52.103 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1966,Deployability,release,release,1966,".794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:423,Modifiability,variab,variables,423,"## Bug Report. ### Affected tool(s) or class(es); CreateReadCountPanelOfNormals. ### Affected version(s); - [ ] Latest public release version [4.1.0.0]. ### Description ; When you run it on a single machine, it trys to use _hadoop_ and failed. ```; $ java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5; 12:33:52.103 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:549,Modifiability,config,configured,549,"## Bug Report. ### Affected tool(s) or class(es); CreateReadCountPanelOfNormals. ### Affected version(s); - [ ] Latest public release version [4.1.0.0]. ### Description ; When you run it on a single machine, it trys to use _hadoop_ and failed. ```; $ java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5; 12:33:52.103 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1652,Modifiability,Config,Config,1652,"share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:4270,Modifiability,config,config,4270,"rogram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: cngb-nas-f17-1: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1631); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:924); 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1504); 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:843); 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1494); 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1626); 	... 21 more; ```. #### Steps to reproduce; On a Linux machine without _Hadoop_, run `java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5` locally. #### Expected behavior; Produce *out.pon.hdf5*. #### Actual behavior; Exit with error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:4343,Modifiability,config,config,4343,"rogram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: cngb-nas-f17-1: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1631); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: cngb-nas-f17-1: Name or service not known; 	at java.base/java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.base/java.net.InetAddress$PlatformNameService.lookupAllHostAddr(InetAddress.java:924); 	at java.base/java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1504); 	at java.base/java.net.InetAddress$NameServiceAddresses.get(InetAddress.java:843); 	at java.base/java.net.InetAddress.getAllByName0(InetAddress.java:1494); 	at java.base/java.net.InetAddress.getLocalHost(InetAddress.java:1626); 	... 21 more; ```. #### Steps to reproduce; On a Linux machine without _Hadoop_, run `java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5` locally. #### Expected behavior; Produce *out.pon.hdf5*. #### Actual behavior; Exit with error.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:610,Performance,Load,Loading,610,"## Bug Report. ### Affected tool(s) or class(es); CreateReadCountPanelOfNormals. ### Affected version(s); - [ ] Latest public release version [4.1.0.0]. ### Description ; When you run it on a single machine, it trys to use _hadoop_ and failed. ```; $ java -jar ../gatk-package-4.1.0.0-local.jar CreateReadCountPanelOfNormals --input in.counts.hdf5 --output out.pon.hdf5; 12:33:52.103 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:2022,Performance,load,load,2022,"upport and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hell",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1424,Security,access,access,1424,"_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1483,Security,access,access,1483,"_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1511,Security,secur,security,1511," GCS hadoop connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1520,Security,authenticat,authentication,1520,"connector will not be configured properly; 12:33:52.162 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1638,Security,secur,security,1638,"om jar:file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1754,Security,secur,security,1754,".so; 12:33:53.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFacto",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1763,Security,authenticat,authentication,1763,"3.793 INFO CreateReadCountPanelOfNormals - ------------------------------------------------------------; 12:33:53.794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1820,Security,access,access,1820,".794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1881,Security,access,access,1881,".794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:1921,Security,access,access,1921,".794 INFO CreateReadCountPanelOfNormals - The Genome Analysis Toolkit (GATK) v4.1.0.0; 12:33:53.794 INFO CreateReadCountPanelOfNormals - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Initializing engine; 12:33:53.797 INFO CreateReadCountPanelOfNormals - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/02/18 12:33:53 INFO SparkContext: Running Spark version 2.2.0; WARNING: An illegal reflective access operation has occurred; WARNING: Illegal reflective access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5686:2469,Security,validat,validateSettings,2469," access by org.apache.hadoop.security.authentication.util.KerberosUtil (file:/share/FGI2017B/pub/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar) to method sun.security.krb5.Config.getInstance(); WARNING: Please consider reporting this to the maintainers of org.apache.hadoop.security.authentication.util.KerberosUtil; WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations; WARNING: All illegal access operations will be denied in a future release; 12:33:54.187 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 12:33:54.263 INFO CreateReadCountPanelOfNormals - Shutting down engine; [February 18, 2019 at 12:33:54 PM CST] org.broadinstitute.hellbender.tools.copynumber.CreateReadCountPanelOfNormals done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=2147483648; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5686
https://github.com/broadinstitute/gatk/issues/5687:324,Availability,error,errors,324,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description . Functotator MAF output does not properly show genotypes from somatic multi-tumor VCF files produced using M2. It looks like only genotypes from the first tumor sample are shown, but there are no errors or messages to point the user to this fact. Only a single `tumor_sample` and `normal_sample` are reported in the MAF header:. ```; ##normal_sample=xxx; ##source=FilterMutectCalls; ##source=Funcotator; ##source=Mutect2; ##tumor_sample=xxx; ```. Conversely, instructing `Funcotator` to output VCF format properly adds the annotation to the INFO field while retaining the FORMAT-level genotypes works well. However, in this case the VCF header also only lists a single tumor_sample (take note: this is a separate bug, though mostly aesthetics) despite all tumor genotypes being included. #### Steps to reproduce; Run Funcotator with output format set to MAF on any multi-tumor VCF file. #### Expected behavior; Either one of:; 1. `Funcotator` should return an error when trying to process multi-tumor VCF to MAF output; 2. `Funcotator` MAF should output multiple lines per funcotation for each tumor sample, indicating the comparison in the `Tumor_Sample_Barcode` and `Normal_Sample_Barcode` columns.; 3. `Funcotator` should not output genotype information when processing multi-tumor VCF to MAF (this could also be an additional Funcotator parameter that must be switched on when requesting MAF output). #### Actual behavior; Funcotator runs without errors or warnings and the output file is missing genotypes for the other tumor samples",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5687
https://github.com/broadinstitute/gatk/issues/5687:1088,Availability,error,error,1088,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description . Functotator MAF output does not properly show genotypes from somatic multi-tumor VCF files produced using M2. It looks like only genotypes from the first tumor sample are shown, but there are no errors or messages to point the user to this fact. Only a single `tumor_sample` and `normal_sample` are reported in the MAF header:. ```; ##normal_sample=xxx; ##source=FilterMutectCalls; ##source=Funcotator; ##source=Mutect2; ##tumor_sample=xxx; ```. Conversely, instructing `Funcotator` to output VCF format properly adds the annotation to the INFO field while retaining the FORMAT-level genotypes works well. However, in this case the VCF header also only lists a single tumor_sample (take note: this is a separate bug, though mostly aesthetics) despite all tumor genotypes being included. #### Steps to reproduce; Run Funcotator with output format set to MAF on any multi-tumor VCF file. #### Expected behavior; Either one of:; 1. `Funcotator` should return an error when trying to process multi-tumor VCF to MAF output; 2. `Funcotator` MAF should output multiple lines per funcotation for each tumor sample, indicating the comparison in the `Tumor_Sample_Barcode` and `Normal_Sample_Barcode` columns.; 3. `Funcotator` should not output genotype information when processing multi-tumor VCF to MAF (this could also be an additional Funcotator parameter that must be switched on when requesting MAF output). #### Actual behavior; Funcotator runs without errors or warnings and the output file is missing genotypes for the other tumor samples",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5687
https://github.com/broadinstitute/gatk/issues/5687:1579,Availability,error,errors,1579,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description . Functotator MAF output does not properly show genotypes from somatic multi-tumor VCF files produced using M2. It looks like only genotypes from the first tumor sample are shown, but there are no errors or messages to point the user to this fact. Only a single `tumor_sample` and `normal_sample` are reported in the MAF header:. ```; ##normal_sample=xxx; ##source=FilterMutectCalls; ##source=Funcotator; ##source=Mutect2; ##tumor_sample=xxx; ```. Conversely, instructing `Funcotator` to output VCF format properly adds the annotation to the INFO field while retaining the FORMAT-level genotypes works well. However, in this case the VCF header also only lists a single tumor_sample (take note: this is a separate bug, though mostly aesthetics) despite all tumor genotypes being included. #### Steps to reproduce; Run Funcotator with output format set to MAF on any multi-tumor VCF file. #### Expected behavior; Either one of:; 1. `Funcotator` should return an error when trying to process multi-tumor VCF to MAF output; 2. `Funcotator` MAF should output multiple lines per funcotation for each tumor sample, indicating the comparison in the `Tumor_Sample_Barcode` and `Normal_Sample_Barcode` columns.; 3. `Funcotator` should not output genotype information when processing multi-tumor VCF to MAF (this could also be an additional Funcotator parameter that must be switched on when requesting MAF output). #### Actual behavior; Funcotator runs without errors or warnings and the output file is missing genotypes for the other tumor samples",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5687
https://github.com/broadinstitute/gatk/issues/5687:102,Deployability,release,release,102,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description . Functotator MAF output does not properly show genotypes from somatic multi-tumor VCF files produced using M2. It looks like only genotypes from the first tumor sample are shown, but there are no errors or messages to point the user to this fact. Only a single `tumor_sample` and `normal_sample` are reported in the MAF header:. ```; ##normal_sample=xxx; ##source=FilterMutectCalls; ##source=Funcotator; ##source=Mutect2; ##tumor_sample=xxx; ```. Conversely, instructing `Funcotator` to output VCF format properly adds the annotation to the INFO field while retaining the FORMAT-level genotypes works well. However, in this case the VCF header also only lists a single tumor_sample (take note: this is a separate bug, though mostly aesthetics) despite all tumor genotypes being included. #### Steps to reproduce; Run Funcotator with output format set to MAF on any multi-tumor VCF file. #### Expected behavior; Either one of:; 1. `Funcotator` should return an error when trying to process multi-tumor VCF to MAF output; 2. `Funcotator` MAF should output multiple lines per funcotation for each tumor sample, indicating the comparison in the `Tumor_Sample_Barcode` and `Normal_Sample_Barcode` columns.; 3. `Funcotator` should not output genotype information when processing multi-tumor VCF to MAF (this could also be an additional Funcotator parameter that must be switched on when requesting MAF output). #### Actual behavior; Funcotator runs without errors or warnings and the output file is missing genotypes for the other tumor samples",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5687
https://github.com/broadinstitute/gatk/issues/5687:334,Integrability,message,messages,334,"## Bug Report. ### Affected tool(s) or class(es); `Funcotator`. ### Affected version(s); GATK 4.1.0.0 release. ### Description . Functotator MAF output does not properly show genotypes from somatic multi-tumor VCF files produced using M2. It looks like only genotypes from the first tumor sample are shown, but there are no errors or messages to point the user to this fact. Only a single `tumor_sample` and `normal_sample` are reported in the MAF header:. ```; ##normal_sample=xxx; ##source=FilterMutectCalls; ##source=Funcotator; ##source=Mutect2; ##tumor_sample=xxx; ```. Conversely, instructing `Funcotator` to output VCF format properly adds the annotation to the INFO field while retaining the FORMAT-level genotypes works well. However, in this case the VCF header also only lists a single tumor_sample (take note: this is a separate bug, though mostly aesthetics) despite all tumor genotypes being included. #### Steps to reproduce; Run Funcotator with output format set to MAF on any multi-tumor VCF file. #### Expected behavior; Either one of:; 1. `Funcotator` should return an error when trying to process multi-tumor VCF to MAF output; 2. `Funcotator` MAF should output multiple lines per funcotation for each tumor sample, indicating the comparison in the `Tumor_Sample_Barcode` and `Normal_Sample_Barcode` columns.; 3. `Funcotator` should not output genotype information when processing multi-tumor VCF to MAF (this could also be an additional Funcotator parameter that must be switched on when requesting MAF output). #### Actual behavior; Funcotator runs without errors or warnings and the output file is missing genotypes for the other tumor samples",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5687
https://github.com/broadinstitute/gatk/pull/5688:187,Modifiability,Refactor,Refactor,187,"Closes #4893. Closes #5086. Closes #5684. Closes #4500. Makes #4933, #4958, and #5085 possible. @takutosato Failing tests are superficial. You can begin reviewing. . This is a big PR:. * Refactor of all M2 filtering. Each filter has its own class, and the filtering engine ties it all together.; * Learn allele fraction clustering and somatic SNV and indel priors.; * More probabilistic filters.; * All filters have a common probabilistic threshold.; * M2 determines threshold automatically.; * Rewrite of all M2 documentation.; * Several filters, including strand bias and normal artifact, learn their own parameters. @LeeTL1220 M2 validations look really, really good. @meganshand Once this goes in mitochondria best practices will need to be tweaked again. We can merge the dangling tails homoplasmic fix before merging this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5688
https://github.com/broadinstitute/gatk/pull/5688:495,Modifiability,Rewrite,Rewrite,495,"Closes #4893. Closes #5086. Closes #5684. Closes #4500. Makes #4933, #4958, and #5085 possible. @takutosato Failing tests are superficial. You can begin reviewing. . This is a big PR:. * Refactor of all M2 filtering. Each filter has its own class, and the filtering engine ties it all together.; * Learn allele fraction clustering and somatic SNV and indel priors.; * More probabilistic filters.; * All filters have a common probabilistic threshold.; * M2 determines threshold automatically.; * Rewrite of all M2 documentation.; * Several filters, including strand bias and normal artifact, learn their own parameters. @LeeTL1220 M2 validations look really, really good. @meganshand Once this goes in mitochondria best practices will need to be tweaked again. We can merge the dangling tails homoplasmic fix before merging this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5688
https://github.com/broadinstitute/gatk/pull/5688:633,Security,validat,validations,633,"Closes #4893. Closes #5086. Closes #5684. Closes #4500. Makes #4933, #4958, and #5085 possible. @takutosato Failing tests are superficial. You can begin reviewing. . This is a big PR:. * Refactor of all M2 filtering. Each filter has its own class, and the filtering engine ties it all together.; * Learn allele fraction clustering and somatic SNV and indel priors.; * More probabilistic filters.; * All filters have a common probabilistic threshold.; * M2 determines threshold automatically.; * Rewrite of all M2 documentation.; * Several filters, including strand bias and normal artifact, learn their own parameters. @LeeTL1220 M2 validations look really, really good. @meganshand Once this goes in mitochondria best practices will need to be tweaked again. We can merge the dangling tails homoplasmic fix before merging this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5688
https://github.com/broadinstitute/gatk/pull/5688:116,Testability,test,tests,116,"Closes #4893. Closes #5086. Closes #5684. Closes #4500. Makes #4933, #4958, and #5085 possible. @takutosato Failing tests are superficial. You can begin reviewing. . This is a big PR:. * Refactor of all M2 filtering. Each filter has its own class, and the filtering engine ties it all together.; * Learn allele fraction clustering and somatic SNV and indel priors.; * More probabilistic filters.; * All filters have a common probabilistic threshold.; * M2 determines threshold automatically.; * Rewrite of all M2 documentation.; * Several filters, including strand bias and normal artifact, learn their own parameters. @LeeTL1220 M2 validations look really, really good. @meganshand Once this goes in mitochondria best practices will need to be tweaked again. We can merge the dangling tails homoplasmic fix before merging this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5688
https://github.com/broadinstitute/gatk/pull/5688:298,Usability,Learn,Learn,298,"Closes #4893. Closes #5086. Closes #5684. Closes #4500. Makes #4933, #4958, and #5085 possible. @takutosato Failing tests are superficial. You can begin reviewing. . This is a big PR:. * Refactor of all M2 filtering. Each filter has its own class, and the filtering engine ties it all together.; * Learn allele fraction clustering and somatic SNV and indel priors.; * More probabilistic filters.; * All filters have a common probabilistic threshold.; * M2 determines threshold automatically.; * Rewrite of all M2 documentation.; * Several filters, including strand bias and normal artifact, learn their own parameters. @LeeTL1220 M2 validations look really, really good. @meganshand Once this goes in mitochondria best practices will need to be tweaked again. We can merge the dangling tails homoplasmic fix before merging this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5688
https://github.com/broadinstitute/gatk/pull/5688:591,Usability,learn,learn,591,"Closes #4893. Closes #5086. Closes #5684. Closes #4500. Makes #4933, #4958, and #5085 possible. @takutosato Failing tests are superficial. You can begin reviewing. . This is a big PR:. * Refactor of all M2 filtering. Each filter has its own class, and the filtering engine ties it all together.; * Learn allele fraction clustering and somatic SNV and indel priors.; * More probabilistic filters.; * All filters have a common probabilistic threshold.; * M2 determines threshold automatically.; * Rewrite of all M2 documentation.; * Several filters, including strand bias and normal artifact, learn their own parameters. @LeeTL1220 M2 validations look really, really good. @meganshand Once this goes in mitochondria best practices will need to be tweaked again. We can merge the dangling tails homoplasmic fix before merging this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5688
https://github.com/broadinstitute/gatk/issues/5690:310,Availability,Error,Error,310,Mutect2 (GATK 4.1.0.0) fails occasionally in smith waterman native library as below. stderr is attached. I can also provide core dump if necessary. [stderr.tar.gz](https://github.com/broadinstitute/gatk/files/2880800/stderr.tar.gz). ```; 07:30:59.335 INFO ProgressMeter - 17:78451657 627.7 1223980 1950.0; *** Error in `java': munmap_chunk(): invalid pointer: 0x00002ba8e50b7740 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7ab54)[0x2ba8df926b54]; /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/2cebc7be-fe23-4787-9095-9b91227c6526/call-M2/shard-13/attempt-2/tmp.945f1f83/libgkl_smithwaterman5575294852416409537.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x2ba9aee21fa8]; /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/2cebc7be-fe23-4787-9095-9b91227c6526/call-M2/shard-13/attempt-2/tmp.945f1f83/libgkl_smithwaterman5575294852416409537.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x2ba9aee21bf8]; [0x2ba8e8f6675a]; ======= Memory map: ========; 00400000-00401000 r-xp 00000000 08:03 5769910 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/jre/bin/java; 00600000-00601000 r--p 00000000 08:03 5769910 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/jre/bin/java; ...; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690
https://github.com/broadinstitute/gatk/issues/5690:487,Testability,log,log,487,Mutect2 (GATK 4.1.0.0) fails occasionally in smith waterman native library as below. stderr is attached. I can also provide core dump if necessary. [stderr.tar.gz](https://github.com/broadinstitute/gatk/files/2880800/stderr.tar.gz). ```; 07:30:59.335 INFO ProgressMeter - 17:78451657 627.7 1223980 1950.0; *** Error in `java': munmap_chunk(): invalid pointer: 0x00002ba8e50b7740 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7ab54)[0x2ba8df926b54]; /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/2cebc7be-fe23-4787-9095-9b91227c6526/call-M2/shard-13/attempt-2/tmp.945f1f83/libgkl_smithwaterman5575294852416409537.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x2ba9aee21fa8]; /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/2cebc7be-fe23-4787-9095-9b91227c6526/call-M2/shard-13/attempt-2/tmp.945f1f83/libgkl_smithwaterman5575294852416409537.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x2ba9aee21bf8]; [0x2ba8e8f6675a]; ======= Memory map: ========; 00400000-00401000 r-xp 00000000 08:03 5769910 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/jre/bin/java; 00600000-00601000 r--p 00000000 08:03 5769910 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/jre/bin/java; ...; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690
https://github.com/broadinstitute/gatk/issues/5690:731,Testability,log,log,731,Mutect2 (GATK 4.1.0.0) fails occasionally in smith waterman native library as below. stderr is attached. I can also provide core dump if necessary. [stderr.tar.gz](https://github.com/broadinstitute/gatk/files/2880800/stderr.tar.gz). ```; 07:30:59.335 INFO ProgressMeter - 17:78451657 627.7 1223980 1950.0; *** Error in `java': munmap_chunk(): invalid pointer: 0x00002ba8e50b7740 ***; ======= Backtrace: =========; /lib64/libc.so.6(+0x7ab54)[0x2ba8df926b54]; /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/2cebc7be-fe23-4787-9095-9b91227c6526/call-M2/shard-13/attempt-2/tmp.945f1f83/libgkl_smithwaterman5575294852416409537.so(_Z19runSWOnePairBT_avx2iiiiPhS_iiaPcPs+0x338)[0x2ba9aee21fa8]; /gpfs/data/software/cromwell/log/cromwell-executions/Mutect2/2cebc7be-fe23-4787-9095-9b91227c6526/call-M2/shard-13/attempt-2/tmp.945f1f83/libgkl_smithwaterman5575294852416409537.so(Java_com_intel_gkl_smithwaterman_IntelSmithWaterman_alignNative+0xd8)[0x2ba9aee21bf8]; [0x2ba8e8f6675a]; ======= Memory map: ========; 00400000-00401000 r-xp 00000000 08:03 5769910 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/jre/bin/java; 00600000-00601000 r--p 00000000 08:03 5769910 /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.111-1.b15.el7_2.x86_64/jre/bin/java; ...; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5690
https://github.com/broadinstitute/gatk/issues/5692:264,Deployability,release,release,264,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692
https://github.com/broadinstitute/gatk/issues/5692:470,Security,validat,validates,470,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692
https://github.com/broadinstitute/gatk/issues/5692:334,Testability,test,test,334,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692
https://github.com/broadinstitute/gatk/issues/5692:402,Testability,log,logs,402,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692
https://github.com/broadinstitute/gatk/issues/5692:545,Testability,log,logging,545,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692
https://github.com/broadinstitute/gatk/issues/5692:819,Testability,log,log,819,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692
https://github.com/broadinstitute/gatk/issues/5692:900,Testability,Log,Logs,900,"## Bug Report. ### Affected tool(s) or class(es); SAMSequenceDictionary function in IndexUtils.java: https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/utils/IndexUtils.java. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [date of test?]. ### Description ; The SAMSequenceDictionary function always logs a warning when a file is passed in. Needs an if statement that validates whether or not the file is actually a sequence dictionary before logging the warning. #### Steps to reproduce; Run GATK's HaplotypeCaller with a --dbsnp option set, or just pass a sequence dictionary into the SAMSequenceDictionary function directly. #### Expected behavior; Should use the --dbsnp file that I pass in if valid, rather than log a warning and creating a separate sequence dictionary. #### Actual behavior; Logs a warning and instead tries to create a new sequence dictionary based on the index file it finds",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5692
https://github.com/broadinstitute/gatk/pull/5693:299,Availability,error,error,299,"@meganshand Could you review this? It fixes most of the homoplasmic missed calls in broadinstitute/dsp-spec-ops#116. I reviewed all of the false positive that were introduced to our somatic validations when attempting to make this the default in non-mitochondria mode. Everything was due to mapping error, which I do not expect to be an issue in mitochondria, and not an inherent problem with recovering more dangling ends. You will still want to run this branch through some of your validations, however. In mitochondria mode it's the same as the dangling-1-29.jar that I shared earlier with you and Sarah.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693
https://github.com/broadinstitute/gatk/pull/5693:393,Availability,recover,recovering,393,"@meganshand Could you review this? It fixes most of the homoplasmic missed calls in broadinstitute/dsp-spec-ops#116. I reviewed all of the false positive that were introduced to our somatic validations when attempting to make this the default in non-mitochondria mode. Everything was due to mapping error, which I do not expect to be an issue in mitochondria, and not an inherent problem with recovering more dangling ends. You will still want to run this branch through some of your validations, however. In mitochondria mode it's the same as the dangling-1-29.jar that I shared earlier with you and Sarah.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693
https://github.com/broadinstitute/gatk/pull/5693:393,Safety,recover,recovering,393,"@meganshand Could you review this? It fixes most of the homoplasmic missed calls in broadinstitute/dsp-spec-ops#116. I reviewed all of the false positive that were introduced to our somatic validations when attempting to make this the default in non-mitochondria mode. Everything was due to mapping error, which I do not expect to be an issue in mitochondria, and not an inherent problem with recovering more dangling ends. You will still want to run this branch through some of your validations, however. In mitochondria mode it's the same as the dangling-1-29.jar that I shared earlier with you and Sarah.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693
https://github.com/broadinstitute/gatk/pull/5693:190,Security,validat,validations,190,"@meganshand Could you review this? It fixes most of the homoplasmic missed calls in broadinstitute/dsp-spec-ops#116. I reviewed all of the false positive that were introduced to our somatic validations when attempting to make this the default in non-mitochondria mode. Everything was due to mapping error, which I do not expect to be an issue in mitochondria, and not an inherent problem with recovering more dangling ends. You will still want to run this branch through some of your validations, however. In mitochondria mode it's the same as the dangling-1-29.jar that I shared earlier with you and Sarah.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693
https://github.com/broadinstitute/gatk/pull/5693:484,Security,validat,validations,484,"@meganshand Could you review this? It fixes most of the homoplasmic missed calls in broadinstitute/dsp-spec-ops#116. I reviewed all of the false positive that were introduced to our somatic validations when attempting to make this the default in non-mitochondria mode. Everything was due to mapping error, which I do not expect to be an issue in mitochondria, and not an inherent problem with recovering more dangling ends. You will still want to run this branch through some of your validations, however. In mitochondria mode it's the same as the dangling-1-29.jar that I shared earlier with you and Sarah.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5693
https://github.com/broadinstitute/gatk/issues/5695:123,Deployability,release,release,123,"## Bug Report. ### Affected tool(s) or class(es); `Mutect2` and `FilterMutectCalls`. ### Affected version(s); GATK 4.1.0.0 release (though I suspect this was also in 4.0.x). ### Description ; M2 force-calling multi-allelic sites using GGA mode results in automatic `mappinq_quality` filter if one allele does not have any reads mapped, even if the other variant allele has good mapping quality. In my testing this happened when I force-called the following IDH1 mutations:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO; 2 209113112 . C A . . .; 2 209113112 . C T . . .; ```. The first listed is the common IDH1 R132H variant and the second listed is the less common but equally pathogenic R132L variant. The purpose for force-calling both variants is that one wants to absolutely exclude the possibility of either variant at this site. In most my tests, variants would fail `FilterMutectCalls` because only one of the multi-allelic alleles would be present, eg. *(representative except of `Mutect2` + `FilterMutectCalls` output)*. ```; VAR 	2	209113112	.	C	A,T; FILTER	mapping_quality;read_position; INFO	MMQ=60,0,60; FORMAT	GT:AD 0/1/2:56,0,47; ```. Only in rare cases would the variant site `PASS`, and that's when there would be at least one variant read in the second allele, eg:. ```; VAR 	2	209113112	.	C	A,T; FILTER	PASS; INFO	MMQ=60,60,60; FORMAT	GT:AD 0/1/2:28,1,16; ```. #### Steps to reproduce; To reproduce this specific example you will need a BAM file with a C>T mutation at `2:209113112`, and no reads with an `A` at this position. You will then need to run M2 and `FilterMutectCalls` with `--genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles ...` set to a VCF with both lines as shown in the description. #### Expected behavior; `FilterMutectCalls` should not apply a `mapping_quality` filter if one of the alleles would pass. Perhaps the mapping quality filter could ignore alleles with zero mapped reads, rather than assigning a MMQ of zero by default? *Note that this issue would also",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5695
https://github.com/broadinstitute/gatk/issues/5695:401,Testability,test,testing,401,"## Bug Report. ### Affected tool(s) or class(es); `Mutect2` and `FilterMutectCalls`. ### Affected version(s); GATK 4.1.0.0 release (though I suspect this was also in 4.0.x). ### Description ; M2 force-calling multi-allelic sites using GGA mode results in automatic `mappinq_quality` filter if one allele does not have any reads mapped, even if the other variant allele has good mapping quality. In my testing this happened when I force-called the following IDH1 mutations:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO; 2 209113112 . C A . . .; 2 209113112 . C T . . .; ```. The first listed is the common IDH1 R132H variant and the second listed is the less common but equally pathogenic R132L variant. The purpose for force-calling both variants is that one wants to absolutely exclude the possibility of either variant at this site. In most my tests, variants would fail `FilterMutectCalls` because only one of the multi-allelic alleles would be present, eg. *(representative except of `Mutect2` + `FilterMutectCalls` output)*. ```; VAR 	2	209113112	.	C	A,T; FILTER	mapping_quality;read_position; INFO	MMQ=60,0,60; FORMAT	GT:AD 0/1/2:56,0,47; ```. Only in rare cases would the variant site `PASS`, and that's when there would be at least one variant read in the second allele, eg:. ```; VAR 	2	209113112	.	C	A,T; FILTER	PASS; INFO	MMQ=60,60,60; FORMAT	GT:AD 0/1/2:28,1,16; ```. #### Steps to reproduce; To reproduce this specific example you will need a BAM file with a C>T mutation at `2:209113112`, and no reads with an `A` at this position. You will then need to run M2 and `FilterMutectCalls` with `--genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles ...` set to a VCF with both lines as shown in the description. #### Expected behavior; `FilterMutectCalls` should not apply a `mapping_quality` filter if one of the alleles would pass. Perhaps the mapping quality filter could ignore alleles with zero mapped reads, rather than assigning a MMQ of zero by default? *Note that this issue would also",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5695
https://github.com/broadinstitute/gatk/issues/5695:846,Testability,test,tests,846,"## Bug Report. ### Affected tool(s) or class(es); `Mutect2` and `FilterMutectCalls`. ### Affected version(s); GATK 4.1.0.0 release (though I suspect this was also in 4.0.x). ### Description ; M2 force-calling multi-allelic sites using GGA mode results in automatic `mappinq_quality` filter if one allele does not have any reads mapped, even if the other variant allele has good mapping quality. In my testing this happened when I force-called the following IDH1 mutations:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO; 2 209113112 . C A . . .; 2 209113112 . C T . . .; ```. The first listed is the common IDH1 R132H variant and the second listed is the less common but equally pathogenic R132L variant. The purpose for force-calling both variants is that one wants to absolutely exclude the possibility of either variant at this site. In most my tests, variants would fail `FilterMutectCalls` because only one of the multi-allelic alleles would be present, eg. *(representative except of `Mutect2` + `FilterMutectCalls` output)*. ```; VAR 	2	209113112	.	C	A,T; FILTER	mapping_quality;read_position; INFO	MMQ=60,0,60; FORMAT	GT:AD 0/1/2:56,0,47; ```. Only in rare cases would the variant site `PASS`, and that's when there would be at least one variant read in the second allele, eg:. ```; VAR 	2	209113112	.	C	A,T; FILTER	PASS; INFO	MMQ=60,60,60; FORMAT	GT:AD 0/1/2:28,1,16; ```. #### Steps to reproduce; To reproduce this specific example you will need a BAM file with a C>T mutation at `2:209113112`, and no reads with an `A` at this position. You will then need to run M2 and `FilterMutectCalls` with `--genotyping-mode GENOTYPE_GIVEN_ALLELES --alleles ...` set to a VCF with both lines as shown in the description. #### Expected behavior; `FilterMutectCalls` should not apply a `mapping_quality` filter if one of the alleles would pass. Perhaps the mapping quality filter could ignore alleles with zero mapped reads, rather than assigning a MMQ of zero by default? *Note that this issue would also",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5695
https://github.com/broadinstitute/gatk/issues/5696:371,Testability,Test,Test,371,"## Bug Report. ### Affected tool(s) or class(es); - `HaplotypeCaller` with the arguments `--max-mnp-distance 5`. ### Affected version(s); - gatk version 4.0.12.0. ### Description . For a genomes in a bottle reference sample, I have observed a genotype discordance as follows:. ref: `CA`; alleles: `CG` and `TG`. | Callset | Genotype |; | --- | --- |; | GIAB | `1/2` |; | Test | `0/2` |. As you can see, the test dataset improperly calls a reference allele (`CA`) when it should have called the first alternate instead. The GIAB callset has two phased records:; <details>; <summary> GIAB callset </summary>. ```; chr2 241815307 . C T 50 PASS platforms=3;platformnames=Illumina,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 0|1:241815307_C_T:286:53,44:47,41:1082; chr2 241815308 . A G 50 PASS platforms=3;platformnames=Illumina,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 1|1:241815307_C_T:287:0,98:0,89:1; ```; </details>. while the test callset has the following single record:. <details>; <summary> Test callset </summary>. ```; chr2 241815307 . CA TG 1756.77 PASS AC=1;AF=0.5;AN=2;BaseQRankSum=-0.802;ClippingRankSum=0.521;DP=85;ExcessHet=3.0103;FS=2.902;MLEAC=1;MLEAF=0.5;MQ=60.0;MQRankSum=0.0;QD=20.67;ReadPosRankSum=-1.858;SOR=0.571 GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:37,48:85:16,25,0:21,23,0:99:1785,0,1406; ```; </details>. Next",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696
https://github.com/broadinstitute/gatk/issues/5696:407,Testability,test,test,407,"## Bug Report. ### Affected tool(s) or class(es); - `HaplotypeCaller` with the arguments `--max-mnp-distance 5`. ### Affected version(s); - gatk version 4.0.12.0. ### Description . For a genomes in a bottle reference sample, I have observed a genotype discordance as follows:. ref: `CA`; alleles: `CG` and `TG`. | Callset | Genotype |; | --- | --- |; | GIAB | `1/2` |; | Test | `0/2` |. As you can see, the test dataset improperly calls a reference allele (`CA`) when it should have called the first alternate instead. The GIAB callset has two phased records:; <details>; <summary> GIAB callset </summary>. ```; chr2 241815307 . C T 50 PASS platforms=3;platformnames=Illumina,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 0|1:241815307_C_T:286:53,44:47,41:1082; chr2 241815308 . A G 50 PASS platforms=3;platformnames=Illumina,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 1|1:241815307_C_T:287:0,98:0,89:1; ```; </details>. while the test callset has the following single record:. <details>; <summary> Test callset </summary>. ```; chr2 241815307 . CA TG 1756.77 PASS AC=1;AF=0.5;AN=2;BaseQRankSum=-0.802;ClippingRankSum=0.521;DP=85;ExcessHet=3.0103;FS=2.902;MLEAC=1;MLEAF=0.5;MQ=60.0;MQRankSum=0.0;QD=20.67;ReadPosRankSum=-1.858;SOR=0.571 GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:37,48:85:16,25,0:21,23,0:99:1785,0,1406; ```; </details>. Next",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696
https://github.com/broadinstitute/gatk/issues/5696:1604,Testability,test,test,1604,"15307 . C T 50 PASS platforms=3;platformnames=Illumina,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 0|1:241815307_C_T:286:53,44:47,41:1082; chr2 241815308 . A G 50 PASS platforms=3;platformnames=Illumina,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 1|1:241815307_C_T:287:0,98:0,89:1; ```; </details>. while the test callset has the following single record:. <details>; <summary> Test callset </summary>. ```; chr2 241815307 . CA TG 1756.77 PASS AC=1;AF=0.5;AN=2;BaseQRankSum=-0.802;ClippingRankSum=0.521;DP=85;ExcessHet=3.0103;FS=2.902;MLEAC=1;MLEAF=0.5;MQ=60.0;MQRankSum=0.0;QD=20.67;ReadPosRankSum=-1.858;SOR=0.571 GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:37,48:85:16,25,0:21,23,0:99:1785,0,1406; ```; </details>. Next, I looked at the reads from the haplotype assembly BAM supporting each of the three possible alleles in the test callset:. | Allele | # of reads |; | --- | --- |; | ref (`CA`) | 1 read |; | alt1 (`CG`) | 36 reads |; | alt 2 (`TG`) | 48 reads |. You can see the sum of the count of reads in the table is equal to the depth and sum of allelic depths in the VCF record. It is clear from the haplotype assembly BAM that the call should be `1/2` (`CG/TG`), but the finall call is `0/2` (`CA/TG`). . I then re-ran `HaplotypeCaller` without the `--max-mnp-distance 5` argument and got the variant call I expected, which matches",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696
https://github.com/broadinstitute/gatk/issues/5696:1672,Testability,Test,Test,1672,"na,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 0|1:241815307_C_T:286:53,44:47,41:1082; chr2 241815308 . A G 50 PASS platforms=3;platformnames=Illumina,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 1|1:241815307_C_T:287:0,98:0,89:1; ```; </details>. while the test callset has the following single record:. <details>; <summary> Test callset </summary>. ```; chr2 241815307 . CA TG 1756.77 PASS AC=1;AF=0.5;AN=2;BaseQRankSum=-0.802;ClippingRankSum=0.521;DP=85;ExcessHet=3.0103;FS=2.902;MLEAC=1;MLEAF=0.5;MQ=60.0;MQRankSum=0.0;QD=20.67;ReadPosRankSum=-1.858;SOR=0.571 GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:37,48:85:16,25,0:21,23,0:99:1785,0,1406; ```; </details>. Next, I looked at the reads from the haplotype assembly BAM supporting each of the three possible alleles in the test callset:. | Allele | # of reads |; | --- | --- |; | ref (`CA`) | 1 read |; | alt1 (`CG`) | 36 reads |; | alt 2 (`TG`) | 48 reads |. You can see the sum of the count of reads in the table is equal to the depth and sum of allelic depths in the VCF record. It is clear from the haplotype assembly BAM that the call should be `1/2` (`CG/TG`), but the finall call is `0/2` (`CA/TG`). . I then re-ran `HaplotypeCaller` without the `--max-mnp-distance 5` argument and got the variant call I expected, which matches the GIAB variant call:. <details>; <summary> Test ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696
https://github.com/broadinstitute/gatk/issues/5696:2110,Testability,test,test,2110,":241815307_C_T:286:53,44:47,41:1082; chr2 241815308 . A G 50 PASS platforms=3;platformnames=Illumina,CG,10X;datasets=4;datasetnames=HiSeqPE100x,CGnormal,10XChromium,HiSeqMatePair;callsets=6;callsetnames=HiSeqPE100xSentieon,CGnormal,HiSeqPE100xfreebayes,10XSentieonhaplo,HiSeqMatePairSentieon,HiSeqMatePairfreebayes;callable=CS_HiSeqPE100xSentieon_callable,CS_CGnormal_callable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 1|1:241815307_C_T:287:0,98:0,89:1; ```; </details>. while the test callset has the following single record:. <details>; <summary> Test callset </summary>. ```; chr2 241815307 . CA TG 1756.77 PASS AC=1;AF=0.5;AN=2;BaseQRankSum=-0.802;ClippingRankSum=0.521;DP=85;ExcessHet=3.0103;FS=2.902;MLEAC=1;MLEAF=0.5;MQ=60.0;MQRankSum=0.0;QD=20.67;ReadPosRankSum=-1.858;SOR=0.571 GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:37,48:85:16,25,0:21,23,0:99:1785,0,1406; ```; </details>. Next, I looked at the reads from the haplotype assembly BAM supporting each of the three possible alleles in the test callset:. | Allele | # of reads |; | --- | --- |; | ref (`CA`) | 1 read |; | alt1 (`CG`) | 36 reads |; | alt 2 (`TG`) | 48 reads |. You can see the sum of the count of reads in the table is equal to the depth and sum of allelic depths in the VCF record. It is clear from the haplotype assembly BAM that the call should be `1/2` (`CG/TG`), but the finall call is `0/2` (`CA/TG`). . I then re-ran `HaplotypeCaller` without the `--max-mnp-distance 5` argument and got the variant call I expected, which matches the GIAB variant call:. <details>; <summary> Test callset without `--max-mnp-distance 5` </summary>. ```; chr2 241815307 . C T,<NON_REF> 1717.77 . BaseQRankSum=-1.137;ClippingRankSum=1.185;DP=83;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQandDP=298800,83;ReadPosRankSum=-1.967 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:36,47,0:83:15,24,0:21,23,0:99:0|1:241815307_C_T:1746,0,1375,1855,1516,3371:241815307:15,21,24,2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696
https://github.com/broadinstitute/gatk/issues/5696:2668,Testability,Test,Test,2668,"307_C_T:287:0,98:0,89:1; ```; </details>. while the test callset has the following single record:. <details>; <summary> Test callset </summary>. ```; chr2 241815307 . CA TG 1756.77 PASS AC=1;AF=0.5;AN=2;BaseQRankSum=-0.802;ClippingRankSum=0.521;DP=85;ExcessHet=3.0103;FS=2.902;MLEAC=1;MLEAF=0.5;MQ=60.0;MQRankSum=0.0;QD=20.67;ReadPosRankSum=-1.858;SOR=0.571 GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:37,48:85:16,25,0:21,23,0:99:1785,0,1406; ```; </details>. Next, I looked at the reads from the haplotype assembly BAM supporting each of the three possible alleles in the test callset:. | Allele | # of reads |; | --- | --- |; | ref (`CA`) | 1 read |; | alt1 (`CG`) | 36 reads |; | alt 2 (`TG`) | 48 reads |. You can see the sum of the count of reads in the table is equal to the depth and sum of allelic depths in the VCF record. It is clear from the haplotype assembly BAM that the call should be `1/2` (`CG/TG`), but the finall call is `0/2` (`CA/TG`). . I then re-ran `HaplotypeCaller` without the `--max-mnp-distance 5` argument and got the variant call I expected, which matches the GIAB variant call:. <details>; <summary> Test callset without `--max-mnp-distance 5` </summary>. ```; chr2 241815307 . C T,<NON_REF> 1717.77 . BaseQRankSum=-1.137;ClippingRankSum=1.185;DP=83;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQandDP=298800,83;ReadPosRankSum=-1.967 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:36,47,0:83:15,24,0:21,23,0:99:0|1:241815307_C_T:1746,0,1375,1855,1516,3371:241815307:15,21,24,23; chr2 241815308 . A G,<NON_REF> 3429.77 . BaseQRankSum=0.293;ClippingRankSum=0.601;DP=83;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;MQRankSum=0.000;RAW_MQandDP=298800,83;ReadPosRankSum=-1.420 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 1|1:1,82,0:83:1,38,0:0,44,0:99:0|1:241815307_C_T:3458,206,0,3461,246,3500:241815307:1,0,38,44; ```; </details>. I have a work-around where I can post-GATK join phased variants into an MNP, but it looks like there is a bug in this version of `HaplotypeCaller`.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696
https://github.com/broadinstitute/gatk/issues/5696:2375,Usability,clear,clear,2375,"llable,CS_HiSeqPE100xfreebayes_callable,CS_10XSentieonhaplo_callable;filt=CS_CGnormal_filt GT:PS:DP:ADALL:AD:GQ 1|1:241815307_C_T:287:0,98:0,89:1; ```; </details>. while the test callset has the following single record:. <details>; <summary> Test callset </summary>. ```; chr2 241815307 . CA TG 1756.77 PASS AC=1;AF=0.5;AN=2;BaseQRankSum=-0.802;ClippingRankSum=0.521;DP=85;ExcessHet=3.0103;FS=2.902;MLEAC=1;MLEAF=0.5;MQ=60.0;MQRankSum=0.0;QD=20.67;ReadPosRankSum=-1.858;SOR=0.571 GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:37,48:85:16,25,0:21,23,0:99:1785,0,1406; ```; </details>. Next, I looked at the reads from the haplotype assembly BAM supporting each of the three possible alleles in the test callset:. | Allele | # of reads |; | --- | --- |; | ref (`CA`) | 1 read |; | alt1 (`CG`) | 36 reads |; | alt 2 (`TG`) | 48 reads |. You can see the sum of the count of reads in the table is equal to the depth and sum of allelic depths in the VCF record. It is clear from the haplotype assembly BAM that the call should be `1/2` (`CG/TG`), but the finall call is `0/2` (`CA/TG`). . I then re-ran `HaplotypeCaller` without the `--max-mnp-distance 5` argument and got the variant call I expected, which matches the GIAB variant call:. <details>; <summary> Test callset without `--max-mnp-distance 5` </summary>. ```; chr2 241815307 . C T,<NON_REF> 1717.77 . BaseQRankSum=-1.137;ClippingRankSum=1.185;DP=83;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQandDP=298800,83;ReadPosRankSum=-1.967 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 0|1:36,47,0:83:15,24,0:21,23,0:99:0|1:241815307_C_T:1746,0,1375,1855,1516,3371:241815307:15,21,24,23; chr2 241815308 . A G,<NON_REF> 3429.77 . BaseQRankSum=0.293;ClippingRankSum=0.601;DP=83;ExcessHet=3.0103;MLEAC=2,0;MLEAF=1.00,0.00;MQRankSum=0.000;RAW_MQandDP=298800,83;ReadPosRankSum=-1.420 GT:AD:DP:F1R2:F2R1:GQ:PGT:PID:PL:PS:SB 1|1:1,82,0:83:1,38,0:0,44,0:99:0|1:241815307_C_T:3458,206,0,3461,246,3500:241815307:1,0,38,44; ```; </details>. I have a work-around where",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5696
https://github.com/broadinstitute/gatk/pull/5697:64,Deployability,pipeline,pipeline,64,"With allele-specific annotations and filtering in the new exome pipeline, this should be helpful. (I was using it for debugging merging Mutect2 perAlleleAnnotations for MT data.)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5697
https://github.com/broadinstitute/gatk/issues/5698:1258,Availability,error,errors,1258,"tion ; I think there's a problem with the StrandOddsRatio (SOR) annotation and the `--map-mnp-distance` flag. I'm looking at a small region of NA24143 (one of the GIAB samples). There's a pair of SNPs in very close proximity. When called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independently. Since the first SNP is hom-var and the second is het, I would have expected the SOR value for the MNP call to closely mirror that of the het SNP. My suspicion is that what's going on here is probably that the calculation is bein",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698
https://github.com/broadinstitute/gatk/issues/5698:145,Deployability,release,release,145,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller with `--max-mnp-distance` filter. ### Affected version(s); - [x] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; I think there's a problem with the StrandOddsRatio (SOR) annotation and the `--map-mnp-distance` flag. I'm looking at a small region of NA24143 (one of the GIAB samples). There's a pair of SNPs in very close proximity. When called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698
https://github.com/broadinstitute/gatk/issues/5698:1236,Safety,avoid,avoids,1236,"tion ; I think there's a problem with the StrandOddsRatio (SOR) annotation and the `--map-mnp-distance` flag. I'm looking at a small region of NA24143 (one of the GIAB samples). There's a pair of SNPs in very close proximity. When called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independently. Since the first SNP is hom-var and the second is het, I would have expected the SOR value for the MNP call to closely mirror that of the het SNP. My suspicion is that what's going on here is probably that the calculation is bein",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698
https://github.com/broadinstitute/gatk/issues/5698:1435,Safety,detect,detect,1435,"n called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independently. Since the first SNP is hom-var and the second is het, I would have expected the SOR value for the MNP call to closely mirror that of the het SNP. My suspicion is that what's going on here is probably that the calculation is being run using the contingency table for the hom-var SNP that's first in the MNP, perhaps filtered to only reads that span the whole MNP, since the value is marginally lower. #### Steps to reproduce; I will try and post a BAM snippe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698
https://github.com/broadinstitute/gatk/issues/5698:214,Testability,test,test,214,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller with `--max-mnp-distance` filter. ### Affected version(s); - [x] Latest public release version [4.1.0.0]; - [ ] Latest master branch as of [date of test?]. ### Description ; I think there's a problem with the StrandOddsRatio (SOR) annotation and the `--map-mnp-distance` flag. I'm looking at a small region of NA24143 (one of the GIAB samples). There's a pair of SNPs in very close proximity. When called without the MNP output I get a pair of variants as follows (some info removed for clarity), coordinates are HG19:. ```; chr4 5743509 . C T 5903.03 . FS=0.000;QD=25.36;SOR=9.825 GT:AD:DP:GQ:PL 1/1:0,135:135:99:5917,406,0; chr4 5743512 . T C 2766.60 . FS=0.000;QD=21.12;SOR=0.983 GT:AD:DP:GQ:PL 0/1:57,74:131:99:2774,0,2060; ```. I'm trying to get permission to share the BAM over this region, but the key information is that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independen",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698
https://github.com/broadinstitute/gatk/issues/5698:2521,Testability,test,test,2521,"that every single read that spans or is in proximity to these variants is on the R strand. There is zero F strand coverage. This seems reasonable. It's a bit odd to me that the first SNP which is hom-var has a SOR value of 9.825, but it's homozygous so it's more or less irrelevant. Looking at the code, I think the problem here is that the code avoids divide-by-zero errors by adding pseudo-counts of `1.0` to the table, which for homozygous variants with no coverage on one strand creates a weird situation. I think it would be better to just detect if _all_ coverage is on one strand and short-circuit the calculation, but I digress. The real problem comes when running with `--max-mnp-distance 5`. Then I get this single variant:. ```; chr4 5743509 . CTAT TTAC,TTAT 5506.10 . FS=0.000;QD=25.36;SOR=9.750 GT:AD:DP:GQ:PL 1/2:0,74,56:130:99:5523,2213,2060,3016,0,2774; ```. Now I have a het variant with an SOR of 9.75. This seems really wrong to me - note how FS is 0.0. Again all coverage of all alleles is on one strand. And the het SNP that forms part of this MNP had an SOR of 0.983 when called independently. Since the first SNP is hom-var and the second is het, I would have expected the SOR value for the MNP call to closely mirror that of the het SNP. My suspicion is that what's going on here is probably that the calculation is being run using the contingency table for the hom-var SNP that's first in the MNP, perhaps filtered to only reads that span the whole MNP, since the value is marginally lower. #### Steps to reproduce; I will try and post a BAM snippet later, but essentially to reproduce I think a synthetic test case with the following properties would work:. 1. Coverage on only one strand; 2. A HomVar SNP at position n; 3. A Het SNP at position n+2; 4. Calls made with `--max-mnp-distance 5`. #### Expected behavior; SOR value should be close in value to that of the het-snp called in isolation. #### Actual behavior; SOR value is in fact similar to that of the HomVar snp.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5698
https://github.com/broadinstitute/gatk/pull/5699:133,Security,Expose,Exposed,133,-Cleaned up intermediate files in gCNV WDL. Closes #5382.; -Added output of MAD values as floats in somatic CNV WDL. Closes #5591.; -Exposed boot disk space for Oncotator in somatic CNV WDL. Closes #3566.; -Added check to skip outlier truncation if number of matrix elements exceeds Integer.MAX_VALUE in CreateReadCountPanelOfNormals. Closes #4734.; -Fixed some issues concerning intervals in DetermineGermlineContigPloidy documentation.; -Miscellaneous boy scout activities.; -Fixed non-kebab-case argument in CollectAllelicCountsSpark and other minor issues. See #5478.; -Improved consistency of style and input/output validation across CNV tools. Closes #4825. Closes #5744.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5699
https://github.com/broadinstitute/gatk/pull/5699:621,Security,validat,validation,621,-Cleaned up intermediate files in gCNV WDL. Closes #5382.; -Added output of MAD values as floats in somatic CNV WDL. Closes #5591.; -Exposed boot disk space for Oncotator in somatic CNV WDL. Closes #3566.; -Added check to skip outlier truncation if number of matrix elements exceeds Integer.MAX_VALUE in CreateReadCountPanelOfNormals. Closes #4734.; -Fixed some issues concerning intervals in DetermineGermlineContigPloidy documentation.; -Miscellaneous boy scout activities.; -Fixed non-kebab-case argument in CollectAllelicCountsSpark and other minor issues. See #5478.; -Improved consistency of style and input/output validation across CNV tools. Closes #4825. Closes #5744.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5699
https://github.com/broadinstitute/gatk/issues/5700:1062,Deployability,update,updated,1062,"I am trying to understand the calculation of the [`StrandOddsRatio`](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_annotator_StrandOddsRatio.php). The online documentation and javadoc for the [`StrandOddsRatio` class](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java#L40) shows it as:; ```latex; $$ refRatio = \frac{max(X[0][0], X[0][1])}{min(X[0][0], X[0][1} $$; ```. Nonetheless, my reading of [the code](https://github.com/broadinstitute/gatk/blob/master/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java#L102) shows it as; ```latex; $$ refRatio = \frac{min(X[0][0], X[0][1])}{max(X[0][0], X[0][1} $$; ```; The code is:; ```java; final double refRatio = min(t00, t01)/ max(t00, t01);; ```. The docs say its max/min while the code does min/max. The same is true for the docs and implementation of `altRatio`. It looks like either a bug, or the docs need to be updated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5700
https://github.com/broadinstitute/gatk/pull/5701:225,Performance,perform,performance,225,"* Added the possibility of requesting a gridded output intervals set in ProcessIntervals.; * Also added a min-interval-length argument in case we want to skip smallish intervals (e.g.; at the end of contig).; * Some possible performance improvements in filtering bins that only contain Ns (was using a Stream<Byte>, a bit abusive). * IntervalUtils has now a rutine to write a interval file out of an stream of intervals. Works with NIO.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5701
https://github.com/broadinstitute/gatk/issues/5702:1419,Availability,error,errors,1419,"Gbp genome that is roughly 30M entries. Easily a text interval_list in its simplest form would need around 30ch for each interval that bump it up to 900MB . However one could express the same list just like:. `* *:100`. where the first asterisk stands for ""any contig"", the second stands for ""whole contig"" and the 100 means into 100bp adjacent intervals. from 7ch to 900M??? A few more example as to how such a language could look like:. ```; chr1 # the entire chr1; chr1 * # same; chr1,chr2 # both chr1 and chr2, in full.; * # all contigs in full.; * * # same.; chr1 100-200 # sigle interval from 100-200 on chr1.; chr1 { 100-200 } # same; chr1 { # same; 100-200; }; * 100-200 # 100-200 at every contig.; chr1,chr2 100-200 # only on chr1 and chr2; chr1 *200 # from 1-200 i.e. start to 200.; chr1 4000* # from 4000 to the end of chr1.; chr1 4000 # only position 4000; chr1 4M # only position 4 million. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncating down to 20bp if necessary. ; chr1 { # we can combine interval specs in blocks if they apply to the same contig(s).; 1M-2M:150(20) # from 1 to 2Mbp 150 intervals with 20bp gap; 20M-25M # a big interval from 20 to 25M.; 40012451-40023451 # another standalone interval ; } . ```; ## Interval exclusion; We could specify the exclused interval in the same file:; ```; chr20 *:200 exclude",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:2005,Availability,down,down,2005,"200 # sigle interval from 100-200 on chr1.; chr1 { 100-200 } # same; chr1 { # same; 100-200; }; * 100-200 # 100-200 at every contig.; chr1,chr2 100-200 # only on chr1 and chr2; chr1 *200 # from 1-200 i.e. start to 200.; chr1 4000* # from 4000 to the end of chr1.; chr1 4000 # only position 4000; chr1 4M # only position 4 million. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncating down to 20bp if necessary. ; chr1 { # we can combine interval specs in blocks if they apply to the same contig(s).; 1M-2M:150(20) # from 1 to 2Mbp 150 intervals with 20bp gap; 20M-25M # a big interval from 20 to 25M.; 40012451-40023451 # another standalone interval ; } . ```; ## Interval exclusion; We could specify the exclused interval in the same file:; ```; chr20 *:200 exclude *10000 11000000+10000 32510000* # 200bp intervals except telomere and centromere regions. chr20 { # another way using blocks.; *:200; } excl {; *10000 ; 11000000+10000 ; 32510000*; }. ```. ## Arbitrary interval list. Some other tools cannot specify intervals if these are very specific... for example in exome analysis targets do not fall at regular intervals and are tailor to the capture used. In this case explicit listing is not avoidable. However there are ways to gain. For one thing the language above allows to pack intervals on the same contig on the block so savi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:329,Deployability,pipeline,pipeline,329,"Some tools work with a large list of intervals. In some case these are quite repetitive and they could specify in a single line but due to the need to enumerate each interval explicitly in the interval lists it might result in a uncessary large file, potentially GB in size. ## Repetitive intervals. For example the SV detection pipeline collects read counts at 100bp intervals. In a 3.2Gbp genome that is roughly 30M entries. Easily a text interval_list in its simplest form would need around 30ch for each interval that bump it up to 900MB . However one could express the same list just like:. `* *:100`. where the first asterisk stands for ""any contig"", the second stands for ""whole contig"" and the 100 means into 100bp adjacent intervals. from 7ch to 900M??? A few more example as to how such a language could look like:. ```; chr1 # the entire chr1; chr1 * # same; chr1,chr2 # both chr1 and chr2, in full.; * # all contigs in full.; * * # same.; chr1 100-200 # sigle interval from 100-200 on chr1.; chr1 { 100-200 } # same; chr1 { # same; 100-200; }; * 100-200 # 100-200 at every contig.; chr1,chr2 100-200 # only on chr1 and chr2; chr1 *200 # from 1-200 i.e. start to 200.; chr1 4000* # from 4000 to the end of chr1.; chr1 4000 # only position 4000; chr1 4M # only position 4 million. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:3152,Energy Efficiency,reduce,reduce,3152,"ion. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncating down to 20bp if necessary. ; chr1 { # we can combine interval specs in blocks if they apply to the same contig(s).; 1M-2M:150(20) # from 1 to 2Mbp 150 intervals with 20bp gap; 20M-25M # a big interval from 20 to 25M.; 40012451-40023451 # another standalone interval ; } . ```; ## Interval exclusion; We could specify the exclused interval in the same file:; ```; chr20 *:200 exclude *10000 11000000+10000 32510000* # 200bp intervals except telomere and centromere regions. chr20 { # another way using blocks.; *:200; } excl {; *10000 ; 11000000+10000 ; 32510000*; }. ```. ## Arbitrary interval list. Some other tools cannot specify intervals if these are very specific... for example in exome analysis targets do not fall at regular intervals and are tailor to the capture used. In this case explicit listing is not avoidable. However there are ways to gain. For one thing the language above allows to pack intervals on the same contig on the block so saving to specify the name at each line. (e.g. ```chr1 { 100-200 3124-5681 ... }````. . However real gains would come from ""publishing"" those list under some unique identifier or stable URL that reduce the need of marshaling the whole interval-file file every time. These lest could be retrieved and cached locally by the engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:1601,Modifiability,extend,extending,1601,"* *:100`. where the first asterisk stands for ""any contig"", the second stands for ""whole contig"" and the 100 means into 100bp adjacent intervals. from 7ch to 900M??? A few more example as to how such a language could look like:. ```; chr1 # the entire chr1; chr1 * # same; chr1,chr2 # both chr1 and chr2, in full.; * # all contigs in full.; * * # same.; chr1 100-200 # sigle interval from 100-200 on chr1.; chr1 { 100-200 } # same; chr1 { # same; 100-200; }; * 100-200 # 100-200 at every contig.; chr1,chr2 100-200 # only on chr1 and chr2; chr1 *200 # from 1-200 i.e. start to 200.; chr1 4000* # from 4000 to the end of chr1.; chr1 4000 # only position 4000; chr1 4M # only position 4 million. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncating down to 20bp if necessary. ; chr1 { # we can combine interval specs in blocks if they apply to the same contig(s).; 1M-2M:150(20) # from 1 to 2Mbp 150 intervals with 20bp gap; 20M-25M # a big interval from 20 to 25M.; 40012451-40023451 # another standalone interval ; } . ```; ## Interval exclusion; We could specify the exclused interval in the same file:; ```; chr20 *:200 exclude *10000 11000000+10000 32510000* # 200bp intervals except telomere and centromere regions. chr20 { # another way using blocks.; *:200; } excl {; *10000 ; 11000000+10000 ; 32510000*; }. ```. ## Arbitrary interval",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:3257,Performance,cache,cached,3257,"ion. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncating down to 20bp if necessary. ; chr1 { # we can combine interval specs in blocks if they apply to the same contig(s).; 1M-2M:150(20) # from 1 to 2Mbp 150 intervals with 20bp gap; 20M-25M # a big interval from 20 to 25M.; 40012451-40023451 # another standalone interval ; } . ```; ## Interval exclusion; We could specify the exclused interval in the same file:; ```; chr20 *:200 exclude *10000 11000000+10000 32510000* # 200bp intervals except telomere and centromere regions. chr20 { # another way using blocks.; *:200; } excl {; *10000 ; 11000000+10000 ; 32510000*; }. ```. ## Arbitrary interval list. Some other tools cannot specify intervals if these are very specific... for example in exome analysis targets do not fall at regular intervals and are tailor to the capture used. In this case explicit listing is not avoidable. However there are ways to gain. For one thing the language above allows to pack intervals on the same contig on the block so saving to specify the name at each line. (e.g. ```chr1 { 100-200 3124-5681 ... }````. . However real gains would come from ""publishing"" those list under some unique identifier or stable URL that reduce the need of marshaling the whole interval-file file every time. These lest could be retrieved and cached locally by the engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:319,Safety,detect,detection,319,"Some tools work with a large list of intervals. In some case these are quite repetitive and they could specify in a single line but due to the need to enumerate each interval explicitly in the interval lists it might result in a uncessary large file, potentially GB in size. ## Repetitive intervals. For example the SV detection pipeline collects read counts at 100bp intervals. In a 3.2Gbp genome that is roughly 30M entries. Easily a text interval_list in its simplest form would need around 30ch for each interval that bump it up to 900MB . However one could express the same list just like:. `* *:100`. where the first asterisk stands for ""any contig"", the second stands for ""whole contig"" and the 100 means into 100bp adjacent intervals. from 7ch to 900M??? A few more example as to how such a language could look like:. ```; chr1 # the entire chr1; chr1 * # same; chr1,chr2 # both chr1 and chr2, in full.; * # all contigs in full.; * * # same.; chr1 100-200 # sigle interval from 100-200 on chr1.; chr1 { 100-200 } # same; chr1 { # same; 100-200; }; * 100-200 # 100-200 at every contig.; chr1,chr2 100-200 # only on chr1 and chr2; chr1 *200 # from 1-200 i.e. start to 200.; chr1 4000* # from 4000 to the end of chr1.; chr1 4000 # only position 4000; chr1 4M # only position 4 million. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:1693,Safety,avoid,avoiding,1693,"00 means into 100bp adjacent intervals. from 7ch to 900M??? A few more example as to how such a language could look like:. ```; chr1 # the entire chr1; chr1 * # same; chr1,chr2 # both chr1 and chr2, in full.; * # all contigs in full.; * * # same.; chr1 100-200 # sigle interval from 100-200 on chr1.; chr1 { 100-200 } # same; chr1 { # same; 100-200; }; * 100-200 # 100-200 at every contig.; chr1,chr2 100-200 # only on chr1 and chr2; chr1 *200 # from 1-200 i.e. start to 200.; chr1 4000* # from 4000 to the end of chr1.; chr1 4000 # only position 4000; chr1 4M # only position 4 million. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncating down to 20bp if necessary. ; chr1 { # we can combine interval specs in blocks if they apply to the same contig(s).; 1M-2M:150(20) # from 1 to 2Mbp 150 intervals with 20bp gap; 20M-25M # a big interval from 20 to 25M.; 40012451-40023451 # another standalone interval ; } . ```; ## Interval exclusion; We could specify the exclused interval in the same file:; ```; chr20 *:200 exclude *10000 11000000+10000 32510000* # 200bp intervals except telomere and centromere regions. chr20 { # another way using blocks.; *:200; } excl {; *10000 ; 11000000+10000 ; 32510000*; }. ```. ## Arbitrary interval list. Some other tools cannot specify intervals if these are very specific... for example in exome analys",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:2821,Safety,avoid,avoidable,2821,"ion. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncating down to 20bp if necessary. ; chr1 { # we can combine interval specs in blocks if they apply to the same contig(s).; 1M-2M:150(20) # from 1 to 2Mbp 150 intervals with 20bp gap; 20M-25M # a big interval from 20 to 25M.; 40012451-40023451 # another standalone interval ; } . ```; ## Interval exclusion; We could specify the exclused interval in the same file:; ```; chr20 *:200 exclude *10000 11000000+10000 32510000* # 200bp intervals except telomere and centromere regions. chr20 { # another way using blocks.; *:200; } excl {; *10000 ; 11000000+10000 ; 32510000*; }. ```. ## Arbitrary interval list. Some other tools cannot specify intervals if these are very specific... for example in exome analysis targets do not fall at regular intervals and are tailor to the capture used. In this case explicit listing is not avoidable. However there are ways to gain. For one thing the language above allows to pack intervals on the same contig on the block so saving to specify the name at each line. (e.g. ```chr1 { 100-200 3124-5681 ... }````. . However real gains would come from ""publishing"" those list under some unique identifier or stable URL that reduce the need of marshaling the whole interval-file file every time. These lest could be retrieved and cached locally by the engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/issues/5702:462,Usability,simpl,simplest,462,"Some tools work with a large list of intervals. In some case these are quite repetitive and they could specify in a single line but due to the need to enumerate each interval explicitly in the interval lists it might result in a uncessary large file, potentially GB in size. ## Repetitive intervals. For example the SV detection pipeline collects read counts at 100bp intervals. In a 3.2Gbp genome that is roughly 30M entries. Easily a text interval_list in its simplest form would need around 30ch for each interval that bump it up to 900MB . However one could express the same list just like:. `* *:100`. where the first asterisk stands for ""any contig"", the second stands for ""whole contig"" and the 100 means into 100bp adjacent intervals. from 7ch to 900M??? A few more example as to how such a language could look like:. ```; chr1 # the entire chr1; chr1 * # same; chr1,chr2 # both chr1 and chr2, in full.; * # all contigs in full.; * * # same.; chr1 100-200 # sigle interval from 100-200 on chr1.; chr1 { 100-200 } # same; chr1 { # same; 100-200; }; * 100-200 # 100-200 at every contig.; chr1,chr2 100-200 # only on chr1 and chr2; chr1 *200 # from 1-200 i.e. start to 200.; chr1 4000* # from 4000 to the end of chr1.; chr1 4000 # only position 4000; chr1 4M # only position 4 million. M=10^6, k/K=10^3 ; chr1 10000-99 # from 10000 to 10099... ; # perhaps is best not to accept this as it might silence user input errors.; # but what about instead?; chr1 100[00-99]; chr1 10000+100 # 100 bps starting at 10000 so 10000-10099; chr1 4k # only poistion 4000.; chr20 1M+32K # from position 1 million extending to the following 32Kbps.; chr20 1M1+32K # from position 1 million and 1 instead. (avoiding all those 0s). chr1 *:200 # consecutive 200bp intervals for the entire chromosome; chr1 *:200(100) # 200bp intervals with 100 gaps; chr1 *:200/20 # 200bp intervals with an overlap of 20bp.; chr1 *:20/200 # 200bp starting every 20 positions (so 180bp overlap); chr1 *:200~20 # 200bp intervals truncat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5702
https://github.com/broadinstitute/gatk/pull/5707:110,Testability,test,test,110,Can handle .tsv and .tsv.gz input and output files. ; In theory NIO ready. ; @Header lines are preserved.; No test added so far but you should be able to test it out on a given file(s) or bucket url(s) like so:. `./gatk CondenseReadCounts --input input.100bp.tsv -O output.1000bp.tsv.gz -f 10 --out-bin-length 1000`. The last argument ```--out-bin-length 1000``` is optional and make sure that only those bins with exactly 1000 bases are output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5707
https://github.com/broadinstitute/gatk/pull/5707:154,Testability,test,test,154,Can handle .tsv and .tsv.gz input and output files. ; In theory NIO ready. ; @Header lines are preserved.; No test added so far but you should be able to test it out on a given file(s) or bucket url(s) like so:. `./gatk CondenseReadCounts --input input.100bp.tsv -O output.1000bp.tsv.gz -f 10 --out-bin-length 1000`. The last argument ```--out-bin-length 1000``` is optional and make sure that only those bins with exactly 1000 bases are output.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5707
https://github.com/broadinstitute/gatk/pull/5708:27,Deployability,pipeline,pipeline,27,"This pulls the bulk of the pipeline into a separate subworkflow so that the validations (with the mixture samples) can be run. The mixtures have already been subset and tagged, which is why the rest of the pipeline needed to be extracted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5708
https://github.com/broadinstitute/gatk/pull/5708:206,Deployability,pipeline,pipeline,206,"This pulls the bulk of the pipeline into a separate subworkflow so that the validations (with the mixture samples) can be run. The mixtures have already been subset and tagged, which is why the rest of the pipeline needed to be extracted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5708
https://github.com/broadinstitute/gatk/pull/5708:76,Security,validat,validations,76,"This pulls the bulk of the pipeline into a separate subworkflow so that the validations (with the mixture samples) can be run. The mixtures have already been subset and tagged, which is why the rest of the pipeline needed to be extracted.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5708
https://github.com/broadinstitute/gatk/issues/5709:36,Testability,test,testUtils,36,Let's consolidate them all into src/testUtils/java/org/broadinstitute/hellbender/testutils/VariantContextTestUtils.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5709
https://github.com/broadinstitute/gatk/issues/5709:81,Testability,test,testutils,81,Let's consolidate them all into src/testUtils/java/org/broadinstitute/hellbender/testutils/VariantContextTestUtils.java,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5709
https://github.com/broadinstitute/gatk/issues/5711:318,Safety,avoid,avoid,318,"We acknowledge that in targeted capture the reads will predominantly from one strand at the ends of the targets, but we still try to calculate the strand bias for alt vs. ref. The so-called StrandOddsRatio I wrote when I was young and naive (just don't blame me for the name) adds pseudo counts to the calculations to avoid dividing by zero, but this becomes disastrous at hom-var sites where all the reads are in one direction, e.g. homVar SB:0,0,0,135 (from #5698). The expected behavior should be to return a value that's typical of an unbiased site because this isn't a reason to filter the variant.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5711
https://github.com/broadinstitute/gatk/issues/5714:32,Availability,error,error,32,"@samuelklee I'm running into an error for the `cnv_germline_case_scattered_workflow` WDL pipeline to create a Panel of Normals. It seems during the _GermlineCNVCallerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO Germ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:528,Availability,error,error,528,"@samuelklee I'm running into an error for the `cnv_germline_case_scattered_workflow` WDL pipeline to create a Panel of Normals. It seems during the _GermlineCNVCallerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO Germ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:655,Availability,avail,available,655,"@samuelklee I'm running into an error for the `cnv_germline_case_scattered_workflow` WDL pipeline to create a Panel of Normals. It seems during the _GermlineCNVCallerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO Germ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:748,Availability,error,error,748,"@samuelklee I'm running into an error for the `cnv_germline_case_scattered_workflow` WDL pipeline to create a Panel of Normals. It seems during the _GermlineCNVCallerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO Germ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:908,Availability,error,error,908,"@samuelklee I'm running into an error for the `cnv_germline_case_scattered_workflow` WDL pipeline to create a Panel of Normals. It seems during the _GermlineCNVCallerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO Germ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:21637,Availability,Error,Error,21637,"ackages/theano/gof/cmodule.py"", line 1118, in module_from_key; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1017, in _get_from_key; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 691, in _get_module; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 302, in dlimport; File ""<frozen importlib._bootstrap>"", line 961, in _find_and_load; File ""<frozen importlib._bootstrap>"", line 950, in _find_and_load_unlocked; File ""<frozen importlib._bootstrap>"", line 655, in _load_unlocked; File ""<frozen importlib._bootstrap_external>"", line 674, in exec_module; File ""<frozen importlib._bootstrap_external>"", line 780, in get_code; File ""<frozen importlib._bootstrap_external>"", line 832, in get_data; OSError: [Errno 23] Too many open files in system: '/gatk/local_mnt/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/tmp_yxu5we5/__init__.py'; Error in atexit._run_exitfuncs:; Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1492, in _on_atexit; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1295, in clear_old; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 746, in refresh; OSError: [Errno 23] Too many open files in system: '/gatk/local_mnt/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'; 23:39:07.445 DEBUG ScriptExecutor - Result: 1; 23:39:07.447 INFO GermlineCNVCaller - Shutting down engine; [February 22, 2019 11:39:07 PM UTC] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.14 minutes.; Runtime.totalMemory()=2305818624; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:22280,Availability,down,down,22280,", line 674, in exec_module; File ""<frozen importlib._bootstrap_external>"", line 780, in get_code; File ""<frozen importlib._bootstrap_external>"", line 832, in get_data; OSError: [Errno 23] Too many open files in system: '/gatk/local_mnt/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64/tmp_yxu5we5/__init__.py'; Error in atexit._run_exitfuncs:; Traceback (most recent call last):; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1492, in _on_atexit; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1295, in clear_old; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 746, in refresh; OSError: [Errno 23] Too many open files in system: '/gatk/local_mnt/.theano/compiledir_Linux-3.10-el7.x86_64-x86_64-with-debian-stretch-sid-x86_64-3.6.2-64'; 23:39:07.445 DEBUG ScriptExecutor - Result: 1; 23:39:07.447 INFO GermlineCNVCaller - Shutting down engine; [February 22, 2019 11:39:07 PM UTC] org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller done. Elapsed time: 2.14 minutes.; Runtime.totalMemory()=2305818624; org.broadinstitute.hellbender.utils.python.PythonScriptExecutorException: ; python exited with 1; Command Line: python /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/cohort_denoising_calling.1650827882847090378.py --ploidy_calls_path=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/execution/contig-ploidy-calls-dir --output_calls_path=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/execution/out/csi_batch1-4_wes_gcnv_pon-calls --output_tracking_path=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:29354,Availability,error,error-rate,29354,flow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-1425124017/P0000480.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1072891920/P0000481.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-724059439/P0000992.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:29833,Availability,error,error,29833,"shard-12910/inputs/-724059439/P0000992.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:30689,Availability,down,down,30689,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:30777,Availability,down,down,30777,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:30868,Availability,down,down,30868,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31220,Availability,down,down,31220," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31465,Availability,down,down,31465," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31512,Availability,down,down,31512," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31606,Availability,down,down,31606," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31692,Availability,down,down,31692," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31784,Availability,down,down,31784," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31940,Availability,down,down,31940," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32028,Availability,down,down,32028," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32128,Availability,down,down,32128," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32334,Availability,down,down,32334," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32430,Availability,down,down,32430," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32740,Availability,down,down,32740," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:89,Deployability,pipeline,pipeline,89,"@samuelklee I'm running into an error for the `cnv_germline_case_scattered_workflow` WDL pipeline to create a Panel of Normals. It seems during the _GermlineCNVCallerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO Germ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:188,Deployability,pipeline,pipeline,188,"@samuelklee I'm running into an error for the `cnv_germline_case_scattered_workflow` WDL pipeline to create a Panel of Normals. It seems during the _GermlineCNVCallerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO Germ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:3764,Deployability,Configurat,Configuration,3764,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:19826,Deployability,continuous,continuous,19826,"most recent call last):; File ""/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/cohort_denoising_calling.1650827882847090378.py"", line 143, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/task_cohort_denoising_calling.py"", line 140, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 754, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 515, in Var; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 869, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 250, in logp; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/tensor/var.py"", line 155, in __mul__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 935, in make_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 839, in make_c_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1190, in make_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1131, in __compile__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1586, in cthunk_factory; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1118, in module_from_key; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1017, in _get_from_ke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:30206,Deployability,update,update-convergence-threshold,30206,"nv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-0",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32143,Integrability,message,messages,32143," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32349,Integrability,message,messages,32349," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32445,Integrability,message,messages,32445," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:3748,Modifiability,Config,ConfigFactory,3748,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:3764,Modifiability,Config,Configuration,3764,NVCaller - HTSJDK Defaults.CREATE_MD5 : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.execut,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:3812,Modifiability,Config,ConfigFactory,3812, INFO GermlineCNVCaller - HTSJDK Defaults.CUSTOM_READER_FACTORY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:3867,Modifiability,Config,ConfigFactory,3867,"RY : ; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.DISABLE_SNAPPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:3933,Modifiability,Config,ConfigFactory,3933,"PPY_COMPRESSOR : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4011,Modifiability,Config,ConfigFactory,4011,"ller - HTSJDK Defaults.EBI_REFERENCE_SERVICE_URL_MASK : https://www.ebi.ac.uk/ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engin",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4089,Modifiability,Config,ConfigFactory,4089,"ena/cram/md5/%s; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.NON_ZERO_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4167,Modifiability,Config,ConfigFactory,4167,"O_BUFFER_SIZE : 131072; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFacto",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4245,Modifiability,Config,ConfigFactory,4245,"REFERENCE_FASTA : null; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4310,Modifiability,Config,ConfigFactory,4310,"JDK Defaults.SAM_FLAG_FIELD_FORMAT : DECIMAL; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4385,Modifiability,Config,ConfigFactory,4385,"Caller - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDef",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4452,Modifiability,Config,ConfigFactory,4452,":37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflate",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4527,Modifiability,Config,ConfigFactory,4527,"SAMTOOLS : true; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4596,Modifiability,Config,ConfigFactory,4596,"s.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:37:00.975 INFO GermlineCNVCaller - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.9",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4673,Modifiability,Config,ConfigFactory,4673," - HTSJDK Defaults.USE_CRAM_REF_DOWNLOAD : false; 23:37:00.976 DEBUG ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4742,Modifiability,Config,ConfigFactory,4742,"ConfigFactory - Configuration file values: ; 23:37:00.982 DEBUG ConfigFactory - gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4813,Modifiability,Config,ConfigFactory,4813,"gcsMaxRetries = 20; 23:37:00.982 DEBUG ConfigFactory - gcsProjectForRequesterPays = ; 23:37:00.982 DEBUG ConfigFactory - gatk_stacktrace_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcn",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:4943,Modifiability,Config,ConfigFactory,4943,"race_on_user_exception = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_read_samtools = false; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:5049,Modifiability,Config,ConfigFactory,5049,"lse; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_samtools = true; 23:37:00.982 DEBUG ConfigFactory - samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initializ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:5163,Modifiability,Config,ConfigFactory,5163,"samjdk.use_async_io_write_tribble = false; 23:37:00.983 DEBUG ConfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:5224,Modifiability,Config,ConfigFactory,5224,"nfigFactory - samjdk.compression_level = 2; 23:37:00.983 DEBUG ConfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:38:14.588 INFO GermlineCNVCaller - Intervals specified...; 23:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:5290,Modifiability,Config,ConfigFactory,5290,"nfigFactory - spark.kryoserializer.buffer.max = 512m; 23:37:00.983 DEBUG ConfigFactory - spark.driver.maxResultSize = 0; 23:37:00.983 DEBUG ConfigFactory - spark.driver.userClassPathFirst = true; 23:37:00.983 DEBUG ConfigFactory - spark.io.compression.codec = lzf; 23:37:00.983 DEBUG ConfigFactory - spark.yarn.executor.memoryOverhead = 600; 23:37:00.983 DEBUG ConfigFactory - spark.driver.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - spark.executor.extraJavaOptions = ; 23:37:00.983 DEBUG ConfigFactory - codec_packages = [htsjdk.variant, htsjdk.tribble, org.broadinstitute.hellbender.utils.codecs]; 23:37:00.983 DEBUG ConfigFactory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:38:14.588 INFO GermlineCNVCaller - Intervals specified...; 23:38:14.590 DEBUG GenomeLocParser - Prepared reference sequence cont",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:27539,Modifiability,variab,variable,27539,.executeScript(PythonScriptExecutor.java:151); at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:121); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.executeGermlineCNVCallerPythonScript(GermlineCNVCaller.java:441); at org.broadinstitute.hellbender.tools.copynumber.GermlineCNVCaller.doWork(GermlineCNVCaller.java:288); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10G -jar /gatk/gatk-package-4.1.0.0-local.jar GermlineCNVCaller --run-mode COHORT -L /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-25145621/SureSelect_Human_All_Exon_V6_UTR.1based.preprocessed.filtered.scattered.8543.interval_list --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/371827342/P0000335.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-1425124017/P0000480.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:1164,Performance,Load,Loading,1164,"llerCohortMode_ step, the pipeline opens up tens-of-thousands of files that it doesn't close, causing the system to crash. This seems to happen for me both with the Docker image and Standalone GATK4.1.0.0 jar. It reminds me [of this issue mentioned on the forums from GATK3.8](https://gatkforums.broadinstitute.org/gatk/discussion/12791/too-many-open-files) but the error still occurs even if I limit that step to a single thread. I'm running on a Red Had HPC with 16 threads and 200GB of RAM available and using Cromwell v34. After checked with the manager for my cluster it seems the error occurred when over 60K files were opened simultaneously so this looks to me more like a memory leak than a ulimit issue. Here's the output from a typical error file:. ```Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023; 23:36:58.837 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:36:58.940 DEBUG NativeLibraryLoader - Extracting libgkl_compression.so to /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/libgkl_compression7867300459324040837.so; 23:37:00.969 INFO GermlineCNVCaller - ------------------------------------------------------------; 23:37:00.970 INFO GermlineCNVCaller - The Genome Analysis Toolkit (GATK) v4.1.0.0; 23:37:00.970 INFO GermlineCNVCaller - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:37:00.970 INFO GermlineCNVCaller - Executing as user@e15b680c0241 on Linux v3.10.0-327.36.1.el7.x86_64 amd64; 23:37:00.970 INFO GermlineCNVCaller - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 23:37:00.971 INFO GermlineCNVCaller - Start Date/Time: February",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:11675,Performance,perform,performed,11675,eLocParser - GL000219.1 (179198 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000224.1 (179693 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000223.1 (180455 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000195.1 (182896 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000212.1 (186858 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000222.1 (186861 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000200.1 (187035 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000193.1 (189789 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000194.1 (191469 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000225.1 (211173 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000192.1 (547496 bp); 23:38:14.601 DEBUG GenomeLocParser - NC_007605 (171823 bp); 23:38:14.601 DEBUG GenomeLocParser - hs37d5 (35477943 bp); 23:38:15.218 INFO IntervalArgumentCollection - Processing 13461 bp from intervals; 23:38:15.222 INFO GermlineCNVCaller - No annotated intervals were provided...; 23:38:15.222 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 23:38:15.235 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 23:38:15.236 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:38:15.237 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/371827342/P0000335.b37.counts.hdf5 (1 / 5); 23:38:15.653 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-1425124017/P0000480.b37.counts.hdf5 (2 / 5); 23:38:15.905 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1072891920/P,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32136,Performance,queue,queued,32136," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32342,Performance,queue,queued,32342," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32438,Performance,queue,queued,32438," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:30715,Safety,Timeout,Timeout,30715,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:30806,Safety,Timeout,Timeout,30806,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:30902,Safety,Timeout,Timeout,30902,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:30955,Safety,Abort,Aborting,30955,"r-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Aborting all running workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting dow",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31248,Safety,Timeout,Timeout,31248," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31541,Safety,Timeout,Timeout,31541," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31627,Safety,Timeout,Timeout,31627," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31719,Safety,Timeout,Timeout,31719," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31812,Safety,Timeout,Timeout,31812," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:31963,Safety,Timeout,Timeout,31963," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:32043,Safety,Timeout,Timeout,32043," workflows.; [2019-02-22 23:50:02,53] [info] JobExecutionTokenDispenser stopped; [2019-02-22 23:50:02,53] [info] WorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] WorkflowLogCopyRouter stopped; [2019-02-22 23:50:02,61] [info] Shutting down WorkflowManagerActor - Timeout = 3600 seconds; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor All workflows finished; [2019-02-22 23:50:02,61] [info] WorkflowManagerActor stopped; [2019-02-22 23:50:02,61] [info] Connection pools shut down; [2019-02-22 23:50:02,61] [info] Shutting down SubWorkflowStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down JobStoreActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down CallCacheWriteActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down ServiceRegistryActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] SubWorkflowStoreActor stopped; [2019-02-22 23:50:02,61] [info] Shutting down DockerHashActor - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] Shutting down IoProxy - Timeout = 1800 seconds; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] JobStoreActor stopped; [2019-02-22 23:50:02,61] [info] CallCacheWriteActor stopped; [2019-02-22 23:50:02,61] [info] KvWriteActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,61] [info] WriteMetadataActor Shutting down: 0 queued messages to process; [2019-02-22 23:50:02,62] [info] DockerHashActor stopped; [2019-02-22 23:50:02,62] [info] IoProxy stopped; [2019-02-22 23:50:02,62] [info] ServiceRegistryActor stopped; [2019-02-22 23:50:02,65] [info] Database closed; [2019-02-22 23:50:02,65] [info] Stream materializer shut down; Workflow 098a389e-b298-4324-8a8c-9f46f05708b5 transitioned to state Failed; [2019-02-22 23:50:02,75] [info] Automatic shutdown of the async connection; [2019-02-22 23:50:02,75] [info] Gracefully shutdown sentry threads.; [2019-02-",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:11805,Security,Validat,Validating,11805,000195.1 (182896 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000212.1 (186858 bp); 23:38:14.600 DEBUG GenomeLocParser - GL000222.1 (186861 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000200.1 (187035 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000193.1 (189789 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000194.1 (191469 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000225.1 (211173 bp); 23:38:14.601 DEBUG GenomeLocParser - GL000192.1 (547496 bp); 23:38:14.601 DEBUG GenomeLocParser - NC_007605 (171823 bp); 23:38:14.601 DEBUG GenomeLocParser - hs37d5 (35477943 bp); 23:38:15.218 INFO IntervalArgumentCollection - Processing 13461 bp from intervals; 23:38:15.222 INFO GermlineCNVCaller - No annotated intervals were provided...; 23:38:15.222 INFO GermlineCNVCaller - No GC-content annotations for intervals found; explicit GC-bias correction will not be performed...; 23:38:15.235 INFO GermlineCNVCaller - Running the tool in the COHORT mode...; 23:38:15.236 INFO GermlineCNVCaller - Validating and aggregating data from input read-count files...; 23:38:15.237 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/371827342/P0000335.b37.counts.hdf5 (1 / 5); 23:38:15.653 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-1425124017/P0000480.b37.counts.hdf5 (2 / 5); 23:38:15.905 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1072891920/P0000481.b37.counts.hdf5 (3 / 5); 23:38:16.174 INFO GermlineCNVCaller - Aggregating read-count file /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:5992,Testability,log,logger,5992,Factory - read_filter_packages = [org.broadinstitute.hellbender.engine.filters]; 23:37:00.983 DEBUG ConfigFactory - annotation_packages = [org.broadinstitute.hellbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:38:14.588 INFO GermlineCNVCaller - Intervals specified...; 23:38:14.590 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:38:14.591 DEBUG GenomeLocParser - 1 (249250621 bp); 23:38:14.591 DEBUG GenomeLocParser - 2 (243199373 bp); 23:38:14.591 DEBUG GenomeLocParser - 3 (198022430 bp); 23:38:14.591 DEBUG GenomeLocParser - 4 (191154276 bp); 23:38:14.591 DEBUG GenomeLocParser - 5 (180915260 bp); 23:38:14.592 DEBUG GenomeLocParser - 6 (171115067 bp); 23:38:14.592 DEBUG GenomeLocParser - 7 (159138663 bp); 23:38:14.592 DEBUG GenomeLocParser - 8 (146364022 bp); 23:38:14.592 DEBUG GenomeLocParser - 9 (141213431 bp); 23:38:14.592 DEBUG GenomeLocParser - 10 (135534747 bp); 23:38:14.592 DEBUG GenomeLocParser - 11 (135006516 bp); 23:38:14.592 DE,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:6118,Testability,log,logging,6118,lbender.tools.walkers.annotator]; 23:37:00.983 DEBUG ConfigFactory - cloudPrefetchBuffer = 40; 23:37:00.983 DEBUG ConfigFactory - cloudIndexPrefetchBuffer = -1; 23:37:00.983 DEBUG ConfigFactory - createOutputBamIndex = true; 23:37:00.984 INFO GermlineCNVCaller - Deflater: IntelDeflater; 23:37:00.984 INFO GermlineCNVCaller - Inflater: IntelInflater; 23:37:00.984 INFO GermlineCNVCaller - GCS max retries/reopens: 20; 23:37:00.984 INFO GermlineCNVCaller - Requester pays: disabled; 23:37:00.984 INFO GermlineCNVCaller - Initializing engine; 23:37:00.990 DEBUG ScriptExecutor - Executing:; 23:37:00.991 DEBUG ScriptExecutor - python; 23:37:00.991 DEBUG ScriptExecutor - -c; 23:37:00.991 DEBUG ScriptExecutor - import gcnvkernel. 23:38:13.336 DEBUG ScriptExecutor - Result: 0; 23:38:13.341 INFO GermlineCNVCaller - Done initializing engine; log4j:WARN No appenders could be found for logger (org.broadinstitute.hdf5.HDF5Library).; log4j:WARN Please initialize the log4j system properly.; log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.; 23:38:14.588 INFO GermlineCNVCaller - Intervals specified...; 23:38:14.590 DEBUG GenomeLocParser - Prepared reference sequence contig dictionary; 23:38:14.591 DEBUG GenomeLocParser - 1 (249250621 bp); 23:38:14.591 DEBUG GenomeLocParser - 2 (243199373 bp); 23:38:14.591 DEBUG GenomeLocParser - 3 (198022430 bp); 23:38:14.591 DEBUG GenomeLocParser - 4 (191154276 bp); 23:38:14.591 DEBUG GenomeLocParser - 5 (180915260 bp); 23:38:14.592 DEBUG GenomeLocParser - 6 (171115067 bp); 23:38:14.592 DEBUG GenomeLocParser - 7 (159138663 bp); 23:38:14.592 DEBUG GenomeLocParser - 8 (146364022 bp); 23:38:14.592 DEBUG GenomeLocParser - 9 (141213431 bp); 23:38:14.592 DEBUG GenomeLocParser - 10 (135534747 bp); 23:38:14.592 DEBUG GenomeLocParser - 11 (135006516 bp); 23:38:14.592 DEBUG GenomeLocParser - 12 (133851895 bp); 23:38:14.593 DEBUG GenomeLocParser - 13 (115169878 bp); 23:38:14.593 DEBUG GenomeLocParser - 14 (107349540 bp); 23:38:14,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:19855,Testability,log,logp,19855,"xecutions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/tmp.cd408023/cohort_denoising_calling.1650827882847090378.py"", line 143, in <module>; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/tasks/task_cohort_denoising_calling.py"", line 140, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 197, in __call__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/gcnvkernel/models/model_denoising_calling.py"", line 754, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/distribution.py"", line 39, in __new__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 515, in Var; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/model.py"", line 869, in __init__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/pymc3/distributions/continuous.py"", line 250, in logp; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/tensor/var.py"", line 155, in __mul__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 670, in __call__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 935, in make_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/op.py"", line 839, in make_c_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1190, in make_thunk; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1131, in __compile__; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cc.py"", line 1586, in cthunk_factory; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1118, in module_from_key; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 1017, in _get_from_key; File ""/opt/miniconda/envs/gatk/lib/python3.6/site-pack",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:29456,Testability,log,log-mean-bias-standard-deviation,29456,"t /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1072891920/P0000481.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-724059439/P0000992.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: Wo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:29764,Testability,log,log-emission-samples-per-round,29764,"shard-12910/inputs/-724059439/P0000992.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:29800,Testability,log,log-emission-sampling-median-rel-error,29800,"shard-12910/inputs/-724059439/P0000992.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRou",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:29847,Testability,log,log-emission-sampling-rounds,29847,"-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2019-02-22 23:50:02,48] [info] Shutting down WorkflowStoreActor - Timeout = 5 seconds; [2019-02-22 23:50:02,49] [info] Shutting down WorkflowLogCopyRouter - Timeout = 5 seconds; [2019-02-22 23:50:02,53] [info] Shutting down JobExecutionTokenDispenser - Timeout = 5 seconds; [2019-02-2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/issues/5714:29702,Usability,learn,learning-rate,29702,"unts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/-724059439/P0000992.b37.counts.hdf5 --input /gatk/local_mnt/cromwell-executions/CNVGermlineCohortWorkflow/098a389e-b298-4324-8a8c-9f46f05708b5/call-GermlineCNVCallerCohortMode/shard-12910/inputs/1773956498/P0001010.b37.counts.hdf5 --contig-ploidy-calls contig-ploidy-calls-dir --interval-merging-rule OVERLAPPING_ONLY --output out --output-prefix csi_batch1-4_wes_gcnv_pon --verbosity DEBUG --p-alt 1e-6 --p-active 1e-2 --cnv-coherence-length 10000.0 --class-coherence-length 10000.0 --max-copy-number 5 --max-bias-factors 5 --mapping-error-rate 0.01 --interval-psi-scale 0.001 --sample-psi-scale 0.0001 --depth-correction-tau 10000.0 --log-mean-bias-standard-deviation 0.1 --init-ard-rel-unexplained-variance 0.1 --num-gc-bins 20 --gc-curve-standard-deviation 1.0 --copy-number-posterior-expectation-mode HYBRID --enable-bias-factors true --active-class-padding-hybrid-mode 50000 --learning-rate 0.05 --adamax-beta-1 0.9 --adamax-beta-2 0.99 --log-emission-samples-per-round 50 --log-emission-sampling-median-rel-error 0.005 --log-emission-sampling-rounds 10 --max-advi-iter-first-epoch 5000 --max-advi-iter-subsequent-epochs 100 --min-training-epochs 10 --max-training-epochs 100 --initial-temperature 2.0 --num-thermal-advi-iters 2500 --convergence-snr-averaging-window 500 --convergence-snr-trigger-threshold 0.1 --convergence-snr-countdown-window 10 --max-calling-iters 10 --caller-update-convergence-threshold 0.001 --caller-internal-admixing-rate 0.75 --caller-external-admixing-rate 1.00 --disable-annealing false. [2019-02-22 23:49:20,42] [info] WorkflowManagerActor WorkflowActor-098a389e-b298-4324-8a8c-9f46f05708b5 is in a terminal state: WorkflowFailedState; [2019-02-22 23:50:01,65] [info] SingleWorkflowRunnerActor workflow finished with status 'Failed'.; [2019-02-22 23:50:02,38] [info] Workflow polling stopped; [2",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5714
https://github.com/broadinstitute/gatk/pull/5715:274,Availability,down,down,274,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:860,Availability,redundant,redundant,860,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:994,Availability,down,down,994,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:1695,Availability,mainten,maintenance,1695,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:1870,Availability,down,down,1870,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:725,Deployability,release,released,725,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:420,Energy Efficiency,reduce,reduce,420,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:1118,Performance,optimiz,optimize,1118,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:850,Safety,avoid,avoided,850,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:860,Safety,redund,redundant,860,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:773,Security,Hash,HashMultiSet,773,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/pull/5715:888,Usability,Simpl,SimpleCountCollection,888,"Goal was to get WGS coverage collection at 100bp at ~15 cents per sample. Since this is I/O bound (takes ~2 hours to stream or localize a BAM, or about the same to decompress a CRAM), cost reduction can be most easily achieved by reducing the memory requirements and moving down to a cheaper VM. . Memory requirements at 100bp are dominated by manipulations of the list of ~30M intervals. There were a few easy fixes to reduce requirements that did not require changing the collection method (which can be easily modified for future investigations, see #4551):. -removed WellformedReadFilter. See #5233. EDIT: We decided after PR review to retain this filter by default and disable it at the WDL level when Best Practices is released. Leaving the issue open.; -initialized HashMultiSet capacity; -removed unnecessary call to OverlapDetector.getAll; -avoided a redundant defensive copy in SimpleCountCollection; -used per-contig OverlapDetectors, rather than a global one. This brought the cost down to ~9 cents per sample using n1-standard-2's with 7.5GB of memory when collecting on BAMs with NIO. Note that I didn't optimize disk size, which accounts for ~50% of the total cost and is unused when running with NIO, so we are closer to ~5 cents per sample. It is possible that using CRAMs with or without NIO and with or without SSDs might be cheaper. Note that OverlapDetectors may be overkill for our case, since bins are guaranteed to be sorted and non-overlapping and queries are also sorted. We could probably roll something that is O(1) in memory. However, since we are I/O bound, as long as we are satisfied with the current cost, I am willing to sacrifice memory for implementation and maintenance costs, as well as the option to change strategies easily. In any case, @lbergelson found some easy wins in OverlapDetector that may further bring the memory usage down, and will issue a fix in htsjdk soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5715
https://github.com/broadinstitute/gatk/issues/5716:1075,Deployability,pipeline,pipeline,1075,"In order of priority:. 1) The ability to query and/or stream intervals for locatable collections might reduce the overhead of file localization in the germline workflows---even though we only run GermlineCNVCaller on a subset of intervals in any particular shard, we localize the entire read-count file. This could be enabled in the parent class to benefit all locatable files, but since it will probably require indexing, we should use only when necessary.; 2) Memory requirements for some tools could be reduced by avoiding intermediate creation of an internally held list, streaming it directly instead.; 3) NIO streaming of entire files to/from buckets could be easily added to the relevant CSV/HDF5 read/write classes. Apart from the first issue, I don't think this really adds much, since the largest files are only ~1GB (and most seg files are much smaller) and are typically cheap to localize for single samples. See also #3976, #4004, #4717, and #5715 for context. I think we should first demonstrate if the first issue is really the dominating cost in the germline pipeline. If not, we should first focus on optimizing inference. The other issues are much lower priority.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716
https://github.com/broadinstitute/gatk/issues/5716:103,Energy Efficiency,reduce,reduce,103,"In order of priority:. 1) The ability to query and/or stream intervals for locatable collections might reduce the overhead of file localization in the germline workflows---even though we only run GermlineCNVCaller on a subset of intervals in any particular shard, we localize the entire read-count file. This could be enabled in the parent class to benefit all locatable files, but since it will probably require indexing, we should use only when necessary.; 2) Memory requirements for some tools could be reduced by avoiding intermediate creation of an internally held list, streaming it directly instead.; 3) NIO streaming of entire files to/from buckets could be easily added to the relevant CSV/HDF5 read/write classes. Apart from the first issue, I don't think this really adds much, since the largest files are only ~1GB (and most seg files are much smaller) and are typically cheap to localize for single samples. See also #3976, #4004, #4717, and #5715 for context. I think we should first demonstrate if the first issue is really the dominating cost in the germline pipeline. If not, we should first focus on optimizing inference. The other issues are much lower priority.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716
https://github.com/broadinstitute/gatk/issues/5716:506,Energy Efficiency,reduce,reduced,506,"In order of priority:. 1) The ability to query and/or stream intervals for locatable collections might reduce the overhead of file localization in the germline workflows---even though we only run GermlineCNVCaller on a subset of intervals in any particular shard, we localize the entire read-count file. This could be enabled in the parent class to benefit all locatable files, but since it will probably require indexing, we should use only when necessary.; 2) Memory requirements for some tools could be reduced by avoiding intermediate creation of an internally held list, streaming it directly instead.; 3) NIO streaming of entire files to/from buckets could be easily added to the relevant CSV/HDF5 read/write classes. Apart from the first issue, I don't think this really adds much, since the largest files are only ~1GB (and most seg files are much smaller) and are typically cheap to localize for single samples. See also #3976, #4004, #4717, and #5715 for context. I think we should first demonstrate if the first issue is really the dominating cost in the germline pipeline. If not, we should first focus on optimizing inference. The other issues are much lower priority.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716
https://github.com/broadinstitute/gatk/issues/5716:1118,Performance,optimiz,optimizing,1118,"In order of priority:. 1) The ability to query and/or stream intervals for locatable collections might reduce the overhead of file localization in the germline workflows---even though we only run GermlineCNVCaller on a subset of intervals in any particular shard, we localize the entire read-count file. This could be enabled in the parent class to benefit all locatable files, but since it will probably require indexing, we should use only when necessary.; 2) Memory requirements for some tools could be reduced by avoiding intermediate creation of an internally held list, streaming it directly instead.; 3) NIO streaming of entire files to/from buckets could be easily added to the relevant CSV/HDF5 read/write classes. Apart from the first issue, I don't think this really adds much, since the largest files are only ~1GB (and most seg files are much smaller) and are typically cheap to localize for single samples. See also #3976, #4004, #4717, and #5715 for context. I think we should first demonstrate if the first issue is really the dominating cost in the germline pipeline. If not, we should first focus on optimizing inference. The other issues are much lower priority.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716
https://github.com/broadinstitute/gatk/issues/5716:517,Safety,avoid,avoiding,517,"In order of priority:. 1) The ability to query and/or stream intervals for locatable collections might reduce the overhead of file localization in the germline workflows---even though we only run GermlineCNVCaller on a subset of intervals in any particular shard, we localize the entire read-count file. This could be enabled in the parent class to benefit all locatable files, but since it will probably require indexing, we should use only when necessary.; 2) Memory requirements for some tools could be reduced by avoiding intermediate creation of an internally held list, streaming it directly instead.; 3) NIO streaming of entire files to/from buckets could be easily added to the relevant CSV/HDF5 read/write classes. Apart from the first issue, I don't think this really adds much, since the largest files are only ~1GB (and most seg files are much smaller) and are typically cheap to localize for single samples. See also #3976, #4004, #4717, and #5715 for context. I think we should first demonstrate if the first issue is really the dominating cost in the germline pipeline. If not, we should first focus on optimizing inference. The other issues are much lower priority.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5716
https://github.com/broadinstitute/gatk/issues/5717:600,Availability,error,errors,600,"User Question: I'm trying to speed up the process of calling variants using SPARK. I have access to a slurm HPC cluster, so I guess it's not that straightforward to run GATK in a proper distributed master-slave architecture (if there is any tutorial on how to setup slurm jobs to use GATK Spark tools on multiple nodes I would appreciate it a lot). ; Therefore, I run GATK in local mode with some SPARK threads, eventually speeding up the process by parallelising the number of samples processed simultaneously with GNU parallel. But then, I'm having troubles because some samples crash due to SPARK errors. Perhaps you could send my logs to the developers ? I'm trying to run 8 parallel GATk jobs (8 samples) using 5 Spark cpus on each in a node with 40 cpus. . Best,; Pedro. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/56193#Comment_56193",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5717
https://github.com/broadinstitute/gatk/issues/5717:90,Security,access,access,90,"User Question: I'm trying to speed up the process of calling variants using SPARK. I have access to a slurm HPC cluster, so I guess it's not that straightforward to run GATK in a proper distributed master-slave architecture (if there is any tutorial on how to setup slurm jobs to use GATK Spark tools on multiple nodes I would appreciate it a lot). ; Therefore, I run GATK in local mode with some SPARK threads, eventually speeding up the process by parallelising the number of samples processed simultaneously with GNU parallel. But then, I'm having troubles because some samples crash due to SPARK errors. Perhaps you could send my logs to the developers ? I'm trying to run 8 parallel GATk jobs (8 samples) using 5 Spark cpus on each in a node with 40 cpus. . Best,; Pedro. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/56193#Comment_56193",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5717
https://github.com/broadinstitute/gatk/issues/5717:634,Testability,log,logs,634,"User Question: I'm trying to speed up the process of calling variants using SPARK. I have access to a slurm HPC cluster, so I guess it's not that straightforward to run GATK in a proper distributed master-slave architecture (if there is any tutorial on how to setup slurm jobs to use GATK Spark tools on multiple nodes I would appreciate it a lot). ; Therefore, I run GATK in local mode with some SPARK threads, eventually speeding up the process by parallelising the number of samples processed simultaneously with GNU parallel. But then, I'm having troubles because some samples crash due to SPARK errors. Perhaps you could send my logs to the developers ? I'm trying to run 8 parallel GATk jobs (8 samples) using 5 Spark cpus on each in a node with 40 cpus. . Best,; Pedro. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/56193#Comment_56193",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5717
https://github.com/broadinstitute/gatk/issues/5719:1306,Deployability,release,release,1306,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5719
https://github.com/broadinstitute/gatk/issues/5719:1376,Testability,test,test,1376,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5719
https://github.com/broadinstitute/gatk/issues/5719:1476,Testability,log,logs,1476,"gatk).; - Search the existing github issues to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5719
https://github.com/broadinstitute/gatk/pull/5721:91,Availability,error,errors,91,"These are the changes needed to run on a whole genome in strict mode. We get out of memory errors without these changes. Reads downsampling was missing for the part where `AssemblyRegion`s are filled with reads - this PR adds it in. Downsampling is not deterministic yet, since that depends on #5437, but that's an orthogonal issue so it's OK to merge this change and add #5437 later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5721
https://github.com/broadinstitute/gatk/pull/5721:127,Availability,down,downsampling,127,"These are the changes needed to run on a whole genome in strict mode. We get out of memory errors without these changes. Reads downsampling was missing for the part where `AssemblyRegion`s are filled with reads - this PR adds it in. Downsampling is not deterministic yet, since that depends on #5437, but that's an orthogonal issue so it's OK to merge this change and add #5437 later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5721
https://github.com/broadinstitute/gatk/pull/5721:233,Availability,Down,Downsampling,233,"These are the changes needed to run on a whole genome in strict mode. We get out of memory errors without these changes. Reads downsampling was missing for the part where `AssemblyRegion`s are filled with reads - this PR adds it in. Downsampling is not deterministic yet, since that depends on #5437, but that's an orthogonal issue so it's OK to merge this change and add #5437 later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5721
https://github.com/broadinstitute/gatk/pull/5721:283,Integrability,depend,depends,283,"These are the changes needed to run on a whole genome in strict mode. We get out of memory errors without these changes. Reads downsampling was missing for the part where `AssemblyRegion`s are filled with reads - this PR adds it in. Downsampling is not deterministic yet, since that depends on #5437, but that's an orthogonal issue so it's OK to merge this change and add #5437 later.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5721
https://github.com/broadinstitute/gatk/issues/5722:1334,Usability,simpl,simple,1334,"When each haplotype has a deletion and they overlap, but the first is low quality, the resulting reference block gets the wrong END. Input:; ```; chr22	10515074	.	AGAAG	A,<NON_REF>	0	.	DP=10;ExcessHet=3.0103;MLEAC=0,0;MLEAF=0.00,0.00;RAW_MQ=19758.00	GT:AD:DP:GQ:PL:SB	0/0:6,0,0:6:18:0,18,253,18,254,254:5,1,0,0; chr22	10515076	.	AAGGAAGG	A,<NON_REF>	112.73	.	BaseQRankSum=-0.365;ClippingRankSum=0.365;DP=10;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.365;RAW_MQ=19758.00;ReadPosRankSum=2.383	GT:AD:DP:GQ:PL:SB	0/1:6,4,0:10:99:150,0,211,168,223,391:5,1,3,1; ```. Current behavior:; ```; chr22 10515074 . A <NON_REF> . . END=10515078 GT:DP:GQ:MIN_DP:PL 0/0:6:18:6:0,18,254; chr22 10515076 . AAGGAAGG A,<NON_REF> 121.77 . DP=10;MQRankSum=0.365;MQ_DP=10;QUALapprox=150;RAW_MQ=19758.00;ReadPosRankSum=2.383;VarDP=10 GT:AD:DP:GQ:PL:SB 0/1:6,4,0:10:99:150,0,211,168,223,391:5,1,3,1; ```. Expected behavior:; ```; chr22 10515074 . A <NON_REF> . . END=10515075 GT:DP:GQ:MIN_DP:PL 0/0:6:18:6:0,18,254; chr22 10515076 . AAGGAAGG A,<NON_REF> 121.77 . DP=10;MQRankSum=0.365;MQ_DP=10;QUALapprox=150;RAW_MQ=19758.00;ReadPosRankSum=2.383;VarDP=10 GT:AD:DP:GQ:PL:SB 0/1:6,4,0:10:99:150,0,211,168,223,391:5,1,3,1; ```; such that the END of the ref block from the first deletion doesn't exceed the start of the next deletion. This may be as simple as checking the start of the next variant when a deletion is converted to a hom ref block.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5722
https://github.com/broadinstitute/gatk/pull/5723:41,Deployability,integrat,integration,41,Share more code between walker and Spark integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5723
https://github.com/broadinstitute/gatk/pull/5723:41,Integrability,integrat,integration,41,Share more code between walker and Spark integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5723
https://github.com/broadinstitute/gatk/pull/5723:53,Testability,test,tests,53,Share more code between walker and Spark integration tests.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5723
https://github.com/broadinstitute/gatk/pull/5728:0,Deployability,Update,Update,0,"Update all to createVCFWriter(Path) (not File). As a result, all VCF tools should now be able to write to Cloud (including Funcolator). This commit also removes createVCFWriter(File), ensuring all new; code retains the ability to write to Cloud. This touches many files, but the changes are mostly the same everywhere. Only `FilterByOrientationBias.java` is a bit different since it also needed code to add a suffix to a file name. . Fixes #5726",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5728
https://github.com/broadinstitute/gatk/issues/5729:34,Availability,error,error,34,"Hi GATK Team. I got the following error when using MarkDupliactesSpark tool:. `A USER ERROR has occurred: Couldn't write file /media/Berechnungen/190218_NB501654_0110_AHT7VFBGX9/0046-19_Exom (Intelligenzminderung)/0046-19.dedup.bam because writing failed with exception File file:/media/Berechnungen/190218_NB501654_0110_AHT7VFBGX9/0046-19_Exom%20(Intelligenzminderung)/0046-19.dedup.bam.parts/header does not exist`. I looked at the specified path and the `header` file is in the right directory. ; Is it possible, that MDSpark replaces a space in the filepath to %20 and at the end does not replace it back to space, so it does not finde the `header` file?. Thanks in advance; Stefan . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23509/markduplicatespark-does-not-find-file-though-it-is-there/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5729
https://github.com/broadinstitute/gatk/issues/5729:86,Availability,ERROR,ERROR,86,"Hi GATK Team. I got the following error when using MarkDupliactesSpark tool:. `A USER ERROR has occurred: Couldn't write file /media/Berechnungen/190218_NB501654_0110_AHT7VFBGX9/0046-19_Exom (Intelligenzminderung)/0046-19.dedup.bam because writing failed with exception File file:/media/Berechnungen/190218_NB501654_0110_AHT7VFBGX9/0046-19_Exom%20(Intelligenzminderung)/0046-19.dedup.bam.parts/header does not exist`. I looked at the specified path and the `header` file is in the right directory. ; Is it possible, that MDSpark replaces a space in the filepath to %20 and at the end does not replace it back to space, so it does not finde the `header` file?. Thanks in advance; Stefan . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23509/markduplicatespark-does-not-find-file-though-it-is-there/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5729
https://github.com/broadinstitute/gatk/issues/5730:599,Testability,test,test,599,"Looking back at some Slack chatter with @mbabadi from September 2018, I was seeing some runs of GermlineCNVCaller that yielded slightly different results (mostly slight shifts in breakpoints). Not sure if this is still a problem (or if @vruano or @mwalker174 encountered this), but if so, we should check RNG seeds and initializations in the inference code. @mbabadi also raised the possibility of the choice of BLAS library introducing non-determinism. Results of DetermineGermlineContigPloidy were identical. Note also that PostprocessGermlineCNVCallsIntegrationTest checks for reproducibility on test files. See also #4375.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5730
https://github.com/broadinstitute/gatk/pull/5735:78,Deployability,update,updated,78,Now the Mutect2 WDL has all the hooks to run Funcotator (`Funcotator.wdl` was updated as well). The `Funcotate` task in the M2 WDL is identical to that same task in `Funcotator.wdl`. Fixes #5253,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5735
https://github.com/broadinstitute/gatk/issues/5738:1990,Testability,Assert,AssertionError,1990,"756355.tsv --contig_ploidy_prior_table=contig_ploidy_priort_hg38.tsv `--output_model_path=cohort_output/normal_cohort-model`. `Stdout:`; `Stderr: Traceback (most recent call last):; File ""/tmp/cohort_determine_ploidy_and_depth.8087328354193956874.py"", line 67, in <module>; ""--caller_update_convergence_threshold"": 1e-3; File ""/home/grayl/.local/lib/python3.6/site-packages/gcnvkernel/tasks/inference_task_base.py"", line 638, in expose_args ""arguments; possible choices are: {1}"".format(override_default_arg, valid_args); AssertionError: Initializer argument of which the default is to be overridden --num_thermal_advi_iters is not a valid initializer arguments; possible choices are: {'--track_model_params_every', '--learning_rate', '--disable_annealing', '--random_seed', '--adamax_beta2', '--convergence_snr_trigger_threshold', '--track_model_params', '--sampler_smoothing_window', '--param_tracker_config', '--caller_update_convergence_threshold', '--caller_summary_statistics_reducer', '--caller_admixing_rate', '--initial_temperature', '--self', '--convergence_snr_averaging_window', '--max_advi_iter_first_epoch', '--num_thermal_epochs', '--disable_sampler', '--min_training_epochs', '--max_calling_iters', '--max_advi_iter_subsequent_epochs', '--adamax_beta1', '--log_emission_sampling_median_rel_error', '--total_grad_norm_constraint', '--convergence_snr_countdown_window', '--log_emission_samples_per_round', '--max_training_epochs', '--log_emission_sampling_rounds', '--disable_caller', '--obj_n_mc'} at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126) at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170) at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151)`. #### Steps to reproduce; `gatk DetermineGerml",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5738
https://github.com/broadinstitute/gatk/issues/5738:3946,Testability,Assert,AssertionError,3946,"ide_default_arg, valid_args); AssertionError: Initializer argument of which the default is to be overridden --num_thermal_advi_iters is not a valid initializer arguments; possible choices are: {'--track_model_params_every', '--learning_rate', '--disable_annealing', '--random_seed', '--adamax_beta2', '--convergence_snr_trigger_threshold', '--track_model_params', '--sampler_smoothing_window', '--param_tracker_config', '--caller_update_convergence_threshold', '--caller_summary_statistics_reducer', '--caller_admixing_rate', '--initial_temperature', '--self', '--convergence_snr_averaging_window', '--max_advi_iter_first_epoch', '--num_thermal_epochs', '--disable_sampler', '--min_training_epochs', '--max_calling_iters', '--max_advi_iter_subsequent_epochs', '--adamax_beta1', '--log_emission_sampling_median_rel_error', '--total_grad_norm_constraint', '--convergence_snr_countdown_window', '--log_emission_samples_per_round', '--max_training_epochs', '--log_emission_sampling_rounds', '--disable_caller', '--obj_n_mc'} at org.broadinstitute.hellbender.utils.python.PythonExecutorBase.getScriptException(PythonExecutorBase.java:75) at org.broadinstitute.hellbender.utils.runtime.ScriptExecutor.executeCuratedArgs(ScriptExecutor.java:126) at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeArgs(PythonScriptExecutor.java:170) at org.broadinstitute.hellbender.utils.python.PythonScriptExecutor.executeScript(PythonScriptExecutor.java:151)`. #### Steps to reproduce; `gatk DetermineGermlineContigPloidy --input 1_AH285.counts.hdf5 --input 2_AH287.counts.hdf5 --input 3_AH288.counts.hdf5 --input 4_AH290.counts.hdf5 --input 5_AH291.counts.hdf5 --input 6_AH292.counts.hdf5 --input 7_AH295.counts.hdf5 --input 8_AH296.counts.hdf5 --contig-ploidy-priors contig_ploidy_priort_hg38.tsv --output cohort_output --output-prefix normal_cohort`. #### Expected behavior; Should identify key for caller_update_convergence_threshold. #### Actual behavior; Script terminates with AssertionError",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5738
https://github.com/broadinstitute/gatk/issues/5739:325,Availability,error,error,325,"## Bug Report. ### Affected tool(s) or class(es); GATK 4.1.0.0 AnalyzeCovariates. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; The csv produced by AnalyzeCovariates is invalid. It doesn't escape commas in fields, resulting in an error in the R script. #### Steps to reproduce; If you have a comma in the readgroup in a BAM, this will happen. #### Expected behavior; It should produce valid csv files, and then be able to properly produce the plots. #### Actual behavior; Commas in read group names result in malformed (unescaped) csv where it's impossible to parse fields properly. This results in the following R script error:; ```; Error in read.table(file = file, header = header, sep = sep, quote = quote, :; more columns than column names; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5739
https://github.com/broadinstitute/gatk/issues/5739:717,Availability,error,error,717,"## Bug Report. ### Affected tool(s) or class(es); GATK 4.1.0.0 AnalyzeCovariates. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; The csv produced by AnalyzeCovariates is invalid. It doesn't escape commas in fields, resulting in an error in the R script. #### Steps to reproduce; If you have a comma in the readgroup in a BAM, this will happen. #### Expected behavior; It should produce valid csv files, and then be able to properly produce the plots. #### Actual behavior; Commas in read group names result in malformed (unescaped) csv where it's impossible to parse fields properly. This results in the following R script error:; ```; Error in read.table(file = file, header = header, sep = sep, quote = quote, :; more columns than column names; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5739
https://github.com/broadinstitute/gatk/issues/5739:730,Availability,Error,Error,730,"## Bug Report. ### Affected tool(s) or class(es); GATK 4.1.0.0 AnalyzeCovariates. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; The csv produced by AnalyzeCovariates is invalid. It doesn't escape commas in fields, resulting in an error in the R script. #### Steps to reproduce; If you have a comma in the readgroup in a BAM, this will happen. #### Expected behavior; It should produce valid csv files, and then be able to properly produce the plots. #### Actual behavior; Commas in read group names result in malformed (unescaped) csv where it's impossible to parse fields properly. This results in the following R script error:; ```; Error in read.table(file = file, header = header, sep = sep, quote = quote, :; more columns than column names; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5739
https://github.com/broadinstitute/gatk/issues/5739:127,Deployability,release,release,127,"## Bug Report. ### Affected tool(s) or class(es); GATK 4.1.0.0 AnalyzeCovariates. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; The csv produced by AnalyzeCovariates is invalid. It doesn't escape commas in fields, resulting in an error in the R script. #### Steps to reproduce; If you have a comma in the readgroup in a BAM, this will happen. #### Expected behavior; It should produce valid csv files, and then be able to properly produce the plots. #### Actual behavior; Commas in read group names result in malformed (unescaped) csv where it's impossible to parse fields properly. This results in the following R script error:; ```; Error in read.table(file = file, header = header, sep = sep, quote = quote, :; more columns than column names; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5739
https://github.com/broadinstitute/gatk/issues/5739:197,Testability,test,test,197,"## Bug Report. ### Affected tool(s) or class(es); GATK 4.1.0.0 AnalyzeCovariates. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; The csv produced by AnalyzeCovariates is invalid. It doesn't escape commas in fields, resulting in an error in the R script. #### Steps to reproduce; If you have a comma in the readgroup in a BAM, this will happen. #### Expected behavior; It should produce valid csv files, and then be able to properly produce the plots. #### Actual behavior; Commas in read group names result in malformed (unescaped) csv where it's impossible to parse fields properly. This results in the following R script error:; ```; Error in read.table(file = file, header = header, sep = sep, quote = quote, :; more columns than column names; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5739
https://github.com/broadinstitute/gatk/issues/5740:291,Availability,avail,available,291,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:4624,Availability,Error,Error,4624,"NFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing error; path=/mnt/fargen/experiments/joint_call/data/genomicsdb/run1/chr1$11981$12351/.__cd28ac27-6a06-422f-a674-2acfbdb072d1140406632785664_1551359040166; errno=22(Invalid argument); terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr1$11981$12351 to disk; TileDB error message :",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:4674,Availability,error,error,4674,"NFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing error; path=/mnt/fargen/experiments/joint_call/data/genomicsdb/run1/chr1$11981$12351/.__cd28ac27-6a06-422f-a674-2acfbdb072d1140406632785664_1551359040166; errno=22(Invalid argument); terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr1$11981$12351 to disk; TileDB error message :",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:4989,Availability,Error,Error,4989,"NFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing error; path=/mnt/fargen/experiments/joint_call/data/genomicsdb/run1/chr1$11981$12351/.__cd28ac27-6a06-422f-a674-2acfbdb072d1140406632785664_1551359040166; errno=22(Invalid argument); terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr1$11981$12351 to disk; TileDB error message :",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:5048,Availability,error,error,5048,"NFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing error; path=/mnt/fargen/experiments/joint_call/data/genomicsdb/run1/chr1$11981$12351/.__cd28ac27-6a06-422f-a674-2acfbdb072d1140406632785664_1551359040166; errno=22(Invalid argument); terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr1$11981$12351 to disk; TileDB error message :",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:360,Deployability,Release,Release,360,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:644,Deployability,configurat,configuration,644,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:952,Deployability,install,install,952,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:1055,Deployability,install,install,1055,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:1459,Deployability,install,install,1459,"ABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12; 12:52:37.522 INFO GenomicsDBImport - Start Date/Time: 28 February 2019 12:52:35 WET; 12:52:37.522 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.522 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.523 INFO GenomicsDBImport - HTSJDK Ve",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:867,Integrability,wrap,wrapper,867,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:5054,Integrability,message,message,5054,"NFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing error; path=/mnt/fargen/experiments/joint_call/data/genomicsdb/run1/chr1$11981$12351/.__cd28ac27-6a06-422f-a674-2acfbdb072d1140406632785664_1551359040166; errno=22(Invalid argument); terminate called after throwing an instance of 'VariantStorageManagerException'; what(): VariantStorageManagerException exception : Error while syncing array chr1$11981$12351 to disk; TileDB error message :",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:644,Modifiability,config,configuration,644,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:1345,Performance,Load,Loading,1345,"ntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12; 12:52:37.522 INFO GenomicsDBImport - Start Date/Time: 28 February 2019 12:52:35 WET; 12:52:37.522 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.522 INFO GenomicsDBImport - ------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:3580,Performance,perform,performance,3580,"efaults.COMPRESSION_LEVEL : 2; 12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:52:37.524 INFO GenomicsDBImport - Deflater: IntelDeflater; 12:52:37.524 INFO GenomicsDBImport - Inflater: IntelInflater; 12:52:37.524 INFO GenomicsDBImport - GCS max retries/reopens: 20; 12:52:37.524 INFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:3652,Performance,perform,performance,3652,"12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 12:52:37.524 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 12:52:37.524 INFO GenomicsDBImport - Deflater: IntelDeflater; 12:52:37.524 INFO GenomicsDBImport - Inflater: IntelInflater; 12:52:37.524 INFO GenomicsDBImport - GCS max retries/reopens: 20; 12:52:37.524 INFO GenomicsDBImport - Requester pays: disabled; 12:52:37.524 INFO GenomicsDBImport - Initializing engine; 12:52:38.096 INFO FeatureManager - Using codec BEDCodec to read file file:///mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:43.641 INFO IntervalArgumentCollection - Processing 134492644 bp from intervals; 12:52:43.720 WARN GenomicsDBImport - A large number of intervals were specified. Using more than 100 intervals in a single import is not recommended and can cause performance to suffer. If GVCF data only exists within those intervals, performance can be improved by aggregating intervals with the merge-input-intervals argument.; 12:52:43.722 INFO GenomicsDBImport - Done initializing engine; 12:52:44.113 INFO GenomicsDBImport - Vid Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vidmap.json; 12:52:44.113 INFO GenomicsDBImport - Callset Map JSON file will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/callset.json; 12:52:44.114 INFO GenomicsDBImport - Complete VCF Header will be written to /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/vcfheader.vcf; 12:52:44.114 INFO GenomicsDBImport - Importing to array - /mnt/fargen/experiments/joint_call/data/genomicsdb/run1/genomicsdb_array; 12:52:44.114 INFO ProgressMeter - Starting traversal; 12:52:44.115 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 13:03:44.100 INFO GenomicsDBImport - Importing batch 1 with 2 samples; [TileDB::FileSystem] Error: (sync_path) Cannot sync file; File syncing ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/issues/5740:851,Testability,Log,Log,851,"As discussed in #5608 with @nalinigans. . ## Software version. GATK v4.1.0.0-32-g213f99c-SNAPSHOT. ## OS/Platform. ```; $ uname -a; Linux hnpv-fargenCompute01 4.4.0-101-generic #124-Ubuntu SMP Fri Nov 10 18:29:59 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux. $ lsb_release -a; No LSB modules are available.; Distributor ID: Ubuntu; Description: Ubuntu 16.04.3 LTS; Release: 16.04; Codename: xenial; ```. ## Command . ```; TILEDB_DISABLE_FILE_LOCKING=1. gatk --java-options ""-Xmx4g -Xms4g"" GenomicsDBImport \; -V [GVCF file] \; -V [GVCF file] \; --genomicsdb-workspace-path data/genomicsdb/run1 \; --tmp-dir=tmp \; -L [target BED file]; ```. ## CIFS configuration. /etc/fstab:; ```; /[servername]/[mountame] /mnt/[mountname] cifs credentials=/root/.smbcredentials,iocharset=utf8,uid=1004,gid=1005,file_mode=0770,dir_mode=0770,noperm,mfsymlinks 0 0; ```. ## Log. Using GATK wrapper script /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk; Running:; /mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/bin/gatk GenomicsDBImport -V data/gvcf/FN000009.g.vcf.gz -V old_data/FN000010.g.vcf.gz --genomicsdb-workspace-path data/genomicsdb/run1 --tmp-dir=tmp -L /mnt/fargen/resources/sureselect_human_all_exon_v6_utr_grch38/S07604624_Padded.bed; 12:52:35.654 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/mnt/fargen/experiments/joint_call/gatk_27-02-2019_213f99c/gatk/build/install/gatk/lib/gkl-0.8.6.jar!/com/intel/gkl/native/libgkl_compression.so; 12:52:37.520 INFO GenomicsDBImport - ------------------------------------------------------------; 12:52:37.521 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.1.0.0-32-g213f99c-SNAPSHOT; 12:52:37.521 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 12:52:37.521 INFO GenomicsDBImport - Executing as olavur@hnpv-fargenCompute01.heilsunet.fo on Linux v4.4.0-101-generic amd64; 12:52:37.521 INFO Geno",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5740
https://github.com/broadinstitute/gatk/pull/5741:209,Deployability,pipeline,pipeline,209,"Rather than start off the exome genotyping task with a step to merge the query intervals for better GDB performance, make it an option as for Import in #5540 . This is necessary to get the exome joint calling pipeline onto official GATK release dockers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5741
https://github.com/broadinstitute/gatk/pull/5741:237,Deployability,release,release,237,"Rather than start off the exome genotyping task with a step to merge the query intervals for better GDB performance, make it an option as for Import in #5540 . This is necessary to get the exome joint calling pipeline onto official GATK release dockers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5741
https://github.com/broadinstitute/gatk/pull/5741:104,Performance,perform,performance,104,"Rather than start off the exome genotyping task with a step to merge the query intervals for better GDB performance, make it an option as for Import in #5540 . This is necessary to get the exome joint calling pipeline onto official GATK release dockers.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5741
https://github.com/broadinstitute/gatk/issues/5744:464,Availability,failure,failure,464,"@LeeTL1220 mentioned having difficulty running gCNV on REBC WGS normals. As far as I can tell, other than runs failing due to quota or FC issues, there were some runs that failed because the WDL version and the Docker version were not in sync. Specifically, these runs failed because the WDL was more recent than the Docker, causing the ScatterIntervals task (which was running an out-of-date IntervalListTools in the latter) to ""fail silently"". The behavior upon failure, for reasons due to the somewhat awkward format of the IntervalListTools output, is to output just a single shard by simply copying the original intervals list; this is handled in bash. This single-shard run then failed due to OOM in the gCNV step. Just to be clear, everything typically works fine when the versions are in sync. But you could imagine that even then IntervalListTools could fail for other reasons, in which case we'd probably fail misleadingly at the gCNV step again. So let's modify the WDL so we fail at the appropriate place. Apologies to @asmirnov239, who I think pointed this weirdness out in the original PR review.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5744
https://github.com/broadinstitute/gatk/issues/5744:589,Usability,simpl,simply,589,"@LeeTL1220 mentioned having difficulty running gCNV on REBC WGS normals. As far as I can tell, other than runs failing due to quota or FC issues, there were some runs that failed because the WDL version and the Docker version were not in sync. Specifically, these runs failed because the WDL was more recent than the Docker, causing the ScatterIntervals task (which was running an out-of-date IntervalListTools in the latter) to ""fail silently"". The behavior upon failure, for reasons due to the somewhat awkward format of the IntervalListTools output, is to output just a single shard by simply copying the original intervals list; this is handled in bash. This single-shard run then failed due to OOM in the gCNV step. Just to be clear, everything typically works fine when the versions are in sync. But you could imagine that even then IntervalListTools could fail for other reasons, in which case we'd probably fail misleadingly at the gCNV step again. So let's modify the WDL so we fail at the appropriate place. Apologies to @asmirnov239, who I think pointed this weirdness out in the original PR review.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5744
https://github.com/broadinstitute/gatk/issues/5744:732,Usability,clear,clear,732,"@LeeTL1220 mentioned having difficulty running gCNV on REBC WGS normals. As far as I can tell, other than runs failing due to quota or FC issues, there were some runs that failed because the WDL version and the Docker version were not in sync. Specifically, these runs failed because the WDL was more recent than the Docker, causing the ScatterIntervals task (which was running an out-of-date IntervalListTools in the latter) to ""fail silently"". The behavior upon failure, for reasons due to the somewhat awkward format of the IntervalListTools output, is to output just a single shard by simply copying the original intervals list; this is handled in bash. This single-shard run then failed due to OOM in the gCNV step. Just to be clear, everything typically works fine when the versions are in sync. But you could imagine that even then IntervalListTools could fail for other reasons, in which case we'd probably fail misleadingly at the gCNV step again. So let's modify the WDL so we fail at the appropriate place. Apologies to @asmirnov239, who I think pointed this weirdness out in the original PR review.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5744
https://github.com/broadinstitute/gatk/issues/5747:76,Deployability,update,updated,76,"`TableReader` and `TableWriter` only work on `java.io.File`, and need to be updated to accept `java.nio.Path` so we can Path-enable the tools that have code paths that depend on this package, like FilterByOrientationBias.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5747
https://github.com/broadinstitute/gatk/issues/5747:168,Integrability,depend,depend,168,"`TableReader` and `TableWriter` only work on `java.io.File`, and need to be updated to accept `java.nio.Path` so we can Path-enable the tools that have code paths that depend on this package, like FilterByOrientationBias.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5747
https://github.com/broadinstitute/gatk/issues/5749:66,Testability,test,testing,66,"Contains mostly very simple utility methods, but could stand some testing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5749
https://github.com/broadinstitute/gatk/issues/5749:21,Usability,simpl,simple,21,"Contains mostly very simple utility methods, but could stand some testing.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5749
https://github.com/broadinstitute/gatk/issues/5754:430,Availability,down,downstream,430,"This is set to an unexposed 250 samples. Since the sampling and posterior estimation is done online to prevent the entire samples x intervals x dCR samples matrix from causing OOM, I think we suffer from lack of vectorization. However, the good news is that we can probably get by with far fewer samples, say ~20) if all we want are estimates of posterior mean and standard deviation---not even sure if the latter will be used by downstream analyses anytime soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5754
https://github.com/broadinstitute/gatk/pull/5755:6,Deployability,update,update,6,Quick update to docs to include an FAQ section with the first common question populated. Already pushed to forum.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5755
https://github.com/broadinstitute/gatk/pull/5758:298,Modifiability,inherit,inherit,298,Closes #5352. This removes unused command line arguments from Mutect2. @LeeTL1220 This PR looks big but it's really just a lot of moving around `static` methods and giving `HaplotypeCallerArgumentCollection` a `StandardCallerCollection` as a member instead of having`AssemblyBasedCallerCollection` inherit from it.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5758
https://github.com/broadinstitute/gatk/pull/5761:148,Testability,Test,Tests,148,This is a first draft/iteration on my CEMBA Type Caller in order to jump start the review process. Here are some of my own suggestions to add:. Add Tests with a sample methylated bam as input and a corresponding vcf as expected exact match output; Add constants for VCF header names; Add constants for VCF header descriptions ; Change program group,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5761
https://github.com/broadinstitute/gatk/pull/5762:148,Testability,Test,Tests,148,This is a first draft/iteration on my CEMBA Type Caller in order to jump start the review process. Here are some of my own suggestions to add:. Add Tests with a sample methylated bam as input and a corresponding vcf as expected exact match output; Add constants for VCF header names; Add constants for VCF header descriptions; Change program group,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5762
https://github.com/broadinstitute/gatk/issues/5763:54,Deployability,integrat,integration,54,This should be done after #5688 does in. Some Mutect2 integration tests should be deleted once this is done.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5763
https://github.com/broadinstitute/gatk/issues/5763:54,Integrability,integrat,integration,54,This should be done after #5688 does in. Some Mutect2 integration tests should be deleted once this is done.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5763
https://github.com/broadinstitute/gatk/issues/5763:66,Testability,test,tests,66,This should be done after #5688 does in. Some Mutect2 integration tests should be deleted once this is done.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5763
https://github.com/broadinstitute/gatk/issues/5764:156,Deployability,Update,Update,156,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764
https://github.com/broadinstitute/gatk/issues/5764:599,Deployability,install,install,599,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764
https://github.com/broadinstitute/gatk/issues/5764:634,Deployability,install,install,634,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764
https://github.com/broadinstitute/gatk/issues/5764:739,Integrability,depend,dependencies,739,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764
https://github.com/broadinstitute/gatk/issues/5764:984,Integrability,depend,dependencies---if,984,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764
https://github.com/broadinstitute/gatk/issues/5764:334,Modifiability,Config,Configure,334,"There appears to be a memory leak in gCNV coming from Theano 0.9.0, possibly fixed in https://github.com/Theano/Theano/pull/5832. A few possible fixes:. 1) Update Theano to the latest 1.0.4 version. I've tried this and it looks like the leak goes away. Need to confirm reproducibility of results between versions, see also #5730.; 2) Configure Theano 0.9.0 to use MKL, rather than OpenBLAS. It appears the leak is only an issue with the latter. This is a little more complicated, since I now realize that MKL is not actually fully utilized (if at all) in our conda environment. For example, we `pip install numpy`, rather than `conda install` a version from the `default` channel that is compiled against MKL. So we'd need to change a few dependencies in the environment which might have implications for VQSR-CNN. See also #4074. @lucidtronix any thoughts? @jamesemery and @cmnbroad might also be interested, as this could have pretty drastic implications for the size of the python dependencies---if we go with option 1, we might be able to get rid of MKL, etc. Not sure if the memory leak manifests the same across all architectures. Note that I believe this is a separate issue from #5714.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5764
https://github.com/broadinstitute/gatk/issues/5765:250,Usability,Simpl,SimpleInterval,250,"## Bug Report. ### Affected tool(s) or class(es); org.broadinstitute.hellbender.utils.activityprofile.BandPassActivityProfile. ### Affected version(s); Latest master branch as of March 6, 2019. ### Description ; Line 147 should read:; ```Java; final SimpleInterval loc = getLocForOffset(superState.getLoc(), i); ```; instead of; ```Java; final SimpleInterval loc = getLocForOffset(justAddedState.getLoc(), i); ```. Likewise, line 154 should read:; ```Java; states.add(superState); ```; instead of; ```Java; states.add(justAddedState); ```. Otherwise, the location changes in org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile, line 241 doesn't make a lot of sense, and neither does the extension of `getMaxProbPropagationDistance()` in org.broadinstitute.hellbender.utils.activityprofile.BandPassActivityProfile, lines 100-102.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5765
https://github.com/broadinstitute/gatk/issues/5765:344,Usability,Simpl,SimpleInterval,344,"## Bug Report. ### Affected tool(s) or class(es); org.broadinstitute.hellbender.utils.activityprofile.BandPassActivityProfile. ### Affected version(s); Latest master branch as of March 6, 2019. ### Description ; Line 147 should read:; ```Java; final SimpleInterval loc = getLocForOffset(superState.getLoc(), i); ```; instead of; ```Java; final SimpleInterval loc = getLocForOffset(justAddedState.getLoc(), i); ```. Likewise, line 154 should read:; ```Java; states.add(superState); ```; instead of; ```Java; states.add(justAddedState); ```. Otherwise, the location changes in org.broadinstitute.hellbender.utils.activityprofile.ActivityProfile, line 241 doesn't make a lot of sense, and neither does the extension of `getMaxProbPropagationDistance()` in org.broadinstitute.hellbender.utils.activityprofile.BandPassActivityProfile, lines 100-102.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5765
https://github.com/broadinstitute/gatk/issues/5766:459,Availability,error,error,459,"----. ## Bug Report. ### Affected tool(s) or class(es); - GATK; - gcnvkernel ; - theano. ### Affected version(s); - GATK 4.1.0.0; - gcnvkernel 0.0.7; - theano 0.9.0; - GCC 7.3.0. ### Description ; I have installed the python package theano(which is a requirement of gcnvkernel) with python 3.6.6 which is compiled with gcc 7.3.0. I am not using the conda environment to install these packages.; Then i tried to run theano-nose, but is giving me the following error:. ```sh. $ theano-nose; --; ; You can find the C code in this temporary file: /tmp/theano_compilation_error_gp0ar1kx; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2942,Availability,error,error,2942," = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3110,Availability,error,error,3110,"K}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see descrip",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3341,Availability,error,error,3341,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3453,Availability,error,error,3453,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3492,Availability,error,error,3492,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3636,Availability,error,errors,3636,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:204,Deployability,install,installed,204,"----. ## Bug Report. ### Affected tool(s) or class(es); - GATK; - gcnvkernel ; - theano. ### Affected version(s); - GATK 4.1.0.0; - gcnvkernel 0.0.7; - theano 0.9.0; - GCC 7.3.0. ### Description ; I have installed the python package theano(which is a requirement of gcnvkernel) with python 3.6.6 which is compiled with gcc 7.3.0. I am not using the conda environment to install these packages.; Then i tried to run theano-nose, but is giving me the following error:. ```sh. $ theano-nose; --; ; You can find the C code in this temporary file: /tmp/theano_compilation_error_gp0ar1kx; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:370,Deployability,install,install,370,"----. ## Bug Report. ### Affected tool(s) or class(es); - GATK; - gcnvkernel ; - theano. ### Affected version(s); - GATK 4.1.0.0; - gcnvkernel 0.0.7; - theano 0.9.0; - GCC 7.3.0. ### Description ; I have installed the python package theano(which is a requirement of gcnvkernel) with python 3.6.6 which is compiled with gcc 7.3.0. I am not using the conda environment to install these packages.; Then i tried to run theano-nose, but is giving me the following error:. ```sh. $ theano-nose; --; ; You can find the C code in this temporary file: /tmp/theano_compilation_error_gp0ar1kx; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:855,Deployability,INSTALL,INSTALLDIRGATK,855,"----. ## Bug Report. ### Affected tool(s) or class(es); - GATK; - gcnvkernel ; - theano. ### Affected version(s); - GATK 4.1.0.0; - gcnvkernel 0.0.7; - theano 0.9.0; - GCC 7.3.0. ### Description ; I have installed the python package theano(which is a requirement of gcnvkernel) with python 3.6.6 which is compiled with gcc 7.3.0. I am not using the conda environment to install these packages.; Then i tried to run theano-nose, but is giving me the following error:. ```sh. $ theano-nose; --; ; You can find the C code in this temporary file: /tmp/theano_compilation_error_gp0ar1kx; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:1291,Deployability,INSTALL,INSTALLDIRGATK,1291," the python package theano(which is a requirement of gcnvkernel) with python 3.6.6 which is compiled with gcc 7.3.0. I am not using the conda environment to install these packages.; Then i tried to run theano-nose, but is giving me the following error:. ```sh. $ theano-nose; --; ; You can find the C code in this temporary file: /tmp/theano_compilation_error_gp0ar1kx; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o: is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compil",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:1728,Deployability,INSTALL,INSTALLDIRGATK,1728,"ound.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x1a): is not found.; library inux-gnu/7.3.0/crtbeginS.o(.text+0x6b): is not found.; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:1863,Deployability,INSTALL,INSTALLDIRGATK,1863,"""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 81, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:1981,Deployability,INSTALL,INSTALLDIRGATK,1981,"force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2108,Deployability,INSTALL,INSTALLDIRGATK,2108,"211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTabl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2234,Deployability,INSTALL,INSTALLDIRGATK,2234,"ccurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2372,Deployability,INSTALL,INSTALLDIRGATK,2372,"in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2493,Deployability,INSTALL,INSTALLDIRGATK,2493,". Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2615,Deployability,INSTALL,INSTALLDIRGATK,2615,"uring handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, an",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2733,Deployability,INSTALL,INSTALLDIRGATK,2733,"le ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . -------------------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2951,Deployability,INSTALL,INSTALLDIRGCC,2951," = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3119,Deployability,INSTALL,INSTALLDIRGCC,3119,"K}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see descrip",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3260,Deployability,INSTALL,INSTALLDIRGCC,3260,"-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functio",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3372,Deployability,INSTALL,INSTALLDIRGCC,3372,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3543,Deployability,install,installed,3543,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3925,Integrability,message,message,3925,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:2092,Modifiability,config,config,2092,"211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 105, in <module>; actual_version, force_compile, _need_reload)); ImportError: Version check of the existing lazylinker compiled file. Looking for version 0.211, but found None. Extra debug information: force_compile=False, _need_reload=True; ; During handling of the above exception, another exception occurred:; ; Traceback (most recent call last):; File ""${INSTALLDIRGATK}/bin/theano-nose"", line 11, in <module>; load_entry_point('Theano==1.0.4', 'console_scripts', 'theano-nose')(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 207, in main; result = main_function(); File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/bin/theano_nose.py"", line 45, in main_function; from theano import config; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/__init__.py"", line 110, in <module>; from theano.compile import (; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/__init__.py"", line 12, in <module>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTabl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3745,Testability,test,tests,3745,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/issues/5766:3882,Testability,test,tested,3882,"odule>; from theano.compile.mode import *; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/compile/mode.py"", line 11, in <module>; import theano.gof.vm; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/vm.py"", line 674, in <module>; from . import lazylinker_c; File ""${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/lazylinker_c.py"", line 140, in <module>; preargs=args); File ${INSTALLDIRGATK}/lib/python3.6/site-packages/theano/gof/cmodule.py"", line 2396, in compile_str; (status, compile_stderr.replace('\n', '. '))); Exception: Compilation failed (return status=1): /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_deregisterTMCloneTable. /usr/bin/ld.gold: error: ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o: unsupported reloc 42 against global symbol _ITM_registerTMCloneTable. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x1a): error: unsupported reloc 42. ${INSTALLDIRGCC}/bin/../lib/gcc/x86_64-pc-linux-gnu/7.3.0/crtbeginS.o(.text+0x6b): error: unsupported reloc 42. collect2: error: ld returned 1 exit status. ```. Then I have installed theano with python 3.6.6 which is compiled with gcc 5.4.0, and it was giving me no errors. ```sh. $ theano-nose . ----------------------------------------------------------------------; Ran 0 tests in 0.012s. OK; ```. The Theano toolchain issue might be caused by theano not being actively developed anymore. Probably they never tested it with newer toolchains.; See this message that is also on the Theano github page.; https://groups.google.com/d/msg/theano-users/7Poq8BZutbY/rNCIfvAEAwAJ. #### Steps to reproduce; see description. #### Expected behavior; see description. #### Actual behavior; see description. ----. ## Feature request; - Switch from pymc3/Theano to another framework that offers the same functionality; - Modify the depedencies of gcnvkernel. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5766
https://github.com/broadinstitute/gatk/pull/5769:443,Deployability,Update,Update,443,"- Running in mitochonrial mode; - Link to liquid biopsy blogpost will need to be added once it is posted; - Specifying tumor name is no longer necessary; - Af-of-alleles-not-in-resource is dynamically adjusted for modes; - Joint calling on multiple tumor and normal samples; - Mention and/or link to FilterMutectCalls, Funcotator and CreateSomaticPanelOfNormals; - Remove caveat and state instead M2's ability to handle extreme high depths; - Update HaplotypeCaller link from v3 to current; - Add 'Notes' section and make ordered list; - Move example AF resource snippet to 'Notes' section",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5769
https://github.com/broadinstitute/gatk/issues/5771:318,Availability,error,error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation,318,"See some issues---mostly stemming from the HDF5 library and the BLAS library optionally used by MLlib SVD at e.g. https://gatkforums.broadinstitute.org/gatk/discussion/23591/createreadcountpanelofnormals-in-gatk4-1-doesnt-output-valid-hdf5-files#latest; https://gatkforums.broadinstitute.org/gatk/discussion/12537/get-error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation; https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv/p1. Would also be nice to to turn down the verbosity of Spark logging, which emits a ridiculous amount of messages for a simple SVD. I think this is a relatively ancient issue (https://github.com/broadinstitute/gatk/issues/1370), not sure if it's been resolved for other Spark tools since.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5771
https://github.com/broadinstitute/gatk/issues/5771:600,Availability,down,down,600,"See some issues---mostly stemming from the HDF5 library and the BLAS library optionally used by MLlib SVD at e.g. https://gatkforums.broadinstitute.org/gatk/discussion/23591/createreadcountpanelofnormals-in-gatk4-1-doesnt-output-valid-hdf5-files#latest; https://gatkforums.broadinstitute.org/gatk/discussion/12537/get-error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation; https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv/p1. Would also be nice to to turn down the verbosity of Spark logging, which emits a ridiculous amount of messages for a simple SVD. I think this is a relatively ancient issue (https://github.com/broadinstitute/gatk/issues/1370), not sure if it's been resolved for other Spark tools since.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5771
https://github.com/broadinstitute/gatk/issues/5771:672,Integrability,message,messages,672,"See some issues---mostly stemming from the HDF5 library and the BLAS library optionally used by MLlib SVD at e.g. https://gatkforums.broadinstitute.org/gatk/discussion/23591/createreadcountpanelofnormals-in-gatk4-1-doesnt-output-valid-hdf5-files#latest; https://gatkforums.broadinstitute.org/gatk/discussion/12537/get-error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation; https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv/p1. Would also be nice to to turn down the verbosity of Spark logging, which emits a ridiculous amount of messages for a simple SVD. I think this is a relatively ancient issue (https://github.com/broadinstitute/gatk/issues/1370), not sure if it's been resolved for other Spark tools since.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5771
https://github.com/broadinstitute/gatk/issues/5771:628,Testability,log,logging,628,"See some issues---mostly stemming from the HDF5 library and the BLAS library optionally used by MLlib SVD at e.g. https://gatkforums.broadinstitute.org/gatk/discussion/23591/createreadcountpanelofnormals-in-gatk4-1-doesnt-output-valid-hdf5-files#latest; https://gatkforums.broadinstitute.org/gatk/discussion/12537/get-error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation; https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv/p1. Would also be nice to to turn down the verbosity of Spark logging, which emits a ridiculous amount of messages for a simple SVD. I think this is a relatively ancient issue (https://github.com/broadinstitute/gatk/issues/1370), not sure if it's been resolved for other Spark tools since.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5771
https://github.com/broadinstitute/gatk/issues/5771:687,Usability,simpl,simple,687,"See some issues---mostly stemming from the HDF5 library and the BLAS library optionally used by MLlib SVD at e.g. https://gatkforums.broadinstitute.org/gatk/discussion/23591/createreadcountpanelofnormals-in-gatk4-1-doesnt-output-valid-hdf5-files#latest; https://gatkforums.broadinstitute.org/gatk/discussion/12537/get-error-when-using-createreadcountpanelofnormals-in-calling-somatic-copy-number-variation; https://gatkforums.broadinstitute.org/gatk/discussion/11461/gatk-4-0-1-2-no-non-zero-singular-values-were-found-in-creating-a-panel-of-normals-for-somatic-cnv/p1. Would also be nice to to turn down the verbosity of Spark logging, which emits a ridiculous amount of messages for a simple SVD. I think this is a relatively ancient issue (https://github.com/broadinstitute/gatk/issues/1370), not sure if it's been resolved for other Spark tools since.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5771
https://github.com/broadinstitute/gatk/pull/5772:87,Usability,Simpl,Simply,87,"Slightly speculative PR to try and address (or rather, create a workaround for) #5727. Simply removes the restriction that physical phasing cannot be used in non-ERC mode.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5772
https://github.com/broadinstitute/gatk/issues/5773:124,Testability,log,logic,124,Currently Funcotator has an issue where it will only annotate with Locatable Funcotation Factories. This is because of some logic in `DataSourceFuncotationFactory` around whether the `featureList` is compatible.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5773
https://github.com/broadinstitute/gatk/pull/5774:20,Deployability,integrat,integration,20,Primarily the large integration tests check for the non-locatable; funcotation factories producing data. Regenerated the expected output from large tests. Fixes #5773,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5774
https://github.com/broadinstitute/gatk/pull/5774:20,Integrability,integrat,integration,20,Primarily the large integration tests check for the non-locatable; funcotation factories producing data. Regenerated the expected output from large tests. Fixes #5773,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5774
https://github.com/broadinstitute/gatk/pull/5774:32,Testability,test,tests,32,Primarily the large integration tests check for the non-locatable; funcotation factories producing data. Regenerated the expected output from large tests. Fixes #5773,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5774
https://github.com/broadinstitute/gatk/pull/5774:148,Testability,test,tests,148,Primarily the large integration tests check for the non-locatable; funcotation factories producing data. Regenerated the expected output from large tests. Fixes #5773,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5774
https://github.com/broadinstitute/gatk/issues/5776:57,Deployability,update,update,57,"@cmnbroad correct me if I'm wrong, but we currently just update the conda env if it is already present. This will add new packages that were added to the yml but will not remove old packages that were removed from the yml. Note that the update --prune option seems like it should take care of this, but see https://github.com/conda/conda/issues/7279.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5776
https://github.com/broadinstitute/gatk/issues/5776:237,Deployability,update,update,237,"@cmnbroad correct me if I'm wrong, but we currently just update the conda env if it is already present. This will add new packages that were added to the yml but will not remove old packages that were removed from the yml. Note that the update --prune option seems like it should take care of this, but see https://github.com/conda/conda/issues/7279.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5776
https://github.com/broadinstitute/gatk/issues/5777:236,Availability,error,error,236,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); GATK 4.1.0.0. ### Description ; Funcotator does not perform any annotation on a minimal VCF with canonical cancer variants and returns the following error:. ```; 23:28:30.519 INFO Funcotator - Initializing Funcotator Engine...; 23:28:30.523 INFO Funcotator - Creating a VCF file for output: file:xxx/sandbox/idh.funcotated.vcf; 23:28:30.541 INFO ProgressMeter - Starting traversal; 23:28:30.541 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:28:30.652 INFO ProgressMeter - unmapped 0.0 15 8108.1; 23:28:30.652 INFO ProgressMeter - Traversal complete. Processed 15 total variants in 0.0 minutes.; 23:28:30.652 WARN Funcotator - ================================================================================; 23:28:30.652 WARN Funcotator - _ _ _ __ __ _ _ _ _; 23:28:30.652 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || |; 23:28:30.652 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || |; 23:28:30.653 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_|; 23:28:30.653 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_); 23:28:30.653 WARN Funcotator - |___/; 23:28:30.653 WARN Funcotator - --------------------------------------------------------------------------------; 23:28:30.653 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this; 23:28:30.653 WARN Funcotator - run was misconfigured.; 23:28:30.653 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 23:28:30.653 WARN Funcotator - ================================================================================; ```. There is no reason to assume that there is any issue with the data sources or run parameters. They have worked fine using a different VCF that had completed INFO tags. #### Steps to reproduce; Run Funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5777
https://github.com/broadinstitute/gatk/issues/5777:387,Modifiability,sandbox,sandbox,387,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); GATK 4.1.0.0. ### Description ; Funcotator does not perform any annotation on a minimal VCF with canonical cancer variants and returns the following error:. ```; 23:28:30.519 INFO Funcotator - Initializing Funcotator Engine...; 23:28:30.523 INFO Funcotator - Creating a VCF file for output: file:xxx/sandbox/idh.funcotated.vcf; 23:28:30.541 INFO ProgressMeter - Starting traversal; 23:28:30.541 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:28:30.652 INFO ProgressMeter - unmapped 0.0 15 8108.1; 23:28:30.652 INFO ProgressMeter - Traversal complete. Processed 15 total variants in 0.0 minutes.; 23:28:30.652 WARN Funcotator - ================================================================================; 23:28:30.652 WARN Funcotator - _ _ _ __ __ _ _ _ _; 23:28:30.652 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || |; 23:28:30.652 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || |; 23:28:30.653 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_|; 23:28:30.653 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_); 23:28:30.653 WARN Funcotator - |___/; 23:28:30.653 WARN Funcotator - --------------------------------------------------------------------------------; 23:28:30.653 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this; 23:28:30.653 WARN Funcotator - run was misconfigured.; 23:28:30.653 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 23:28:30.653 WARN Funcotator - ================================================================================; ```. There is no reason to assume that there is any issue with the data sources or run parameters. They have worked fine using a different VCF that had completed INFO tags. #### Steps to reproduce; Run Funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5777
https://github.com/broadinstitute/gatk/issues/5777:139,Performance,perform,perform,139,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); GATK 4.1.0.0. ### Description ; Funcotator does not perform any annotation on a minimal VCF with canonical cancer variants and returns the following error:. ```; 23:28:30.519 INFO Funcotator - Initializing Funcotator Engine...; 23:28:30.523 INFO Funcotator - Creating a VCF file for output: file:xxx/sandbox/idh.funcotated.vcf; 23:28:30.541 INFO ProgressMeter - Starting traversal; 23:28:30.541 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:28:30.652 INFO ProgressMeter - unmapped 0.0 15 8108.1; 23:28:30.652 INFO ProgressMeter - Traversal complete. Processed 15 total variants in 0.0 minutes.; 23:28:30.652 WARN Funcotator - ================================================================================; 23:28:30.652 WARN Funcotator - _ _ _ __ __ _ _ _ _; 23:28:30.652 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || |; 23:28:30.652 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || |; 23:28:30.653 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_|; 23:28:30.653 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_); 23:28:30.653 WARN Funcotator - |___/; 23:28:30.653 WARN Funcotator - --------------------------------------------------------------------------------; 23:28:30.653 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this; 23:28:30.653 WARN Funcotator - run was misconfigured.; 23:28:30.653 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 23:28:30.653 WARN Funcotator - ================================================================================; ```. There is no reason to assume that there is any issue with the data sources or run parameters. They have worked fine using a different VCF that had completed INFO tags. #### Steps to reproduce; Run Funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5777
https://github.com/broadinstitute/gatk/issues/5777:387,Testability,sandbox,sandbox,387,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); GATK 4.1.0.0. ### Description ; Funcotator does not perform any annotation on a minimal VCF with canonical cancer variants and returns the following error:. ```; 23:28:30.519 INFO Funcotator - Initializing Funcotator Engine...; 23:28:30.523 INFO Funcotator - Creating a VCF file for output: file:xxx/sandbox/idh.funcotated.vcf; 23:28:30.541 INFO ProgressMeter - Starting traversal; 23:28:30.541 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 23:28:30.652 INFO ProgressMeter - unmapped 0.0 15 8108.1; 23:28:30.652 INFO ProgressMeter - Traversal complete. Processed 15 total variants in 0.0 minutes.; 23:28:30.652 WARN Funcotator - ================================================================================; 23:28:30.652 WARN Funcotator - _ _ _ __ __ _ _ _ _; 23:28:30.652 WARN Funcotator - | || || | \ \ / /_ _ _ __ _ __ (_)_ __ __ _ | || || |; 23:28:30.652 WARN Funcotator - | || || | \ \ /\ / / _` | '__| '_ \| | '_ \ / _` | | || || |; 23:28:30.653 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_|; 23:28:30.653 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_); 23:28:30.653 WARN Funcotator - |___/; 23:28:30.653 WARN Funcotator - --------------------------------------------------------------------------------; 23:28:30.653 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this; 23:28:30.653 WARN Funcotator - run was misconfigured.; 23:28:30.653 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 23:28:30.653 WARN Funcotator - ================================================================================; ```. There is no reason to assume that there is any issue with the data sources or run parameters. They have worked fine using a different VCF that had completed INFO tags. #### Steps to reproduce; Run Funcotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5777
https://github.com/broadinstitute/gatk/issues/5777:2064,Testability,test,test,2064,"| || |; 23:28:30.653 WARN Funcotator - |_||_||_| \ \V V / (_| | | | | | | | | | | (_| | |_||_||_|; 23:28:30.653 WARN Funcotator - (_)(_)(_) \_/\_/ \__,_|_| |_| |_|_|_| |_|\__, | (_)(_)(_); 23:28:30.653 WARN Funcotator - |___/; 23:28:30.653 WARN Funcotator - --------------------------------------------------------------------------------; 23:28:30.653 WARN Funcotator - Only IGRs were produced for this dataset. This STRONGLY indicates that this; 23:28:30.653 WARN Funcotator - run was misconfigured.; 23:28:30.653 WARN Funcotator - You MUST check your data sources to make sure they are correct for these data.; 23:28:30.653 WARN Funcotator - ================================================================================; ```. There is no reason to assume that there is any issue with the data sources or run parameters. They have worked fine using a different VCF that had completed INFO tags. #### Steps to reproduce; Run Funcotator with the following command:. ```; gatk Funcotator --variant ./test.vcf --reference human_g1k_v37_decoy.fasta --ref-versio; n hg19 --data-sources-path funcotator_dataSources.v1.6.20190124s --output idh.funcotated.vcf --output-file-format VCF --verbosity DEBUG; ```. on the following minimal VCF with canonical cancer variants:. ```; ##fileformat=VCFv4.1; ##contig=<ID=2,length=243199373,assembly=b37>; ##contig=<ID=5,length=180915260,assembly=b37>; ##contig=<ID=15,length=102531392,assembly=b37>; #CHROM POS ID REF ALT QUAL FILTER INFO; 2 209113112 . C A . . .; 2 209113112 . C T . . .; 2 209113113 . G C . . .; 2 209113113 . G A . . .; 2 209113113 . G T . . .; 5 1295169 . G A . . .; 5 1295228 . G A . . .; 5 1295242 . GG AA . . .; 5 1295250 . G A . . .; 15 90631837 . C G . . .; 15 90631837 . C A . . .; 15 90631838 . C T . . .; 15 90631838 . C A . . .; 15 90631839 . T C . . .; 15 90631839 . T A . . .; ```. #### Expected behavior; The input VCF consists of canonical variants only, they should be annotated. #### Actual behavior; No annotations are returned",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5777
https://github.com/broadinstitute/gatk/issues/5780:1147,Availability,down,downsample,1147,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780
https://github.com/broadinstitute/gatk/issues/5780:1377,Availability,down,downsampling,1377,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780
https://github.com/broadinstitute/gatk/issues/5780:556,Energy Efficiency,reduce,reduce,556,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780
https://github.com/broadinstitute/gatk/issues/5780:285,Performance,perform,performance,285,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780
https://github.com/broadinstitute/gatk/issues/5780:1399,Performance,perform,performed,1399,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780
https://github.com/broadinstitute/gatk/issues/5780:932,Safety,avoid,avoid,932,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780
https://github.com/broadinstitute/gatk/issues/5780:1123,Safety,detect,detected,1123,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780
https://github.com/broadinstitute/gatk/issues/5780:438,Usability,simpl,simply,438,"Users (@yfarjoun @jnoms) have been reporting high run times in PathSeq when the samples have a large proportion (on the order of 10%+) of microbial reads. PathSeq was designed to run on samples with low numbers (<1%) microbial reads, but there are two ways users can currently improve performance when that's not the case:. 1) Run the 3 PathSeq tools individually (Filter, Align, and Score) instead of using `PathSeqPipelineSpark`, which simply runs those in series. This will eliminate over-allocation of resources during Filter and Score. This also will reduce the likelihood that Spark will recompute parts of the graph when it is low on memory/disk. ; 2) Enable `--skip-pre-bwa-repartition`, see https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_spark_pathseq_PathSeqPipelineSpark.php#--skip-pre-bwa-repartition; 3) Omit metric file outputs. This may also help Spark to avoid recomputing tasks from earlier parts of the graph. Planned features to help improve this:; 1) Automatically enable `--skip-pre-bwa-repartition` when a large amount of non-host reads is detected.; 2) Option to downsample the input BAM on the fly. This is also useful for estimating the proportion of non-host contamination.; 3) Option to limit the number of non-host reads that are processed. This is essentially equivalent to (2), but the downsampling would be performed after host filtering and could be used when the fraction of non-host reads is unknown a priori.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5780
https://github.com/broadinstitute/gatk/pull/5781:70,Deployability,update,updated,70,-Changed sampling of denoised copy ratios to address memory spike and updated output formats and filenames. Partially addresses #5754.; -Updated theano version to 1.0.4 and changed numpy install source to conda defaults to enable MKL.; -Updated theano flags to use MKL and OpenMP elemwise. Closes #5764.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781
https://github.com/broadinstitute/gatk/pull/5781:137,Deployability,Update,Updated,137,-Changed sampling of denoised copy ratios to address memory spike and updated output formats and filenames. Partially addresses #5754.; -Updated theano version to 1.0.4 and changed numpy install source to conda defaults to enable MKL.; -Updated theano flags to use MKL and OpenMP elemwise. Closes #5764.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781
https://github.com/broadinstitute/gatk/pull/5781:187,Deployability,install,install,187,-Changed sampling of denoised copy ratios to address memory spike and updated output formats and filenames. Partially addresses #5754.; -Updated theano version to 1.0.4 and changed numpy install source to conda defaults to enable MKL.; -Updated theano flags to use MKL and OpenMP elemwise. Closes #5764.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781
https://github.com/broadinstitute/gatk/pull/5781:237,Deployability,Update,Updated,237,-Changed sampling of denoised copy ratios to address memory spike and updated output formats and filenames. Partially addresses #5754.; -Updated theano version to 1.0.4 and changed numpy install source to conda defaults to enable MKL.; -Updated theano flags to use MKL and OpenMP elemwise. Closes #5764.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5781
https://github.com/broadinstitute/gatk/issues/5783:62,Availability,error,error,62,"Please find below the user report of this issue:. Here is the error I got:; [February 25, 2019 8:18:10 PM PST] org.broadinstitute.hellbender.tools.funcotator.Funcotator done. Elapsed time: 11.08 minutes.; Runtime.totalMemory()=5379719168; java.lang.StringIndexOutOfBoundsException: String index out of range: 545; at java.lang.String.substring(String.java:1951); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.initializeForDeletion(ProteinChangeInfo.java:192); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.<init>(ProteinChangeInfo.java:96); at org.broadinstitute.hellbender.tools.funcotator.ProteinChangeInfo.create(ProteinChangeInfo.java:371); [...]. and here is offending record:; `chr12 70747693 . TAAAAAAA T,TAAAA,TAAAAA,TAAAAAA,TAAAAAAAA . artifact_in_normal;germline_risk;multiallelic CONTQ=93;DP=537;ECNT=1;GERMQ=253,113,0,0,18;MBQ=36,24,36,28,36,33;MFRL=293,529,291,288,325,299;MMQ=60,29,60,60,60,60;MPOS=43,43,41,44,26;NALOD=0.912,0.217,-2.040e+00,-1.342e+01,-4.057e+00;NLOD=20.49,13.33,2.58,-1.476e+01,2.13;POPAF=2.27,1.08,2.19,2.53,5.40;REF_BASES=GCAAGCCTTCTAAAAAAAAAA;RPA=25,18,22,23,24,26;RU=A;SAAF=0.394,0.404,0.420;SAPP=0.019,0.015,0.965;STR;TLOD=3.88,7.29,6.62,36.73,3.85 GT:AD:AF:DP:F1R2:F2R1 0/0:21,0,1,7,14,6:0.011,0.025,0.099,0.262,0.124:49:14,0,1,6,7,4:7,0,0,1,7,2 0/1/2/3/4/5:65,3,11,20,47,18:0.016,0.057,0.076,0.245,0.092:164:32,3,6,14,30,8:33,0,5,6,17,10`. As a result the Funcotator output is truncated. Bug?. Thanks!; Ivan. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23631/funcotator-reproducibly-crushes-on-specific-wes-vcf-record-produced-by-gatk4-1-0-java-1-8-0-45/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5783
https://github.com/broadinstitute/gatk/issues/5784:244,Performance,perform,perform,244,"See https://gatkforums.broadinstitute.org/gatk/discussion/23646/determinegermlinecontigploidy-issue-when-using-more-samples#latest. This includes DetermineGermlineContigPloidy, GermlineCNVCaller, and CreateReadCountPanelOfNormals. We currently perform a check that no duplicate count files are input, and I believe the python tools perform a check for duplicate sample names (although I'm not sure about gCNV). We could fail early on duplicate sample names on the Java side; alternatively, we could remove these checks from the python side if we don't want this to be a requirement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5784
https://github.com/broadinstitute/gatk/issues/5784:332,Performance,perform,perform,332,"See https://gatkforums.broadinstitute.org/gatk/discussion/23646/determinegermlinecontigploidy-issue-when-using-more-samples#latest. This includes DetermineGermlineContigPloidy, GermlineCNVCaller, and CreateReadCountPanelOfNormals. We currently perform a check that no duplicate count files are input, and I believe the python tools perform a check for duplicate sample names (although I'm not sure about gCNV). We could fail early on duplicate sample names on the Java side; alternatively, we could remove these checks from the python side if we don't want this to be a requirement.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5784
https://github.com/broadinstitute/gatk/pull/5785:109,Testability,test,test,109,All subclasses do the same. This involves updating all callers to pass a `Path` instead of a `File`. Added a test in `TableUtilsUnitTest` that writes a table to an in-memory `Path` to demonstrate we're not using files underneath (it works). Added a similar test in `TableReaderUnitTest` that reads from an in-memory `Path`. Fixes #5747,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5785
https://github.com/broadinstitute/gatk/pull/5785:257,Testability,test,test,257,All subclasses do the same. This involves updating all callers to pass a `Path` instead of a `File`. Added a test in `TableUtilsUnitTest` that writes a table to an in-memory `Path` to demonstrate we're not using files underneath (it works). Added a similar test in `TableReaderUnitTest` that reads from an in-memory `Path`. Fixes #5747,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5785
https://github.com/broadinstitute/gatk/pull/5786:191,Availability,recover,recovering,191,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786
https://github.com/broadinstitute/gatk/pull/5786:539,Availability,recover,recovery,539,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786
https://github.com/broadinstitute/gatk/pull/5786:1003,Performance,optimiz,optimize,1003,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786
https://github.com/broadinstitute/gatk/pull/5786:1527,Performance,perform,performance,1527,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786
https://github.com/broadinstitute/gatk/pull/5786:191,Safety,recover,recovering,191,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786
https://github.com/broadinstitute/gatk/pull/5786:539,Safety,recover,recovery,539,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786
https://github.com/broadinstitute/gatk/pull/5786:1018,Testability,log,log,1018,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786
https://github.com/broadinstitute/gatk/pull/5786:1791,Testability,test,tested,1791,"This fixes a bug that @meganshand found. Here's the background:. By construction we do not assemble graphs with cycles (although @kvg has something to say about this). However, in rare cases recovering a dangling end may create a cycle. Although it's debatable whether this is an issue for the new, Dijkstra's algorithm-based best haplotype finding algorithm, we remove cycles before finding best haplotypes. It seems that the code for removing cycles can go into an infinite loop when, as in Mutect2's mitochondria mode, we allow for the recovery of forked dangling ends. This PR deletes a single line. `parentVertices` is the set of previously visited vertices in the depth-first search. When an edge is incident on one of these vertices it creates a cycle and we mark it for removal. My best guess (@ldgauthier could you be an extra set of brain? @droazen you're welcome to look, too.) is that the idea behind removing a `currentVertex` from `parentVertices` once all its edges were processed was to optimize the O(log n) cost of subsequent `parentVertices.contains` calls. Since it's a depth-first search, you would think that `currentVertex` will never be seen again and that this is innocuous. However, if some other branch of the depth-first search that is not descended from `currentVertex` also leads to a cycle that goes through `currentVertex`, forgetting that it has been visited creates a huge problem. I believe that forked dangling ends create this possibility. . Removing the line in question will incur a tiny performance cost, if any. By the time we get here the graph has been zipped into a `SeqGraph`, so it doesn't have very many vertices. In any case, `Set.contains` is not an expensive operation. We might even save runtime by eliminating all the `Set.remove`. I have tested this on several WGS samples and it does no harm.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5786
https://github.com/broadinstitute/gatk/pull/5787:12,Modifiability,Inherit,Inherited,12,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787
https://github.com/broadinstitute/gatk/pull/5787:42,Modifiability,inherit,inherit,42,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787
https://github.com/broadinstitute/gatk/pull/5787:186,Modifiability,inherit,inherited,186,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787
https://github.com/broadinstitute/gatk/pull/5787:244,Modifiability,extend,extends,244,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787
https://github.com/broadinstitute/gatk/pull/5787:22,Testability,test,test,22,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787
https://github.com/broadinstitute/gatk/pull/5787:85,Testability,test,testng,85,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787
https://github.com/broadinstitute/gatk/pull/5787:131,Testability,test,tests,131,"This fixes 'Inherited test methods do not inherit groups', https://github.com/cbeust/testng/issues/182. This is needed to run e.g. tests in the Spark group, since in some cases they are inherited. See e.g. PrintReadsSparkIntegrationTest, which extends AbstractPrintReadsIntegrationTest.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5787
https://github.com/broadinstitute/gatk/issues/5790:80,Deployability,pipeline,pipeline,80,The return value of picard tools goes to standard output which makes it hard to pipeline these tools nicely.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5790
https://github.com/broadinstitute/gatk/issues/5791:1932,Availability,recover,recover,1932,"different in HC invocation was that the first one used `--dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791
https://github.com/broadinstitute/gatk/issues/5791:2174,Availability,error,error,2174,"-dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that are problematic, so we could have our cake and eat it too.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791
https://github.com/broadinstitute/gatk/issues/5791:2884,Performance,perform,performance,2884,"-dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that are problematic, so we could have our cake and eat it too.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791
https://github.com/broadinstitute/gatk/issues/5791:1932,Safety,recover,recover,1932,"different in HC invocation was that the first one used `--dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791
https://github.com/broadinstitute/gatk/issues/5791:2149,Safety,detect,detect,2149,"-dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but that make the HC run approximately 5 times slower, which is obviously not ideal. What I'd like to suggest is that the HC have some automated way to detect when this kind of error is likely to happen or has happened, and work around it. My suggestion(s) would be:. 1. I _think_ this really only happens in repetitive regions. I wonder if it would be possible to have the HC automatically trim active regions when assembly at kmer size 10 works, and disable it when it has to escalate to a higher kmer size? . 2. Trim the active region, but retain the untrimmed active region also. Genotype using the trimmed region. If any allele receives count=0, re-genotype using the untrimmed regions. My thought here is that I think not trimming the active regions really only makes a difference at a small fraction of sites, on the order of 1/1000, but to rescue those sites we have to pay a 5x performance penalty at every site. It would be great if trimming could be auto-disabled at only those sites that are problematic, so we could have our cake and eat it too.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791
https://github.com/broadinstitute/gatk/issues/5791:628,Usability,simpl,simple,628,"## Feature request. ### Tool(s) or class(es) involved; HaplotypeCaller (latest version from master as of 3/12/2019). ### Description; I'm running into situations where the HaplotypeCaller does the wrong thing when genotyping, when it is allowed to trim active regions. A good example is a fairly high frequency variant at `HG19:chr11 -6411935` aka `HG38:chr11:6390705`. I'm looking at this data in the 24 samples that Broad did deep PCR-free 2x250 WGS sequencing on for the 1000G project. In these samples there are a pair of variants at that locus, one is a SNP and the other is a 6bp deletion. It's right on the boundary of a simple sequence repeat. What appears to happen is that when the active region is trimmed, sometimes the deletion allele is lost. From looking at the gVCFs created and also at the output with `--debug` it's clear the allele is discovered, but then when genotyping is done on the trimmed active region the allele disappears. Here's an example pair of calls where the only different in HC invocation was that the first one used `--dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791
https://github.com/broadinstitute/gatk/issues/5791:834,Usability,clear,clear,834,"## Feature request. ### Tool(s) or class(es) involved; HaplotypeCaller (latest version from master as of 3/12/2019). ### Description; I'm running into situations where the HaplotypeCaller does the wrong thing when genotyping, when it is allowed to trim active regions. A good example is a fairly high frequency variant at `HG19:chr11 -6411935` aka `HG38:chr11:6390705`. I'm looking at this data in the 24 samples that Broad did deep PCR-free 2x250 WGS sequencing on for the 1000G project. In these samples there are a pair of variants at that locus, one is a SNP and the other is a 6bp deletion. It's right on the boundary of a simple sequence repeat. What appears to happen is that when the active region is trimmed, sometimes the deletion allele is lost. From looking at the gVCFs created and also at the output with `--debug` it's clear the allele is discovered, but then when genotyping is done on the trimmed active region the allele disappears. Here's an example pair of calls where the only different in HC invocation was that the first one used `--dont-trim-active-regions true`:. ```; chr11 6411935 rs3838786 TGCTGGC CGCTGGC,T,<NON_REF> 4029.06 . DB;DP=118;ExcessHet=3.0103;MLEAC=1,1,0;MLEAF=0.500,0.500,0.00;RAW_MQandDP=424800,118;REF_BASES=ATGGGCCTGGTGCTGGCGCTG GT:AD:DP:F1R2:F2R1:GQ:PL:SB 1/2:0,62,40,0:102:0,31,23,0:0,31,17,0:99:4046,1646,1982,2435,0,2437,4113,1933,2560,4431:0,0,54,48; ```. and the second one didn't:. ```; chr11 6411935 rs3838786 TGCTGGC T,CGCTGGC,<NON_REF> 2308.64 . BaseQRankSum=-1.312;ClippingRankSum=0.877;DB;DP=119;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=428400,119;REF_BASES=ATGGGCCTGGTGCTGGCGCTG;ReadPosRankSum=0.255 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/2:7,0,65,0:72:1,0,34,0:6,0,31,0:99:2316,2364,2996,0,269,1897,2506,2977,1274,3798:1,6,34,31; ```. Note how in the second case, there are two alts in the gVCF, but only one of them has depth!. The only way to recover these cases is to run with `--dont-trim-active-regions`, but ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5791
https://github.com/broadinstitute/gatk/issues/5793:119,Deployability,release,release,119,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs 4.0.0.12. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I've run into a weird case where GenotypeGVCFs is doing something unexpected. I have a gVCF with the following entry in it:. ```; chr11 6637739 . ATTTTT A,AT,ATT,ATTT,ATTTT,ATTTTTT,<NON_REF> 565.73 . BaseQRankSum=-0.014;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;MLEAC=0,0,0,1,0,0,0;MLEAF=0,0,0,0.5,0,0,0;MQRankSum=0;RAW_MQandDP=338400,94;REF_BASES=GCCGGCCTGGATTTTTTTTTT;ReadPosRankSum=-0.812 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/4:9,3,3,11,15,8,3,0:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:56:603,504,1526,335,1171,1118,56,661,640,608,0,362,313,183,335,336,500,389,187,171,527,655,864,622,277,169,466,1026,597,1101,953,645,465,625,861,1133:8,1,33,10; ```. It's a messy site for sure, an indel in a long homopolymer-T, but I think that's a separate issue. If I run the following on that gVCF:. ```; gatk GenotypeGVCFs \; -R hg19.fa -V test.g.vcf -O test.vcf \; -A ClippingRankSumTest -A Coverage -A ExcessHet -A FisherStrand \; -A MappingQualityRankSumTest -A OxoGReadCounts -A QualByDepth -A ReadPosRankSumTest \; -A ReferenceBases -A RMSMappingQuality -A StrandOddsRatio -A TandemRepeat \; -L chr11:6637730-6637750 \; -stand-call-conf 18.0 \; ```. then I get the following output to the VCF just like I'd expect:. ```; chr11 6637739 . ATT A 565.73 . AC=1;AF=0.500;AN=2;BaseQRankSum=-1.400e-02;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;FS=1.779;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.00;QD=23.57;REF_BASES=GCCGGCCTGGATTTTTTTTTT;RPA=15,13;RU=T;ReadPosRankSum=-8.120e-01;SOR=0.386;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:9,15:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:99:603,0,335; ```. QUAL is unchanged since I'm genotyping a single-sample gVCF. However, if I raise my `-stand-call-conf` threshold to 19.0, GenotypeGVCFs no longer outputs any variants. 565.73 >> 19.0, so I'm confused as to why that var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793
https://github.com/broadinstitute/gatk/issues/5793:189,Testability,test,test,189,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs 4.0.0.12. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I've run into a weird case where GenotypeGVCFs is doing something unexpected. I have a gVCF with the following entry in it:. ```; chr11 6637739 . ATTTTT A,AT,ATT,ATTT,ATTTT,ATTTTTT,<NON_REF> 565.73 . BaseQRankSum=-0.014;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;MLEAC=0,0,0,1,0,0,0;MLEAF=0,0,0,0.5,0,0,0;MQRankSum=0;RAW_MQandDP=338400,94;REF_BASES=GCCGGCCTGGATTTTTTTTTT;ReadPosRankSum=-0.812 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/4:9,3,3,11,15,8,3,0:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:56:603,504,1526,335,1171,1118,56,661,640,608,0,362,313,183,335,336,500,389,187,171,527,655,864,622,277,169,466,1026,597,1101,953,645,465,625,861,1133:8,1,33,10; ```. It's a messy site for sure, an indel in a long homopolymer-T, but I think that's a separate issue. If I run the following on that gVCF:. ```; gatk GenotypeGVCFs \; -R hg19.fa -V test.g.vcf -O test.vcf \; -A ClippingRankSumTest -A Coverage -A ExcessHet -A FisherStrand \; -A MappingQualityRankSumTest -A OxoGReadCounts -A QualByDepth -A ReadPosRankSumTest \; -A ReferenceBases -A RMSMappingQuality -A StrandOddsRatio -A TandemRepeat \; -L chr11:6637730-6637750 \; -stand-call-conf 18.0 \; ```. then I get the following output to the VCF just like I'd expect:. ```; chr11 6637739 . ATT A 565.73 . AC=1;AF=0.500;AN=2;BaseQRankSum=-1.400e-02;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;FS=1.779;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.00;QD=23.57;REF_BASES=GCCGGCCTGGATTTTTTTTTT;RPA=15,13;RU=T;ReadPosRankSum=-8.120e-01;SOR=0.386;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:9,15:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:99:603,0,335; ```. QUAL is unchanged since I'm genotyping a single-sample gVCF. However, if I raise my `-stand-call-conf` threshold to 19.0, GenotypeGVCFs no longer outputs any variants. 565.73 >> 19.0, so I'm confused as to why that var",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793
https://github.com/broadinstitute/gatk/issues/5793:1040,Testability,test,test,1040,"class(es); GenotypeGVCFs 4.0.0.12. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I've run into a weird case where GenotypeGVCFs is doing something unexpected. I have a gVCF with the following entry in it:. ```; chr11 6637739 . ATTTTT A,AT,ATT,ATTT,ATTTT,ATTTTTT,<NON_REF> 565.73 . BaseQRankSum=-0.014;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;MLEAC=0,0,0,1,0,0,0;MLEAF=0,0,0,0.5,0,0,0;MQRankSum=0;RAW_MQandDP=338400,94;REF_BASES=GCCGGCCTGGATTTTTTTTTT;ReadPosRankSum=-0.812 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/4:9,3,3,11,15,8,3,0:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:56:603,504,1526,335,1171,1118,56,661,640,608,0,362,313,183,335,336,500,389,187,171,527,655,864,622,277,169,466,1026,597,1101,953,645,465,625,861,1133:8,1,33,10; ```. It's a messy site for sure, an indel in a long homopolymer-T, but I think that's a separate issue. If I run the following on that gVCF:. ```; gatk GenotypeGVCFs \; -R hg19.fa -V test.g.vcf -O test.vcf \; -A ClippingRankSumTest -A Coverage -A ExcessHet -A FisherStrand \; -A MappingQualityRankSumTest -A OxoGReadCounts -A QualByDepth -A ReadPosRankSumTest \; -A ReferenceBases -A RMSMappingQuality -A StrandOddsRatio -A TandemRepeat \; -L chr11:6637730-6637750 \; -stand-call-conf 18.0 \; ```. then I get the following output to the VCF just like I'd expect:. ```; chr11 6637739 . ATT A 565.73 . AC=1;AF=0.500;AN=2;BaseQRankSum=-1.400e-02;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;FS=1.779;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.00;QD=23.57;REF_BASES=GCCGGCCTGGATTTTTTTTTT;RPA=15,13;RU=T;ReadPosRankSum=-8.120e-01;SOR=0.386;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:9,15:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:99:603,0,335; ```. QUAL is unchanged since I'm genotyping a single-sample gVCF. However, if I raise my `-stand-call-conf` threshold to 19.0, GenotypeGVCFs no longer outputs any variants. 565.73 >> 19.0, so I'm confused as to why that variant is no longer emitted. #### Steps t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793
https://github.com/broadinstitute/gatk/issues/5793:1054,Testability,test,test,1054,"otypeGVCFs 4.0.0.12. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I've run into a weird case where GenotypeGVCFs is doing something unexpected. I have a gVCF with the following entry in it:. ```; chr11 6637739 . ATTTTT A,AT,ATT,ATTT,ATTTT,ATTTTTT,<NON_REF> 565.73 . BaseQRankSum=-0.014;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;MLEAC=0,0,0,1,0,0,0;MLEAF=0,0,0,0.5,0,0,0;MQRankSum=0;RAW_MQandDP=338400,94;REF_BASES=GCCGGCCTGGATTTTTTTTTT;ReadPosRankSum=-0.812 GT:AD:DP:F1R2:F2R1:GQ:PL:SB 0/4:9,3,3,11,15,8,3,0:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:56:603,504,1526,335,1171,1118,56,661,640,608,0,362,313,183,335,336,500,389,187,171,527,655,864,622,277,169,466,1026,597,1101,953,645,465,625,861,1133:8,1,33,10; ```. It's a messy site for sure, an indel in a long homopolymer-T, but I think that's a separate issue. If I run the following on that gVCF:. ```; gatk GenotypeGVCFs \; -R hg19.fa -V test.g.vcf -O test.vcf \; -A ClippingRankSumTest -A Coverage -A ExcessHet -A FisherStrand \; -A MappingQualityRankSumTest -A OxoGReadCounts -A QualByDepth -A ReadPosRankSumTest \; -A ReferenceBases -A RMSMappingQuality -A StrandOddsRatio -A TandemRepeat \; -L chr11:6637730-6637750 \; -stand-call-conf 18.0 \; ```. then I get the following output to the VCF just like I'd expect:. ```; chr11 6637739 . ATT A 565.73 . AC=1;AF=0.500;AN=2;BaseQRankSum=-1.400e-02;ClippingRankSum=0.508;DP=94;ExcessHet=3.0103;FS=1.779;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.00;QD=23.57;REF_BASES=GCCGGCCTGGATTTTTTTTTT;RPA=15,13;RU=T;ReadPosRankSum=-8.120e-01;SOR=0.386;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:9,15:52:8,2,2,8,12,6,3,0:1,1,1,3,3,2,0,0:99:603,0,335; ```. QUAL is unchanged since I'm genotyping a single-sample gVCF. However, if I raise my `-stand-call-conf` threshold to 19.0, GenotypeGVCFs no longer outputs any variants. 565.73 >> 19.0, so I'm confused as to why that variant is no longer emitted. #### Steps to reproduce; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5793
https://github.com/broadinstitute/gatk/issues/5798:299,Availability,error,error,299,"@kvg reports that running the Intel deflater via GATK on long reads data intermittently produces corrupt bam outputs. His specific use case is sharding a single unaligned bam file into multiple smaller bams. Running with the JDK deflater (`--use-jdk-deflater`) appears to resolve the issue. Example error when trying to read a corrupt shard (reading with htsjdk produces the same error):. ```; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v1/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v2/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v3/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v4/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v5/ -nr 100000. $ samtools view intel_compression/v1/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v2/sharding_test.000002.bam > /dev/null; [E::bgzf_read] Read block operation failed with error 2 after 30675 of 72043 bytes; [main_samview] truncated file.; $ samtools view intel_compression/v3/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v4/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v5/sharding_test.000002.bam > /dev/null. (Only the second attempt yields a corrupted file; runs before and after appear to be correct, despite nothing changing between steps.); ```. There may be a bug in https://github.com/Intel-HLS/GKL/blob/master/src/main/native/compression/IntelDeflater.cc, perhaps triggered when a read spans many compressed blocks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5798
https://github.com/broadinstitute/gatk/issues/5798:380,Availability,error,error,380,"@kvg reports that running the Intel deflater via GATK on long reads data intermittently produces corrupt bam outputs. His specific use case is sharding a single unaligned bam file into multiple smaller bams. Running with the JDK deflater (`--use-jdk-deflater`) appears to resolve the issue. Example error when trying to read a corrupt shard (reading with htsjdk produces the same error):. ```; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v1/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v2/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v3/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v4/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v5/ -nr 100000. $ samtools view intel_compression/v1/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v2/sharding_test.000002.bam > /dev/null; [E::bgzf_read] Read block operation failed with error 2 after 30675 of 72043 bytes; [main_samview] truncated file.; $ samtools view intel_compression/v3/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v4/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v5/sharding_test.000002.bam > /dev/null. (Only the second attempt yields a corrupted file; runs before and after appear to be correct, despite nothing changing between steps.); ```. There may be a bug in https://github.com/Intel-HLS/GKL/blob/master/src/main/native/compression/IntelDeflater.cc, perhaps triggered when a read spans many compressed blocks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5798
https://github.com/broadinstitute/gatk/issues/5798:1082,Availability,error,error,1082,"@kvg reports that running the Intel deflater via GATK on long reads data intermittently produces corrupt bam outputs. His specific use case is sharding a single unaligned bam file into multiple smaller bams. Running with the JDK deflater (`--use-jdk-deflater`) appears to resolve the issue. Example error when trying to read a corrupt shard (reading with htsjdk produces the same error):. ```; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v1/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v2/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v3/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v4/ -nr 100000; $ java -jar gatk.jar SplitSubreadsByZmw -I sharding_test.bam -O intel_compression/v5/ -nr 100000. $ samtools view intel_compression/v1/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v2/sharding_test.000002.bam > /dev/null; [E::bgzf_read] Read block operation failed with error 2 after 30675 of 72043 bytes; [main_samview] truncated file.; $ samtools view intel_compression/v3/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v4/sharding_test.000002.bam > /dev/null; $ samtools view intel_compression/v5/sharding_test.000002.bam > /dev/null. (Only the second attempt yields a corrupted file; runs before and after appear to be correct, despite nothing changing between steps.); ```. There may be a bug in https://github.com/Intel-HLS/GKL/blob/master/src/main/native/compression/IntelDeflater.cc, perhaps triggered when a read spans many compressed blocks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5798
https://github.com/broadinstitute/gatk/pull/5799:3,Testability,test,test,3,CI test,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5799
https://github.com/broadinstitute/gatk/pull/5801:42,Availability,redundant,redundant,42,"The file extension check in this tool was redundant, since there is a subsequent check; that the input file is in BGZF format. Resolves #5800",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5801
https://github.com/broadinstitute/gatk/pull/5801:42,Safety,redund,redundant,42,"The file extension check in this tool was redundant, since there is a subsequent check; that the input file is in BGZF format. Resolves #5800",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5801
https://github.com/broadinstitute/gatk/issues/5802:655,Availability,error,error,655,"Hi,. I am trying to test the pathseq tutorial following the tutorial on [this]( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ""this"") link. I ran the following commands. bioinfo@bioinfo$ conda activate gatk; (gatk) bioinfo@bioinfo$ gatk PathSeqPipelineSpark \; > --input test_sample.bam \; > --filter-bwa-image hg19mini.fasta.img \; > --kmer-file hg19mini.hss \; > --min-clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:2487,Availability,down,down,2487,"figured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=645922816; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:3887,Availability,error,error,3887, 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: bioinfo: bioinfo: unknown error; 	at java.net.InetAddress.getLocalHost(InetAddress.java:1505); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: bioinfo: unknown error; 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); 	at java.net.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:4719,Availability,error,error,4719,itute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: bioinfo: bioinfo: unknown error; 	at java.net.InetAddress.getLocalHost(InetAddress.java:1505); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: bioinfo: unknown error; 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); 	at java.net.InetAddress.getLocalHost(InetAddress.java:1500); 	... 21 more; . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23594/java-related-error-encountered-while-running-gatk-pathseqpipelinespark/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:5142,Availability,error,error-encountered-while-running-gatk-pathseqpipelinespark,5142,itute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: bioinfo: bioinfo: unknown error; 	at java.net.InetAddress.getLocalHost(InetAddress.java:1505); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: bioinfo: unknown error; 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); 	at java.net.InetAddress.getLocalHost(InetAddress.java:1500); 	... 21 more; . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23594/java-related-error-encountered-while-running-gatk-pathseqpipelinespark/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:163,Deployability,pipeline,pipeline,163,"Hi,. I am trying to test the pathseq tutorial following the tutorial on [this]( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ""this"") link. I ran the following commands. bioinfo@bioinfo$ conda activate gatk; (gatk) bioinfo@bioinfo$ gatk PathSeqPipelineSpark \; > --input test_sample.bam \; > --filter-bwa-image hg19mini.fasta.img \; > --kmer-file hg19mini.hss \; > --min-clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:692,Deployability,Install,Installers,692,"Hi,. I am trying to test the pathseq tutorial following the tutorial on [this]( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ""this"") link. I ran the following commands. bioinfo@bioinfo$ conda activate gatk; (gatk) bioinfo@bioinfo$ gatk PathSeqPipelineSpark \; > --input test_sample.bam \; > --filter-bwa-image hg19mini.fasta.img \; > --kmer-file hg19mini.hss \; > --min-clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:943,Deployability,Install,Installers,943,"Hi,. I am trying to test the pathseq tutorial following the tutorial on [this]( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ""this"") link. I ran the following commands. bioinfo@bioinfo$ conda activate gatk; (gatk) bioinfo@bioinfo$ gatk PathSeqPipelineSpark \; > --input test_sample.bam \; > --filter-bwa-image hg19mini.fasta.img \; > --kmer-file hg19mini.hss \; > --min-clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:1606,Deployability,Install,Installers,1606,"res-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark d",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:1361,Modifiability,variab,variables,1361,clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:1487,Modifiability,config,configured,1487,clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:4532,Modifiability,config,config,4532,itute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: bioinfo: bioinfo: unknown error; 	at java.net.InetAddress.getLocalHost(InetAddress.java:1505); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: bioinfo: unknown error; 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); 	at java.net.InetAddress.getLocalHost(InetAddress.java:1500); 	... 21 more; . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23594/java-related-error-encountered-while-running-gatk-pathseqpipelinespark/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:4605,Modifiability,config,config,4605,itute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.net.UnknownHostException: bioinfo: bioinfo: unknown error; 	at java.net.InetAddress.getLocalHost(InetAddress.java:1505); 	at org.apache.spark.util.Utils$.findLocalInetAddress(Utils.scala:891); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress$lzycompute(Utils.scala:884); 	at org.apache.spark.util.Utils$.org$apache$spark$util$Utils$$localIpAddress(Utils.scala:884); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at org.apache.spark.util.Utils$$anonfun$localHostName$1.apply(Utils.scala:941); 	at scala.Option.getOrElse(Option.scala:121); 	at org.apache.spark.util.Utils$.localHostName(Utils.scala:941); 	at org.apache.spark.internal.config.package$.<init>(package.scala:204); 	at org.apache.spark.internal.config.package$.<clinit>(package.scala); 	... 12 more; Caused by: java.net.UnknownHostException: bioinfo: unknown error; 	at java.net.Inet6AddressImpl.lookupAllHostAddr(Native Method); 	at java.net.InetAddress$2.lookupAllHostAddr(InetAddress.java:928); 	at java.net.InetAddress.getAddressesFromNameService(InetAddress.java:1323); 	at java.net.InetAddress.getLocalHost(InetAddress.java:1500); 	... 21 more; . This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23594/java-related-error-encountered-while-running-gatk-pathseqpipelinespark/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:1548,Performance,Load,Loading,1548,"_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstit",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:2344,Performance,load,load,2344,"- Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=645922816; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:2767,Security,validat,validateSettings,2767,"---------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:57:41.739 INFO PathSeqPipelineSpark - Initializing engine; 18:57:41.739 INFO PathSeqPipelineSpark - Done initializing engine; Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties; 19/03/05 18:57:41 INFO SparkContext: Running Spark version 2.2.0; 18:57:41.968 WARN NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable; 18:57:42.155 INFO PathSeqPipelineSpark - Shutting down engine; [5 March, 2019 6:57:42 PM IST] org.broadinstitute.hellbender.tools.spark.pathseq.PathSeqPipelineSpark done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=645922816; Exception in thread ""main"" java.lang.ExceptionInInitializerError; 	at org.apache.spark.SparkConf.validateSettings(SparkConf.scala:546); 	at org.apache.spark.SparkContext.<init>(SparkContext.scala:373); 	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.createSparkContext(SparkContextFactory.java:178); 	at org.broadinstitute.hellbender.engine.spark.SparkContextFactory.getSparkContext(SparkContextFactory.java:110); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:28); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5802:20,Testability,test,test,20,"Hi,. I am trying to test the pathseq tutorial following the tutorial on [this]( https://gatkforums.broadinstitute.org/gatk/discussion/10913/how-to-run-the-pathseq-pipeline ""this"") link. I ran the following commands. bioinfo@bioinfo$ conda activate gatk; (gatk) bioinfo@bioinfo$ gatk PathSeqPipelineSpark \; > --input test_sample.bam \; > --filter-bwa-image hg19mini.fasta.img \; > --kmer-file hg19mini.hss \; > --min-clipped-read-length 70 \; > --microbe-fasta e_coli_k12.fasta \; > --microbe-bwa-image e_coli_k12.fasta.img \; > --taxonomy-file e_coli_k12.db \; > --output output.pathseq.bam \; > --scores-output output.pathseq.txt. And encountered below error:. Using GATK jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar PathSeqPipelineSpark --input test_sample.bam --filter-bwa-image hg19mini.fasta.img --kmer-file hg19mini.hss --min-clipped-read-length 70 --microbe-fasta e_coli_k12.fasta --microbe-bwa-image e_coli_k12.fasta.img --taxonomy-file e_coli_k12.db --output output.pathseq.bam --scores-output output.pathseq.txt; 18:57:39.629 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 18:57:39.729 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/bioinfo/Installers/gatk4/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:57:41.594 INFO PathSeqPipelineSpark - ------------------------------------------------------------; 18:57:41.594 INFO PathSeqPipelineSpark - The Genome Analysis Toolkit (GATK) v4.1.0.0; 18:57:41.594 INFO PathSeqPipelineSpark - For support and documentation go to https://software.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5802
https://github.com/broadinstitute/gatk/issues/5804:51,Availability,error,errors,51,"As pointed out by Julian, I incorrectly propagated errors in one part of the model translation. Not sure if this has any effect on ABSOLUTE results, but we can fix it up. Probably should re-examine some of the other expressions as well---would be great to finally get feedback on whether these are at all sensible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5804
https://github.com/broadinstitute/gatk/issues/5804:268,Usability,feedback,feedback,268,"As pointed out by Julian, I incorrectly propagated errors in one part of the model translation. Not sure if this has any effect on ABSOLUTE results, but we can fix it up. Probably should re-examine some of the other expressions as well---would be great to finally get feedback on whether these are at all sensible.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5804
https://github.com/broadinstitute/gatk/issues/5805:1351,Availability,error,error,1351,"GATK doesn't seem to be working on Windows any longer, at least HaploTypeCaller crashes with the following. Tested with GATK 4.0.9.0, 4.0.11.0 and 4.1.0.0. ```; java.exe -jar gatk-package-4.1.0.0-local.jar HaplotypeCaller -R refa.fa -I refa.bam -O bam_chramma.vcf; java.nio.file.InvalidPathException: Illegal char <:> at index 2: /C:/Users/Teemu/AppData/Local/Temp/; at sun.nio.fs.WindowsPathParser.normalize(Unknown Source); at sun.nio.fs.WindowsPathParser.parse(Unknown Source); at sun.nio.fs.WindowsPathParser.parse(Unknown Source); at sun.nio.fs.WindowsPath.parse(Unknown Source); at sun.nio.fs.WindowsFileSystem.getPath(Unknown Source); at java.nio.file.Paths.get(Unknown Source); at org.broadinstitute.hellbender.utils.io.IOUtils.getPath(IOUtils.java:759); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:161); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). ```. The last version I have tested where this still works is 4.0.8.1. It is caused most likely because of the forward slash before C:. There are several examples of the error that can be googled. Here's one:. https://bugs.eclipse.org/bugs/show_bug.cgi?id=518079",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5805
https://github.com/broadinstitute/gatk/issues/5805:108,Testability,Test,Tested,108,"GATK doesn't seem to be working on Windows any longer, at least HaploTypeCaller crashes with the following. Tested with GATK 4.0.9.0, 4.0.11.0 and 4.1.0.0. ```; java.exe -jar gatk-package-4.1.0.0-local.jar HaplotypeCaller -R refa.fa -I refa.bam -O bam_chramma.vcf; java.nio.file.InvalidPathException: Illegal char <:> at index 2: /C:/Users/Teemu/AppData/Local/Temp/; at sun.nio.fs.WindowsPathParser.normalize(Unknown Source); at sun.nio.fs.WindowsPathParser.parse(Unknown Source); at sun.nio.fs.WindowsPathParser.parse(Unknown Source); at sun.nio.fs.WindowsPath.parse(Unknown Source); at sun.nio.fs.WindowsFileSystem.getPath(Unknown Source); at java.nio.file.Paths.get(Unknown Source); at org.broadinstitute.hellbender.utils.io.IOUtils.getPath(IOUtils.java:759); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:161); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). ```. The last version I have tested where this still works is 4.0.8.1. It is caused most likely because of the forward slash before C:. There are several examples of the error that can be googled. Here's one:. https://bugs.eclipse.org/bugs/show_bug.cgi?id=518079",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5805
https://github.com/broadinstitute/gatk/issues/5805:1210,Testability,test,tested,1210,"GATK doesn't seem to be working on Windows any longer, at least HaploTypeCaller crashes with the following. Tested with GATK 4.0.9.0, 4.0.11.0 and 4.1.0.0. ```; java.exe -jar gatk-package-4.1.0.0-local.jar HaplotypeCaller -R refa.fa -I refa.bam -O bam_chramma.vcf; java.nio.file.InvalidPathException: Illegal char <:> at index 2: /C:/Users/Teemu/AppData/Local/Temp/; at sun.nio.fs.WindowsPathParser.normalize(Unknown Source); at sun.nio.fs.WindowsPathParser.parse(Unknown Source); at sun.nio.fs.WindowsPathParser.parse(Unknown Source); at sun.nio.fs.WindowsPath.parse(Unknown Source); at sun.nio.fs.WindowsFileSystem.getPath(Unknown Source); at java.nio.file.Paths.get(Unknown Source); at org.broadinstitute.hellbender.utils.io.IOUtils.getPath(IOUtils.java:759); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:161); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:211); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:160); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:203); at org.broadinstitute.hellbender.Main.main(Main.java:289). ```. The last version I have tested where this still works is 4.0.8.1. It is caused most likely because of the forward slash before C:. There are several examples of the error that can be googled. Here's one:. https://bugs.eclipse.org/bugs/show_bug.cgi?id=518079",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5805
https://github.com/broadinstitute/gatk/issues/5807:1679,Availability,avail,available,1679,":17.382 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrows",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:2206,Availability,error,errors,2206,"T --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinsti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:2429,Availability,error,error,2429,"lse --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:46:42.813 INFO BaseRecalibrator - Executing as mpmachado@lx-bioinfo02 on Linux v2.6.32-696.23.1.el6.x86_64 amd64; 15:46:42.814 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:2736,Availability,ERROR,ERROR,2736," ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:46:42.813 INFO BaseRecalibrator - Executing as mpmachado@lx-bioinfo02 on Linux v2.6.32-696.23.1.el6.x86_64 amd64; 15:46:42.814 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 15:46:42.814 INFO BaseRecalibrator - Start Date/Time: March 7, 2019 3:46:35 PM UTC; 15:46:42.815 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.815 INFO BaseRecalibrator - ------------------------------------------------------------; 15:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:5754,Availability,down,down,5754,"z; 15:46:44.076 WARN IndexUtils - Feature file ""snp151common_tablebrowser.bed.bgz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 15:46:44.500 WARN IndexUtils - Feature file ""snp151flagged_tablebrowser.bed.bgz"" appears to contain no sequence dictionary. Attempting to retrieve a sequence dictionary from the associated index file; 15:46:44.798 INFO BaseRecalibrator - Done initializing engine; 15:46:44.936 INFO BaseRecalibrationEngine - The covariates being used here:; 15:46:44.936 INFO BaseRecalibrationEngine - 	ReadGroupCovariate; 15:46:44.937 INFO BaseRecalibrationEngine - 	QualityScoreCovariate; 15:46:44.937 INFO BaseRecalibrationEngine - 	ContextCovariate; 15:46:44.937 INFO BaseRecalibrationEngine - 	CycleCovariate; 15:46:44.953 INFO ProgressMeter - Starting traversal; 15:46:44.953 INFO ProgressMeter - Current Locus Elapsed Minutes Reads Processed Reads/Minute; 15:46:45.866 INFO BaseRecalibrator - Shutting down engine; [March 7, 2019 3:46:45 PM UTC] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 0.17 minutes.; Runtime.totalMemory()=731381760; java.lang.IllegalArgumentException: fromIndex(64) > toIndex(62); 	at java.util.Arrays.rangeCheck(Arrays.java:113); 	at java.util.Arrays.fill(Arrays.java:3044); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateKnownSites(BaseRecalibrationEngine.java:354); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.calculateSkipArray(BaseRecalibrationEngine.java:322); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:137); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:185); 	at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:91); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at ja",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:8860,Availability,down,downsampled,8860,"KTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz; ```. I downsampled the fastq files and got similar results.; However, when giving only the reduced known-sites file (`--known-sites snp151flagged_tablebrowser.bed.bgz`) and specifying two intervals (`--intervals chr22 --intervals chrY`), it worked. I attached the downsampled bam file and the reduced known-sites file [here](https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049/#Comment_57049), and the reference file can be found [here](ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Eukaryotes/vertebrates_mammals/Homo_sapiens/GRCh38/seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna). I hope you can help me understanding what is going on and how to fix it. Thank you in advance. Best regards,. Miguel. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049#Comment_57049",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:9117,Availability,down,downsampled,9117,"KTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz; ```. I downsampled the fastq files and got similar results.; However, when giving only the reduced known-sites file (`--known-sites snp151flagged_tablebrowser.bed.bgz`) and specifying two intervals (`--intervals chr22 --intervals chrY`), it worked. I attached the downsampled bam file and the reduced known-sites file [here](https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049/#Comment_57049), and the reference file can be found [here](ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Eukaryotes/vertebrates_mammals/Homo_sapiens/GRCh38/seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna). I hope you can help me understanding what is going on and how to fix it. Thank you in advance. Best regards,. Miguel. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049#Comment_57049",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:8944,Energy Efficiency,reduce,reduced,8944,"KTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz; ```. I downsampled the fastq files and got similar results.; However, when giving only the reduced known-sites file (`--known-sites snp151flagged_tablebrowser.bed.bgz`) and specifying two intervals (`--intervals chr22 --intervals chrY`), it worked. I attached the downsampled bam file and the reduced known-sites file [here](https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049/#Comment_57049), and the reference file can be found [here](ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Eukaryotes/vertebrates_mammals/Homo_sapiens/GRCh38/seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna). I hope you can help me understanding what is going on and how to fix it. Thank you in advance. Best regards,. Miguel. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049#Comment_57049",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:9146,Energy Efficiency,reduce,reduced,9146,"KTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz; ```. I downsampled the fastq files and got similar results.; However, when giving only the reduced known-sites file (`--known-sites snp151flagged_tablebrowser.bed.bgz`) and specifying two intervals (`--intervals chr22 --intervals chrY`), it worked. I attached the downsampled bam file and the reduced known-sites file [here](https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049/#Comment_57049), and the reference file can be found [here](ftp://ftp.ncbi.nlm.nih.gov/genomes/archive/old_genbank/Eukaryotes/vertebrates_mammals/Homo_sapiens/GRCh38/seqs_for_alignment_pipelines/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna). I hope you can help me understanding what is going on and how to fix it. Thank you in advance. Best regards,. Miguel. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/57049#Comment_57049",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:7253,Integrability,wrap,wrapAndCopyInto,7253,libration.BaseRecalibrationEngine.calculateSkipArray(BaseRecalibrationEngine.java:322); 	at org.broadinstitute.hellbender.utils.recalibration.BaseRecalibrationEngine.processRead(BaseRecalibrationEngine.java:137); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:185); 	at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:91); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:89); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:704,Performance,Load,Loading,704,"Hi everyone,. I'm facing a similar issue with GATK v4.1.0.0 (HTSJDK v2.18.2 and Picard v2.18.25). I'm using GATK Docker image broadinstitute/gatk:4.1.0.0. Following what I read here, I checked the bam file and everything seems fine:; `gatk ValidateSamFile --INPUT sorted.bam --MODE SUMMARY`; ```; Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY; 16:08:17.382 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. La",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:1791,Performance,perform,performed,1791,"0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:2807,Performance,Load,Loading,2807,"out the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:46:42.813 INFO BaseRecalibrator - Executing as mpmachado@lx-bioinfo02 on Linux v2.6.32-696.23.1.el6.x86_64 amd64; 15:46:42.814 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 15:46:42.814 INFO BaseRecalibrator - Start Date/Time: March 7, 2019 3:46:35 PM UTC; 15:46:42.815 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.815 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.817 INFO BaseRecalibrator - HTSJDK Version: 2.18",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:240,Security,Validat,ValidateSamFile,240,"Hi everyone,. I'm facing a similar issue with GATK v4.1.0.0 (HTSJDK v2.18.2 and Picard v2.18.25). I'm using GATK Docker image broadinstitute/gatk:4.1.0.0. Following what I read here, I checked the bam file and everything seems fine:; `gatk ValidateSamFile --INPUT sorted.bam --MODE SUMMARY`; ```; Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY; 16:08:17.382 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. La",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:562,Security,Validat,ValidateSamFile,562,"Hi everyone,. I'm facing a similar issue with GATK v4.1.0.0 (HTSJDK v2.18.2 and Picard v2.18.25). I'm using GATK Docker image broadinstitute/gatk:4.1.0.0. Following what I read here, I checked the bam file and everything seems fine:; `gatk ValidateSamFile --INPUT sorted.bam --MODE SUMMARY`; ```; Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY; 16:08:17.382 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. La",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:861,Security,Validat,ValidateSamFile,861,"Hi everyone,. I'm facing a similar issue with GATK v4.1.0.0 (HTSJDK v2.18.2 and Picard v2.18.25). I'm using GATK Docker image broadinstitute/gatk:4.1.0.0. Following what I read here, I checked the bam file and everything seems fine:; `gatk ValidateSamFile --INPUT sorted.bam --MODE SUMMARY`; ```; Using GATK jar /gatk/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /gatk/gatk-package-4.1.0.0-local.jar ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY; 16:08:17.382 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. La",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:1751,Security,Validat,ValidateSamFile,1751,"0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:1770,Security,validat,validation,1770,"0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu Mar 07 16:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:1834,Security,validat,validations,1834,"6:08:17 UTC 2019] ValidateSamFile --INPUT CQ-NEQAS-2018.ILLUMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from j",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:1907,Security,Validat,Validated,1907,"UMINA.library.000000000-BCFDC.1.1.sorted.bam --MODE SUMMARY --MAX_OUTPUT 100 --IGNORE_WARNINGS false --VALIDATE_INDEX true --INDEX_VALIDATION_STRINGENCY EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:2078,Security,Validat,Validated,2078,"EXHAUSTIVE --IS_BISULFITE_SEQUENCED false --MAX_OPEN_TEMP_FILES 8000 --SKIP_MATE_VALIDATION false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/issues/5807:2262,Security,Validat,ValidateSamFile,2262,"E_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu Mar 07 16:08:24 UTC 2019] Executing as mpmachado@lx-bioinfo02 on Linux 2.6.32-696.23.1.el6.x86_64 amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.0.0; WARNING 2019-03-07 16:08:24 ValidateSamFile NM validation cannot be performed without the reference. All other validations will still occur.; INFO 2019-03-07 16:10:25 SamFileValidator Validated Read 10,000,000 records. Elapsed time: 00:02:00s. Time for last 10,000,000: 120s. Last read position: chr9:32,633,613; INFO 2019-03-07 16:12:22 SamFileValidator Validated Read 20,000,000 records. Elapsed time: 00:03:58s. Time for last 10,000,000: 117s. Last read position: chrM:11,340; No errors found; [Thu Mar 07 16:13:05 UTC 2019] picard.sam.ValidateSamFile done. Elapsed time: 4.79 minutes.; Runtime.totalMemory()=2602041344; Tool returned:; 0; ```. But when run BaseRecalibrator got the _fromIndex toIndex_ error:; `gatk BaseRecalibrator --input sorted.bam --output sorted.baserecalibrator_report.txt --reference GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.bowtie_index.fasta --use-original-qualities true --known-sites snp151common_tablebrowser.bed.bgz --known-sites snp151flagged_tablebrowser.bed.bgz`; ```; ERROR: return code 3; STDERR:; 15:46:35.795 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 15:46:42.808 INFO BaseRecalibrator - ------------------------------------------------------------; 15:46:42.810 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 15:46:42.810 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 15:46:42.813 INFO BaseRecalibrator - Execut",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5807
https://github.com/broadinstitute/gatk/pull/5811:6,Availability,failure,failures,6,"Fixes failures caused by 1) sample names containing `@`, which pandas interprets as a midline comment character, and 2) cohorts where all samples have numerical names, in which case leading zeros may be stripped when pandas infers the column dtype to be int. Added some commits to address a few more issues. Closes #5778. Closes #5809.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5811
https://github.com/broadinstitute/gatk/pull/5814:114,Energy Efficiency,reduce,reduces,114,"Closes #5775. @takutosato This doesn't affect M2 results (well, actually it improves sensitivity by 0.01%) but it reduces runtime by about 5% and makes the docs and code cleaner. @jamesemery could you verify that in abstracting out `Log10Cache` as `IntToDoubleFunctionCache` I didn't spoil its thread safety?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5814
https://github.com/broadinstitute/gatk/pull/5814:301,Safety,safe,safety,301,"Closes #5775. @takutosato This doesn't affect M2 results (well, actually it improves sensitivity by 0.01%) but it reduces runtime by about 5% and makes the docs and code cleaner. @jamesemery could you verify that in abstracting out `Log10Cache` as `IntToDoubleFunctionCache` I didn't spoil its thread safety?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5814
https://github.com/broadinstitute/gatk/pull/5817:5,Modifiability,refactor,refactored,5,Also refactored the `VcfOutputRenderer` sanitization code. Fixes #5671,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5817
https://github.com/broadinstitute/gatk/pull/5817:40,Security,sanitiz,sanitization,40,Also refactored the `VcfOutputRenderer` sanitization code. Fixes #5671,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5817
https://github.com/broadinstitute/gatk/pull/5818:5,Deployability,update,update,5,This update means the pipeline won't die on single-ended reads (it just filters them out).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5818
https://github.com/broadinstitute/gatk/pull/5818:22,Deployability,pipeline,pipeline,22,This update means the pipeline won't die on single-ended reads (it just filters them out).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5818
https://github.com/broadinstitute/gatk/pull/5819:337,Deployability,Integrat,Integration,337,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819
https://github.com/broadinstitute/gatk/pull/5819:337,Integrability,Integrat,Integration,337,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819
https://github.com/broadinstitute/gatk/pull/5819:481,Modifiability,variab,variable,481,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819
https://github.com/broadinstitute/gatk/pull/5819:455,Safety,detect,detect,455,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819
https://github.com/broadinstitute/gatk/pull/5819:183,Testability,test,tests,183,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819
https://github.com/broadinstitute/gatk/pull/5819:221,Testability,test,tests,221,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819
https://github.com/broadinstitute/gatk/pull/5819:281,Testability,test,tests,281,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819
https://github.com/broadinstitute/gatk/pull/5819:349,Testability,test,tests,349,"Fixes #5751 and #4591. Longer term we'll still want to do package version-checking/verification per https://github.com/broadinstitute/gatk/issues/4995 as well. @jamesemery I included tests for this change, but I need the tests to only run when the conda env is NOT activated. Unit tests are always run on the docker image, so thats out. Integration tests are run on both the docker and the travis image, so I throw a skip exception on the docker, which I detect using the ""CI"" env variable. But that seems fragile and confusing. Is there a better way to do this ?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5819
https://github.com/broadinstitute/gatk/issues/5820:116,Availability,failure,failure,116,"Its possible to specify CNN inference size argument values that cause the Python process run out of memory, and the failure mode appears to be the java process hangs. Its not clear whether its always possible to recover from this using the global exception handler we currently install on the Python side - we need to explore a bit to see if the handler is being invoked on OOM; whether catching the OOM exception explicitly would help, or if we need an alternative reporting strategy for low-memory conditions. Attached is a log provided by @bhanugandham from a run in a Terra notebook that failed and that exhibited a hang that we assume was due to OOM, and that was resolved by reducing the inference batch size. [gatkStreamingProcessJournal-772629669.txt](https://github.com/broadinstitute/gatk/files/2988819/gatkStreamingProcessJournal-772629669.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5820
https://github.com/broadinstitute/gatk/issues/5820:212,Availability,recover,recover,212,"Its possible to specify CNN inference size argument values that cause the Python process run out of memory, and the failure mode appears to be the java process hangs. Its not clear whether its always possible to recover from this using the global exception handler we currently install on the Python side - we need to explore a bit to see if the handler is being invoked on OOM; whether catching the OOM exception explicitly would help, or if we need an alternative reporting strategy for low-memory conditions. Attached is a log provided by @bhanugandham from a run in a Terra notebook that failed and that exhibited a hang that we assume was due to OOM, and that was resolved by reducing the inference batch size. [gatkStreamingProcessJournal-772629669.txt](https://github.com/broadinstitute/gatk/files/2988819/gatkStreamingProcessJournal-772629669.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5820
https://github.com/broadinstitute/gatk/issues/5820:278,Deployability,install,install,278,"Its possible to specify CNN inference size argument values that cause the Python process run out of memory, and the failure mode appears to be the java process hangs. Its not clear whether its always possible to recover from this using the global exception handler we currently install on the Python side - we need to explore a bit to see if the handler is being invoked on OOM; whether catching the OOM exception explicitly would help, or if we need an alternative reporting strategy for low-memory conditions. Attached is a log provided by @bhanugandham from a run in a Terra notebook that failed and that exhibited a hang that we assume was due to OOM, and that was resolved by reducing the inference batch size. [gatkStreamingProcessJournal-772629669.txt](https://github.com/broadinstitute/gatk/files/2988819/gatkStreamingProcessJournal-772629669.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5820
https://github.com/broadinstitute/gatk/issues/5820:212,Safety,recover,recover,212,"Its possible to specify CNN inference size argument values that cause the Python process run out of memory, and the failure mode appears to be the java process hangs. Its not clear whether its always possible to recover from this using the global exception handler we currently install on the Python side - we need to explore a bit to see if the handler is being invoked on OOM; whether catching the OOM exception explicitly would help, or if we need an alternative reporting strategy for low-memory conditions. Attached is a log provided by @bhanugandham from a run in a Terra notebook that failed and that exhibited a hang that we assume was due to OOM, and that was resolved by reducing the inference batch size. [gatkStreamingProcessJournal-772629669.txt](https://github.com/broadinstitute/gatk/files/2988819/gatkStreamingProcessJournal-772629669.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5820
https://github.com/broadinstitute/gatk/issues/5820:526,Testability,log,log,526,"Its possible to specify CNN inference size argument values that cause the Python process run out of memory, and the failure mode appears to be the java process hangs. Its not clear whether its always possible to recover from this using the global exception handler we currently install on the Python side - we need to explore a bit to see if the handler is being invoked on OOM; whether catching the OOM exception explicitly would help, or if we need an alternative reporting strategy for low-memory conditions. Attached is a log provided by @bhanugandham from a run in a Terra notebook that failed and that exhibited a hang that we assume was due to OOM, and that was resolved by reducing the inference batch size. [gatkStreamingProcessJournal-772629669.txt](https://github.com/broadinstitute/gatk/files/2988819/gatkStreamingProcessJournal-772629669.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5820
https://github.com/broadinstitute/gatk/issues/5820:175,Usability,clear,clear,175,"Its possible to specify CNN inference size argument values that cause the Python process run out of memory, and the failure mode appears to be the java process hangs. Its not clear whether its always possible to recover from this using the global exception handler we currently install on the Python side - we need to explore a bit to see if the handler is being invoked on OOM; whether catching the OOM exception explicitly would help, or if we need an alternative reporting strategy for low-memory conditions. Attached is a log provided by @bhanugandham from a run in a Terra notebook that failed and that exhibited a hang that we assume was due to OOM, and that was resolved by reducing the inference batch size. [gatkStreamingProcessJournal-772629669.txt](https://github.com/broadinstitute/gatk/files/2988819/gatkStreamingProcessJournal-772629669.txt)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5820
https://github.com/broadinstitute/gatk/issues/5821:488,Availability,error,error,488,"I ran GATK 4.1.0.0 Mutect2 on a small (~1Mb) targeted panel. I am using a normal control that is not the same individual (basically to exclude technical artifacts), so I do expect to see more variants than with a proper matched normal. I was getting around 100-300 variants per sample with GATK 4.0.6.0. I am still roughly in the same range for some samples GATK 4.1.0.0, but I am getting 0 for some. The problem seems to be at the FilterMutectCalls stage where I am seeing the following error:; ```; [March 19, 2019 10:43:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8851030016; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:227); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:79); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821
https://github.com/broadinstitute/gatk/issues/5821:708,Availability,error,errorRate,708,"I ran GATK 4.1.0.0 Mutect2 on a small (~1Mb) targeted panel. I am using a normal control that is not the same individual (basically to exclude technical artifacts), so I do expect to see more variants than with a proper matched normal. I was getting around 100-300 variants per sample with GATK 4.0.6.0. I am still roughly in the same range for some samples GATK 4.1.0.0, but I am getting 0 for some. The problem seems to be at the FilterMutectCalls stage where I am seeing the following error:; ```; [March 19, 2019 10:43:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8851030016; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:227); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:79); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821
https://github.com/broadinstitute/gatk/issues/5821:882,Availability,error,errorProbToQual,882,"I ran GATK 4.1.0.0 Mutect2 on a small (~1Mb) targeted panel. I am using a normal control that is not the same individual (basically to exclude technical artifacts), so I do expect to see more variants than with a proper matched normal. I was getting around 100-300 variants per sample with GATK 4.0.6.0. I am still roughly in the same range for some samples GATK 4.1.0.0, but I am getting 0 for some. The problem seems to be at the FilterMutectCalls stage where I am seeing the following error:; ```; [March 19, 2019 10:43:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8851030016; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:227); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:79); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821
https://github.com/broadinstitute/gatk/issues/5821:974,Availability,error,errorProbToQual,974,"I ran GATK 4.1.0.0 Mutect2 on a small (~1Mb) targeted panel. I am using a normal control that is not the same individual (basically to exclude technical artifacts), so I do expect to see more variants than with a proper matched normal. I was getting around 100-300 variants per sample with GATK 4.0.6.0. I am still roughly in the same range for some samples GATK 4.1.0.0, but I am getting 0 for some. The problem seems to be at the FilterMutectCalls stage where I am seeing the following error:; ```; [March 19, 2019 10:43:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8851030016; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:227); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:79); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821
https://github.com/broadinstitute/gatk/issues/5821:1923,Integrability,wrap,wrapAndCopyInto,1923,tute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:79); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverseVariants(TwoPassVariantWalker.java:74); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.traverse(TwoPassVariantWalker.java:27); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:966); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLin,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821
https://github.com/broadinstitute/gatk/issues/5821:801,Security,validat,validateArg,801,"I ran GATK 4.1.0.0 Mutect2 on a small (~1Mb) targeted panel. I am using a normal control that is not the same individual (basically to exclude technical artifacts), so I do expect to see more variants than with a proper matched normal. I was getting around 100-300 variants per sample with GATK 4.0.6.0. I am still roughly in the same range for some samples GATK 4.1.0.0, but I am getting 0 for some. The problem seems to be at the FilterMutectCalls stage where I am seeing the following error:; ```; [March 19, 2019 10:43:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls done. Elapsed time: 0.04 minutes.; Runtime.totalMemory()=8851030016; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:227); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:211); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.applyContaminationFilter(Mutect2FilteringEngine.java:79); at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2FilteringEngine.calculateFilters(Mutect2FilteringEngine.java:518); at org.broadinstitute.hellbender.tools.walkers.mutect.FilterMutectCalls.firstPassApply(FilterMutectCalls.java:130); at org.broadinstitute.hellbender.engine.TwoPassVariantWalker.lambda$traverseVariants$0(TwoPassVariantWalker.java:76); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$For",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5821
https://github.com/broadinstitute/gatk/issues/5822:698,Availability,mask,masked,698,"While Funcotator does not support GVCF files, using it with one illustrated the following issue:. In a multiallelic variant, it is possible to have a ""best"" transcript that matches one allele, and a different ""best"" transcript that matches another allele. The following variant is such a case:. ```; <HG38>; #CHROM POS ID REF ALT QUAL FILTER INFO; chr1	55063514	.	G	A,<NON_REF>	1000	.	.; ```. For this case, the Gencode funcotations come out as:; `A` -> `PCSK9_MISSENSE_p.R670H`; `<NON_REF>` -> `USP24_COULD_NOT_DETERMINE`. This becomes a problem for the `VcfOutputRenderer`. ----. This phenomenon has to do with symbolic alleles. The fix is to change the logic for when we create funcotations for masked / symbolic alleles.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5822
https://github.com/broadinstitute/gatk/issues/5822:656,Testability,log,logic,656,"While Funcotator does not support GVCF files, using it with one illustrated the following issue:. In a multiallelic variant, it is possible to have a ""best"" transcript that matches one allele, and a different ""best"" transcript that matches another allele. The following variant is such a case:. ```; <HG38>; #CHROM POS ID REF ALT QUAL FILTER INFO; chr1	55063514	.	G	A,<NON_REF>	1000	.	.; ```. For this case, the Gencode funcotations come out as:; `A` -> `PCSK9_MISSENSE_p.R670H`; `<NON_REF>` -> `USP24_COULD_NOT_DETERMINE`. This becomes a problem for the `VcfOutputRenderer`. ----. This phenomenon has to do with symbolic alleles. The fix is to change the logic for when we create funcotations for masked / symbolic alleles.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5822
https://github.com/broadinstitute/gatk/pull/5825:418,Integrability,message,messages,418,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:463,Performance,tune,tune,463,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:55,Testability,log,logging,55,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:169,Testability,log,logging,169,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:555,Testability,log,logging,555,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:629,Testability,log,logging,629,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:730,Testability,log,log,730,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:813,Testability,log,logging,813,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:958,Testability,test,tests,958,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:993,Testability,test,test,993,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:1052,Testability,log,logging,1052,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5825:1109,Testability,log,logging,1109,"Resolves issue #1370 by exposing the setting for Spark logging level with a separate CLI argument (`--spark-verbosity`). Also stifles overly-verbose executor INFO-level logging by setting the default to WARN. This was much cleaner to implement than just applying the existing `--verbosity` to the Spark context for a few reasons. First, Spark's `INFO` level is far too verbose, but most GATK tools provide useful INFO messages. This gives the user the ability to tune them separately. Also, Spark offers more levels through log4j, and mapping from htsjdk logging levels with a bunch of `if` statements didn't seem ideal. Verbose logging only seems to be a problem with when running `/gatk/gatk` inside the GATK docker (eg PathSeq log files easily in 100's of MB). Strangely, however, I found there was no verbose logging if I cloned GATK inside the docker and rebuilt from source with `gradlew buildAll`. Since I can't pinpoint the cause, I haven't included tests for this, but I don't expect test coverage to drop. Note that Spark generates some INFO logging when the context is initialized. After that, the logging level is corrected.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5825
https://github.com/broadinstitute/gatk/pull/5826:158,Availability,down,down,158,"This read filter removes unnmapped reads and reads with unmapped mates. When used in combination with `MateOnSameContigOrNoMappedMateReadFilter` this subsets down to reads only on chrM whose mate is also on chrM. If we only used the `MateOnSameContigOrNoMappedMateReadFilter` we end up with reads whose mate is unmapped still in the BAM, but not the unmapped read, which causes problems downstream in the mitochondria pipeline. This read filter will make the subsetting step faster when we no longer need the NuMTs. I would really appreciate this getting in before the next release (on Tuesday). (fyi @droazen) @ldgauthier @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5826
https://github.com/broadinstitute/gatk/pull/5826:387,Availability,down,downstream,387,"This read filter removes unnmapped reads and reads with unmapped mates. When used in combination with `MateOnSameContigOrNoMappedMateReadFilter` this subsets down to reads only on chrM whose mate is also on chrM. If we only used the `MateOnSameContigOrNoMappedMateReadFilter` we end up with reads whose mate is unmapped still in the BAM, but not the unmapped read, which causes problems downstream in the mitochondria pipeline. This read filter will make the subsetting step faster when we no longer need the NuMTs. I would really appreciate this getting in before the next release (on Tuesday). (fyi @droazen) @ldgauthier @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5826
https://github.com/broadinstitute/gatk/pull/5826:418,Deployability,pipeline,pipeline,418,"This read filter removes unnmapped reads and reads with unmapped mates. When used in combination with `MateOnSameContigOrNoMappedMateReadFilter` this subsets down to reads only on chrM whose mate is also on chrM. If we only used the `MateOnSameContigOrNoMappedMateReadFilter` we end up with reads whose mate is unmapped still in the BAM, but not the unmapped read, which causes problems downstream in the mitochondria pipeline. This read filter will make the subsetting step faster when we no longer need the NuMTs. I would really appreciate this getting in before the next release (on Tuesday). (fyi @droazen) @ldgauthier @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5826
https://github.com/broadinstitute/gatk/pull/5826:574,Deployability,release,release,574,"This read filter removes unnmapped reads and reads with unmapped mates. When used in combination with `MateOnSameContigOrNoMappedMateReadFilter` this subsets down to reads only on chrM whose mate is also on chrM. If we only used the `MateOnSameContigOrNoMappedMateReadFilter` we end up with reads whose mate is unmapped still in the BAM, but not the unmapped read, which causes problems downstream in the mitochondria pipeline. This read filter will make the subsetting step faster when we no longer need the NuMTs. I would really appreciate this getting in before the next release (on Tuesday). (fyi @droazen) @ldgauthier @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5826
https://github.com/broadinstitute/gatk/pull/5827:308,Deployability,release,release,308,"After #5688 we need to adjust some of the default values when in mitochondria mode. I tested these parameters with the mixture samples and it looked good. In particular note that the TLOD divided by depth filter is no longer needed, so the default would now be 0. Again, would love to get this in before the release on Tuesday (fyi @droazen). @ldgauthier @davidbenjamin @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827
https://github.com/broadinstitute/gatk/pull/5827:86,Testability,test,tested,86,"After #5688 we need to adjust some of the default values when in mitochondria mode. I tested these parameters with the mixture samples and it looked good. In particular note that the TLOD divided by depth filter is no longer needed, so the default would now be 0. Again, would love to get this in before the release on Tuesday (fyi @droazen). @ldgauthier @davidbenjamin @jsotobroad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5827
https://github.com/broadinstitute/gatk/pull/5831:364,Modifiability,refactor,refactoring,364,"Closes #5085. This improves validations a bit immediately but more importantly will enable more intelligent filtering on the level of haplotypes, which I think is critical to some of the messy data we have seen. This will also benefit HaplotypeCaller some day. @LeeTL1220 Could you review as far as changes to Mutect2 are concerned?. @droazen Could you review the refactoring of `ReadLikelihoods` / extracting `MoleculeLikelihoods`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831
https://github.com/broadinstitute/gatk/pull/5831:28,Security,validat,validations,28,"Closes #5085. This improves validations a bit immediately but more importantly will enable more intelligent filtering on the level of haplotypes, which I think is critical to some of the messy data we have seen. This will also benefit HaplotypeCaller some day. @LeeTL1220 Could you review as far as changes to Mutect2 are concerned?. @droazen Could you review the refactoring of `ReadLikelihoods` / extracting `MoleculeLikelihoods`?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5831
https://github.com/broadinstitute/gatk/pull/5834:95,Security,sanitiz,sanitized,95,Added in code to change how the best transcript is determined.; Added `#` as a character to be sanitized by `VCFOutputRenderer`. (#5817); Fixes #5822,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5834
https://github.com/broadinstitute/gatk/issues/5836:507,Deployability,pipeline,pipeline,507,"Dear @sooheelee . I am wondering if I need to perform base recalibration on the normal bam files before I proceed with creating the panel of normals, which will then be used for somatic mutation calling with MuTect2. . The tutorial (https://software.broadinstitute.org/gatk/documentation/article?id=11136) and the documentation (https://software.broadinstitute.org/gatk/documentation/article?id=11127) suggest to me that baseRecalibration is not required, but I wanted to confirm before proceeding with the pipeline. Regards,; Sangjin Lee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5836
https://github.com/broadinstitute/gatk/issues/5836:46,Performance,perform,perform,46,"Dear @sooheelee . I am wondering if I need to perform base recalibration on the normal bam files before I proceed with creating the panel of normals, which will then be used for somatic mutation calling with MuTect2. . The tutorial (https://software.broadinstitute.org/gatk/documentation/article?id=11136) and the documentation (https://software.broadinstitute.org/gatk/documentation/article?id=11127) suggest to me that baseRecalibration is not required, but I wanted to confirm before proceeding with the pipeline. Regards,; Sangjin Lee",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5836
https://github.com/broadinstitute/gatk/issues/5838:743,Deployability,upgrade,upgrade,743,"htsjdk has had a bug for a while where it permissively would read any BCF 2.x version if x was greater than or equal to 1 (it only supports BCF 2.1, not version 2.2). In practical terms this didn't matter because BCF 2.2 uses ""IDX"" fields in some header lines, and those were rejected by htsjdk anyway. However, a recently change in htsjdk causes it to accept additional fields in header lines, so BCF 2.2 is no longer rejected due to the presence of these fields. . IndexFeatureFile `testUncompressedBCF2_2Index` is a negative test that relies on the fact that BCF2.2 is rejected, so its currently disabled. The BCF version checking is fixed in later versions of htsjdk (https://github.com/samtools/htsjdk/issues/1323), so at the next htsjdk upgrade, `testUncompressedBCF2_2Index` should be reenabled (though it might throw a different exception).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5838
https://github.com/broadinstitute/gatk/issues/5838:528,Testability,test,test,528,"htsjdk has had a bug for a while where it permissively would read any BCF 2.x version if x was greater than or equal to 1 (it only supports BCF 2.1, not version 2.2). In practical terms this didn't matter because BCF 2.2 uses ""IDX"" fields in some header lines, and those were rejected by htsjdk anyway. However, a recently change in htsjdk causes it to accept additional fields in header lines, so BCF 2.2 is no longer rejected due to the presence of these fields. . IndexFeatureFile `testUncompressedBCF2_2Index` is a negative test that relies on the fact that BCF2.2 is rejected, so its currently disabled. The BCF version checking is fixed in later versions of htsjdk (https://github.com/samtools/htsjdk/issues/1323), so at the next htsjdk upgrade, `testUncompressedBCF2_2Index` should be reenabled (though it might throw a different exception).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5838
https://github.com/broadinstitute/gatk/issues/5839:25,Deployability,release,release,25,"The BCFCodec in the next release of htsjdk (after 2.19) will reject any BCF input that is greater than BCF 2.1 (see https://github.com/broadinstitute/gatk/issues/5838 and https://github.com/samtools/htsjdk/issues/1323). However, GenomicsDB uses htslib, which generates version 2.2 output, to create BCF streams for GATK (with the BCF IDX fields removed). This will no longer work with post-2.19 htsjdk versions. Since GATK bypasses codec discovery and provides the codec directly for GenomicsDB inputs, the proposed solution is to change to the BCF codec in htsjdk to delegate version checking to an overridable method(!), and then provide a subclassed codec in GATK that has relaxed version checking.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5839
https://github.com/broadinstitute/gatk/pull/5840:552,Deployability,pipeline,pipeline,552,"@takutosato Most of the files changed are just due to a change in method signature. Any significant block of ""new"" code is just your code moved to a different class. This PR does a few things:. * Move logic from the orientation bias annotation into the filter.; * Package F1R2 counts and learned orientation bias models in .tar.gz files to simplify command lines and accomodate multiple samples.; * Make all orientation bias tools fully multi-sample.; *Extract a backend for CollectF1R2Counts and use this backend inside Mutect2. I have tested the new pipeline on Firecloud. Do you have time to review this before the release tomorrow? If not, I can ask Lee.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840
https://github.com/broadinstitute/gatk/pull/5840:618,Deployability,release,release,618,"@takutosato Most of the files changed are just due to a change in method signature. Any significant block of ""new"" code is just your code moved to a different class. This PR does a few things:. * Move logic from the orientation bias annotation into the filter.; * Package F1R2 counts and learned orientation bias models in .tar.gz files to simplify command lines and accomodate multiple samples.; * Make all orientation bias tools fully multi-sample.; *Extract a backend for CollectF1R2Counts and use this backend inside Mutect2. I have tested the new pipeline on Firecloud. Do you have time to review this before the release tomorrow? If not, I can ask Lee.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840
https://github.com/broadinstitute/gatk/pull/5840:201,Testability,log,logic,201,"@takutosato Most of the files changed are just due to a change in method signature. Any significant block of ""new"" code is just your code moved to a different class. This PR does a few things:. * Move logic from the orientation bias annotation into the filter.; * Package F1R2 counts and learned orientation bias models in .tar.gz files to simplify command lines and accomodate multiple samples.; * Make all orientation bias tools fully multi-sample.; *Extract a backend for CollectF1R2Counts and use this backend inside Mutect2. I have tested the new pipeline on Firecloud. Do you have time to review this before the release tomorrow? If not, I can ask Lee.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840
https://github.com/broadinstitute/gatk/pull/5840:537,Testability,test,tested,537,"@takutosato Most of the files changed are just due to a change in method signature. Any significant block of ""new"" code is just your code moved to a different class. This PR does a few things:. * Move logic from the orientation bias annotation into the filter.; * Package F1R2 counts and learned orientation bias models in .tar.gz files to simplify command lines and accomodate multiple samples.; * Make all orientation bias tools fully multi-sample.; *Extract a backend for CollectF1R2Counts and use this backend inside Mutect2. I have tested the new pipeline on Firecloud. Do you have time to review this before the release tomorrow? If not, I can ask Lee.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840
https://github.com/broadinstitute/gatk/pull/5840:288,Usability,learn,learned,288,"@takutosato Most of the files changed are just due to a change in method signature. Any significant block of ""new"" code is just your code moved to a different class. This PR does a few things:. * Move logic from the orientation bias annotation into the filter.; * Package F1R2 counts and learned orientation bias models in .tar.gz files to simplify command lines and accomodate multiple samples.; * Make all orientation bias tools fully multi-sample.; *Extract a backend for CollectF1R2Counts and use this backend inside Mutect2. I have tested the new pipeline on Firecloud. Do you have time to review this before the release tomorrow? If not, I can ask Lee.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840
https://github.com/broadinstitute/gatk/pull/5840:340,Usability,simpl,simplify,340,"@takutosato Most of the files changed are just due to a change in method signature. Any significant block of ""new"" code is just your code moved to a different class. This PR does a few things:. * Move logic from the orientation bias annotation into the filter.; * Package F1R2 counts and learned orientation bias models in .tar.gz files to simplify command lines and accomodate multiple samples.; * Make all orientation bias tools fully multi-sample.; *Extract a backend for CollectF1R2Counts and use this backend inside Mutect2. I have tested the new pipeline on Firecloud. Do you have time to review this before the release tomorrow? If not, I can ask Lee.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5840
https://github.com/broadinstitute/gatk/issues/5841:287,Availability,down,downloaded,287,"## Feature request. ### Tool(s) or class(es) involved; M2 WDL and FC deployment of M2. ### Description; We should specify the same file listed in the GATK forum (https://gatkforums.broadinstitute.org/gatk/discussion/4154/howto-install-and-run-oncotator-for-the-first-time), which can be downloaded from: https://personal.broadinstitute.org/lichtens/oncobeta/tx_exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt; This should be used as the default transcript selection list for funcotator (@jonn-smith I assume that funcotator and oncotator use the same format for this file. Please confirm.). The only time you would not want this file is if you are not running on hg19. For other references, ideally, we would want different lists. - This list needs to be put into a bucket (gatk-best-practices?); - Please notify @bshifaw for deployment in the FC featured workspace in the appropriate funcotator parameter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5841
https://github.com/broadinstitute/gatk/issues/5841:69,Deployability,deploy,deployment,69,"## Feature request. ### Tool(s) or class(es) involved; M2 WDL and FC deployment of M2. ### Description; We should specify the same file listed in the GATK forum (https://gatkforums.broadinstitute.org/gatk/discussion/4154/howto-install-and-run-oncotator-for-the-first-time), which can be downloaded from: https://personal.broadinstitute.org/lichtens/oncobeta/tx_exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt; This should be used as the default transcript selection list for funcotator (@jonn-smith I assume that funcotator and oncotator use the same format for this file. Please confirm.). The only time you would not want this file is if you are not running on hg19. For other references, ideally, we would want different lists. - This list needs to be put into a bucket (gatk-best-practices?); - Please notify @bshifaw for deployment in the FC featured workspace in the appropriate funcotator parameter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5841
https://github.com/broadinstitute/gatk/issues/5841:227,Deployability,install,install-and-run-oncotator-for-the-first-time,227,"## Feature request. ### Tool(s) or class(es) involved; M2 WDL and FC deployment of M2. ### Description; We should specify the same file listed in the GATK forum (https://gatkforums.broadinstitute.org/gatk/discussion/4154/howto-install-and-run-oncotator-for-the-first-time), which can be downloaded from: https://personal.broadinstitute.org/lichtens/oncobeta/tx_exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt; This should be used as the default transcript selection list for funcotator (@jonn-smith I assume that funcotator and oncotator use the same format for this file. Please confirm.). The only time you would not want this file is if you are not running on hg19. For other references, ideally, we would want different lists. - This list needs to be put into a bucket (gatk-best-practices?); - Please notify @bshifaw for deployment in the FC featured workspace in the appropriate funcotator parameter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5841
https://github.com/broadinstitute/gatk/issues/5841:821,Deployability,deploy,deployment,821,"## Feature request. ### Tool(s) or class(es) involved; M2 WDL and FC deployment of M2. ### Description; We should specify the same file listed in the GATK forum (https://gatkforums.broadinstitute.org/gatk/discussion/4154/howto-install-and-run-oncotator-for-the-first-time), which can be downloaded from: https://personal.broadinstitute.org/lichtens/oncobeta/tx_exact_uniprot_matches.AKT1_CRLF2_FGFR1.txt; This should be used as the default transcript selection list for funcotator (@jonn-smith I assume that funcotator and oncotator use the same format for this file. Please confirm.). The only time you would not want this file is if you are not running on hg19. For other references, ideally, we would want different lists. - This list needs to be put into a bucket (gatk-best-practices?); - Please notify @bshifaw for deployment in the FC featured workspace in the appropriate funcotator parameter.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5841
https://github.com/broadinstitute/gatk/pull/5842:113,Deployability,pipeline,pipelines,113,"This adds a hard filter for low variant allele fraction calls. We will not turn this on by default in any of our pipelines, but it will give users an easy option to filter everything below a certain VAF that they don't care about. It also adds a hard filter for low alt depth calls based on a threshold from the median autosomal coverage (that must be supplied as an argument). It takes the cutoff from a Poisson with a mean of 1.5 * median coverage (to account for NuMTs with 3 copies in the autosome) and is tuned to catch 99% of the false positives (which we know will also catch lots of true positives). . It also removes the Polymorphic NUMT annotation (since that's basically what's going into the filter at this point).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842
https://github.com/broadinstitute/gatk/pull/5842:630,Modifiability,Polymorphi,Polymorphic,630,"This adds a hard filter for low variant allele fraction calls. We will not turn this on by default in any of our pipelines, but it will give users an easy option to filter everything below a certain VAF that they don't care about. It also adds a hard filter for low alt depth calls based on a threshold from the median autosomal coverage (that must be supplied as an argument). It takes the cutoff from a Poisson with a mean of 1.5 * median coverage (to account for NuMTs with 3 copies in the autosome) and is tuned to catch 99% of the false positives (which we know will also catch lots of true positives). . It also removes the Polymorphic NUMT annotation (since that's basically what's going into the filter at this point).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842
https://github.com/broadinstitute/gatk/pull/5842:510,Performance,tune,tuned,510,"This adds a hard filter for low variant allele fraction calls. We will not turn this on by default in any of our pipelines, but it will give users an easy option to filter everything below a certain VAF that they don't care about. It also adds a hard filter for low alt depth calls based on a threshold from the median autosomal coverage (that must be supplied as an argument). It takes the cutoff from a Poisson with a mean of 1.5 * median coverage (to account for NuMTs with 3 copies in the autosome) and is tuned to catch 99% of the false positives (which we know will also catch lots of true positives). . It also removes the Polymorphic NUMT annotation (since that's basically what's going into the filter at this point).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5842
https://github.com/broadinstitute/gatk/pull/5844:28,Testability,test,test,28,"Fix botched squash, clarify test asserts. Fixes #5650",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5844
https://github.com/broadinstitute/gatk/pull/5844:33,Testability,assert,asserts,33,"Fix botched squash, clarify test asserts. Fixes #5650",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5844
https://github.com/broadinstitute/gatk/issues/5845:50,Availability,error,error,50,"I just tried Mutect2 from GATK 4.1.1.0 and got an error:; ```; A USER ERROR has occurred: standard-min-confidence-threshold-for-calling is not a recognized option; ```. From the [online documentation](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_mutect_Mutect2.php):. > Note that the default was changed from 10.0 to 30.0 in version 4.1.0.0 to accompany the switch to use the the new quality score by default. Thus, it was still maintained in 4.1.0.0. Based on that, I am surprised that it was removed. There are actually a few other parameters that got dropped. For example, `normal-artifact-lod` from `FilterMutectCalls`. Are these changes explained somewhere?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5845
https://github.com/broadinstitute/gatk/issues/5845:70,Availability,ERROR,ERROR,70,"I just tried Mutect2 from GATK 4.1.1.0 and got an error:; ```; A USER ERROR has occurred: standard-min-confidence-threshold-for-calling is not a recognized option; ```. From the [online documentation](https://software.broadinstitute.org/gatk/documentation/tooldocs/4.1.0.0/org_broadinstitute_hellbender_tools_walkers_mutect_Mutect2.php):. > Note that the default was changed from 10.0 to 30.0 in version 4.1.0.0 to accompany the switch to use the the new quality score by default. Thus, it was still maintained in 4.1.0.0. Based on that, I am surprised that it was removed. There are actually a few other parameters that got dropped. For example, `normal-artifact-lod` from `FilterMutectCalls`. Are these changes explained somewhere?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5845
https://github.com/broadinstitute/gatk/issues/5846:306,Energy Efficiency,reduce,reduce,306,"These are both `@Advanced` tool arguments, but they don't seem to impact the number of threads used by Python, based both on my own results and those reported in [this forum post](https://gatkforums.broadinstitute.org/gatk/discussion/comment/57482#Comment_57482). Setting OMP_NUM_THREADS to 1 does seem to reduce the number of threads used by python though.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5846
https://github.com/broadinstitute/gatk/pull/5847:148,Deployability,Update,Updates,148,"Tests are not passing because I'm now using NIO in the WDL. I'll need to fix that, but the WDL itself should be ready for review. . The changes:; - Updates the pipeline for the new Mutect2 Filtering scheme and pulls filtering after the liftover and recombining of the VCF. ; - Makes the subsetting of the WGS bam fast by using PrintReads over just chrM instead of traversing the whole bam for NuMT mates.; - Moves polymorphic NuMTs based on autosomal coverage to a filter (it was an annotation before); - Adds option to hard filter by VAF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5847
https://github.com/broadinstitute/gatk/pull/5847:160,Deployability,pipeline,pipeline,160,"Tests are not passing because I'm now using NIO in the WDL. I'll need to fix that, but the WDL itself should be ready for review. . The changes:; - Updates the pipeline for the new Mutect2 Filtering scheme and pulls filtering after the liftover and recombining of the VCF. ; - Makes the subsetting of the WGS bam fast by using PrintReads over just chrM instead of traversing the whole bam for NuMT mates.; - Moves polymorphic NuMTs based on autosomal coverage to a filter (it was an annotation before); - Adds option to hard filter by VAF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5847
https://github.com/broadinstitute/gatk/pull/5847:414,Modifiability,polymorphi,polymorphic,414,"Tests are not passing because I'm now using NIO in the WDL. I'll need to fix that, but the WDL itself should be ready for review. . The changes:; - Updates the pipeline for the new Mutect2 Filtering scheme and pulls filtering after the liftover and recombining of the VCF. ; - Makes the subsetting of the WGS bam fast by using PrintReads over just chrM instead of traversing the whole bam for NuMT mates.; - Moves polymorphic NuMTs based on autosomal coverage to a filter (it was an annotation before); - Adds option to hard filter by VAF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5847
https://github.com/broadinstitute/gatk/pull/5847:0,Testability,Test,Tests,0,"Tests are not passing because I'm now using NIO in the WDL. I'll need to fix that, but the WDL itself should be ready for review. . The changes:; - Updates the pipeline for the new Mutect2 Filtering scheme and pulls filtering after the liftover and recombining of the VCF. ; - Makes the subsetting of the WGS bam fast by using PrintReads over just chrM instead of traversing the whole bam for NuMT mates.; - Moves polymorphic NuMTs based on autosomal coverage to a filter (it was an annotation before); - Adds option to hard filter by VAF",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5847
https://github.com/broadinstitute/gatk/issues/5848:120,Deployability,release,release,120,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1.1.0. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 3/31/2019. ### Description ; It looks like PR #5840 did a lot of refactoring to the way F1R2/F2R1 annotations are computed. Along the way it looks like `OxoGReadCounts` was renamed to `OrientationBiasReadCounts`. This is, unfortunately for some, a non-backwards compatible change as any pipeline that uses `-A OxoGReadCounts` will now fail. I'm not sure if there's a deprecation mechanism for annotations that would inform users of this, and I'm not sure there's a whole lot to be done at this point. I'm logging this issue mainly so anyone else who runs into this will find the answer quickly. Might be nice to add a line to the 4.1.1.0 release notes though noting this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5848
https://github.com/broadinstitute/gatk/issues/5848:457,Deployability,pipeline,pipeline,457,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1.1.0. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 3/31/2019. ### Description ; It looks like PR #5840 did a lot of refactoring to the way F1R2/F2R1 annotations are computed. Along the way it looks like `OxoGReadCounts` was renamed to `OrientationBiasReadCounts`. This is, unfortunately for some, a non-backwards compatible change as any pipeline that uses `-A OxoGReadCounts` will now fail. I'm not sure if there's a deprecation mechanism for annotations that would inform users of this, and I'm not sure there's a whole lot to be done at this point. I'm logging this issue mainly so anyone else who runs into this will find the answer quickly. Might be nice to add a line to the 4.1.1.0 release notes though noting this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5848
https://github.com/broadinstitute/gatk/issues/5848:808,Deployability,release,release,808,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1.1.0. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 3/31/2019. ### Description ; It looks like PR #5840 did a lot of refactoring to the way F1R2/F2R1 annotations are computed. Along the way it looks like `OxoGReadCounts` was renamed to `OrientationBiasReadCounts`. This is, unfortunately for some, a non-backwards compatible change as any pipeline that uses `-A OxoGReadCounts` will now fail. I'm not sure if there's a deprecation mechanism for annotations that would inform users of this, and I'm not sure there's a whole lot to be done at this point. I'm logging this issue mainly so anyone else who runs into this will find the answer quickly. Might be nice to add a line to the 4.1.1.0 release notes though noting this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5848
https://github.com/broadinstitute/gatk/issues/5848:235,Modifiability,refactor,refactoring,235,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1.1.0. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 3/31/2019. ### Description ; It looks like PR #5840 did a lot of refactoring to the way F1R2/F2R1 annotations are computed. Along the way it looks like `OxoGReadCounts` was renamed to `OrientationBiasReadCounts`. This is, unfortunately for some, a non-backwards compatible change as any pipeline that uses `-A OxoGReadCounts` will now fail. I'm not sure if there's a deprecation mechanism for annotations that would inform users of this, and I'm not sure there's a whole lot to be done at this point. I'm logging this issue mainly so anyone else who runs into this will find the answer quickly. Might be nice to add a line to the 4.1.1.0 release notes though noting this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5848
https://github.com/broadinstitute/gatk/issues/5848:675,Testability,log,logging,675,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller 4.1.1.0. ### Affected version(s); - [x] Latest public release version; - [x] Latest master branch as of 3/31/2019. ### Description ; It looks like PR #5840 did a lot of refactoring to the way F1R2/F2R1 annotations are computed. Along the way it looks like `OxoGReadCounts` was renamed to `OrientationBiasReadCounts`. This is, unfortunately for some, a non-backwards compatible change as any pipeline that uses `-A OxoGReadCounts` will now fail. I'm not sure if there's a deprecation mechanism for annotations that would inform users of this, and I'm not sure there's a whole lot to be done at this point. I'm logging this issue mainly so anyone else who runs into this will find the answer quickly. Might be nice to add a line to the 4.1.1.0 release notes though noting this change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5848
https://github.com/broadinstitute/gatk/pull/5849:149,Energy Efficiency,schedul,scheduler,149,"…al memory (as opposed to resident memory) under control. This solves (I think) a long-time problem for anyone using the GATK under SGE or any other scheduler that imposes hard limits on _virual_ memory. The posting at this link describes in detail what is going on:. https://www.ibm.com/developerworks/community/blogs/kevgrig/entry/linux_glibc_2_10_rhel_6_malloc_may_show_excessive_virtual_memory_usage?. TL;DR: there was a change in `malloc` in `glibc` several years ago that attempts to make memory allocation more efficient in multi-threaded apps on multi-core machines, by creating many memory pools (arenas) from which allocation requests are satisfied. On systems with lots of CPUs it can cause virtual memory usage to balloon up to many times the heap size (e.g. we see 30GB VIRT with -Xmx4G and < 4G resident). In some very limited testing I didn't see any significant performance change from limiting the number of arenas. My suspicion is that since Java is allocating fairly large blocks of memory using `malloc` and then allocating internal to the JVM this shouldn't have much if any affect on Java programs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5849
https://github.com/broadinstitute/gatk/pull/5849:518,Energy Efficiency,efficient,efficient,518,"…al memory (as opposed to resident memory) under control. This solves (I think) a long-time problem for anyone using the GATK under SGE or any other scheduler that imposes hard limits on _virual_ memory. The posting at this link describes in detail what is going on:. https://www.ibm.com/developerworks/community/blogs/kevgrig/entry/linux_glibc_2_10_rhel_6_malloc_may_show_excessive_virtual_memory_usage?. TL;DR: there was a change in `malloc` in `glibc` several years ago that attempts to make memory allocation more efficient in multi-threaded apps on multi-core machines, by creating many memory pools (arenas) from which allocation requests are satisfied. On systems with lots of CPUs it can cause virtual memory usage to balloon up to many times the heap size (e.g. we see 30GB VIRT with -Xmx4G and < 4G resident). In some very limited testing I didn't see any significant performance change from limiting the number of arenas. My suspicion is that since Java is allocating fairly large blocks of memory using `malloc` and then allocating internal to the JVM this shouldn't have much if any affect on Java programs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5849
https://github.com/broadinstitute/gatk/pull/5849:531,Performance,multi-thread,multi-threaded,531,"…al memory (as opposed to resident memory) under control. This solves (I think) a long-time problem for anyone using the GATK under SGE or any other scheduler that imposes hard limits on _virual_ memory. The posting at this link describes in detail what is going on:. https://www.ibm.com/developerworks/community/blogs/kevgrig/entry/linux_glibc_2_10_rhel_6_malloc_may_show_excessive_virtual_memory_usage?. TL;DR: there was a change in `malloc` in `glibc` several years ago that attempts to make memory allocation more efficient in multi-threaded apps on multi-core machines, by creating many memory pools (arenas) from which allocation requests are satisfied. On systems with lots of CPUs it can cause virtual memory usage to balloon up to many times the heap size (e.g. we see 30GB VIRT with -Xmx4G and < 4G resident). In some very limited testing I didn't see any significant performance change from limiting the number of arenas. My suspicion is that since Java is allocating fairly large blocks of memory using `malloc` and then allocating internal to the JVM this shouldn't have much if any affect on Java programs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5849
https://github.com/broadinstitute/gatk/pull/5849:878,Performance,perform,performance,878,"…al memory (as opposed to resident memory) under control. This solves (I think) a long-time problem for anyone using the GATK under SGE or any other scheduler that imposes hard limits on _virual_ memory. The posting at this link describes in detail what is going on:. https://www.ibm.com/developerworks/community/blogs/kevgrig/entry/linux_glibc_2_10_rhel_6_malloc_may_show_excessive_virtual_memory_usage?. TL;DR: there was a change in `malloc` in `glibc` several years ago that attempts to make memory allocation more efficient in multi-threaded apps on multi-core machines, by creating many memory pools (arenas) from which allocation requests are satisfied. On systems with lots of CPUs it can cause virtual memory usage to balloon up to many times the heap size (e.g. we see 30GB VIRT with -Xmx4G and < 4G resident). In some very limited testing I didn't see any significant performance change from limiting the number of arenas. My suspicion is that since Java is allocating fairly large blocks of memory using `malloc` and then allocating internal to the JVM this shouldn't have much if any affect on Java programs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5849
https://github.com/broadinstitute/gatk/pull/5849:841,Testability,test,testing,841,"…al memory (as opposed to resident memory) under control. This solves (I think) a long-time problem for anyone using the GATK under SGE or any other scheduler that imposes hard limits on _virual_ memory. The posting at this link describes in detail what is going on:. https://www.ibm.com/developerworks/community/blogs/kevgrig/entry/linux_glibc_2_10_rhel_6_malloc_may_show_excessive_virtual_memory_usage?. TL;DR: there was a change in `malloc` in `glibc` several years ago that attempts to make memory allocation more efficient in multi-threaded apps on multi-core machines, by creating many memory pools (arenas) from which allocation requests are satisfied. On systems with lots of CPUs it can cause virtual memory usage to balloon up to many times the heap size (e.g. we see 30GB VIRT with -Xmx4G and < 4G resident). In some very limited testing I didn't see any significant performance change from limiting the number of arenas. My suspicion is that since Java is allocating fairly large blocks of memory using `malloc` and then allocating internal to the JVM this shouldn't have much if any affect on Java programs.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5849
https://github.com/broadinstitute/gatk/issues/5850:1476,Energy Efficiency,reduce,reduces,1476,"## Documentation request; Filing issue at the request of @samuelklee stemming from the discussion in https://github.com/broadinstitute/gatk/pull/5829. ### Tool(s) or class(es) involved; GermlineCNVCaller and related tools. . ### Description ; In particular, I am currently needing information for the gCNV tutorial writeup for the following parameters. What direction increases sensitivity?. - `--depth-correction-tau` has a default of 10000.0 (10K) and defines the precision of read-depth concordance with the global depth value.; - `--p-active` has a default of 1e-2 (0.01) and defines the expected probability of CNV events.; - `p-alt` has a default of 1e-6 (0.000001) and defines the prior probability of CNV states. . Here are some other parameters of particular interest and descriptions I worked on with help from @mwalker174 and @samuelklee:; - Decreasing `--class-coherence-length` from its default of 10,000bp to 1000bp decreases the expected length of contiguous segments. Factor for bin size when tuning. ; - Decreasing `--cnv-coherence-length` from its default 10,000bp to 1000bp decreases the expected length of CNV events. Factor for bin size when tuning. ; - Turning off `--enable-bias-factors` from the default `true` state to `false` turns off active discovery of learnable bias factors. This should always be on for targeted exome data and in general can be turned off for WGS data. ; - Decreasing `--interval-psi-scale` from its default of 0.001 to 1.0E-6 reduces the scale the tool considers normal in per-interval noise.; - Decreasing `--log-mean-bias-standard-deviation` from its default of 0.1 to 0.01 reduces what is considered normal noise in bias factors.; - Decreasing `--sample-psi-scale` from its default of 0.0001 to 1.0E-6 reduces the scale that is considered normal in sample-to-sample variance. . In general, all of the parameter descriptions could be friendlier. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5850
https://github.com/broadinstitute/gatk/issues/5850:1626,Energy Efficiency,reduce,reduces,1626,"## Documentation request; Filing issue at the request of @samuelklee stemming from the discussion in https://github.com/broadinstitute/gatk/pull/5829. ### Tool(s) or class(es) involved; GermlineCNVCaller and related tools. . ### Description ; In particular, I am currently needing information for the gCNV tutorial writeup for the following parameters. What direction increases sensitivity?. - `--depth-correction-tau` has a default of 10000.0 (10K) and defines the precision of read-depth concordance with the global depth value.; - `--p-active` has a default of 1e-2 (0.01) and defines the expected probability of CNV events.; - `p-alt` has a default of 1e-6 (0.000001) and defines the prior probability of CNV states. . Here are some other parameters of particular interest and descriptions I worked on with help from @mwalker174 and @samuelklee:; - Decreasing `--class-coherence-length` from its default of 10,000bp to 1000bp decreases the expected length of contiguous segments. Factor for bin size when tuning. ; - Decreasing `--cnv-coherence-length` from its default 10,000bp to 1000bp decreases the expected length of CNV events. Factor for bin size when tuning. ; - Turning off `--enable-bias-factors` from the default `true` state to `false` turns off active discovery of learnable bias factors. This should always be on for targeted exome data and in general can be turned off for WGS data. ; - Decreasing `--interval-psi-scale` from its default of 0.001 to 1.0E-6 reduces the scale the tool considers normal in per-interval noise.; - Decreasing `--log-mean-bias-standard-deviation` from its default of 0.1 to 0.01 reduces what is considered normal noise in bias factors.; - Decreasing `--sample-psi-scale` from its default of 0.0001 to 1.0E-6 reduces the scale that is considered normal in sample-to-sample variance. . In general, all of the parameter descriptions could be friendlier. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5850
https://github.com/broadinstitute/gatk/issues/5850:1755,Energy Efficiency,reduce,reduces,1755,"## Documentation request; Filing issue at the request of @samuelklee stemming from the discussion in https://github.com/broadinstitute/gatk/pull/5829. ### Tool(s) or class(es) involved; GermlineCNVCaller and related tools. . ### Description ; In particular, I am currently needing information for the gCNV tutorial writeup for the following parameters. What direction increases sensitivity?. - `--depth-correction-tau` has a default of 10000.0 (10K) and defines the precision of read-depth concordance with the global depth value.; - `--p-active` has a default of 1e-2 (0.01) and defines the expected probability of CNV events.; - `p-alt` has a default of 1e-6 (0.000001) and defines the prior probability of CNV states. . Here are some other parameters of particular interest and descriptions I worked on with help from @mwalker174 and @samuelklee:; - Decreasing `--class-coherence-length` from its default of 10,000bp to 1000bp decreases the expected length of contiguous segments. Factor for bin size when tuning. ; - Decreasing `--cnv-coherence-length` from its default 10,000bp to 1000bp decreases the expected length of CNV events. Factor for bin size when tuning. ; - Turning off `--enable-bias-factors` from the default `true` state to `false` turns off active discovery of learnable bias factors. This should always be on for targeted exome data and in general can be turned off for WGS data. ; - Decreasing `--interval-psi-scale` from its default of 0.001 to 1.0E-6 reduces the scale the tool considers normal in per-interval noise.; - Decreasing `--log-mean-bias-standard-deviation` from its default of 0.1 to 0.01 reduces what is considered normal noise in bias factors.; - Decreasing `--sample-psi-scale` from its default of 0.0001 to 1.0E-6 reduces the scale that is considered normal in sample-to-sample variance. . In general, all of the parameter descriptions could be friendlier. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5850
https://github.com/broadinstitute/gatk/issues/5850:1560,Testability,log,log-mean-bias-standard-deviation,1560,"## Documentation request; Filing issue at the request of @samuelklee stemming from the discussion in https://github.com/broadinstitute/gatk/pull/5829. ### Tool(s) or class(es) involved; GermlineCNVCaller and related tools. . ### Description ; In particular, I am currently needing information for the gCNV tutorial writeup for the following parameters. What direction increases sensitivity?. - `--depth-correction-tau` has a default of 10000.0 (10K) and defines the precision of read-depth concordance with the global depth value.; - `--p-active` has a default of 1e-2 (0.01) and defines the expected probability of CNV events.; - `p-alt` has a default of 1e-6 (0.000001) and defines the prior probability of CNV states. . Here are some other parameters of particular interest and descriptions I worked on with help from @mwalker174 and @samuelklee:; - Decreasing `--class-coherence-length` from its default of 10,000bp to 1000bp decreases the expected length of contiguous segments. Factor for bin size when tuning. ; - Decreasing `--cnv-coherence-length` from its default 10,000bp to 1000bp decreases the expected length of CNV events. Factor for bin size when tuning. ; - Turning off `--enable-bias-factors` from the default `true` state to `false` turns off active discovery of learnable bias factors. This should always be on for targeted exome data and in general can be turned off for WGS data. ; - Decreasing `--interval-psi-scale` from its default of 0.001 to 1.0E-6 reduces the scale the tool considers normal in per-interval noise.; - Decreasing `--log-mean-bias-standard-deviation` from its default of 0.1 to 0.01 reduces what is considered normal noise in bias factors.; - Decreasing `--sample-psi-scale` from its default of 0.0001 to 1.0E-6 reduces the scale that is considered normal in sample-to-sample variance. . In general, all of the parameter descriptions could be friendlier. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5850
https://github.com/broadinstitute/gatk/issues/5850:1282,Usability,learn,learnable,1282,"## Documentation request; Filing issue at the request of @samuelklee stemming from the discussion in https://github.com/broadinstitute/gatk/pull/5829. ### Tool(s) or class(es) involved; GermlineCNVCaller and related tools. . ### Description ; In particular, I am currently needing information for the gCNV tutorial writeup for the following parameters. What direction increases sensitivity?. - `--depth-correction-tau` has a default of 10000.0 (10K) and defines the precision of read-depth concordance with the global depth value.; - `--p-active` has a default of 1e-2 (0.01) and defines the expected probability of CNV events.; - `p-alt` has a default of 1e-6 (0.000001) and defines the prior probability of CNV states. . Here are some other parameters of particular interest and descriptions I worked on with help from @mwalker174 and @samuelklee:; - Decreasing `--class-coherence-length` from its default of 10,000bp to 1000bp decreases the expected length of contiguous segments. Factor for bin size when tuning. ; - Decreasing `--cnv-coherence-length` from its default 10,000bp to 1000bp decreases the expected length of CNV events. Factor for bin size when tuning. ; - Turning off `--enable-bias-factors` from the default `true` state to `false` turns off active discovery of learnable bias factors. This should always be on for targeted exome data and in general can be turned off for WGS data. ; - Decreasing `--interval-psi-scale` from its default of 0.001 to 1.0E-6 reduces the scale the tool considers normal in per-interval noise.; - Decreasing `--log-mean-bias-standard-deviation` from its default of 0.1 to 0.01 reduces what is considered normal noise in bias factors.; - Decreasing `--sample-psi-scale` from its default of 0.0001 to 1.0E-6 reduces the scale that is considered normal in sample-to-sample variance. . In general, all of the parameter descriptions could be friendlier. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5850
https://github.com/broadinstitute/gatk/issues/5851:13,Deployability,install,install,13,"We currently install v4.3.3, which is quite a few releases behind the current version v4.6.8. The preferred method for activating/deactivating has changed, and the method we currently suggest/use (`source activate`) is [deprecated](https://github.com/conda/conda/releases/tag/4.4.0) and has been replaced with conda commands. The readme/online doc should be modified to reflect these changes as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5851
https://github.com/broadinstitute/gatk/issues/5851:50,Deployability,release,releases,50,"We currently install v4.3.3, which is quite a few releases behind the current version v4.6.8. The preferred method for activating/deactivating has changed, and the method we currently suggest/use (`source activate`) is [deprecated](https://github.com/conda/conda/releases/tag/4.4.0) and has been replaced with conda commands. The readme/online doc should be modified to reflect these changes as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5851
https://github.com/broadinstitute/gatk/issues/5851:263,Deployability,release,releases,263,"We currently install v4.3.3, which is quite a few releases behind the current version v4.6.8. The preferred method for activating/deactivating has changed, and the method we currently suggest/use (`source activate`) is [deprecated](https://github.com/conda/conda/releases/tag/4.4.0) and has been replaced with conda commands. The readme/online doc should be modified to reflect these changes as well.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5851
https://github.com/broadinstitute/gatk/issues/5852:71,Availability,error,error-using-postprocessgermlinecnvcalls,71,See https://gatkforums.broadinstitute.org/gatk/discussion/23793/python-error-using-postprocessgermlinecnvcalls,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5852
https://github.com/broadinstitute/gatk/pull/5853:179,Usability,simpl,simply,179,"@takutosato This indicates that something went wrong with `CalculateContamination`, but I would rather have `FilterMutectCalls` finish and say every variant is contamination than simply fail. It is a separate issue to deal with overestimates of contamination, which can occur with gene panels (too little territory to get hom alts).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5853
https://github.com/broadinstitute/gatk/issues/5854:248,Availability,error,error,248,"## Bug Report. ### Affected tool(s) or class(es); BaseRecalibratorSpark; BQSRPipelineSpark. ### Affected version(s); 4.1.0.0. ### Description ; We are running into a problem using BaseRecalibratorSpark. The tool fails soon after starting. The same error appears with the same bam file on different machines. Additionally, vanilla BaseRecalibrator works just fine on these bams (so I don't think the issue is with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1160,Availability,error,error,1160,"oblem using BaseRecalibratorSpark. The tool fails soon after starting. The same error appears with the same bam file on different machines. Additionally, vanilla BaseRecalibrator works just fine on these bams (so I don't think the issue is with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1262,Energy Efficiency,schedul,scheduler,1262,"the same bam file on different machines. Additionally, vanilla BaseRecalibrator works just fine on these bams (so I don't think the issue is with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1302,Energy Efficiency,schedul,scheduler,1302,"lla BaseRecalibrator works just fine on these bams (so I don't think the issue is with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1401,Energy Efficiency,schedul,scheduler,1401,"s with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProces",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1499,Energy Efficiency,schedul,scheduler,1499,"orSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1753,Energy Efficiency,schedul,scheduler,1753,"y, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1834,Energy Efficiency,schedul,scheduler,1834,"asn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1940,Energy Efficiency,schedul,scheduler,1940,"found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperati",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:2090,Energy Efficiency,schedul,scheduler,2090,"bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.r",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:2179,Energy Efficiency,schedul,scheduler,2179, to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperat,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:2277,Energy Efficiency,schedul,scheduler,2277,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.with,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:2373,Energy Efficiency,schedul,scheduler,2373,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at o,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:2538,Energy Efficiency,schedul,scheduler,2538,.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128); 	at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:439); 	at org.apache.spark.api,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:2765,Energy Efficiency,reduce,reduce,2765,DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128); 	at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:439); 	at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:38); 	at org.broadinstitute.hellbender.to,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:3049,Energy Efficiency,reduce,reduce,3049,on.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1026); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1008); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1.apply(RDD.scala:1151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362); 	at org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1128); 	at org.apache.spark.api.java.JavaRDDLike$class.treeAggregate(JavaRDDLike.scala:439); 	at org.apache.spark.api.java.AbstractJavaRDDLike.treeAggregate(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.transforms.BaseRecalibratorSparkFn.apply(BaseRecalibratorSparkFn.java:38); 	at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:132); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.b,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:6743,Energy Efficiency,schedul,scheduler,6743,egate(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 19/03/26 20:02:39 INFO ShutdownHookManager: Shutdown hook called; 19/03/26 20:02:39 INFO ShutdownHookManager: Deleting directory /docker/working/7dd5e9aa-fa24-45ca-9979-13623c0ff8d5/a0d4bfdf-66b4-47af-b002-3c3935a7b633/spark-44911f4d-4d54-42b0-b6d1-35614170c1fc; Using GATK jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:6823,Energy Efficiency,schedul,scheduler,6823,1$$anonfun$24.apply(RDD.scala:1136); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$24.apply(RDD.scala:1136); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 19/03/26 20:02:39 INFO ShutdownHookManager: Shutdown hook called; 19/03/26 20:02:39 INFO ShutdownHookManager: Deleting directory /docker/working/7dd5e9aa-fa24-45ca-9979-13623c0ff8d5/a0d4bfdf-66b4-47af-b002-3c3935a7b633/spark-44911f4d-4d54-42b0-b6d1-35614170c1fc; Using GATK jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10876M -Djava.io.tmpdir=./ -jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:6903,Energy Efficiency,schedul,scheduler,6903,ggregate$1$$anonfun$24.apply(RDD.scala:1136); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 19/03/26 20:02:39 INFO ShutdownHookManager: Shutdown hook called; 19/03/26 20:02:39 INFO ShutdownHookManager: Deleting directory /docker/working/7dd5e9aa-fa24-45ca-9979-13623c0ff8d5/a0d4bfdf-66b4-47af-b002-3c3935a7b633/spark-44911f4d-4d54-42b0-b6d1-35614170c1fc; Using GATK jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10876M -Djava.io.tmpdir=./ -jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar BaseRecalibratorSpark --spark-master local[8] -R /docker/refer,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1166,Integrability,message,message,1166,"oblem using BaseRecalibratorSpark. The tool fails soon after starting. The same error appears with the same bam file on different machines. Additionally, vanilla BaseRecalibrator works just fine on these bams (so I don't think the issue is with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:7028,Performance,concurren,concurrent,7028,la:1137); 	at org.apache.spark.rdd.RDD$$anonfun$treeAggregate$1$$anonfun$25.apply(RDD.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 19/03/26 20:02:39 INFO ShutdownHookManager: Shutdown hook called; 19/03/26 20:02:39 INFO ShutdownHookManager: Deleting directory /docker/working/7dd5e9aa-fa24-45ca-9979-13623c0ff8d5/a0d4bfdf-66b4-47af-b002-3c3935a7b633/spark-44911f4d-4d54-42b0-b6d1-35614170c1fc; Using GATK jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10876M -Djava.io.tmpdir=./ -jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar BaseRecalibratorSpark --spark-master local[8] -R /docker/reference/Data/B37/GATKBundle/2.8_subset_arup_v0.1/human_g1k_v37_decoy_phiXAdaptr.fasta -I bam/rmdup_gatkrealign.bam -O gatk_base_r,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:7113,Performance,concurren,concurrent,7113,.scala:1137); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:797); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96); 	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53); 	at org.apache.spark.scheduler.Task.run(Task.scala:108); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617); 	at java.lang.Thread.run(Thread.java:745); 19/03/26 20:02:39 INFO ShutdownHookManager: Shutdown hook called; 19/03/26 20:02:39 INFO ShutdownHookManager: Deleting directory /docker/working/7dd5e9aa-fa24-45ca-9979-13623c0ff8d5/a0d4bfdf-66b4-47af-b002-3c3935a7b633/spark-44911f4d-4d54-42b0-b6d1-35614170c1fc; Using GATK jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx10876M -Djava.io.tmpdir=./ -jar /docker/reference/Apps/GATK/4.1.0.0/gatk-package-4.1.0.0-local.jar BaseRecalibratorSpark --spark-master local[8] -R /docker/reference/Data/B37/GATKBundle/2.8_subset_arup_v0.1/human_g1k_v37_decoy_phiXAdaptr.fasta -I bam/rmdup_gatkrealign.bam -O gatk_base_recal_table.txt --known-sites /docker/reference/Data/B37/DbSNP/dbSNP_147_20160601/All_,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1433,Safety,abort,abortStage,1433,"are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1531,Safety,abort,abortStage,1531,"ther bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:1776,Safety,abort,abortStage,1776,"here are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2022); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2119); 	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:770,Security,Validat,ValidateSamFile,770,"## Bug Report. ### Affected tool(s) or class(es); BaseRecalibratorSpark; BQSRPipelineSpark. ### Affected version(s); 4.1.0.0. ### Description ; We are running into a problem using BaseRecalibratorSpark. The tool fails soon after starting. The same error appears with the same bam file on different machines. Additionally, vanilla BaseRecalibrator works just fine on these bams (so I don't think the issue is with the bam). They are all suffering from the same/similar stacktrace. We've had BaseRecalibratorSpark work fine on other bam files. Additionally, changing the number of threads still results in the same stacktrace. I've also tried running the BQSRPipelineSpark to see if that would suffer the same issue and it fails in the same manner. Additionally, I've run ValidateSamFile. There are some reads missing their mates, but this hasn't presented an issue in other tools (including vanilla BaseRecalibrator). Searching thru the forum, I found an old issue with a similar stacktrace, but that issue appears to occur in GATK 2.4: https://gatkforums.broadinstitute.org/gatk/discussion/3265/bqsrgatherer-exception. In the below stacktrace, I've bolded the error message that seems to occur in each of these samples. `Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(D",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5854:4697,Security,validat,validateArg,4697,"torSparkFn.java:38); 	at org.broadinstitute.hellbender.tools.spark.BaseRecalibratorSpark.runTool(BaseRecalibratorSpark.java:132); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291)`; **Caused by: java.lang.IllegalArgumentException: Table1 1,3 not equal to 88,3**; 	`at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.recalibration.RecalUtils.combineTables(RecalUtils.java:560); 	at org.broadinstitute.hellbender.utils.recalibration.RecalibrationTables.combine(RecalibrationTables.java:144); 	at org.broadinstitute.hellbender.utils.recalibration.RecalibrationTables.inPlaceCombine(RecalibrationTables.java:178); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$toScalaFunction2$1.apply(JavaPairRDD.scala:1037); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157); 	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.aggregate(TraversableOnce.scala:214); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5854
https://github.com/broadinstitute/gatk/issues/5855:309,Availability,error,error,309,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:387,Availability,down,downloaded,387,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:543,Availability,error,error,543,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:857,Availability,Error,Error,857,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:97,Deployability,pipeline,pipeline,97,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:1152,Modifiability,variab,variable,1152,"cription; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:404); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:886,Security,secur,security,886,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:895,Security,access,access,895,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:63,Testability,test,tests,63,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:460,Testability,test,test,460,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:591,Testability,test,tests,591,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:709,Testability,test,tests,709,"## Feature request. ### Tool(s) or class(es) involved; All WDL tests (Mutect2, CNV, Mitochondria pipeline, etc). ### Description; I'd like to be able to include tasks in GATK WDLs that use NIO (tasks with `String input_file` rather than `File input_file`). When I tried this with local git lfs files I got an error saying that the file could not be found (even though the file was being downloaded correctly). I then tried putting the file in `gs://hellbender/test/resources/large`, but when the tool tried to run on travis I got a permission error (see below). It would be great if the WDL tests all ran in the cloud since that's the main way we expect to run these WDLs. It would also be great it the local tests could account for NIO tasks (especially as we want to make more tasks use NIO in the future). ```; com.google.cloud.storage.StorageException: Error code 404 trying to get security access token from Compute Engine metadata for the default service account. This may be because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.asse",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5855:1997,Testability,assert,assertFileIsReadable,1997, because the virtual machine instance does not have permission scopes specified. It is possible to skip checking for Compute Engine metadata by specifying the environment variable NO_GCE_CHECK=true.; at com.google.cloud.storage.spi.v1.HttpStorageRpc.translate(HttpStorageRpc.java:227); at com.google.cloud.storage.spi.v1.HttpStorageRpc.get(HttpStorageRpc.java:438); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:239); at com.google.cloud.storage.StorageImpl$5.call(StorageImpl.java:236); at shaded.cloud_nio.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:105); at com.google.cloud.RetryHelper.run(RetryHelper.java:76); at com.google.cloud.RetryHelper.runWithRetries(RetryHelper.java:50); at com.google.cloud.storage.StorageImpl.get(StorageImpl.java:235); at com.google.cloud.storage.contrib.nio.CloudStorageFileSystemProvider.checkAccess(CloudStorageFileSystemProvider.java:687); at java.nio.file.Files.exists(Files.java:2385); at htsjdk.samtools.util.IOUtil.assertFileIsReadable(IOUtil.java:404); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:206); at org.broadinstitute.hellbender.engine.ReadsDataSource.<init>(ReadsDataSource.java:162); at org.broadinstitute.hellbender.engine.GATKTool.initializeReads(GATKTool.java:391); at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:640); at org.broadinstitute.hellbender.engine.ReadWalker.onStartup(ReadWalker.java:50); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:136); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5855
https://github.com/broadinstitute/gatk/issues/5857:284,Energy Efficiency,Adapt,AdaptiveChainPruner,284,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857
https://github.com/broadinstitute/gatk/issues/5857:57,Integrability,inject,injecting,57,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857
https://github.com/broadinstitute/gatk/issues/5857:834,Integrability,inject,injected,834,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857
https://github.com/broadinstitute/gatk/issues/5857:284,Modifiability,Adapt,AdaptiveChainPruner,284,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857
https://github.com/broadinstitute/gatk/issues/5857:359,Safety,avoid,avoid,359,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857
https://github.com/broadinstitute/gatk/issues/5857:57,Security,inject,injecting,57,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857
https://github.com/broadinstitute/gatk/issues/5857:834,Security,inject,injected,834,"In Mutect2 and HaplotypeCaller, we force-call alleles by injecting them into the ref haplotype, then threading these constructed haplotypes into the assembly graph with a large edge weight. There are several drawbacks to this approach:. * The strange edge weights interfere with the `AdaptiveChainPruner`.; * The large edge weights may not be large enough to avoid pruning when depth is extremely high.; * The alleles may be lost if assembly fails.; * If the alleles actually exist but are in phase with another variant we end up putting an enormous amount of weight on a false haplotype. We can get around these issue with the following method:. * assemble haplotypes without regard to the force-called alleles.; * if an allele is present in these haplotypes, do nothing further.; * otherwise, add a haplotype in which the allele is injected into the reference haplotype. @LeeTL1220 I prototyped this and it seems to resolve the missed forced alleles that Ziao found. @ldgauthier Can you think of any objections to making this change in HaplotypeCaller?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5857
https://github.com/broadinstitute/gatk/pull/5858:57,Testability,log,log-,57,"This does not change any outputs or inputs, which remain log-10. . @takutosato I ran the DREAM challenges before and after and the vcfs are identical. Besides replacing log10 methods with log methods I moved some things from `MathUtils` to `NaturaLogUtils` for clarity.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5858
https://github.com/broadinstitute/gatk/pull/5858:188,Testability,log,log,188,"This does not change any outputs or inputs, which remain log-10. . @takutosato I ran the DREAM challenges before and after and the vcfs are identical. Besides replacing log10 methods with log methods I moved some things from `MathUtils` to `NaturaLogUtils` for clarity.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5858
https://github.com/broadinstitute/gatk/pull/5860:100,Availability,error,error,100,"Added in test condition for AD field with only 1 value in MAF mode. This isn't really a bug, but an error mode that needed more explicit; feedback.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5860
https://github.com/broadinstitute/gatk/pull/5860:9,Testability,test,test,9,"Added in test condition for AD field with only 1 value in MAF mode. This isn't really a bug, but an error mode that needed more explicit; feedback.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5860
https://github.com/broadinstitute/gatk/pull/5860:138,Usability,feedback,feedback,138,"Added in test condition for AD field with only 1 value in MAF mode. This isn't really a bug, but an error mode that needed more explicit; feedback.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5860
https://github.com/broadinstitute/gatk/issues/5862:1086,Availability,error,errors,1086,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:1189,Availability,down,downstream,1189,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:238,Deployability,update,updated,238,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:96,Integrability,message,message,96,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:422,Integrability,message,message,422,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:30,Security,Validat,ValidateVariants,30,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:62,Security,validat,validation,62,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:275,Security,Validat,ValidateVariants,275,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:312,Security,Validat,ValidateVariants,312,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:396,Security,validat,validation,396,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:503,Security,validat,validation,503,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:544,Security,validat,validation,544,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:697,Security,validat,validation,697,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:828,Security,validat,validation-type-to-exclude,828,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:866,Security,validat,validation,866,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:911,Security,validat,validation-type-to-exclude,911,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:1060,Security,validat,validation,1060,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:753,Testability,log,logged,753,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/issues/5862:1026,Testability,log,logged,1026,"There are several cases where ValidateVariants does no actual validation, and issues no warning message. This includes the default case, where the minimal set of required args is provided (these are examples from the doc, which should be updated when this is fixed): . `gatk ValidateVariants -V some.vcf`; `gatk ValidateVariants -V some.vcf -R some.fasta`. Either of these silently results in no validation and no warning message, despite the entire VCF being decoded and traversed, because the default validation type is ""ALL"", which includes validation type ""IDS"". But IDS requires a dbsnp arg, and none was provided, so the code short-circuits out. The default case should probably do whatever validation it can, but at a minimum a warning should be logged. Ironically, if you provide an exclusion on the command line via `--validation-type-to-exclude IDS`, then validation is done. Another no-op case is `--validation-type-to-exclude ALL` (also recommended in the doc), which also should probably be rejected, or at least logged, since it silently does no validation and reports no errors. This tripped up [this user](https://github.com/samtools/htsjdk/issues/1117), and resulted in a downstream BCF issue.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5862
https://github.com/broadinstitute/gatk/pull/5864:81,Testability,test,test,81,Attempts to fix https://github.com/broadinstitute/gatk/issues/5685. I don't have test data from the users who reported the bug but it seemed like it was likely due to this issue.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5864
https://github.com/broadinstitute/gatk/issues/5865:4194,Availability,down,down,4194," Importing to array - /home/WangBS/Analyses/vcf/test/chr02/genomicsdb_array; 23:42:44.276 INFO ProgressMeter - Starting traversal; 23:42:44.276 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 23:42:45.830 INFO GenomicsDBImport - Importing batch 1 with 63 samples; Buffer resized from 37294 bytes to 65464; Buffer resized from 37294 bytes to 65511; Buffer resized from 37293 bytes to 65539; Buffer resized from 37294 bytes to 65447; .....; .....; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; 06:50:14.219 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.220 INFO GenomicsDBImport - Done importing batch 1/1; 06:50:14.221 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.229 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 427.5 minutes.; 06:50:14.236 INFO GenomicsDBImport - Import completed!; 06:50:14.236 INFO GenomicsDBImport - Shutting down engine; [January 27, 2019 6:50:14 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 428.57 minutes.; Runtime.totalMemory()=8988393472; Tool returned:; true; Using GATK jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx24g -jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar GenotypeGVCFs -R /home/WangBS/Reference/Qrobur/Qrob_PM1N.fa -V gendb:///home/WangBS/Analyses/vcf/test/chr02 -all-sites -O /home/WangBS/Analyses/vcf/test/chr02.vcf; 06:50:19.236 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:51:21.116 INFO Genot",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:8106,Availability,down,down,8106,"generated VCF records; WARNING: No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field DS - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field InbreedingCoeff - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field MLEAC - the field will NOT be part of INFO fields in the generated VCF records; WARNING: No valid combination operation found for INFO field MLEAF - the field will NOT be part of INFO fields in the generated VCF records; 06:51:26.212 INFO GenotypeGVCFs - Done initializing engine; 06:51:26.257 INFO ProgressMeter - Starting traversal; 06:51:26.257 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 06:51:26.278 INFO GenotypeGVCFs - Shutting down engine; [January 27, 2019 6:51:26 AM CST] org.broadinstitute.hellbender.tools.walkers.GenotypeGVCFs done. Elapsed time: 1.12 minutes.; Runtime.totalMemory()=1972371456; java.lang.IllegalStateException: There are no sources based on those query parameters; at com.intel.genomicsdb.reader.GenomicsDBFeatureIterator.<init>(GenomicsDBFeatureIterator.java:131); at com.intel.genomicsdb.reader.GenomicsDBFeatureReader.query(GenomicsDBFeatureReader.java:144); at org.broadinstitute.hellbender.engine.FeatureDataSource.refillQueryCache(FeatureDataSource.java:534); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:503); at org.broadinstitute.hellbender.engine.FeatureDataSource.query(FeatureDataSource.java:469); at org.broadinstitute.hellbender.engine.VariantLocusWalker.lambda$traverse$2(VariantLocusWalker.java:144); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:180",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:501,Modifiability,variab,variables,501,"User Reported this issue as shown below: . I used GenomicsDBImport and GenotypeGVCFs to call SNPs in GATK4. Due to large reference genome (1G) and sample size (100). I want to separate the work for each chromosome. It seems that GenomicsDBImport works for all chromosomes, but GenotypeGVCFs only works for chromosome1. Could you please give me some suggestions. Below are commands and log information for chromosome 2. Look forward to hearing from you soon.; Best regards,; Baosheng. ### command, all variables are defined before command lines.; $GATK --java-options ""-Xmx24g"" \; GenomicsDBImport \; ${InputVCF} \; --genomicsdb-workspace-path ${OUTDIR}/chr02 \; -L Qrob_Chr02. $GATK --java-options ""-Xmx48g"" \; GenotypeGVCFs \; -R ${REF} \; -V gendb://${OUTDIR}/chr02 \; -all-sites \; -O ${OUTDIR}/chr02.vcf. ## log file; 23:41:40.274 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:42:41.990 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.990 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 23:42:41.990 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:42:41.991 INFO GenomicsDBImport - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 23:42:41.991 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:42:41.991 INFO GenomicsDBImport - Start Date/Time: January 26, 2019 11:41:40 PM CST; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 23:42:41.991 INFO GenomicsDBImport - Picard Version: 2.18.16; 23:42:41.992 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:862,Performance,Load,Loading,862,"User Reported this issue as shown below: . I used GenomicsDBImport and GenotypeGVCFs to call SNPs in GATK4. Due to large reference genome (1G) and sample size (100). I want to separate the work for each chromosome. It seems that GenomicsDBImport works for all chromosomes, but GenotypeGVCFs only works for chromosome1. Could you please give me some suggestions. Below are commands and log information for chromosome 2. Look forward to hearing from you soon.; Best regards,; Baosheng. ### command, all variables are defined before command lines.; $GATK --java-options ""-Xmx24g"" \; GenomicsDBImport \; ${InputVCF} \; --genomicsdb-workspace-path ${OUTDIR}/chr02 \; -L Qrob_Chr02. $GATK --java-options ""-Xmx48g"" \; GenotypeGVCFs \; -R ${REF} \; -V gendb://${OUTDIR}/chr02 \; -all-sites \; -O ${OUTDIR}/chr02.vcf. ## log file; 23:41:40.274 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:42:41.990 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.990 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 23:42:41.990 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:42:41.991 INFO GenomicsDBImport - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 23:42:41.991 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:42:41.991 INFO GenomicsDBImport - Start Date/Time: January 26, 2019 11:41:40 PM CST; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 23:42:41.991 INFO GenomicsDBImport - Picard Version: 2.18.16; 23:42:41.992 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:4993,Performance,Load,Loading,4993,"; 06:50:14.229 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 427.5 minutes.; 06:50:14.236 INFO GenomicsDBImport - Import completed!; 06:50:14.236 INFO GenomicsDBImport - Shutting down engine; [January 27, 2019 6:50:14 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 428.57 minutes.; Runtime.totalMemory()=8988393472; Tool returned:; true; Using GATK jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx24g -jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar GenotypeGVCFs -R /home/WangBS/Reference/Qrobur/Qrob_PM1N.fa -V gendb:///home/WangBS/Analyses/vcf/test/chr02 -all-sites -O /home/WangBS/Analyses/vcf/test/chr02.vcf; 06:50:19.236 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:51:21.116 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.116 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 06:51:21.116 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:51:21.117 INFO GenotypeGVCFs - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 06:51:21.117 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 06:51:21.117 INFO GenotypeGVCFs - Start Date/Time: January 27, 2019 6:50:19 AM CST; 06:51:21.117 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.117 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.118 INFO GenotypeGVCFs - HTSJDK",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:385,Testability,log,log,385,"User Reported this issue as shown below: . I used GenomicsDBImport and GenotypeGVCFs to call SNPs in GATK4. Due to large reference genome (1G) and sample size (100). I want to separate the work for each chromosome. It seems that GenomicsDBImport works for all chromosomes, but GenotypeGVCFs only works for chromosome1. Could you please give me some suggestions. Below are commands and log information for chromosome 2. Look forward to hearing from you soon.; Best regards,; Baosheng. ### command, all variables are defined before command lines.; $GATK --java-options ""-Xmx24g"" \; GenomicsDBImport \; ${InputVCF} \; --genomicsdb-workspace-path ${OUTDIR}/chr02 \; -L Qrob_Chr02. $GATK --java-options ""-Xmx48g"" \; GenotypeGVCFs \; -R ${REF} \; -V gendb://${OUTDIR}/chr02 \; -all-sites \; -O ${OUTDIR}/chr02.vcf. ## log file; 23:41:40.274 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:42:41.990 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.990 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 23:42:41.990 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:42:41.991 INFO GenomicsDBImport - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 23:42:41.991 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:42:41.991 INFO GenomicsDBImport - Start Date/Time: January 26, 2019 11:41:40 PM CST; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 23:42:41.991 INFO GenomicsDBImport - Picard Version: 2.18.16; 23:42:41.992 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:812,Testability,log,log,812,"User Reported this issue as shown below: . I used GenomicsDBImport and GenotypeGVCFs to call SNPs in GATK4. Due to large reference genome (1G) and sample size (100). I want to separate the work for each chromosome. It seems that GenomicsDBImport works for all chromosomes, but GenotypeGVCFs only works for chromosome1. Could you please give me some suggestions. Below are commands and log information for chromosome 2. Look forward to hearing from you soon.; Best regards,; Baosheng. ### command, all variables are defined before command lines.; $GATK --java-options ""-Xmx24g"" \; GenomicsDBImport \; ${InputVCF} \; --genomicsdb-workspace-path ${OUTDIR}/chr02 \; -L Qrob_Chr02. $GATK --java-options ""-Xmx48g"" \; GenotypeGVCFs \; -R ${REF} \; -V gendb://${OUTDIR}/chr02 \; -all-sites \; -O ${OUTDIR}/chr02.vcf. ## log file; 23:41:40.274 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 23:42:41.990 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.990 INFO GenomicsDBImport - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 23:42:41.990 INFO GenomicsDBImport - For support and documentation go to https://software.broadinstitute.org/gatk/; 23:42:41.991 INFO GenomicsDBImport - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 23:42:41.991 INFO GenomicsDBImport - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 23:42:41.991 INFO GenomicsDBImport - Start Date/Time: January 26, 2019 11:41:40 PM CST; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - ------------------------------------------------------------; 23:42:41.991 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 23:42:41.991 INFO GenomicsDBImport - Picard Version: 2.18.16; 23:42:41.992 INFO GenomicsDBImpor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:2884,Testability,test,test,2884,:42:41.991 INFO GenomicsDBImport - HTSJDK Version: 2.18.1; 23:42:41.991 INFO GenomicsDBImport - Picard Version: 2.18.16; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:42:41.992 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:42:41.992 INFO GenomicsDBImport - Inflater: IntelInflater; 23:42:41.992 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:42:41.992 INFO GenomicsDBImport - Requester pays: disabled; 23:42:41.992 INFO GenomicsDBImport - Initializing engine; 23:42:44.099 INFO IntervalArgumentCollection - Processing 115639695 bp from intervals; 23:42:44.102 INFO GenomicsDBImport - Done initializing engine; 23:42:44.276 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/WangBS/Analyses/vcf/test/chr02/vidmap.json; 23:42:44.276 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/WangBS/Analyses/vcf/test/chr02/callset.json; 23:42:44.276 INFO GenomicsDBImport - Complete VCF Header will be written to /home/WangBS/Analyses/vcf/test/chr02/vcfheader.vcf; 23:42:44.276 INFO GenomicsDBImport - Importing to array - /home/WangBS/Analyses/vcf/test/chr02/genomicsdb_array; 23:42:44.276 INFO ProgressMeter - Starting traversal; 23:42:44.276 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 23:42:45.830 INFO GenomicsDBImport - Importing batch 1 with 63 samples; Buffer resized from 37294 bytes to 65464; Buffer resized from 37294 bytes to 65511; Buffer resized from 37293 bytes to 65539; Buffer resized from 37294 bytes to 65447; .....; .....; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; 06:50:14.219 INFO ProgressMeter - Qrob,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:3012,Testability,test,test,3012,41.992 INFO GenomicsDBImport - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:42:41.992 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:42:41.992 INFO GenomicsDBImport - Inflater: IntelInflater; 23:42:41.992 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:42:41.992 INFO GenomicsDBImport - Requester pays: disabled; 23:42:41.992 INFO GenomicsDBImport - Initializing engine; 23:42:44.099 INFO IntervalArgumentCollection - Processing 115639695 bp from intervals; 23:42:44.102 INFO GenomicsDBImport - Done initializing engine; 23:42:44.276 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/WangBS/Analyses/vcf/test/chr02/vidmap.json; 23:42:44.276 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/WangBS/Analyses/vcf/test/chr02/callset.json; 23:42:44.276 INFO GenomicsDBImport - Complete VCF Header will be written to /home/WangBS/Analyses/vcf/test/chr02/vcfheader.vcf; 23:42:44.276 INFO GenomicsDBImport - Importing to array - /home/WangBS/Analyses/vcf/test/chr02/genomicsdb_array; 23:42:44.276 INFO ProgressMeter - Starting traversal; 23:42:44.276 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 23:42:45.830 INFO GenomicsDBImport - Importing batch 1 with 63 samples; Buffer resized from 37294 bytes to 65464; Buffer resized from 37294 bytes to 65511; Buffer resized from 37293 bytes to 65539; Buffer resized from 37294 bytes to 65447; .....; .....; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; 06:50:14.219 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.220 INFO GenomicsDBImport - Done importing batch 1/1; 06:50:14.221 INFO ProgressMeter - Qrob_Ch,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:3139,Testability,test,test,3139,YNC_IO_READ_FOR_SAMTOOLS : false; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:42:41.992 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:42:41.992 INFO GenomicsDBImport - Inflater: IntelInflater; 23:42:41.992 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:42:41.992 INFO GenomicsDBImport - Requester pays: disabled; 23:42:41.992 INFO GenomicsDBImport - Initializing engine; 23:42:44.099 INFO IntervalArgumentCollection - Processing 115639695 bp from intervals; 23:42:44.102 INFO GenomicsDBImport - Done initializing engine; 23:42:44.276 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/WangBS/Analyses/vcf/test/chr02/vidmap.json; 23:42:44.276 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/WangBS/Analyses/vcf/test/chr02/callset.json; 23:42:44.276 INFO GenomicsDBImport - Complete VCF Header will be written to /home/WangBS/Analyses/vcf/test/chr02/vcfheader.vcf; 23:42:44.276 INFO GenomicsDBImport - Importing to array - /home/WangBS/Analyses/vcf/test/chr02/genomicsdb_array; 23:42:44.276 INFO ProgressMeter - Starting traversal; 23:42:44.276 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 23:42:45.830 INFO GenomicsDBImport - Importing batch 1 with 63 samples; Buffer resized from 37294 bytes to 65464; Buffer resized from 37294 bytes to 65511; Buffer resized from 37293 bytes to 65539; Buffer resized from 37294 bytes to 65447; .....; .....; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; 06:50:14.219 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.220 INFO GenomicsDBImport - Done importing batch 1/1; 06:50:14.221 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.229 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 427.5 minutes.; 06:50:14.23,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:3249,Testability,test,test,3249,"23:42:41.992 INFO GenomicsDBImport - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 23:42:41.992 INFO GenomicsDBImport - Deflater: IntelDeflater; 23:42:41.992 INFO GenomicsDBImport - Inflater: IntelInflater; 23:42:41.992 INFO GenomicsDBImport - GCS max retries/reopens: 20; 23:42:41.992 INFO GenomicsDBImport - Requester pays: disabled; 23:42:41.992 INFO GenomicsDBImport - Initializing engine; 23:42:44.099 INFO IntervalArgumentCollection - Processing 115639695 bp from intervals; 23:42:44.102 INFO GenomicsDBImport - Done initializing engine; 23:42:44.276 INFO GenomicsDBImport - Vid Map JSON file will be written to /home/WangBS/Analyses/vcf/test/chr02/vidmap.json; 23:42:44.276 INFO GenomicsDBImport - Callset Map JSON file will be written to /home/WangBS/Analyses/vcf/test/chr02/callset.json; 23:42:44.276 INFO GenomicsDBImport - Complete VCF Header will be written to /home/WangBS/Analyses/vcf/test/chr02/vcfheader.vcf; 23:42:44.276 INFO GenomicsDBImport - Importing to array - /home/WangBS/Analyses/vcf/test/chr02/genomicsdb_array; 23:42:44.276 INFO ProgressMeter - Starting traversal; 23:42:44.276 INFO ProgressMeter - Current Locus Elapsed Minutes Batches Processed Batches/Minute; 23:42:45.830 INFO GenomicsDBImport - Importing batch 1 with 63 samples; Buffer resized from 37294 bytes to 65464; Buffer resized from 37294 bytes to 65511; Buffer resized from 37293 bytes to 65539; Buffer resized from 37294 bytes to 65447; .....; .....; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; Buffer resized from 65538 bytes to 65539; 06:50:14.219 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.220 INFO GenomicsDBImport - Done importing batch 1/1; 06:50:14.221 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.229 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 427.5 minutes.; 06:50:14.236 INFO GenomicsDBImport - Import completed!; 06:50:14.236 INFO GenomicsDBImport - Shutting down engine; [January 27, 2019 6:50:1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:4886,Testability,test,test,4886,"BImport - Done importing batch 1/1; 06:50:14.221 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.229 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 427.5 minutes.; 06:50:14.236 INFO GenomicsDBImport - Import completed!; 06:50:14.236 INFO GenomicsDBImport - Shutting down engine; [January 27, 2019 6:50:14 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 428.57 minutes.; Runtime.totalMemory()=8988393472; Tool returned:; true; Using GATK jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx24g -jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar GenotypeGVCFs -R /home/WangBS/Reference/Qrobur/Qrob_PM1N.fa -V gendb:///home/WangBS/Analyses/vcf/test/chr02 -all-sites -O /home/WangBS/Analyses/vcf/test/chr02.vcf; 06:50:19.236 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:51:21.116 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.116 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 06:51:21.116 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:51:21.117 INFO GenotypeGVCFs - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 06:51:21.117 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 06:51:21.117 INFO GenotypeGVCFs - Start Date/Time: January 27, 2019 6:50:19 AM CST; 06:51:21.117 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.117 INFO GenotypeGVCFs - --------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/issues/5865:4937,Testability,test,test,4937,"BImport - Done importing batch 1/1; 06:50:14.221 INFO ProgressMeter - Qrob_Chr02:1 427.5 1 0.0; 06:50:14.229 INFO ProgressMeter - Traversal complete. Processed 1 total batches in 427.5 minutes.; 06:50:14.236 INFO GenomicsDBImport - Import completed!; 06:50:14.236 INFO GenomicsDBImport - Shutting down engine; [January 27, 2019 6:50:14 AM CST] org.broadinstitute.hellbender.tools.genomicsdb.GenomicsDBImport done. Elapsed time: 428.57 minutes.; Runtime.totalMemory()=8988393472; Tool returned:; true; Using GATK jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx24g -jar /home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar GenotypeGVCFs -R /home/WangBS/Reference/Qrobur/Qrob_PM1N.fa -V gendb:///home/WangBS/Analyses/vcf/test/chr02 -all-sites -O /home/WangBS/Analyses/vcf/test/chr02.vcf; 06:50:19.236 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/WangBS/software/GATK/gatk/build/libs/gatk-package-4.0.11.0-56-g2c0e9b0-SNAPSHOT-local.jar!/com/intel/gkl/native/libgkl_compression.so; 06:51:21.116 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.116 INFO GenotypeGVCFs - The Genome Analysis Toolkit (GATK) v4.0.11.0-56-g2c0e9b0-SNAPSHOT; 06:51:21.116 INFO GenotypeGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 06:51:21.117 INFO GenotypeGVCFs - Executing as WangBS@cu53 on Linux v3.10.0-693.el7.x86_64 amd64; 06:51:21.117 INFO GenotypeGVCFs - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_131-b12; 06:51:21.117 INFO GenotypeGVCFs - Start Date/Time: January 27, 2019 6:50:19 AM CST; 06:51:21.117 INFO GenotypeGVCFs - ------------------------------------------------------------; 06:51:21.117 INFO GenotypeGVCFs - --------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5865
https://github.com/broadinstitute/gatk/pull/5866:129,Deployability,Update,Update,129,"The version of `conda` we use on the docker uses environment activation commands that are deprecated in newer versions of conda. Update to the latest published version of miniconda, and update the doc to specify the newer commands. Also change the local environment to use `conda env create -f` rather than `conda env update`. Fixes https://github.com/broadinstitute/gatk/issues/5851 and https://github.com/broadinstitute/gatk/issues/5776.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5866
https://github.com/broadinstitute/gatk/pull/5866:186,Deployability,update,update,186,"The version of `conda` we use on the docker uses environment activation commands that are deprecated in newer versions of conda. Update to the latest published version of miniconda, and update the doc to specify the newer commands. Also change the local environment to use `conda env create -f` rather than `conda env update`. Fixes https://github.com/broadinstitute/gatk/issues/5851 and https://github.com/broadinstitute/gatk/issues/5776.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5866
https://github.com/broadinstitute/gatk/pull/5866:318,Deployability,update,update,318,"The version of `conda` we use on the docker uses environment activation commands that are deprecated in newer versions of conda. Update to the latest published version of miniconda, and update the doc to specify the newer commands. Also change the local environment to use `conda env create -f` rather than `conda env update`. Fixes https://github.com/broadinstitute/gatk/issues/5851 and https://github.com/broadinstitute/gatk/issues/5776.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5866
https://github.com/broadinstitute/gatk/issues/5867:101,Availability,error,errors,101,"`VariantWalker` and `VariantContext` do not do variant validation at parse-time. This causes awkward errors on invalid files like the one found here:; https://gatkforums.broadinstitute.org/gatk/discussion/23809/oncotator-for-build-hg38. It would be best if there was a way to properly validate the variants before parsing them. `ValidateVariants` currently doesn't properly work when given default options (#5862). When #5862 is fixed, this may be ignored - I think validating at run-time when iterating over variants may add too much overhead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5867
https://github.com/broadinstitute/gatk/issues/5867:55,Security,validat,validation,55,"`VariantWalker` and `VariantContext` do not do variant validation at parse-time. This causes awkward errors on invalid files like the one found here:; https://gatkforums.broadinstitute.org/gatk/discussion/23809/oncotator-for-build-hg38. It would be best if there was a way to properly validate the variants before parsing them. `ValidateVariants` currently doesn't properly work when given default options (#5862). When #5862 is fixed, this may be ignored - I think validating at run-time when iterating over variants may add too much overhead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5867
https://github.com/broadinstitute/gatk/issues/5867:285,Security,validat,validate,285,"`VariantWalker` and `VariantContext` do not do variant validation at parse-time. This causes awkward errors on invalid files like the one found here:; https://gatkforums.broadinstitute.org/gatk/discussion/23809/oncotator-for-build-hg38. It would be best if there was a way to properly validate the variants before parsing them. `ValidateVariants` currently doesn't properly work when given default options (#5862). When #5862 is fixed, this may be ignored - I think validating at run-time when iterating over variants may add too much overhead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5867
https://github.com/broadinstitute/gatk/issues/5867:329,Security,Validat,ValidateVariants,329,"`VariantWalker` and `VariantContext` do not do variant validation at parse-time. This causes awkward errors on invalid files like the one found here:; https://gatkforums.broadinstitute.org/gatk/discussion/23809/oncotator-for-build-hg38. It would be best if there was a way to properly validate the variants before parsing them. `ValidateVariants` currently doesn't properly work when given default options (#5862). When #5862 is fixed, this may be ignored - I think validating at run-time when iterating over variants may add too much overhead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5867
https://github.com/broadinstitute/gatk/issues/5867:466,Security,validat,validating,466,"`VariantWalker` and `VariantContext` do not do variant validation at parse-time. This causes awkward errors on invalid files like the one found here:; https://gatkforums.broadinstitute.org/gatk/discussion/23809/oncotator-for-build-hg38. It would be best if there was a way to properly validate the variants before parsing them. `ValidateVariants` currently doesn't properly work when given default options (#5862). When #5862 is fixed, this may be ignored - I think validating at run-time when iterating over variants may add too much overhead.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5867
https://github.com/broadinstitute/gatk/issues/5869:158,Availability,error,error,158,"EMR - 5.13.0; Spark - 2.3.0. Running on m5.2xlarge 1 Master and 2 Worker nodes.; While running Haplotype Caller Spark on EMR, running into the stack overflow error. ; `[hadoop@ip-xx.xx.xx.xx gatk]$ ./gatk HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData.cram -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf \; > -- \; > --spark-runner SPARK --spark-master yarn; Using GATK jar /home/hadoop/gatk/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/na",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:17342,Availability,down,down,17342,"ored as bytes in memory (estimated size 23.6 KB, free 360.5 MB); 19/04/08 19:03:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 23.6 KB, free: 365.8 MB); 19/04/08 19:03:27 INFO SparkContext: Created broadcast 4 from newAPIHadoopFile at PathSplitSource.java:96; 19/04/08 19:03:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.9 MB, free 323.6 MB); 19/04/08 19:03:28 INFO SparkUI: Stopped Spark web UI at http://ip-xx.xx.xx.xx.ec2.internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:17455,Availability,down,down,17455,"ored as bytes in memory (estimated size 23.6 KB, free 360.5 MB); 19/04/08 19:03:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 23.6 KB, free: 365.8 MB); 19/04/08 19:03:27 INFO SparkContext: Created broadcast 4 from newAPIHadoopFile at PathSplitSource.java:96; 19/04/08 19:03:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.9 MB, free 323.6 MB); 19/04/08 19:03:28 INFO SparkUI: Stopped Spark web UI at http://ip-xx.xx.xx.xx.ec2.internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:18184,Availability,down,down,18184,"internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScope(FieldSerializerGenericsUtil.java:116); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.newCachedFieldOfGenericType(FieldSerializerGenericsUtil.java:225); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.newCachedField(FieldSerializer.java:368); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.createCachedFields(FieldSerializer.java:331); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:261); 	at com.esotericsoftware.kryo.serial",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:1467,Deployability,configurat,configuration,1467,k/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4062,Deployability,configurat,configuration,4062,allerSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:01:43.730 INFO HaplotypeCallerSpark - Deflater: IntelDeflater; 19:01:43.730 INFO HaplotypeCallerSpark - Inflater: IntelInflater; 19:01:43.730 INFO HaplotypeCallerSpark - GCS max retries/reopens: 20; 19:01:43.730 INFO HaplotypeCallerSpark - Requester pays: disabled; 19:01:43.730 WARN HaplotypeCallerSpark - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:6704,Energy Efficiency,allocate,allocate,6704,"n port 4040.; 19/04/08 19:01:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://ip-xx.xx.xx.xx.ec2.internal:4040; 19/04/08 19:01:44 INFO SparkContext: Added JAR file:/home/hadoop/gatk/build/libs/gatk-spark.jar at spark://ip-xx.xx.xx.xx.ec2.internal:34715/jars/gatk-spark.jar with timestamp 1554750104322; 19/04/08 19:01:44 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances; 19/04/08 19:01:44 INFO GoogleHadoopFileSystemBase: GHFS version: 1.6.3-hadoop2; 19/04/08 19:01:44 INFO RMProxy: Connecting to ResourceManager at ip-xx.xx.xx.xx.ec2.internal/xx.xx.xx.xx:8032; 19/04/08 19:01:45 INFO Client: Requesting a new application from cluster with 2 NodeManagers; 19/04/08 19:01:45 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (24576 MB per container); 19/04/08 19:01:45 INFO Client: Will allocate AM container, with 896 MB memory including 384 MB overhead; 19/04/08 19:01:45 INFO Client: Setting up container launch context for our AM; 19/04/08 19:01:45 INFO Client: Setting up the launch environment for our AM container; 19/04/08 19:01:45 INFO Client: Preparing resources for our AM container; 19/04/08 19:01:45 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.; 19/04/08 19:01:47 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_libs__8661360236699028076.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_libs__8661360236699028076.zip; 19/04/08 19:01:48 INFO Client: Uploading resource file:/etc/spark/conf/hive-site.xml -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:8675,Energy Efficiency,Schedul,SchedulerExtensionServices,8675,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:11996,Energy Efficiency,Schedul,SchedulerBackend,11996,"nagerMaster: Registering BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-xx.xx.xx.xx.ec2.internal:38471 with 366.3 MB RAM, BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManager: external shuffle service port = 7337; 19/04/08 19:01:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1554748821802_0005; 19/04/08 19:01:52 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances; 19/04/08 19:01:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8; 19:01:52.566 INFO HaplotypeCallerSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.7 MB, free 361.6 MB); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 450.5 KB, free 361.2 MB); 19/04/08 19:02:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 450.5 KB, free: 365.9 MB); 19/04/08 19:02:38 INFO SparkContext: Created broadcast 0 from broadcast at CramSource.java:114; 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 290.4 KB, free 360.9 MB); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 23.6 KB, free 360.9 MB); 19/04/08 19:02:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:12026,Energy Efficiency,schedul,scheduling,12026,"nagerMaster: Registering BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-xx.xx.xx.xx.ec2.internal:38471 with 366.3 MB RAM, BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManager: external shuffle service port = 7337; 19/04/08 19:01:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1554748821802_0005; 19/04/08 19:01:52 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances; 19/04/08 19:01:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8; 19:01:52.566 INFO HaplotypeCallerSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.7 MB, free 361.6 MB); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 450.5 KB, free 361.2 MB); 19/04/08 19:02:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 450.5 KB, free: 365.9 MB); 19/04/08 19:02:38 INFO SparkContext: Created broadcast 0 from broadcast at CramSource.java:114; 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 290.4 KB, free 360.9 MB); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 23.6 KB, free 360.9 MB); 19/04/08 19:02:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in me",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:17266,Energy Efficiency,monitor,monitor,17266,"ored as bytes in memory (estimated size 23.6 KB, free 360.5 MB); 19/04/08 19:03:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 23.6 KB, free: 365.8 MB); 19/04/08 19:03:27 INFO SparkContext: Created broadcast 4 from newAPIHadoopFile at PathSplitSource.java:96; 19/04/08 19:03:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.9 MB, free 323.6 MB); 19/04/08 19:03:28 INFO SparkUI: Stopped Spark web UI at http://ip-xx.xx.xx.xx.ec2.internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:17484,Energy Efficiency,Schedul,SchedulerExtensionServices,17484,"ored as bytes in memory (estimated size 23.6 KB, free 360.5 MB); 19/04/08 19:03:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 23.6 KB, free: 365.8 MB); 19/04/08 19:03:27 INFO SparkContext: Created broadcast 4 from newAPIHadoopFile at PathSplitSource.java:96; 19/04/08 19:03:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.9 MB, free 323.6 MB); 19/04/08 19:03:28 INFO SparkUI: Stopped Spark web UI at http://ip-xx.xx.xx.xx.ec2.internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:17521,Energy Efficiency,Schedul,SchedulerExtensionServices,17521,"ored as bytes in memory (estimated size 23.6 KB, free 360.5 MB); 19/04/08 19:03:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 23.6 KB, free: 365.8 MB); 19/04/08 19:03:27 INFO SparkContext: Created broadcast 4 from newAPIHadoopFile at PathSplitSource.java:96; 19/04/08 19:03:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.9 MB, free 323.6 MB); 19/04/08 19:03:28 INFO SparkUI: Stopped Spark web UI at http://ip-xx.xx.xx.xx.ec2.internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:1467,Modifiability,config,configuration,1467,k/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:1709,Modifiability,variab,variables,1709,"ls=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9.85-38.58.amzn1.x86_64 amd64; 19:01:43.729 INFO HaplotypeCallerSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 19:01:43.729 INFO HaplotypeCallerSpark - Start Date/Time: April 8, 2019 7:01:43 PM UTC; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:1835,Modifiability,config,configured,1835,"ls=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9.85-38.58.amzn1.x86_64 amd64; 19:01:43.729 INFO HaplotypeCallerSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 19:01:43.729 INFO HaplotypeCallerSpark - Start Date/Time: April 8, 2019 7:01:43 PM UTC; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4062,Modifiability,config,configuration,4062,allerSpark - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : false; 19:01:43.730 INFO HaplotypeCallerSpark - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 19:01:43.730 INFO HaplotypeCallerSpark - Deflater: IntelDeflater; 19:01:43.730 INFO HaplotypeCallerSpark - Inflater: IntelInflater; 19:01:43.730 INFO HaplotypeCallerSpark - GCS max retries/reopens: 20; 19:01:43.730 INFO HaplotypeCallerSpark - Requester pays: disabled; 19:01:43.730 WARN HaplotypeCallerSpark - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:1896,Performance,Load,Loading,1896," spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO HaplotypeCallerSpark - Executing as hadoop@ip-xx.xx.xx.xx on Linux v4.9.85-38.58.amzn1.x86_64 amd64; 19:01:43.729 INFO HaplotypeCallerSpark - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_171-b10; 19:01:43.729 INFO HaplotypeCallerSpark - Start Date/Time: April 8, 2019 7:01:43 PM UTC; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.730 INFO Haplotyp",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:9108,Performance,queue,queue,9108,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:10319,Performance,queue,queue,10319,"ication_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.xx.xx.ec2.internal, PROXY_URI_BASES -> http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005), /proxy/application_1554748821802_0005; 19/04/08 19:01:51 INFO JettyUtils: Adding filter: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter; 19/04/08 19:01:51 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM); 19/04/08 19:01:52 INFO Client: Application report for application_1554748821802_0005 (state: RUNNING); 19/04/08 19:01:52 INFO Client: ; 	 client token: N/A; 	 diagnostics: N/A; 	 ApplicationMaster host: xx.xx.xx.xx; 	 ApplicationMaster RPC port: 0; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:52 INFO YarnClientSchedulerBackend: Application application_1554748821802_0005 has started running.; 19/04/08 19:01:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38471.; 19/04/08 19:01:52 INFO NettyBlockTransferService: Server created on ip-xx.xx.xx.xx.ec2.internal:38471; 19/04/08 19:01:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 19/04/08 19:01:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-xx.xx.xx.xx.ec2.internal:38471 with 366.3 MB RAM, BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4423,Security,Secur,SecurityManager,4423, - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on port 34715.; 19/04/08 19:01:44 INFO SparkEnv: Registering MapOutputTracker; 19/04/08 19:01:44 INFO SparkEnv: Registering BlockManagerMaster; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 19/04/08 19:01:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-a0da7406-ea5f-4bd9-b3e6-46161a5cbb1b; 19/04/08 19:01:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 19/04/08 19:01:44 INFO SparkEnv: Registering OutputCommitCoordinator; 19/04/08 19:01,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4494,Security,Secur,SecurityManager,4494, - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on port 34715.; 19/04/08 19:01:44 INFO SparkEnv: Registering MapOutputTracker; 19/04/08 19:01:44 INFO SparkEnv: Registering BlockManagerMaster; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 19/04/08 19:01:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-a0da7406-ea5f-4bd9-b3e6-46161a5cbb1b; 19/04/08 19:01:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 19/04/08 19:01:44 INFO SparkEnv: Registering OutputCommitCoordinator; 19/04/08 19:01,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4567,Security,Secur,SecurityManager,4567, - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on port 34715.; 19/04/08 19:01:44 INFO SparkEnv: Registering MapOutputTracker; 19/04/08 19:01:44 INFO SparkEnv: Registering BlockManagerMaster; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 19/04/08 19:01:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-a0da7406-ea5f-4bd9-b3e6-46161a5cbb1b; 19/04/08 19:01:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 19/04/08 19:01:44 INFO SparkEnv: Registering OutputCommitCoordinator; 19/04/08 19:01,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4639,Security,Secur,SecurityManager,4639, - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on port 34715.; 19/04/08 19:01:44 INFO SparkEnv: Registering MapOutputTracker; 19/04/08 19:01:44 INFO SparkEnv: Registering BlockManagerMaster; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 19/04/08 19:01:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-a0da7406-ea5f-4bd9-b3e6-46161a5cbb1b; 19/04/08 19:01:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 19/04/08 19:01:44 INFO SparkEnv: Registering OutputCommitCoordinator; 19/04/08 19:01,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4713,Security,Secur,SecurityManager,4713, - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on port 34715.; 19/04/08 19:01:44 INFO SparkEnv: Registering MapOutputTracker; 19/04/08 19:01:44 INFO SparkEnv: Registering BlockManagerMaster; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 19/04/08 19:01:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-a0da7406-ea5f-4bd9-b3e6-46161a5cbb1b; 19/04/08 19:01:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 19/04/08 19:01:44 INFO SparkEnv: Registering OutputCommitCoordinator; 19/04/08 19:01,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4730,Security,Secur,SecurityManager,4730, - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on port 34715.; 19/04/08 19:01:44 INFO SparkEnv: Registering MapOutputTracker; 19/04/08 19:01:44 INFO SparkEnv: Registering BlockManagerMaster; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 19/04/08 19:01:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-a0da7406-ea5f-4bd9-b3e6-46161a5cbb1b; 19/04/08 19:01:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 19/04/08 19:01:44 INFO SparkEnv: Registering OutputCommitCoordinator; 19/04/08 19:01,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:4747,Security,authenticat,authentication,4747, - . !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: HaplotypeCallerSpark is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 19:01:43.730 INFO HaplotypeCallerSpark - Initializing engine; 19:01:43.730 INFO HaplotypeCallerSpark - Done initializing engine; 19/04/08 19:01:43 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19/04/08 19:01:43 INFO SparkContext: Running Spark version 2.3.0; 19/04/08 19:01:43 INFO SparkContext: Submitted application: HaplotypeCallerSpark; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:43 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:44 INFO Utils: Successfully started service 'sparkDriver' on port 34715.; 19/04/08 19:01:44 INFO SparkEnv: Registering MapOutputTracker; 19/04/08 19:01:44 INFO SparkEnv: Registering BlockManagerMaster; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information; 19/04/08 19:01:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up; 19/04/08 19:01:44 INFO DiskBlockManager: Created local directory at /mnt/tmp/blockmgr-a0da7406-ea5f-4bd9-b3e6-46161a5cbb1b; 19/04/08 19:01:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MB; 19/04/08 19:01:44 INFO SparkEnv: Registering OutputCommitCoordinator; 19/04/08 19:01,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:7925,Security,Secur,SecurityManager,7925,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:7996,Security,Secur,SecurityManager,7996,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:8069,Security,Secur,SecurityManager,8069,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:8141,Security,Secur,SecurityManager,8141,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:8215,Security,Secur,SecurityManager,8215,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:8232,Security,Secur,SecurityManager,8232,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:8249,Security,authenticat,authentication,8249,"ternal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/hive-site.xml; 19/04/08 19:01:48 INFO Client: Uploading resource file:/mnt/tmp/spark-ada67a34-2db0-488c-adf5-7e4607fe989f/__spark_conf__2453357125414211656.zip -> hdfs://ip-xx.xx.xx.xx.ec2.internal:8020/user/hadoop/.sparkStaging/application_1554748821802_0005/__spark_conf__.zip; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls to: hadoop; 19/04/08 19:01:48 INFO SecurityManager: Changing view acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: Changing modify acls groups to: ; 19/04/08 19:01:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); groups with view permissions: Set(); users with modify permissions: Set(hadoop); groups with modify permissions: Set(); 19/04/08 19:01:48 INFO Client: Submitting application application_1554748821802_0005 to ResourceManager; 19/04/08 19:01:48 INFO YarnClientImpl: Submitted application application_1554748821802_0005; 19/04/08 19:01:48 INFO SchedulerExtensionServices: Starting Yarn extension services with app application_1554748821802_0005 and attemptId None; 19/04/08 19:01:49 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:49 INFO Client: ; 	 client token: N/A; 	 diagnostics: AM container is launched, waiting for AM container to Register with RM; 	 ApplicationMaster host: N/A; 	 ApplicationMaster RPC port: -1; 	 queue: default; 	 start time: 1554750108216; 	 final status: UNDEFINED; 	 tracking URL: http://ip-xx.xx.xx.xx.ec2.internal:20888/proxy/application_1554748821802_0005/; 	 user: hadoop; 19/04/08 19:01:50 INFO Client: Application report for application_1554748821802_0005 (state: ACCEPTED); 19/04/08 19:01:51 INFO YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> ip-xx.xx.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:18424,Security,Hash,HashMap,18424,"king each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScope(FieldSerializerGenericsUtil.java:116); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.newCachedFieldOfGenericType(FieldSerializerGenericsUtil.java:225); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.newCachedField(FieldSerializer.java:368); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.createCachedFields(FieldSerializer.java:331); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:261); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:182); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esoterics",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:18446,Security,Hash,HashMap,18446,"tor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScope(FieldSerializerGenericsUtil.java:116); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.newCachedFieldOfGenericType(FieldSerializerGenericsUtil.java:225); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.newCachedField(FieldSerializer.java:368); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.createCachedFields(FieldSerializer.java:331); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:261); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:182); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.se",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:18479,Security,Hash,HashMap,18479," SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScope(FieldSerializerGenericsUtil.java:116); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.newCachedFieldOfGenericType(FieldSerializerGenericsUtil.java:225); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.newCachedField(FieldSerializer.java:368); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.createCachedFields(FieldSerializer.java:331); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:261); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:182); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:18494,Security,Hash,HashMap,18494,"xtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScope(FieldSerializerGenericsUtil.java:116); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.newCachedFieldOfGenericType(FieldSerializerGenericsUtil.java:225); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.newCachedField(FieldSerializer.java:368); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.createCachedFields(FieldSerializer.java:331); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:261); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.rebuildCachedFields(FieldSerializer.java:182); 	at com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:508); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	a",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:249,Testability,test,testdata,249,"EMR - 5.13.0; Spark - 2.3.0. Running on m5.2xlarge 1 Master and 2 Worker nodes.; While running Haplotype Caller Spark on EMR, running into the stack overflow error. ; `[hadoop@ip-xx.xx.xx.xx gatk]$ ./gatk HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData.cram -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf \; > -- \; > --spark-runner SPARK --spark-master yarn; Using GATK jar /home/hadoop/gatk/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/na",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:258,Testability,Test,TestData,258,"EMR - 5.13.0; Spark - 2.3.0. Running on m5.2xlarge 1 Master and 2 Worker nodes.; While running Haplotype Caller Spark on EMR, running into the stack overflow error. ; `[hadoop@ip-xx.xx.xx.xx gatk]$ ./gatk HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData.cram -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf \; > -- \; > --spark-runner SPARK --spark-master yarn; Using GATK jar /home/hadoop/gatk/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/na",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:346,Testability,test,testgatkvcf,346,"EMR - 5.13.0; Spark - 2.3.0. Running on m5.2xlarge 1 Master and 2 Worker nodes.; While running Haplotype Caller Spark on EMR, running into the stack overflow error. ; `[hadoop@ip-xx.xx.xx.xx gatk]$ ./gatk HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData.cram -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf \; > -- \; > --spark-runner SPARK --spark-master yarn; Using GATK jar /home/hadoop/gatk/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/na",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:1300,Testability,test,testdata,1300,/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf \; > -- \; > --spark-runner SPARK --spark-master yarn; Using GATK jar /home/hadoop/gatk/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and document,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:1309,Testability,Test,TestData,1309,/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf \; > -- \; > --spark-runner SPARK --spark-master yarn; Using GATK jar /home/hadoop/gatk/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and document,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:1392,Testability,test,testgatkvcf,1392,rk-runner SPARK --spark-master yarn; Using GATK jar /home/hadoop/gatk/build/libs/gatk-spark.jar; Running:; /usr/lib/spark/bin/spark-submit --master yarn --conf spark.driver.userClassPathFirst=false --conf spark.io.compression.codec=lzf --conf spark.driver.maxResultSize=0 --conf spark.executor.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.driver.extraJavaOptions=-DGATK_STACKTRACE_ON_USER_EXCEPTION=true -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=false -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 --conf spark.kryoserializer.buffer.max=512m --conf spark.yarn.executor.memoryOverhead=600 /home/hadoop/gatk/build/libs/gatk-spark.jar HaplotypeCallerSpark -I hdfs:///user/hadoop/testdata/TestData -R hdfs:///user/hadoop/reference/hg38.fasta -O hdfs:///user/hadoop/output/testgatkvcf.vcf --spark-master yarn; 19/04/08 19:01:40 WARN SparkConf: The configuration key 'spark.yarn.executor.memoryOverhead' has been deprecated as of Spark 2.3 and may be removed in the future. Please use the new key 'spark.executor.memoryOverhead' instead.; 19:01:43.413 WARN SparkContextFactory - Environment variables HELLBENDER_TEST_PROJECT and HELLBENDER_JSON_SERVICE_ACCOUNT_KEY must be set or the GCS hadoop connector will not be configured properly; 19:01:43.565 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/hadoop/gatk/build/libs/gatk-spark.jar!/com/intel/gkl/native/libgkl_compression.so; 19:01:43.728 INFO HaplotypeCallerSpark - ------------------------------------------------------------; 19:01:43.729 INFO HaplotypeCallerSpark - The Genome Analysis Toolkit (GATK) v4.1.1.0-10-g554a0e8-SNAPSHOT; 19:01:43.729 INFO HaplotypeCallerSpark - For support and documentation go to https://software.broadinstitute.org/gatk/; 19:01:43.729 INFO Hap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:11695,Testability,Log,Logging,11695,"e' on port 38471.; 19/04/08 19:01:52 INFO NettyBlockTransferService: Server created on ip-xx.xx.xx.xx.ec2.internal:38471; 19/04/08 19:01:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 19/04/08 19:01:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-xx.xx.xx.xx.ec2.internal:38471 with 366.3 MB RAM, BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManager: external shuffle service port = 7337; 19/04/08 19:01:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1554748821802_0005; 19/04/08 19:01:52 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances; 19/04/08 19:01:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8; 19:01:52.566 INFO HaplotypeCallerSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.7 MB, free 361.6 MB); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 450.5 KB, free 361.2 MB); 19/04/08 19:02:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 450.5 KB, free: 365.9 MB); 19/04/08 19:02:38 INFO SparkContext: Created broadcast 0 from broadcast at CramSource.java:114; 19/04/08 19:02:38 INFO MemoryStore: Block b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:11723,Testability,log,log,11723,"e' on port 38471.; 19/04/08 19:01:52 INFO NettyBlockTransferService: Server created on ip-xx.xx.xx.xx.ec2.internal:38471; 19/04/08 19:01:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy; 19/04/08 19:01:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMasterEndpoint: Registering block manager ip-xx.xx.xx.xx.ec2.internal:38471 with 366.3 MB RAM, BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO BlockManager: external shuffle service port = 7337; 19/04/08 19:01:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ip-xx.xx.xx.xx.ec2.internal, 38471, None); 19/04/08 19:01:52 INFO EventLoggingListener: Logging events to hdfs:/var/log/spark/apps/application_1554748821802_0005; 19/04/08 19:01:52 INFO Utils: Using initial executors = 0, max of spark.dynamicAllocation.initialExecutors, spark.dynamicAllocation.minExecutors and spark.executor.instances; 19/04/08 19:01:52 INFO YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.8; 19:01:52.566 INFO HaplotypeCallerSpark - Spark verbosity set to INFO (see --spark-verbosity argument); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 4.7 MB, free 361.6 MB); 19/04/08 19:02:38 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 450.5 KB, free 361.2 MB); 19/04/08 19:02:38 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 450.5 KB, free: 365.9 MB); 19/04/08 19:02:38 INFO SparkContext: Created broadcast 0 from broadcast at CramSource.java:114; 19/04/08 19:02:38 INFO MemoryStore: Block b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/issues/5869:17809,Usability,clear,cleared,17809,"ored as bytes in memory (estimated size 23.6 KB, free 360.5 MB); 19/04/08 19:03:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ip-xx.xx.xx.xx.ec2.internal:38471 (size: 23.6 KB, free: 365.8 MB); 19/04/08 19:03:27 INFO SparkContext: Created broadcast 4 from newAPIHadoopFile at PathSplitSource.java:96; 19/04/08 19:03:28 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 36.9 MB, free 323.6 MB); 19/04/08 19:03:28 INFO SparkUI: Stopped Spark web UI at http://ip-xx.xx.xx.xx.ec2.internal:4040; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Interrupting monitor thread; 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Shutting down all executors; 19/04/08 19:03:28 INFO YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down; 19/04/08 19:03:28 INFO SchedulerExtensionServices: Stopping SchedulerExtensionServices; (serviceOption=None,; services=List(),; started=false); 19/04/08 19:03:28 INFO YarnClientSchedulerBackend: Stopped; 19/04/08 19:03:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!; 19/04/08 19:03:28 INFO MemoryStore: MemoryStore cleared; 19/04/08 19:03:28 INFO BlockManager: BlockManager stopped; 19/04/08 19:03:28 INFO BlockManagerMaster: BlockManagerMaster stopped; 19/04/08 19:03:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!; 19/04/08 19:03:28 INFO SparkContext: Successfully stopped SparkContext; 19:03:28.389 INFO HaplotypeCallerSpark - Shutting down engine; [April 8, 2019 7:03:28 PM UTC] org.broadinstitute.hellbender.tools.HaplotypeCallerSpark done. Elapsed time: 1.75 minutes.; Runtime.totalMemory()=941096960; Exception in thread ""main"" java.lang.StackOverflowError; 	at java.util.HashMap.putMapEntries(HashMap.java:501); 	at java.util.HashMap.<init>(HashMap.java:490); 	at com.esotericsoftware.kryo.Generics.<init>(Generics.java:47); 	at com.esotericsoftware.kryo.serializers.FieldSerializerGenericsUtil.buildGenericsScop",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5869
https://github.com/broadinstitute/gatk/pull/5870:32,Testability,Test,Tested,32,"Instead of a String previously. Tested like this:. ```bash; $ ./gatk SelectVariants \; --variant src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf \; --select-random-fraction 0.01 \; --output gs://$BUCKET/variants.vcf; ```. and. ```; $ ./gatk SelectVariants \; --variant src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf \; --select-random-fraction 0.01 \; --output /tmp/variants.vcf; ```. If this works, the plan is to change all tools to use this format, so they can all write VCF output to Google Cloud Storage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5870
https://github.com/broadinstitute/gatk/pull/5870:101,Testability,test,test,101,"Instead of a String previously. Tested like this:. ```bash; $ ./gatk SelectVariants \; --variant src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf \; --select-random-fraction 0.01 \; --output gs://$BUCKET/variants.vcf; ```. and. ```; $ ./gatk SelectVariants \; --variant src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf \; --select-random-fraction 0.01 \; --output /tmp/variants.vcf; ```. If this works, the plan is to change all tools to use this format, so they can all write VCF output to Google Cloud Storage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5870
https://github.com/broadinstitute/gatk/pull/5870:295,Testability,test,test,295,"Instead of a String previously. Tested like this:. ```bash; $ ./gatk SelectVariants \; --variant src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf \; --select-random-fraction 0.01 \; --output gs://$BUCKET/variants.vcf; ```. and. ```; $ ./gatk SelectVariants \; --variant src/test/resources/large/VQSR/dbsnp_132_b37.leftAligned.20.1M-10M.vcf \; --select-random-fraction 0.01 \; --output /tmp/variants.vcf; ```. If this works, the plan is to change all tools to use this format, so they can all write VCF output to Google Cloud Storage.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5870
https://github.com/broadinstitute/gatk/issues/5871:502,Availability,failure,failures,502,"## Feature request. ### Tool(s) or class(es) involved; ValidateBasicSomaticShortMutations . ### Description; It turns out that this tool is doing a subset of the CGA tool, MutationValidator. Originally, the understanding (by both DSP and CGA) was the the GATK tool was doing a different algorithm, but this turned out to be incorrect. We should rename the GATK tool perhaps to SomaticShortMutationValidator and cite MutationValidator. Any relevant WDL should be updated to prevent unnecessary workflow failures. @davidbenjamin . (citation does not exist as per last offline meeting with CGA)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5871
https://github.com/broadinstitute/gatk/issues/5871:462,Deployability,update,updated,462,"## Feature request. ### Tool(s) or class(es) involved; ValidateBasicSomaticShortMutations . ### Description; It turns out that this tool is doing a subset of the CGA tool, MutationValidator. Originally, the understanding (by both DSP and CGA) was the the GATK tool was doing a different algorithm, but this turned out to be incorrect. We should rename the GATK tool perhaps to SomaticShortMutationValidator and cite MutationValidator. Any relevant WDL should be updated to prevent unnecessary workflow failures. @davidbenjamin . (citation does not exist as per last offline meeting with CGA)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5871
https://github.com/broadinstitute/gatk/issues/5871:55,Security,Validat,ValidateBasicSomaticShortMutations,55,"## Feature request. ### Tool(s) or class(es) involved; ValidateBasicSomaticShortMutations . ### Description; It turns out that this tool is doing a subset of the CGA tool, MutationValidator. Originally, the understanding (by both DSP and CGA) was the the GATK tool was doing a different algorithm, but this turned out to be incorrect. We should rename the GATK tool perhaps to SomaticShortMutationValidator and cite MutationValidator. Any relevant WDL should be updated to prevent unnecessary workflow failures. @davidbenjamin . (citation does not exist as per last offline meeting with CGA)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5871
https://github.com/broadinstitute/gatk/pull/5873:287,Safety,avoid,avoid,287,"Closes #5821. @bhanugandham With this PR we will no longer have to recommend against using `CalculateContamination` for gene panels. @takutosato This puts in a last-ditch calculation that uses hom ref sites *and* uses sites that didn't get a clear minor allele fraction segmentation. To avoid distorting the signal with LoH hets, it removes the hom ref sites with the highest allele fraction, which will work unless there's a huge amount of CNV. This will result in a slight underestimate, but for a small gene panel there's not much you can do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5873
https://github.com/broadinstitute/gatk/pull/5873:242,Usability,clear,clear,242,"Closes #5821. @bhanugandham With this PR we will no longer have to recommend against using `CalculateContamination` for gene panels. @takutosato This puts in a last-ditch calculation that uses hom ref sites *and* uses sites that didn't get a clear minor allele fraction segmentation. To avoid distorting the signal with LoH hets, it removes the hom ref sites with the highest allele fraction, which will work unless there's a huge amount of CNV. This will result in a slight underestimate, but for a small gene panel there's not much you can do.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5873
https://github.com/broadinstitute/gatk/issues/5875:354,Integrability,depend,depending,354,"For some deletion variants, the end position that is generated for MAF output is not correct. This is due to the transformation on the alleles and positions for MAF output (specifically in `MafOutputRenderer::adjustIndelAlleleInformationForMafOutput`). When the end positions are wrong, they are at least 1 base greater than the actual correct position (depending on the reference allele length).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5875
https://github.com/broadinstitute/gatk/issues/5877:119,Deployability,release,release,119,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877
https://github.com/broadinstitute/gatk/issues/5877:578,Deployability,release,release,578,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877
https://github.com/broadinstitute/gatk/issues/5877:594,Deployability,upgrade,upgraded,594,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877
https://github.com/broadinstitute/gatk/issues/5877:16,Testability,test,test,16,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877
https://github.com/broadinstitute/gatk/issues/5877:234,Testability,test,tests,234,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877
https://github.com/broadinstitute/gatk/issues/5877:285,Testability,test,test,285,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877
https://github.com/broadinstitute/gatk/issues/5877:741,Testability,test,testSortBAMsSharded,741,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877
https://github.com/broadinstitute/gatk/issues/5877:761,Testability,test,tests,761,"Based on a GATK test run using a locally built htsjdk, the following are prerequisites to upgrading to the next htsjdk release (post-2.19.0):. - https://github.com/broadinstitute/gatk/issues/5839; - Fix CountReads and CountReadsSpark tests. These are failing because they specify `src/test/resources/org/broadinstitute/hellbender/tools/chr7_1_20.interval_list` as an interval argument. Since there is now a tribble codec that handles that extension, the file is treated as a picard interval list and parsed by the codec, but the file is not in picard interval list format.; - A release of Disq upgraded to the new htsjdk version (due to CRAM changes - Container.offset renamed to Container.byteOffset); - Fix failing SortSamSparkIntegration.testSortBAMsSharded tests. Under investigation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5877
https://github.com/broadinstitute/gatk/pull/5878:60,Availability,error,error,60,Suppress the `cp: target '/gatk/srcdir' is not a directory` error message that appears in all of the travis logs by creating the target srcdir first in run_unit_tests.sh.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5878
https://github.com/broadinstitute/gatk/pull/5878:66,Integrability,message,message,66,Suppress the `cp: target '/gatk/srcdir' is not a directory` error message that appears in all of the travis logs by creating the target srcdir first in run_unit_tests.sh.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5878
https://github.com/broadinstitute/gatk/pull/5878:108,Testability,log,logs,108,Suppress the `cp: target '/gatk/srcdir' is not a directory` error message that appears in all of the travis logs by creating the target srcdir first in run_unit_tests.sh.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5878
https://github.com/broadinstitute/gatk/pull/5879:383,Deployability,integrat,integration,383,"When parsing interval arguments in GATK using the latest htsjdk, files that end in "".interval_list"" are claimed by the new IntervalListCodec introduced in https://github.com/samtools/htsjdk/pull/1327. This PR renames the one test file in GATK that has a Picard interval list file extension but isn't really a Picard interval list; without this change, CountReads and CountReadsSpark integration tests will fail when we upgrade to the next htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5879
https://github.com/broadinstitute/gatk/pull/5879:419,Deployability,upgrade,upgrade,419,"When parsing interval arguments in GATK using the latest htsjdk, files that end in "".interval_list"" are claimed by the new IntervalListCodec introduced in https://github.com/samtools/htsjdk/pull/1327. This PR renames the one test file in GATK that has a Picard interval list file extension but isn't really a Picard interval list; without this change, CountReads and CountReadsSpark integration tests will fail when we upgrade to the next htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5879
https://github.com/broadinstitute/gatk/pull/5879:383,Integrability,integrat,integration,383,"When parsing interval arguments in GATK using the latest htsjdk, files that end in "".interval_list"" are claimed by the new IntervalListCodec introduced in https://github.com/samtools/htsjdk/pull/1327. This PR renames the one test file in GATK that has a Picard interval list file extension but isn't really a Picard interval list; without this change, CountReads and CountReadsSpark integration tests will fail when we upgrade to the next htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5879
https://github.com/broadinstitute/gatk/pull/5879:225,Testability,test,test,225,"When parsing interval arguments in GATK using the latest htsjdk, files that end in "".interval_list"" are claimed by the new IntervalListCodec introduced in https://github.com/samtools/htsjdk/pull/1327. This PR renames the one test file in GATK that has a Picard interval list file extension but isn't really a Picard interval list; without this change, CountReads and CountReadsSpark integration tests will fail when we upgrade to the next htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5879
https://github.com/broadinstitute/gatk/pull/5879:395,Testability,test,tests,395,"When parsing interval arguments in GATK using the latest htsjdk, files that end in "".interval_list"" are claimed by the new IntervalListCodec introduced in https://github.com/samtools/htsjdk/pull/1327. This PR renames the one test file in GATK that has a Picard interval list file extension but isn't really a Picard interval list; without this change, CountReads and CountReadsSpark integration tests will fail when we upgrade to the next htsjdk.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5879
https://github.com/broadinstitute/gatk/issues/5880:345,Availability,error,error,345,"In our Mutect2 workflow, we run a pair of Normal/Tumor through `CalculateContamination` step, the output of which is used in `FilterMutectCalls`. Since upgrading to `4.1.0.0`, `CalculateContamination` is breaking in cases where there're mismatched of N/T samples. . For e.g., `4.0.11.0` generates the following output:; ```; level contamination error; whole_bam 0.5013841326835697 0.0055644124674135865; ```; And `4.1.0.0` gives the following:; ```; sample contamination error; Run06_Pair07_Tumor 1.0 0.03452380752462225; ```. As a result of the above output files, the next step in our pipeline `FilterMutectCall` is failing (issue related to https://github.com/broadinstitute/gatk/issues/5821)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880
https://github.com/broadinstitute/gatk/issues/5880:471,Availability,error,error,471,"In our Mutect2 workflow, we run a pair of Normal/Tumor through `CalculateContamination` step, the output of which is used in `FilterMutectCalls`. Since upgrading to `4.1.0.0`, `CalculateContamination` is breaking in cases where there're mismatched of N/T samples. . For e.g., `4.0.11.0` generates the following output:; ```; level contamination error; whole_bam 0.5013841326835697 0.0055644124674135865; ```; And `4.1.0.0` gives the following:; ```; sample contamination error; Run06_Pair07_Tumor 1.0 0.03452380752462225; ```. As a result of the above output files, the next step in our pipeline `FilterMutectCall` is failing (issue related to https://github.com/broadinstitute/gatk/issues/5821)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880
https://github.com/broadinstitute/gatk/issues/5880:587,Deployability,pipeline,pipeline,587,"In our Mutect2 workflow, we run a pair of Normal/Tumor through `CalculateContamination` step, the output of which is used in `FilterMutectCalls`. Since upgrading to `4.1.0.0`, `CalculateContamination` is breaking in cases where there're mismatched of N/T samples. . For e.g., `4.0.11.0` generates the following output:; ```; level contamination error; whole_bam 0.5013841326835697 0.0055644124674135865; ```; And `4.1.0.0` gives the following:; ```; sample contamination error; Run06_Pair07_Tumor 1.0 0.03452380752462225; ```. As a result of the above output files, the next step in our pipeline `FilterMutectCall` is failing (issue related to https://github.com/broadinstitute/gatk/issues/5821)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5880
https://github.com/broadinstitute/gatk/issues/5881:215,Safety,detect,detects,215,"@cmnbroad and I have both observed that the `SortSamSparkIntegrationTest.testSortBAMsSharded` tests fail locally on our machines despite the tests apparently working on travis. The tests fail because the comparator detects the files are out of their reported sort order. When I went digging into the failing tests it appears that the files are getting correctly sorted and written out correctly into 2 shards with proper names (`filename-0000` and `filename-0001`). After reading the sharded directory as input, it appears that the two files are read out of order. That is to say that calling `readsRDD.collect()` clearly places all of the `filename-0001` reads before the `filename-0000` reads. . After digging around it appears the problem might lie in Disq somewhere as it appears everything is working as expected until the `abstractSamSource.getReads()` line is encountered in `HtsjdkReadsRddStorage`. I suspect something is going awry with the filesystem mechanism for ordering the input files on our Macs that travis is sidestepping. . Out of curiosity @tomwhite I thought that the sharded output wrote headerless bam chunks, but that appears not to be the case at all? Was I wrong in that assumption or did that change when we switched to Disq.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5881
https://github.com/broadinstitute/gatk/issues/5881:73,Testability,test,testSortBAMsSharded,73,"@cmnbroad and I have both observed that the `SortSamSparkIntegrationTest.testSortBAMsSharded` tests fail locally on our machines despite the tests apparently working on travis. The tests fail because the comparator detects the files are out of their reported sort order. When I went digging into the failing tests it appears that the files are getting correctly sorted and written out correctly into 2 shards with proper names (`filename-0000` and `filename-0001`). After reading the sharded directory as input, it appears that the two files are read out of order. That is to say that calling `readsRDD.collect()` clearly places all of the `filename-0001` reads before the `filename-0000` reads. . After digging around it appears the problem might lie in Disq somewhere as it appears everything is working as expected until the `abstractSamSource.getReads()` line is encountered in `HtsjdkReadsRddStorage`. I suspect something is going awry with the filesystem mechanism for ordering the input files on our Macs that travis is sidestepping. . Out of curiosity @tomwhite I thought that the sharded output wrote headerless bam chunks, but that appears not to be the case at all? Was I wrong in that assumption or did that change when we switched to Disq.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5881
https://github.com/broadinstitute/gatk/issues/5881:94,Testability,test,tests,94,"@cmnbroad and I have both observed that the `SortSamSparkIntegrationTest.testSortBAMsSharded` tests fail locally on our machines despite the tests apparently working on travis. The tests fail because the comparator detects the files are out of their reported sort order. When I went digging into the failing tests it appears that the files are getting correctly sorted and written out correctly into 2 shards with proper names (`filename-0000` and `filename-0001`). After reading the sharded directory as input, it appears that the two files are read out of order. That is to say that calling `readsRDD.collect()` clearly places all of the `filename-0001` reads before the `filename-0000` reads. . After digging around it appears the problem might lie in Disq somewhere as it appears everything is working as expected until the `abstractSamSource.getReads()` line is encountered in `HtsjdkReadsRddStorage`. I suspect something is going awry with the filesystem mechanism for ordering the input files on our Macs that travis is sidestepping. . Out of curiosity @tomwhite I thought that the sharded output wrote headerless bam chunks, but that appears not to be the case at all? Was I wrong in that assumption or did that change when we switched to Disq.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5881
https://github.com/broadinstitute/gatk/issues/5881:141,Testability,test,tests,141,"@cmnbroad and I have both observed that the `SortSamSparkIntegrationTest.testSortBAMsSharded` tests fail locally on our machines despite the tests apparently working on travis. The tests fail because the comparator detects the files are out of their reported sort order. When I went digging into the failing tests it appears that the files are getting correctly sorted and written out correctly into 2 shards with proper names (`filename-0000` and `filename-0001`). After reading the sharded directory as input, it appears that the two files are read out of order. That is to say that calling `readsRDD.collect()` clearly places all of the `filename-0001` reads before the `filename-0000` reads. . After digging around it appears the problem might lie in Disq somewhere as it appears everything is working as expected until the `abstractSamSource.getReads()` line is encountered in `HtsjdkReadsRddStorage`. I suspect something is going awry with the filesystem mechanism for ordering the input files on our Macs that travis is sidestepping. . Out of curiosity @tomwhite I thought that the sharded output wrote headerless bam chunks, but that appears not to be the case at all? Was I wrong in that assumption or did that change when we switched to Disq.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5881
https://github.com/broadinstitute/gatk/issues/5881:181,Testability,test,tests,181,"@cmnbroad and I have both observed that the `SortSamSparkIntegrationTest.testSortBAMsSharded` tests fail locally on our machines despite the tests apparently working on travis. The tests fail because the comparator detects the files are out of their reported sort order. When I went digging into the failing tests it appears that the files are getting correctly sorted and written out correctly into 2 shards with proper names (`filename-0000` and `filename-0001`). After reading the sharded directory as input, it appears that the two files are read out of order. That is to say that calling `readsRDD.collect()` clearly places all of the `filename-0001` reads before the `filename-0000` reads. . After digging around it appears the problem might lie in Disq somewhere as it appears everything is working as expected until the `abstractSamSource.getReads()` line is encountered in `HtsjdkReadsRddStorage`. I suspect something is going awry with the filesystem mechanism for ordering the input files on our Macs that travis is sidestepping. . Out of curiosity @tomwhite I thought that the sharded output wrote headerless bam chunks, but that appears not to be the case at all? Was I wrong in that assumption or did that change when we switched to Disq.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5881
https://github.com/broadinstitute/gatk/issues/5881:308,Testability,test,tests,308,"@cmnbroad and I have both observed that the `SortSamSparkIntegrationTest.testSortBAMsSharded` tests fail locally on our machines despite the tests apparently working on travis. The tests fail because the comparator detects the files are out of their reported sort order. When I went digging into the failing tests it appears that the files are getting correctly sorted and written out correctly into 2 shards with proper names (`filename-0000` and `filename-0001`). After reading the sharded directory as input, it appears that the two files are read out of order. That is to say that calling `readsRDD.collect()` clearly places all of the `filename-0001` reads before the `filename-0000` reads. . After digging around it appears the problem might lie in Disq somewhere as it appears everything is working as expected until the `abstractSamSource.getReads()` line is encountered in `HtsjdkReadsRddStorage`. I suspect something is going awry with the filesystem mechanism for ordering the input files on our Macs that travis is sidestepping. . Out of curiosity @tomwhite I thought that the sharded output wrote headerless bam chunks, but that appears not to be the case at all? Was I wrong in that assumption or did that change when we switched to Disq.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5881
https://github.com/broadinstitute/gatk/issues/5881:614,Usability,clear,clearly,614,"@cmnbroad and I have both observed that the `SortSamSparkIntegrationTest.testSortBAMsSharded` tests fail locally on our machines despite the tests apparently working on travis. The tests fail because the comparator detects the files are out of their reported sort order. When I went digging into the failing tests it appears that the files are getting correctly sorted and written out correctly into 2 shards with proper names (`filename-0000` and `filename-0001`). After reading the sharded directory as input, it appears that the two files are read out of order. That is to say that calling `readsRDD.collect()` clearly places all of the `filename-0001` reads before the `filename-0000` reads. . After digging around it appears the problem might lie in Disq somewhere as it appears everything is working as expected until the `abstractSamSource.getReads()` line is encountered in `HtsjdkReadsRddStorage`. I suspect something is going awry with the filesystem mechanism for ordering the input files on our Macs that travis is sidestepping. . Out of curiosity @tomwhite I thought that the sharded output wrote headerless bam chunks, but that appears not to be the case at all? Was I wrong in that assumption or did that change when we switched to Disq.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5881
https://github.com/broadinstitute/gatk/issues/5884:548,Deployability,release,release,548,"Hi GATK team!; I would like a minimum fragment length filter during calling CNVs (the CountReads step) if possible. @LeeTL1220 helped me to find an option to set a maximum fragment length with `--read_filter FragmentLength --maxFragmentLength <value>` ([link to the GATK forum page](https://gatkforums.broadinstitute.org/gatk/discussion/2338/how-can-i-invoke-read-filters-and-their-arguments)). However, it appears there is no minimum fragment length filter implemented. Would it be possible to include the min fragment filter option in the future release or as a pre-release version?. The reason of asking this filter is:. there are shorter fragments in a FFPE tumor PCR-free WGS (top) compared to the PCR-plus WGS (bottom) from the same individual:; ![image](https://user-images.githubusercontent.com/5141643/56065278-611c9a80-5d42-11e9-8b60-b82ea1d69e87.png); and I observed higher MAD values in the PCR-free samples. I would like to see whether the results improve after discarding the shorter fragments in the FFPE PCR-free WGS.; ![image](https://user-images.githubusercontent.com/5141643/56065600-50b8ef80-5d43-11e9-9160-fb0beb5c11de.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5884
https://github.com/broadinstitute/gatk/issues/5884:568,Deployability,release,release,568,"Hi GATK team!; I would like a minimum fragment length filter during calling CNVs (the CountReads step) if possible. @LeeTL1220 helped me to find an option to set a maximum fragment length with `--read_filter FragmentLength --maxFragmentLength <value>` ([link to the GATK forum page](https://gatkforums.broadinstitute.org/gatk/discussion/2338/how-can-i-invoke-read-filters-and-their-arguments)). However, it appears there is no minimum fragment length filter implemented. Would it be possible to include the min fragment filter option in the future release or as a pre-release version?. The reason of asking this filter is:. there are shorter fragments in a FFPE tumor PCR-free WGS (top) compared to the PCR-plus WGS (bottom) from the same individual:; ![image](https://user-images.githubusercontent.com/5141643/56065278-611c9a80-5d42-11e9-8b60-b82ea1d69e87.png); and I observed higher MAD values in the PCR-free samples. I would like to see whether the results improve after discarding the shorter fragments in the FFPE PCR-free WGS.; ![image](https://user-images.githubusercontent.com/5141643/56065600-50b8ef80-5d43-11e9-9160-fb0beb5c11de.png)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5884
https://github.com/broadinstitute/gatk/pull/5886:27,Testability,test,tests,27,Also improved the existing tests for FragmentLengthReadFilter. Resolves #5884,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5886
https://github.com/broadinstitute/gatk/pull/5887:321,Modifiability,refactor,refactoring,321,"Added the ability to specify IntervalMergingRule.NONE so so that no merging is performed. Also added the ability to request from IntervalArgumentCollection the unmerged user intervals. I have not solved the underlying issue that the GenomeLocSet requires non-overlapping intervals, though I acknowledge that replacing or refactoring that class is the long term solution to this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887
https://github.com/broadinstitute/gatk/pull/5887:79,Performance,perform,performed,79,"Added the ability to specify IntervalMergingRule.NONE so so that no merging is performed. Also added the ability to request from IntervalArgumentCollection the unmerged user intervals. I have not solved the underlying issue that the GenomeLocSet requires non-overlapping intervals, though I acknowledge that replacing or refactoring that class is the long term solution to this problem.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5887
https://github.com/broadinstitute/gatk/issues/5888:51,Integrability,depend,dependencies,51,so non-Docker users have a way to get the required dependencies.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5888
https://github.com/broadinstitute/gatk/issues/5889:930,Availability,down,downstream,930,"## Documentation request. ### Tool(s) or class(es) involved; Readme for M2 in https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl. ### Description ; This is the text currently in the readme, it needs to be updated to feature Funcotator instead of Oncotator:. > Functional annotation (Oncotator); > ; > The M2 WDL can optionally run oncotator for functional annotation and produce a TCGA MAF from the M2 VCF. Oncotator is not a GATK4 tool and is provided in the M2 WDL as a convenience. There are several notes and caveats; > ; > Several parameters should be passed in to populate the TCGA MAF metadata fields. Default values are provided, though we recommend that you specify the values. These parameters are ignored if you do not run oncotator.; > ; > Several fields in a TCGA MAF cannot be generated by M2 and oncotator, such as all fields relating to validation alleles. These will need to be populated by a downstream process created by the user.; > ; > Oncotator does not enforce the TCGA MAF controlled vocabulary, since it is often too restrictive for general use. This is up to the user to specify correctly. Therefore, we cannot guarantee that a TCGA MAF generated here will pass the TCGA Validator. If you are unsure about the ramifications of this statement, then it probably does not concern you.; > ; > More information about Oncotator can be found at: http://archive.broadinstitute.org/cancer/cga/oncotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889
https://github.com/broadinstitute/gatk/issues/5889:225,Deployability,update,updated,225,"## Documentation request. ### Tool(s) or class(es) involved; Readme for M2 in https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl. ### Description ; This is the text currently in the readme, it needs to be updated to feature Funcotator instead of Oncotator:. > Functional annotation (Oncotator); > ; > The M2 WDL can optionally run oncotator for functional annotation and produce a TCGA MAF from the M2 VCF. Oncotator is not a GATK4 tool and is provided in the M2 WDL as a convenience. There are several notes and caveats; > ; > Several parameters should be passed in to populate the TCGA MAF metadata fields. Default values are provided, though we recommend that you specify the values. These parameters are ignored if you do not run oncotator.; > ; > Several fields in a TCGA MAF cannot be generated by M2 and oncotator, such as all fields relating to validation alleles. These will need to be populated by a downstream process created by the user.; > ; > Oncotator does not enforce the TCGA MAF controlled vocabulary, since it is often too restrictive for general use. This is up to the user to specify correctly. Therefore, we cannot guarantee that a TCGA MAF generated here will pass the TCGA Validator. If you are unsure about the ramifications of this statement, then it probably does not concern you.; > ; > More information about Oncotator can be found at: http://archive.broadinstitute.org/cancer/cga/oncotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889
https://github.com/broadinstitute/gatk/issues/5889:873,Security,validat,validation,873,"## Documentation request. ### Tool(s) or class(es) involved; Readme for M2 in https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl. ### Description ; This is the text currently in the readme, it needs to be updated to feature Funcotator instead of Oncotator:. > Functional annotation (Oncotator); > ; > The M2 WDL can optionally run oncotator for functional annotation and produce a TCGA MAF from the M2 VCF. Oncotator is not a GATK4 tool and is provided in the M2 WDL as a convenience. There are several notes and caveats; > ; > Several parameters should be passed in to populate the TCGA MAF metadata fields. Default values are provided, though we recommend that you specify the values. These parameters are ignored if you do not run oncotator.; > ; > Several fields in a TCGA MAF cannot be generated by M2 and oncotator, such as all fields relating to validation alleles. These will need to be populated by a downstream process created by the user.; > ; > Oncotator does not enforce the TCGA MAF controlled vocabulary, since it is often too restrictive for general use. This is up to the user to specify correctly. Therefore, we cannot guarantee that a TCGA MAF generated here will pass the TCGA Validator. If you are unsure about the ramifications of this statement, then it probably does not concern you.; > ; > More information about Oncotator can be found at: http://archive.broadinstitute.org/cancer/cga/oncotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889
https://github.com/broadinstitute/gatk/issues/5889:1217,Security,Validat,Validator,1217,"## Documentation request. ### Tool(s) or class(es) involved; Readme for M2 in https://github.com/broadinstitute/gatk/tree/master/scripts/mutect2_wdl. ### Description ; This is the text currently in the readme, it needs to be updated to feature Funcotator instead of Oncotator:. > Functional annotation (Oncotator); > ; > The M2 WDL can optionally run oncotator for functional annotation and produce a TCGA MAF from the M2 VCF. Oncotator is not a GATK4 tool and is provided in the M2 WDL as a convenience. There are several notes and caveats; > ; > Several parameters should be passed in to populate the TCGA MAF metadata fields. Default values are provided, though we recommend that you specify the values. These parameters are ignored if you do not run oncotator.; > ; > Several fields in a TCGA MAF cannot be generated by M2 and oncotator, such as all fields relating to validation alleles. These will need to be populated by a downstream process created by the user.; > ; > Oncotator does not enforce the TCGA MAF controlled vocabulary, since it is often too restrictive for general use. This is up to the user to specify correctly. Therefore, we cannot guarantee that a TCGA MAF generated here will pass the TCGA Validator. If you are unsure about the ramifications of this statement, then it probably does not concern you.; > ; > More information about Oncotator can be found at: http://archive.broadinstitute.org/cancer/cga/oncotator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5889
https://github.com/broadinstitute/gatk/issues/5891:123,Security,validat,validate,123,"After #5887 goes in. PreprocessIntervals should still allow the use of IntervalMergingRule.OVERLAPPING_ONLY, and we should validate early that intervals are non-overlapping elsewhere.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5891
https://github.com/broadinstitute/gatk/issues/5893:242,Security,expose,exposed,242,"Since https://github.com/broadinstitute/gatk/pull/4711, we use a deletion hook to delete the temp directories created by the R executor, but the R executor also deletes these itself. https://github.com/samtools/htsjdk/pull/1315 seems to have exposed this by propagating the exception from the second attempt. See https://gatkforums.broadinstitute.org/gatk/discussion/comment/46733#Comment_46733.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5893
https://github.com/broadinstitute/gatk/issues/5895:457,Availability,Failure,Failures,457,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:154,Performance,cache,cached,154,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:215,Performance,cache,cached,215,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:307,Performance,cache,cache,307,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1542,Performance,cache,cache,1542,"umentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:530); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1621,Performance,cache,cache,1621,"umentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:530); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1747,Performance,cache,cacheHit,1747,"ance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:530); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(C",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:450,Testability,test,tests,450,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:609,Testability,test,testFuncotatorWithoutValidatingResults,609,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:675,Testability,test,testVcfDatasourceAccountsForAltAlleles,675,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:741,Testability,test,testVcfMafConcordance,741,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:794,Testability,test,testFilter,794,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:837,Testability,test,testGenotypeGivenAllelesMode,837,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:890,Testability,test,testContaminationFilter,890,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:938,Testability,test,testDreamTumorNormal,938,"Results from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.he",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:983,Testability,test,testGivenAllelesMode,983,"ts from a Travis run using a branch that throws whenever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1028,Testability,test,testGivenAllelesZeroCoverage,1028,"henever a FeatureCache miss occurs, and the query interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContex",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1081,Testability,test,testMissingAF,1081,"ery interval is on the same contig as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1119,Testability,test,testPon,1119,"as was the current cached interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(Fe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1151,Testability,test,testTumorOnly,1151,"ed interval, but with a start locus *before* the current cached interval start. This indicates that features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124);",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1232,Testability,test,testCacheHitDetection,1232,"t features that have already been ejected from the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(Ha",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1281,Testability,test,testSingleDataSourceMultipleQueries,1281,"rom the cache are being re-queried, and the corresponding tool might benefit from a smarter ejection strategy. Some of these could be artifacts of the tests. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:530); at org.broa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:1429,Testability,test,testGenotypeGivenAllelesMode,1429,"s. Failures (see https://travis-ci.com/broadinstitute/gatk/builds/108966841):. FuncotatorIntegrationTest.exhaustiveArgumentTest; FuncotatorIntegrationTest.testFuncotatorWithoutValidatingResults; FuncotatorIntegrationTest.testVcfDatasourceAccountsForAltAlleles; FuncotatorIntegrationTest.testVcfMafConcordance; XGBoostEvidenceFilterUnitTest.testFilter; HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode; Mutect2IntegrationTest.testContaminationFilter; Mutect2IntegrationTest.testDreamTumorNormal; Mutect2IntegrationTest.testGivenAllelesMode; Mutect2IntegrationTest.testGivenAllelesZeroCoverage; Mutect2IntegrationTest.testMissingAF; Mutect2IntegrationTest.testPon; Mutect2IntegrationTest.testTumorOnly. Also, these probably don't count, but:; FeatureDataSourceUnitTest.testCacheHitDetection; FeatureDataSourceUnitTest.testSingleDataSourceMultipleQueries. The HC stack was:. `org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode [31mFAILED[39m; org.broadinstitute.hellbender.exceptions.GATKException: Locatable cache miss while attempting to retrieve a previous interval from the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:530); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); at org.broadinstitute.hellbender.engine.AssemblyRegionW",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:3353,Testability,test,testutils,3353,om the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:530); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode(HaplotypeCallerIntegrationTest.java:394); `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5895:3532,Testability,test,testGenotypeGivenAllelesMode,3532,om the locatable cache. New interval: 20:9999980-10000254 Previous: 20:10000555-10001000; at org.broadinstitute.hellbender.engine.FeatureCache.cacheHit(FeatureCache.java:164); at org.broadinstitute.hellbender.engine.FeatureDataSource.queryAndPrefetch(FeatureDataSource.java:497); at org.broadinstitute.hellbender.engine.FeatureManager.getFeatures(FeatureManager.java:340); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:172); at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:530); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:148); at org.broadinstitute.hellbender.Main.instanceMain(Main.java:189); at org.broadinstitute.hellbender.CommandLineProgramTest.runCommandLine(CommandLineProgramTest.java:27); at org.broadinstitute.hellbender.testutils.CommandLineProgramTester.runCommandLine(CommandLineProgramTester.java:97); at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerIntegrationTest.testGenotypeGivenAllelesMode(HaplotypeCallerIntegrationTest.java:394); `,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5895
https://github.com/broadinstitute/gatk/issues/5897:2183,Availability,down,down,2183,"` averageHQSoftClips.mean() > AVERAGE_HQ_SOFTCLIPS_HQ_BASES_THRESHOLD`, that it will mark the `ActivityProfileState` object it constructs as `ActivityProfileState.Type.HIGH_QUALITY_SOFT_CLIPS`. Subsequently in `ActivityProfile.processState()` a `HIGH_QUALITY_SOFT_CLIPS` will end up getting copied twice per the average number of high quality soft clips at a site. What this means in the end is that each HQ softclip site gets its activity likelihood multiplied by the number of softclips found in average at that site, currently the code centers that ""bright spot"" on the base corresponding to the base that corresponds to the edge of the softclip bases. Its worth noting that the way the average is computed in the HaplotypeCallerEngine is that the high quality softclips are averaged against each other with no heed paid to non-softclipped bases. (see `ReferenceConfidenceModel.applyPileupElementRefVsNonRefLikelihoodAndCount()`) It is conceivable that a site could trigger `AVERAGE_HQ_SOFTCLIPS_HQ_BASES_THRESHOLD` with just a single high quality softclipped read of a sufficient length at a site. This may result in the entire site having its activity likelihood multiplied by at least 12x. This is very old behavior dating back to the original HaplotypeCaller so far as I can tell. Its possible since we liberal about our Activity threshold and since we are already smoothing our probability thresholds out so wide that this doesn't make a substantial difference one way or another. It is worth evaluating two questions related to this problem:; 1. Should the probability density around a high quality softclip be smoothed to distribute the probability density farther away than the current maximum of 50 bases? (Effectively what #5767 accomplishes); 2. Should we even be creating these ""bright spots"" at all for what may ultimately be a very small number of high quality softclips at a location? We would need to evaluate what impact toning this down has on active region selection downstream.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5897
https://github.com/broadinstitute/gatk/issues/5897:2219,Availability,down,downstream,2219,"` averageHQSoftClips.mean() > AVERAGE_HQ_SOFTCLIPS_HQ_BASES_THRESHOLD`, that it will mark the `ActivityProfileState` object it constructs as `ActivityProfileState.Type.HIGH_QUALITY_SOFT_CLIPS`. Subsequently in `ActivityProfile.processState()` a `HIGH_QUALITY_SOFT_CLIPS` will end up getting copied twice per the average number of high quality soft clips at a site. What this means in the end is that each HQ softclip site gets its activity likelihood multiplied by the number of softclips found in average at that site, currently the code centers that ""bright spot"" on the base corresponding to the base that corresponds to the edge of the softclip bases. Its worth noting that the way the average is computed in the HaplotypeCallerEngine is that the high quality softclips are averaged against each other with no heed paid to non-softclipped bases. (see `ReferenceConfidenceModel.applyPileupElementRefVsNonRefLikelihoodAndCount()`) It is conceivable that a site could trigger `AVERAGE_HQ_SOFTCLIPS_HQ_BASES_THRESHOLD` with just a single high quality softclipped read of a sufficient length at a site. This may result in the entire site having its activity likelihood multiplied by at least 12x. This is very old behavior dating back to the original HaplotypeCaller so far as I can tell. Its possible since we liberal about our Activity threshold and since we are already smoothing our probability thresholds out so wide that this doesn't make a substantial difference one way or another. It is worth evaluating two questions related to this problem:; 1. Should the probability density around a high quality softclip be smoothed to distribute the probability density farther away than the current maximum of 50 bases? (Effectively what #5767 accomplishes); 2. Should we even be creating these ""bright spots"" at all for what may ultimately be a very small number of high quality softclips at a location? We would need to evaluate what impact toning this down has on active region selection downstream.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5897
https://github.com/broadinstitute/gatk/issues/5900:469,Availability,error,error,469,"I used newly GATK4.1 version, and my scripts are:. java -Xmx300g -jar $GATK Mutect2 \; --dont-use-soft-clipped-bases true \; --tmp-dir $cw/$i/tmp \; --input $DNAbam/ADAR16-DNA-2_NKD180600323/ADAR16-DNA-2_NKD180600323.best.uniq.pair.sort.markdup.bam \; --input $RNAbam/$i/$i.merge.markdup.reheader.bam \; --reference $genome\; --output $cw/$i/$i.dna.rna.vcf \; --normal-sample ADAR16-DNA-2_NKD180600323 \; --tumor-sample $i \; -bamout $cw/$i/$i.support.bam. and tail of error log are:. 12:05:06.287 INFO ProgressMeter - scaffold23905:111448 948.1 636040 670.9; 12:05:30.519 INFO ProgressMeter - scaffold23905:133852 948.5 636120 670.7; 12:05:57.277 INFO ProgressMeter - scaffold23905:147186 949.0 636170 670.4; 12:24:34.669 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 31261.455155273; 12:24:34.670 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 14618.28 sec; INFO	2019-04-13 12:45:11	SortingCollection	Creating merging iterator from 2 files; 13:30:49.708 INFO Mutect2 - Shutting down engine; [April 13, 2019 1:30:49 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 1,035.35 minutes.; Runtime.totalMemory()=238653800448; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421); 	at java.lang.StringBuilder.append(StringBuilder.java:136); 	at htsjdk.samtools.SAMTextHeaderCodec.advanceLine(SAMTextHeaderCodec.java:142); 	at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:97); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.loadDictionary(ReferenceSequenceFileFactory.java:235); 	at htsjdk.samtools.reference.AbstractFastaSequenceFile.(AbstractFastaSequenceFile.java:68)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900
https://github.com/broadinstitute/gatk/issues/5900:1030,Availability,down,down,1030,"ipts are:. java -Xmx300g -jar $GATK Mutect2 \; --dont-use-soft-clipped-bases true \; --tmp-dir $cw/$i/tmp \; --input $DNAbam/ADAR16-DNA-2_NKD180600323/ADAR16-DNA-2_NKD180600323.best.uniq.pair.sort.markdup.bam \; --input $RNAbam/$i/$i.merge.markdup.reheader.bam \; --reference $genome\; --output $cw/$i/$i.dna.rna.vcf \; --normal-sample ADAR16-DNA-2_NKD180600323 \; --tumor-sample $i \; -bamout $cw/$i/$i.support.bam. and tail of error log are:. 12:05:06.287 INFO ProgressMeter - scaffold23905:111448 948.1 636040 670.9; 12:05:30.519 INFO ProgressMeter - scaffold23905:133852 948.5 636120 670.7; 12:05:57.277 INFO ProgressMeter - scaffold23905:147186 949.0 636170 670.4; 12:24:34.669 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 31261.455155273; 12:24:34.670 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 14618.28 sec; INFO	2019-04-13 12:45:11	SortingCollection	Creating merging iterator from 2 files; 13:30:49.708 INFO Mutect2 - Shutting down engine; [April 13, 2019 1:30:49 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 1,035.35 minutes.; Runtime.totalMemory()=238653800448; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421); 	at java.lang.StringBuilder.append(StringBuilder.java:136); 	at htsjdk.samtools.SAMTextHeaderCodec.advanceLine(SAMTextHeaderCodec.java:142); 	at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:97); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.loadDictionary(ReferenceSequenceFileFactory.java:235); 	at htsjdk.samtools.reference.AbstractFastaSequenceFile.(AbstractFastaSequenceFile.java:68); 	at htsjdk.samtools.reference.Abstract",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900
https://github.com/broadinstitute/gatk/issues/5900:1855,Performance,load,loadDictionary,1855,"a Smith-Waterman : 14618.28 sec; INFO	2019-04-13 12:45:11	SortingCollection	Creating merging iterator from 2 files; 13:30:49.708 INFO Mutect2 - Shutting down engine; [April 13, 2019 1:30:49 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 1,035.35 minutes.; Runtime.totalMemory()=238653800448; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421); 	at java.lang.StringBuilder.append(StringBuilder.java:136); 	at htsjdk.samtools.SAMTextHeaderCodec.advanceLine(SAMTextHeaderCodec.java:142); 	at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:97); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.loadDictionary(ReferenceSequenceFileFactory.java:235); 	at htsjdk.samtools.reference.AbstractFastaSequenceFile.(AbstractFastaSequenceFile.java:68); 	at htsjdk.samtools.reference.AbstractIndexedFastaSequenceFile.(AbstractIndexedFastaSequenceFile.java:60); 	at htsjdk.samtools.reference.IndexedFastaSequenceFile.(IndexedFastaSequenceFile.java:80); 	at htsjdk.samtools.reference.IndexedFastaSequenceFile.(IndexedFastaSequenceFile.java:98); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.getReferenceSequenceFile(ReferenceSequenceFileFactory.java:138); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.(CachingIndexedFastaSequenceFile.java:134); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.(CachingIndexedFastaSequenceFile.java:111); 	at org.broadinstitute.hellbender.utils.fasta.CachingIndexedFastaSequenceFile.(CachingIndexedFastaSequenceFile.java:96); 	at org.broadinstitute.hellbender.engine.ReferenceFileSource.(ReferenceFileSource.java:35); 	at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900
https://github.com/broadinstitute/gatk/issues/5900:475,Testability,log,log,475,"I used newly GATK4.1 version, and my scripts are:. java -Xmx300g -jar $GATK Mutect2 \; --dont-use-soft-clipped-bases true \; --tmp-dir $cw/$i/tmp \; --input $DNAbam/ADAR16-DNA-2_NKD180600323/ADAR16-DNA-2_NKD180600323.best.uniq.pair.sort.markdup.bam \; --input $RNAbam/$i/$i.merge.markdup.reheader.bam \; --reference $genome\; --output $cw/$i/$i.dna.rna.vcf \; --normal-sample ADAR16-DNA-2_NKD180600323 \; --tumor-sample $i \; -bamout $cw/$i/$i.support.bam. and tail of error log are:. 12:05:06.287 INFO ProgressMeter - scaffold23905:111448 948.1 636040 670.9; 12:05:30.519 INFO ProgressMeter - scaffold23905:133852 948.5 636120 670.7; 12:05:57.277 INFO ProgressMeter - scaffold23905:147186 949.0 636170 670.4; 12:24:34.669 INFO PairHMM - Total compute time in PairHMM computeLogLikelihoods() : 31261.455155273; 12:24:34.670 INFO SmithWatermanAligner - Total compute time in java Smith-Waterman : 14618.28 sec; INFO	2019-04-13 12:45:11	SortingCollection	Creating merging iterator from 2 files; 13:30:49.708 INFO Mutect2 - Shutting down engine; [April 13, 2019 1:30:49 PM CST] org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2 done. Elapsed time: 1,035.35 minutes.; Runtime.totalMemory()=238653800448; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; 	at java.util.Arrays.copyOf(Arrays.java:3332); 	at java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137); 	at java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121); 	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421); 	at java.lang.StringBuilder.append(StringBuilder.java:136); 	at htsjdk.samtools.SAMTextHeaderCodec.advanceLine(SAMTextHeaderCodec.java:142); 	at htsjdk.samtools.SAMTextHeaderCodec.decode(SAMTextHeaderCodec.java:97); 	at htsjdk.samtools.reference.ReferenceSequenceFileFactory.loadDictionary(ReferenceSequenceFileFactory.java:235); 	at htsjdk.samtools.reference.AbstractFastaSequenceFile.(AbstractFastaSequenceFile.java:68)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5900
https://github.com/broadinstitute/gatk/issues/5903:52,Availability,down,download,52,"ENV: Centos7 JAVA1.8 GTAK4.1.0.0 ; Preparation:; 1. download datasource funcotator_dataSources.v1.6.20190124g; 2. untar the gnomAD. ```shell; fa=/share/share/data/NGS/ref_index/GATK_bundle/hg38/Homo_sapiens_assembly38.fasta; func=/share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g; gatk Funcotator \; 	--variant ../relapse.filtered.snps.indels.vcf \; 	--reference $fa \; 	--ref-version hg38 \; 	--data-sources-path $func \; 	--output relapse.funcotated.maf \; 	--output-file-format MAF; ```; after running the script above, it stucked and show nothing anymore, is there something I ignored ?. ```; Using GATK jar /share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar Funcotator --variant ../relapse.filtered.snps.indels.vcf --reference /share/share/data/NGS/ref_index/GATK_bundle/hg38/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g --output relapse.funcotated.maf --output-file-format MAF; 10:24:47.787 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:50.558 INFO Funcotator - ------------------------------------------------------------; 10:24:50.559 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 10:24:50.559 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:50.560 INFO Funcotator - Executing as javis@node4 on Linux v3.10.0-514.el7.x86_64 amd64; 10:24:50.560 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:50.560 INFO Funcotator - Start Date/Time: April 24, 2019 10:24:4",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5903
https://github.com/broadinstitute/gatk/issues/5903:1311,Performance,Load,Loading,1311,"4g; gatk Funcotator \; 	--variant ../relapse.filtered.snps.indels.vcf \; 	--reference $fa \; 	--ref-version hg38 \; 	--data-sources-path $func \; 	--output relapse.funcotated.maf \; 	--output-file-format MAF; ```; after running the script above, it stucked and show nothing anymore, is there something I ignored ?. ```; Using GATK jar /share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar Funcotator --variant ../relapse.filtered.snps.indels.vcf --reference /share/share/data/NGS/ref_index/GATK_bundle/hg38/Homo_sapiens_assembly38.fasta --ref-version hg38 --data-sources-path /share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g --output relapse.funcotated.maf --output-file-format MAF; 10:24:47.787 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/share/share/soft/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 10:24:50.558 INFO Funcotator - ------------------------------------------------------------; 10:24:50.559 INFO Funcotator - The Genome Analysis Toolkit (GATK) v4.1.0.0; 10:24:50.559 INFO Funcotator - For support and documentation go to https://software.broadinstitute.org/gatk/; 10:24:50.560 INFO Funcotator - Executing as javis@node4 on Linux v3.10.0-514.el7.x86_64 amd64; 10:24:50.560 INFO Funcotator - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_161-b12; 10:24:50.560 INFO Funcotator - Start Date/Time: April 24, 2019 10:24:47 AM CST; 10:24:50.560 INFO Funcotator - ------------------------------------------------------------; 10:24:50.560 INFO Funcotator - ------------------------------------------------------------; 10:24:50.561 INFO Funcotator - HTSJDK Version: 2.18.2; 10:24:50.561 INFO Funcotator - Picard Version: 2.18.25; 1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5903
https://github.com/broadinstitute/gatk/issues/5903:3188,Security,Validat,Validating,3188,--; 10:24:50.561 INFO Funcotator - HTSJDK Version: 2.18.2; 10:24:50.561 INFO Funcotator - Picard Version: 2.18.25; 10:24:50.561 INFO Funcotator - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 10:24:50.561 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 10:24:50.562 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 10:24:50.562 INFO Funcotator - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 10:24:50.562 INFO Funcotator - Deflater: IntelDeflater; 10:24:50.562 INFO Funcotator - Inflater: IntelInflater; 10:24:50.562 INFO Funcotator - GCS max retries/reopens: 20; 10:24:50.562 INFO Funcotator - Requester pays: disabled; 10:24:50.562 INFO Funcotator - Initializing engine; 10:24:51.150 INFO FeatureManager - Using codec VCFCodec to read file file:///sdb/research_th/Exon_Seq/2_classes/joint_out/relapse_joint/t_func/../relapse.filtered.snps.indels.vcf; 10:24:51.387 INFO Funcotator - Done initializing engine; 10:24:51.387 INFO Funcotator - Validating Sequence Dictionaries...; 10:24:51.422 INFO Funcotator - Processing user transcripts/defaults/overrides...; 10:24:51.423 INFO Funcotator - Initializing data sources...; 10:24:51.425 INFO DataSourceUtils - Initializing data sources from directory: /share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g; 10:24:51.427 INFO DataSourceUtils - Data sources version: 1.6.2019124g; 10:24:51.427 INFO DataSourceUtils - Data sources source: ftp://gsapubftp-anonymous@ftp.broadinstitute.org/bundle/funcotator/funcotator_dataSources.v1.6.20190124g.tar.gz; 10:24:51.427 INFO DataSourceUtils - Data sources alternate source: gs://broad-public-datasets/funcotator/funcotator_dataSources.v1.6.20190124g.tar.gz; 10:24:51.436 INFO DataSourceUtils - Resolved data source file path: file:///sdb/research_th/Exon_Seq/2_classes/joint_out/relapse_joint/t_func/clinvar_20180429_hg38.vcf -> file:///share/share/data/NGS/ref_index/GATK_bundle/funcotator_dataSources.v1.6.20190124g/clinvar/hg38,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5903
https://github.com/broadinstitute/gatk/issues/5905:423,Performance,perform,perform,423,"I found that if an indel + snp combination straddles a shard boundary GenotypeGVCFs will produce different calls. More info:. https://gatkforums.broadinstitute.org/gatk/discussion/comment/58230. This is in GATK 3.7. I do not know how this issue is related, would it have been merged in 3.7? https://github.com/broadinstitute/gatk/issues/2735. What is the expected behaviour of sharding in this circumstance? I am unable to perform the test using gatk4+ as it would require immense amounts of recomputation, just to confirm a bug report. Hopefully the devs know if this is expected behaviour or has been fixed already.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5905
https://github.com/broadinstitute/gatk/issues/5905:435,Testability,test,test,435,"I found that if an indel + snp combination straddles a shard boundary GenotypeGVCFs will produce different calls. More info:. https://gatkforums.broadinstitute.org/gatk/discussion/comment/58230. This is in GATK 3.7. I do not know how this issue is related, would it have been merged in 3.7? https://github.com/broadinstitute/gatk/issues/2735. What is the expected behaviour of sharding in this circumstance? I am unable to perform the test using gatk4+ as it would require immense amounts of recomputation, just to confirm a bug report. Hopefully the devs know if this is expected behaviour or has been fixed already.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5905
https://github.com/broadinstitute/gatk/issues/5906:2292,Availability,error,error,2292,"rt**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----; I am getting error while implementing docker images of GATK4 into variant calling workflow. As I connect the images into the WDL script, it first time run properly but as a streamlined input for next step the previous output is not visible...",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5906
https://github.com/broadinstitute/gatk/issues/5906:1306,Deployability,release,release,1306,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----; I am getting error while implementing docke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5906
https://github.com/broadinstitute/gatk/issues/5906:1376,Testability,test,test,1376,"to see if your issue (or something similar) has already been reported. If the issue already exists, you may comment there to inquire about the progress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----; I am getting error while implementing docke",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5906
https://github.com/broadinstitute/gatk/issues/5906:1476,Testability,log,logs,1476,"rogress.; - Determine whether your issue is a **bug report**, a **feature request**, or a **documentation request** (for tool/class javadoc only -- for forum docs please post there); - Consider if your ""issue"" is better addressed on the GATK forum: http://gatkforums.broadinstitute.org/gatk Post there if you have questions about expected tool behavior, output format, unexpected results, or generally any question that does not fit into the categories above; - Use a **concise** yet **descriptive** title; - Choose the corresponding template block below and fill it in, replacing or deleting text in italics (surrounded by `_`) as appropriate; - Delete the other template blocks and this header. ----. ## Bug Report. ### Affected tool(s) or class(es); _Tool/class name(s), special parameters?_. ### Affected version(s); - [ ] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; _Describe the problem below. Provide **screenshots** , **stacktrace** , **logs** where appropriate._. #### Steps to reproduce; _Tell us how to reproduce this issue. If possible, include command lines that reproduce the problem. (The support team may follow up to ask you to upload data to reproduce the issue.)_. #### Expected behavior; _Tell us what should happen_. #### Actual behavior; _Tell us what happens instead_. ----. ## Feature request. ### Tool(s) or class(es) involved; _Tool/class name(s), special parameters?_. ### Description; _Specify whether you want a modification of an existing behavior or addition of a new capability._; _Provide **examples**, **screenshots**, where appropriate._. ----. ## Documentation request. ### Tool(s) or class(es) involved; _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----; I am getting error while implementing docker images of GATK4 into variant calling workflow. As I connect the images into the WDL script, it first time run properly but as a streamlined in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5906
https://github.com/broadinstitute/gatk/issues/5908:1194,Performance,cache,cache,1194,"## Bug Report. ### Affected tool(s) or class(es); Haplotypecaller. ### Affected version(s); 4.1.1 or later. ### Description ; if you look at haplotypecaller.java::callRegion(), you will see these lines:; ```; if (trimmingResult.hasLeftFlankingRegion()) {; result.addAll(referenceModelForNoVariation(trimmingResult.nonVariantLeftFlankRegion(), false, VCpriors));; }; // output variant containing region.; result.addAll(referenceConfidenceModel.calculateRefConfidence(assemblyResult.getReferenceHaplotype(),; calledHaplotypes.getCalledHaplotypes(), assemblyResult.getPaddedReferenceLoc(), regionForGenotyping,; readLikelihoods, genotypingEngine.getPloidyModel(), calledHaplotypes.getCalls(), hcArgs.standardArgs.genotypeArgs.supportVariants != null,; VCpriors));; // output right-flanking non-variant section:; if (trimmingResult.hasRightFlankingRegion()) {; result.addAll(referenceModelForNoVariation(trimmingResult.nonVariantRightFlankRegion(), false, VCpriors));; }. ```; We trim the region left and right and calculate the Reference confidence. The problem is when we call calculateRefConfidence , we might trim reads for the left region and calculate TransientAttributes and put them in the cache. Next time, when we call calculateRefConfidence, we trim the reads for the right region but we still look at the cache which is prepopulated with data for the left region and we use the same data which might result in the wrong output. . if you turn USE_CACHED_READ_INDEL_INFORMATIVENESS_VALUES off, you might get different results which I do not think is the expected outcome. Let me know if you want more information. Thank you.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5908
https://github.com/broadinstitute/gatk/issues/5908:1313,Performance,cache,cache,1313,"## Bug Report. ### Affected tool(s) or class(es); Haplotypecaller. ### Affected version(s); 4.1.1 or later. ### Description ; if you look at haplotypecaller.java::callRegion(), you will see these lines:; ```; if (trimmingResult.hasLeftFlankingRegion()) {; result.addAll(referenceModelForNoVariation(trimmingResult.nonVariantLeftFlankRegion(), false, VCpriors));; }; // output variant containing region.; result.addAll(referenceConfidenceModel.calculateRefConfidence(assemblyResult.getReferenceHaplotype(),; calledHaplotypes.getCalledHaplotypes(), assemblyResult.getPaddedReferenceLoc(), regionForGenotyping,; readLikelihoods, genotypingEngine.getPloidyModel(), calledHaplotypes.getCalls(), hcArgs.standardArgs.genotypeArgs.supportVariants != null,; VCpriors));; // output right-flanking non-variant section:; if (trimmingResult.hasRightFlankingRegion()) {; result.addAll(referenceModelForNoVariation(trimmingResult.nonVariantRightFlankRegion(), false, VCpriors));; }. ```; We trim the region left and right and calculate the Reference confidence. The problem is when we call calculateRefConfidence , we might trim reads for the left region and calculate TransientAttributes and put them in the cache. Next time, when we call calculateRefConfidence, we trim the reads for the right region but we still look at the cache which is prepopulated with data for the left region and we use the same data which might result in the wrong output. . if you turn USE_CACHED_READ_INDEL_INFORMATIVENESS_VALUES off, you might get different results which I do not think is the expected outcome. Let me know if you want more information. Thank you.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5908
https://github.com/broadinstitute/gatk/pull/5909:896,Energy Efficiency,reduce,reduced,896,"`VariantsSparkSink` will always sort variants before writing them out. However, `HaplotypeCallerSpark` always processes reads in coordinate-sorted order, and produces variants in the same order, so there is no need for `VariantsSparkSink` to sort variants. (In fact, in GVCF mode the sort is prohibitive since the engine creates a variant for every locus over the interval of interest, which go through the sort step before being merged into GVCF bands.). This PR removes the sort step for `HaplotypeCallerSpark` (and `PrintVariantsSpark`, which doesn't need it either). All of the concordance unit tests pass, and as an additional sanity check I compared the GVCF output from running regular `HaplotypeCaller` on a large input BAM to `HaplotypeCallerSpark` (with and without variant sorting). Removing variant sorting actually made the GVCF output more similar to regular `HaplotypeCaller` - it reduced the number of differences from three to one. (The one difference is a minor difference in QUAL due to a boundary artifact.) See VCFs in [vcfs.zip](https://github.com/broadinstitute/gatk/files/3134046/vcfs.zip).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5909
https://github.com/broadinstitute/gatk/pull/5909:632,Safety,sanity check,sanity check,632,"`VariantsSparkSink` will always sort variants before writing them out. However, `HaplotypeCallerSpark` always processes reads in coordinate-sorted order, and produces variants in the same order, so there is no need for `VariantsSparkSink` to sort variants. (In fact, in GVCF mode the sort is prohibitive since the engine creates a variant for every locus over the interval of interest, which go through the sort step before being merged into GVCF bands.). This PR removes the sort step for `HaplotypeCallerSpark` (and `PrintVariantsSpark`, which doesn't need it either). All of the concordance unit tests pass, and as an additional sanity check I compared the GVCF output from running regular `HaplotypeCaller` on a large input BAM to `HaplotypeCallerSpark` (with and without variant sorting). Removing variant sorting actually made the GVCF output more similar to regular `HaplotypeCaller` - it reduced the number of differences from three to one. (The one difference is a minor difference in QUAL due to a boundary artifact.) See VCFs in [vcfs.zip](https://github.com/broadinstitute/gatk/files/3134046/vcfs.zip).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5909
https://github.com/broadinstitute/gatk/pull/5909:599,Testability,test,tests,599,"`VariantsSparkSink` will always sort variants before writing them out. However, `HaplotypeCallerSpark` always processes reads in coordinate-sorted order, and produces variants in the same order, so there is no need for `VariantsSparkSink` to sort variants. (In fact, in GVCF mode the sort is prohibitive since the engine creates a variant for every locus over the interval of interest, which go through the sort step before being merged into GVCF bands.). This PR removes the sort step for `HaplotypeCallerSpark` (and `PrintVariantsSpark`, which doesn't need it either). All of the concordance unit tests pass, and as an additional sanity check I compared the GVCF output from running regular `HaplotypeCaller` on a large input BAM to `HaplotypeCallerSpark` (with and without variant sorting). Removing variant sorting actually made the GVCF output more similar to regular `HaplotypeCaller` - it reduced the number of differences from three to one. (The one difference is a minor difference in QUAL due to a boundary artifact.) See VCFs in [vcfs.zip](https://github.com/broadinstitute/gatk/files/3134046/vcfs.zip).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5909
https://github.com/broadinstitute/gatk/issues/5910:56,Availability,avail,available,56,"## Feature request. Make joint genotyping functionality available as a publicly accessible function to another class/program without passing through the GATK command line interface. ### Tool(s) or class(es) involved; VariantContext; GenotypeGVCFs; Underlying engine classes. ### Description; For various use cases where our pipelines produce in-memory VariantContext objects it would be faster and easier to pass these directly to a joint genotyping function and extract the results back into memory rather than writing to VCF, running the GenotypeGVCFs pipeline via the command line interface and then re-ingesting the resultant VCFs. From discussions during the GATK Working Group meetings it appears this request is similar in principle to existing functionality for the HaplotypeCaller that was implemented by ""extracting the engine"" from the HaplotypeCaller walkers so that it can be instantiated outside the command line utility. Ideally, this implementation should make it possible to instantiate any necessary engine classes pass VariantContext objects directly to the GenotypeGVCFs.apply or GenotypeGVCFs.regenotypeVC and receive the re-genotyped VariantContext objects back for further processing from Java code. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910
https://github.com/broadinstitute/gatk/issues/5910:324,Deployability,pipeline,pipelines,324,"## Feature request. Make joint genotyping functionality available as a publicly accessible function to another class/program without passing through the GATK command line interface. ### Tool(s) or class(es) involved; VariantContext; GenotypeGVCFs; Underlying engine classes. ### Description; For various use cases where our pipelines produce in-memory VariantContext objects it would be faster and easier to pass these directly to a joint genotyping function and extract the results back into memory rather than writing to VCF, running the GenotypeGVCFs pipeline via the command line interface and then re-ingesting the resultant VCFs. From discussions during the GATK Working Group meetings it appears this request is similar in principle to existing functionality for the HaplotypeCaller that was implemented by ""extracting the engine"" from the HaplotypeCaller walkers so that it can be instantiated outside the command line utility. Ideally, this implementation should make it possible to instantiate any necessary engine classes pass VariantContext objects directly to the GenotypeGVCFs.apply or GenotypeGVCFs.regenotypeVC and receive the re-genotyped VariantContext objects back for further processing from Java code. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910
https://github.com/broadinstitute/gatk/issues/5910:554,Deployability,pipeline,pipeline,554,"## Feature request. Make joint genotyping functionality available as a publicly accessible function to another class/program without passing through the GATK command line interface. ### Tool(s) or class(es) involved; VariantContext; GenotypeGVCFs; Underlying engine classes. ### Description; For various use cases where our pipelines produce in-memory VariantContext objects it would be faster and easier to pass these directly to a joint genotyping function and extract the results back into memory rather than writing to VCF, running the GenotypeGVCFs pipeline via the command line interface and then re-ingesting the resultant VCFs. From discussions during the GATK Working Group meetings it appears this request is similar in principle to existing functionality for the HaplotypeCaller that was implemented by ""extracting the engine"" from the HaplotypeCaller walkers so that it can be instantiated outside the command line utility. Ideally, this implementation should make it possible to instantiate any necessary engine classes pass VariantContext objects directly to the GenotypeGVCFs.apply or GenotypeGVCFs.regenotypeVC and receive the re-genotyped VariantContext objects back for further processing from Java code. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910
https://github.com/broadinstitute/gatk/issues/5910:171,Integrability,interface,interface,171,"## Feature request. Make joint genotyping functionality available as a publicly accessible function to another class/program without passing through the GATK command line interface. ### Tool(s) or class(es) involved; VariantContext; GenotypeGVCFs; Underlying engine classes. ### Description; For various use cases where our pipelines produce in-memory VariantContext objects it would be faster and easier to pass these directly to a joint genotyping function and extract the results back into memory rather than writing to VCF, running the GenotypeGVCFs pipeline via the command line interface and then re-ingesting the resultant VCFs. From discussions during the GATK Working Group meetings it appears this request is similar in principle to existing functionality for the HaplotypeCaller that was implemented by ""extracting the engine"" from the HaplotypeCaller walkers so that it can be instantiated outside the command line utility. Ideally, this implementation should make it possible to instantiate any necessary engine classes pass VariantContext objects directly to the GenotypeGVCFs.apply or GenotypeGVCFs.regenotypeVC and receive the re-genotyped VariantContext objects back for further processing from Java code. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910
https://github.com/broadinstitute/gatk/issues/5910:584,Integrability,interface,interface,584,"## Feature request. Make joint genotyping functionality available as a publicly accessible function to another class/program without passing through the GATK command line interface. ### Tool(s) or class(es) involved; VariantContext; GenotypeGVCFs; Underlying engine classes. ### Description; For various use cases where our pipelines produce in-memory VariantContext objects it would be faster and easier to pass these directly to a joint genotyping function and extract the results back into memory rather than writing to VCF, running the GenotypeGVCFs pipeline via the command line interface and then re-ingesting the resultant VCFs. From discussions during the GATK Working Group meetings it appears this request is similar in principle to existing functionality for the HaplotypeCaller that was implemented by ""extracting the engine"" from the HaplotypeCaller walkers so that it can be instantiated outside the command line utility. Ideally, this implementation should make it possible to instantiate any necessary engine classes pass VariantContext objects directly to the GenotypeGVCFs.apply or GenotypeGVCFs.regenotypeVC and receive the re-genotyped VariantContext objects back for further processing from Java code. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910
https://github.com/broadinstitute/gatk/issues/5910:80,Security,access,accessible,80,"## Feature request. Make joint genotyping functionality available as a publicly accessible function to another class/program without passing through the GATK command line interface. ### Tool(s) or class(es) involved; VariantContext; GenotypeGVCFs; Underlying engine classes. ### Description; For various use cases where our pipelines produce in-memory VariantContext objects it would be faster and easier to pass these directly to a joint genotyping function and extract the results back into memory rather than writing to VCF, running the GenotypeGVCFs pipeline via the command line interface and then re-ingesting the resultant VCFs. From discussions during the GATK Working Group meetings it appears this request is similar in principle to existing functionality for the HaplotypeCaller that was implemented by ""extracting the engine"" from the HaplotypeCaller walkers so that it can be instantiated outside the command line utility. Ideally, this implementation should make it possible to instantiate any necessary engine classes pass VariantContext objects directly to the GenotypeGVCFs.apply or GenotypeGVCFs.regenotypeVC and receive the re-genotyped VariantContext objects back for further processing from Java code. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5910
https://github.com/broadinstitute/gatk/pull/5911:80,Performance,cache,caches,80,I don't doubt that there could be issues caused by reads with previously filled caches. Ultimately this shouldn't have too significant an impact except in very pathological circumstances with highly repetitive regions or reads that hang beyond a certain length into the next region and happen to have had good looking indel sites without the cirgar actually containing any indels for that read. This should eliminate any of these circumstances entirely so we can be sure the cache is clear before every call. . Fixes #5908,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911
https://github.com/broadinstitute/gatk/pull/5911:475,Performance,cache,cache,475,I don't doubt that there could be issues caused by reads with previously filled caches. Ultimately this shouldn't have too significant an impact except in very pathological circumstances with highly repetitive regions or reads that hang beyond a certain length into the next region and happen to have had good looking indel sites without the cirgar actually containing any indels for that read. This should eliminate any of these circumstances entirely so we can be sure the cache is clear before every call. . Fixes #5908,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911
https://github.com/broadinstitute/gatk/pull/5911:484,Usability,clear,clear,484,I don't doubt that there could be issues caused by reads with previously filled caches. Ultimately this shouldn't have too significant an impact except in very pathological circumstances with highly repetitive regions or reads that hang beyond a certain length into the next region and happen to have had good looking indel sites without the cirgar actually containing any indels for that read. This should eliminate any of these circumstances entirely so we can be sure the cache is clear before every call. . Fixes #5908,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5911
https://github.com/broadinstitute/gatk/issues/5912:771,Availability,redundant,redundant,771,"## Feature request; ### HaplotypeCaller; When running HaplotypeCaller, I get tens of thousands of lines like the following:; ```; 13:02:04.113 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; ```; The problem is they are not informative: you can't tell what position/region caused them. Resulting repetitive messages are redundant.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5912
https://github.com/broadinstitute/gatk/issues/5912:758,Integrability,message,messages,758,"## Feature request; ### HaplotypeCaller; When running HaplotypeCaller, I get tens of thousands of lines like the following:; ```; 13:02:04.113 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; ```; The problem is they are not informative: you can't tell what position/region caused them. Resulting repetitive messages are redundant.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5912
https://github.com/broadinstitute/gatk/issues/5912:771,Safety,redund,redundant,771,"## Feature request; ### HaplotypeCaller; When running HaplotypeCaller, I get tens of thousands of lines like the following:; ```; 13:02:04.113 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN DepthPerSampleHC - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; 13:02:04.113 WARN StrandBiasBySample - Annotation will not be calculated, genotype is not called or alleleLikelihoodMap is null; ```; The problem is they are not informative: you can't tell what position/region caused them. Resulting repetitive messages are redundant.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5912
https://github.com/broadinstitute/gatk/pull/5913:915,Deployability,toggle,toggled,915,"Currently this implementation could probably use more substantial testing, especially of the core functionality. It is also missing the following features from gatk3:; - Depth of coverage by fragments; - Rtable output format. The following important differences exist from gatk3:; - Fixed numerous bugs with output line ordering, now we necessarily associate the right sample with the right data in the output tables. We also sort the output samples lexicographically consistently so the order of the output columns is deterministic.; - Now gene outputs no longer rely on having exactly matching interval overlaps. Instead they are treated as intervals themselves with coverage information being collected for every exon base covered by the gene. There is an argument to demand that a gene must be completely covered by provided intervals to be in the result table. (Perhaps excising introns from coverage could be toggled?); - Intervals are no longer merged when reported in to the interval tables. . The following (new) features have been pushed into future branches:; - Diagnose targets evaluation field (need to decide what partitioning to attach it to, perhaps sample partitioning info?); - Replacement for `.refseq` format for gene lists. ; - Optional output partitioning based a provided list of read filters. . Depends on #5887 ; Resolves #19",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5913
https://github.com/broadinstitute/gatk/pull/5913:1319,Integrability,Depend,Depends,1319,"Currently this implementation could probably use more substantial testing, especially of the core functionality. It is also missing the following features from gatk3:; - Depth of coverage by fragments; - Rtable output format. The following important differences exist from gatk3:; - Fixed numerous bugs with output line ordering, now we necessarily associate the right sample with the right data in the output tables. We also sort the output samples lexicographically consistently so the order of the output columns is deterministic.; - Now gene outputs no longer rely on having exactly matching interval overlaps. Instead they are treated as intervals themselves with coverage information being collected for every exon base covered by the gene. There is an argument to demand that a gene must be completely covered by provided intervals to be in the result table. (Perhaps excising introns from coverage could be toggled?); - Intervals are no longer merged when reported in to the interval tables. . The following (new) features have been pushed into future branches:; - Diagnose targets evaluation field (need to decide what partitioning to attach it to, perhaps sample partitioning info?); - Replacement for `.refseq` format for gene lists. ; - Optional output partitioning based a provided list of read filters. . Depends on #5887 ; Resolves #19",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5913
https://github.com/broadinstitute/gatk/pull/5913:66,Testability,test,testing,66,"Currently this implementation could probably use more substantial testing, especially of the core functionality. It is also missing the following features from gatk3:; - Depth of coverage by fragments; - Rtable output format. The following important differences exist from gatk3:; - Fixed numerous bugs with output line ordering, now we necessarily associate the right sample with the right data in the output tables. We also sort the output samples lexicographically consistently so the order of the output columns is deterministic.; - Now gene outputs no longer rely on having exactly matching interval overlaps. Instead they are treated as intervals themselves with coverage information being collected for every exon base covered by the gene. There is an argument to demand that a gene must be completely covered by provided intervals to be in the result table. (Perhaps excising introns from coverage could be toggled?); - Intervals are no longer merged when reported in to the interval tables. . The following (new) features have been pushed into future branches:; - Diagnose targets evaluation field (need to decide what partitioning to attach it to, perhaps sample partitioning info?); - Replacement for `.refseq` format for gene lists. ; - Optional output partitioning based a provided list of read filters. . Depends on #5887 ; Resolves #19",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5913
https://github.com/broadinstitute/gatk/issues/5914:454,Availability,down,download,454,"## Feature request. ### Tool(s) or class(es) involved; Funcotator (auxiliary file). ### Description; We have a preferred transcript list for hg19 (i.e. GENCODE v19), but we do not have one for hg38 (latest version supported by Funcotator -- currently v29). There is a script in the oncotator repository for helping to create this list, but it does not have the manual curation. Please see @LeeTL1220 for details on this process. The oncotator datasource download page has the automatically-generated transcript list for gencode v19 alongside the automatically-generated+manual curation list. That can be used to suss out differences. Automatically-generated list was obtained by aligning transcripts to uniprot. I am not sure how the transcript IDs changed between GENCODE v19 and v29. So the complexity of this issue is still a bit unknown.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5914
https://github.com/broadinstitute/gatk/issues/5915:531,Availability,error,error,531,"## Feature request. ### Tool(s) or class(es) involved. (sv) VCF producing tool(s). ### Description. The VCF spec allows `POS` column to take value 0, when the suspected event is at a telomere.; The given example is in section 5.4.5 (see example event illustrated in Figure 6 and VCF records below the figure).; However, currently GATK writes VCF via `VariantContext`'s, which defines coordinate 0 as illegal.; I can of course push this feature request to htsjdk, if that is deemed more appropriate. **UPDATE**; Looking back at the error message, it is actually the `SimpleInterval` that I use for constructing the `VariantContext` throwing the error message.; Temporary workaround would be to ""hack"" the POS to be 1 or N, and warn using an INFO annotation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5915
https://github.com/broadinstitute/gatk/issues/5915:644,Availability,error,error,644,"## Feature request. ### Tool(s) or class(es) involved. (sv) VCF producing tool(s). ### Description. The VCF spec allows `POS` column to take value 0, when the suspected event is at a telomere.; The given example is in section 5.4.5 (see example event illustrated in Figure 6 and VCF records below the figure).; However, currently GATK writes VCF via `VariantContext`'s, which defines coordinate 0 as illegal.; I can of course push this feature request to htsjdk, if that is deemed more appropriate. **UPDATE**; Looking back at the error message, it is actually the `SimpleInterval` that I use for constructing the `VariantContext` throwing the error message.; Temporary workaround would be to ""hack"" the POS to be 1 or N, and warn using an INFO annotation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5915
https://github.com/broadinstitute/gatk/issues/5915:501,Deployability,UPDATE,UPDATE,501,"## Feature request. ### Tool(s) or class(es) involved. (sv) VCF producing tool(s). ### Description. The VCF spec allows `POS` column to take value 0, when the suspected event is at a telomere.; The given example is in section 5.4.5 (see example event illustrated in Figure 6 and VCF records below the figure).; However, currently GATK writes VCF via `VariantContext`'s, which defines coordinate 0 as illegal.; I can of course push this feature request to htsjdk, if that is deemed more appropriate. **UPDATE**; Looking back at the error message, it is actually the `SimpleInterval` that I use for constructing the `VariantContext` throwing the error message.; Temporary workaround would be to ""hack"" the POS to be 1 or N, and warn using an INFO annotation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5915
https://github.com/broadinstitute/gatk/issues/5915:537,Integrability,message,message,537,"## Feature request. ### Tool(s) or class(es) involved. (sv) VCF producing tool(s). ### Description. The VCF spec allows `POS` column to take value 0, when the suspected event is at a telomere.; The given example is in section 5.4.5 (see example event illustrated in Figure 6 and VCF records below the figure).; However, currently GATK writes VCF via `VariantContext`'s, which defines coordinate 0 as illegal.; I can of course push this feature request to htsjdk, if that is deemed more appropriate. **UPDATE**; Looking back at the error message, it is actually the `SimpleInterval` that I use for constructing the `VariantContext` throwing the error message.; Temporary workaround would be to ""hack"" the POS to be 1 or N, and warn using an INFO annotation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5915
https://github.com/broadinstitute/gatk/issues/5915:650,Integrability,message,message,650,"## Feature request. ### Tool(s) or class(es) involved. (sv) VCF producing tool(s). ### Description. The VCF spec allows `POS` column to take value 0, when the suspected event is at a telomere.; The given example is in section 5.4.5 (see example event illustrated in Figure 6 and VCF records below the figure).; However, currently GATK writes VCF via `VariantContext`'s, which defines coordinate 0 as illegal.; I can of course push this feature request to htsjdk, if that is deemed more appropriate. **UPDATE**; Looking back at the error message, it is actually the `SimpleInterval` that I use for constructing the `VariantContext` throwing the error message.; Temporary workaround would be to ""hack"" the POS to be 1 or N, and warn using an INFO annotation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5915
https://github.com/broadinstitute/gatk/issues/5915:566,Usability,Simpl,SimpleInterval,566,"## Feature request. ### Tool(s) or class(es) involved. (sv) VCF producing tool(s). ### Description. The VCF spec allows `POS` column to take value 0, when the suspected event is at a telomere.; The given example is in section 5.4.5 (see example event illustrated in Figure 6 and VCF records below the figure).; However, currently GATK writes VCF via `VariantContext`'s, which defines coordinate 0 as illegal.; I can of course push this feature request to htsjdk, if that is deemed more appropriate. **UPDATE**; Looking back at the error message, it is actually the `SimpleInterval` that I use for constructing the `VariantContext` throwing the error message.; Temporary workaround would be to ""hack"" the POS to be 1 or N, and warn using an INFO annotation.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5915
https://github.com/broadinstitute/gatk/issues/5916:73,Usability,clear,clearly,73,"To me, it seems VariantFiltration is incorrectly adding a filter when it clearly should not - unless I am missing something. I am adding a filter when `VAF in normal is > 2%` or `VAF in tumor is < 5%`. For this i am using the following expression:. ```; gatk VariantFiltration -R $ref_fasta -V tmp5 \; --filter-expression 'vc.getGenotype(""3105-T"").getAD().1 / vc.getGenotype(""3105-T"").getDP() < 0.05 ' --filter-name ""low_taf"" \; --filter-expression 'vc.getGenotype(""3105-N"").getAD().1 / vc.getGenotype(""3105-N"").getDP() > 0.02 ' --filter-name ""high_naf"" \; -O tmp6; ```; However, for this critical variant, the tool is incorrectly adding a filter, when TAF is 0.55. ```; tail -n2 tmp5; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT 3105-N 3105-T; 12 25398284 . C T . clustered_events CONTQ=93;DP=529;ECNT=5;GERMQ=93;MBQ=27,27;MFRL=0,0;MMQ=97,91;MPOS=80;NALOD=2.18;NLOD=31.43;POPAF=6.00;REF_BASES=GCCTACGCCACCAGCTCCAAC;SEQQ=93;STRANDQ=93;TLOD=309.98 GT:AD:AF:DP:F1R2:F2R1:SB 0/0:105,0:7.035e-03:105:15,0:84,0:87,18,0,0 0/1:99,123:0.556:222:20,50:71,69:72,27,71,52; tail -n1 tmp6; 12 25398284 . C T . clustered_events;low_taf CONTQ=93;DP=529;ECNT=5;GERMQ=93;MBQ=27,27;MFRL=0,0;MMQ=97,91;MPOS=80;NALOD=2.18;NLOD=31.43;POPAF=6.00;REF_BASES=GCCTACGCCACCAGCTCCAAC;SEQQ=93;STRANDQ=93;TLOD=309.98 GT:AD:AF:DP:F1R2:F2R1:SB 0/0:105,0:7.035e-03:105:15,0:84,0:87,18,0,0 0/1:99,123:0.556:222:20,50:71,69:72,27,71,52; ```. VAF calc:. ```; # GT:AD:AF:DP:F1R2:F2R1:SB; # 0/0:105,0:7.035e-03:105:15,0:84,0:87,18,0,0; # 0/1:99,123:0.556:222:20,50:71,69:72,27,71,52. TAF: 123/222=0.5540541; NAF: 0/105=0 [however 7.035e-03 is reported in AF field which is also fine...]; ```. I have another unrelated question regarding non-mutect files. If a tool has the AD values listed under a different name (in this case AO: allele observed, instead of allele depth), how can I construct a jexl expression to filter the same?. Imagine having the same data as above, with a slightly different naming convention:. ```; AD --> AO; # G",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5916
https://github.com/broadinstitute/gatk/issues/5917:267,Availability,error,error,267,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:475,Availability,down,down,475,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:897,Availability,down,download-software,897,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:4395,Availability,down,down,4395," IndexFeatureFile - GCS max retries/reopens: 20; 09:28:39.196 INFO IndexFeatureFile - Requester pays: disabled; 09:28:39.196 INFO IndexFeatureFile - Initializing engine; 09:28:39.196 INFO IndexFeatureFile - Done initializing engine; 09:28:39.696 INFO FeatureManager - Using codec VCFCodec to read file file:///storage/ppl/yifang/20190327_David_rampseq_Ehsan/data/samtools_sorted_out/SNPs_candidates.g.vcf; 09:28:39.708 INFO ProgressMeter - Starting traversal; 09:28:39.708 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; =========This is the indexed chromosome ===================; 09:28:41.968 INFO ProgressMeter - **chr7B_part2:295520629** 0.0 925631 24596040.7. 09:28:41.968 INFO ProgressMeter - Traversal complete. Processed 925631 total records in 0.0 minutes.; 09:28:42.006 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190327/data/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 09:28:42.006 INFO IndexFeatureFile - Shutting down engine; [May 6, 2019 9:28:42 CST AM] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=2434269184). ```; This is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO Progr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:5675,Availability,down,down,5675," is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 5.42 minutes.; Runtime.totalMemory()=5618270208; ```; Althought no warning/error messages was issued for the indexing of this big genome, I have tried to debug on 3 things I could think of:. 1. The chromosome and the coordinate are sorted ascendandly, although the chromosome names are not simply numeric continuous because of the A/B subgroup for each chromosome.; 2. The genome size difference, for which no clue was aboserved about the chromosome length limits. ; 3. The chromosome names for this big genome is quite long, but I tried the shorter names as A11 for chr1A_part1, A12 for chr1A_part2, ... B72 for chr7B_part2 (42 chromosomes in total), and the problem stayed exactly the same. Not sure what I may have missed. I appreciate any insight of this problem.; Yifang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:5869,Availability,error,error,5869," is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 5.42 minutes.; Runtime.totalMemory()=5618270208; ```; Althought no warning/error messages was issued for the indexing of this big genome, I have tried to debug on 3 things I could think of:. 1. The chromosome and the coordinate are sorted ascendandly, although the chromosome names are not simply numeric continuous because of the A/B subgroup for each chromosome.; 2. The genome size difference, for which no clue was aboserved about the chromosome length limits. ; 3. The chromosome names for this big genome is quite long, but I tried the shorter names as A11 for chr1A_part1, A12 for chr1A_part2, ... B72 for chr7B_part2 (42 chromosomes in total), and the problem stayed exactly the same. Not sure what I may have missed. I appreciate any insight of this problem.; Yifang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:6099,Deployability,continuous,continuous,6099," is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 5.42 minutes.; Runtime.totalMemory()=5618270208; ```; Althought no warning/error messages was issued for the indexing of this big genome, I have tried to debug on 3 things I could think of:. 1. The chromosome and the coordinate are sorted ascendandly, although the chromosome names are not simply numeric continuous because of the A/B subgroup for each chromosome.; 2. The genome size difference, for which no clue was aboserved about the chromosome length limits. ; 3. The chromosome names for this big genome is quite long, but I tried the shorter names as A11 for chr1A_part1, A12 for chr1A_part2, ... B72 for chr7B_part2 (42 chromosomes in total), and the problem stayed exactly the same. Not sure what I may have missed. I appreciate any insight of this problem.; Yifang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:6125,Deployability,A/B,A/B,6125," is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 5.42 minutes.; Runtime.totalMemory()=5618270208; ```; Althought no warning/error messages was issued for the indexing of this big genome, I have tried to debug on 3 things I could think of:. 1. The chromosome and the coordinate are sorted ascendandly, although the chromosome names are not simply numeric continuous because of the A/B subgroup for each chromosome.; 2. The genome size difference, for which no clue was aboserved about the chromosome length limits. ; 3. The chromosome names for this big genome is quite long, but I tried the shorter names as A11 for chr1A_part1, A12 for chr1A_part2, ... B72 for chr7B_part2 (42 chromosomes in total), and the problem stayed exactly the same. Not sure what I may have missed. I appreciate any insight of this problem.; Yifang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:5875,Integrability,message,messages,5875," is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 5.42 minutes.; Runtime.totalMemory()=5618270208; ```; Althought no warning/error messages was issued for the indexing of this big genome, I have tried to debug on 3 things I could think of:. 1. The chromosome and the coordinate are sorted ascendandly, although the chromosome names are not simply numeric continuous because of the A/B subgroup for each chromosome.; 2. The genome size difference, for which no clue was aboserved about the chromosome length limits. ; 3. The chromosome names for this big genome is quite long, but I tried the shorter names as A11 for chr1A_part1, A12 for chr1A_part2, ... B72 for chr7B_part2 (42 chromosomes in total), and the problem stayed exactly the same. Not sure what I may have missed. I appreciate any insight of this problem.; Yifang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:833,Performance,Load,Loading,833,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:322,Safety,detect,detected,322,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:453,Safety,detect,detected,453,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:1166,Safety,detect,detect,1166," indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291). 09:28:39.193 INFO IndexFeatureFile - ------------------------------------------------------------; 09:28:39.193 INFO IndexFeatureFile - The Genome Analysis To",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:1468,Safety,detect,detect,1468,". ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291). 09:28:39.193 INFO IndexFeatureFile - ------------------------------------------------------------; 09:28:39.193 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:28:39.193 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:28:39.194 INFO IndexFeatureFile - Executing as yifang@valiant5 on Linux v4.15.0-45-generic amd64; 09:28:39.194 INFO IndexFeatureFile - Java runtime: Java HotSpo",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:1766,Safety,detect,detect,1766," ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291). 09:28:39.193 INFO IndexFeatureFile - ------------------------------------------------------------; 09:28:39.193 INFO IndexFeatureFile - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:28:39.193 INFO IndexFeatureFile - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:28:39.194 INFO IndexFeatureFile - Executing as yifang@valiant5 on Linux v4.15.0-45-generic amd64; 09:28:39.194 INFO IndexFeatureFile - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_45-b14; 09:28:39.194 INFO IndexFeatureFile - Start Date/Time: May 6, 2019 9:28:38 CST AM; 09:28:39.194 INFO IndexFeatureFile - ------------------------------------------------------------; 09:28:39.194 INFO IndexFeatureFile - ------------------------------------------",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:665,Testability,log,log,665,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:762,Testability,LOG,LOGDIR,762,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:787,Testability,log,log,787,"Hello, ; I was trying to create a candidates SNP list for **GATK4 BaseRecalibrator** to recalibrate the alignment for SNP calling. And I met a problem with the indexing of the my vcf file of candidates SNPs. ; The indexing step and recalibrating step ran without any error, but only very small amount of SNPs (~2900) were detected from a genome **~15Gbp** size, which is definitely not correct as compared with other methods when **~million SNPs** were detected. ; I tracked down the problem is at the indexing step for the candidates vcf file (**925751 SNPs, through HaplotypeCaller**). The problem looks like only the **last chromosome** was indexed.; This is my log file in which the Google engine related part was omitted as I did not use it: ; ```; $ cat ${LOGDIR}/index_candidates.log. (09:28:38.902 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/storage/ppl/yifang/download-software/anaconda3/envs/exome/share/gatk4-4.1.0.0-0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method). ...... May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused; 	at java.net.PlainSocketImpl.socketConnect(Native Method); 	; May 06, 2019 9:28:39 AM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; WARNING: Failed to detect whether we are running on Google Compute Engine.; java.net.ConnectException: Connection refused. ...... 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:4721,Testability,LOG,LOGDIR,4721,"0190327_David_rampseq_Ehsan/data/samtools_sorted_out/SNPs_candidates.g.vcf; 09:28:39.708 INFO ProgressMeter - Starting traversal; 09:28:39.708 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; =========This is the indexed chromosome ===================; 09:28:41.968 INFO ProgressMeter - **chr7B_part2:295520629** 0.0 925631 24596040.7. 09:28:41.968 INFO ProgressMeter - Traversal complete. Processed 925631 total records in 0.0 minutes.; 09:28:42.006 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190327/data/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 09:28:42.006 INFO IndexFeatureFile - Shutting down engine; [May 6, 2019 9:28:42 CST AM] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=2434269184). ```; This is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:4746,Testability,log,log,4746,"eq_Ehsan/data/samtools_sorted_out/SNPs_candidates.g.vcf; 09:28:39.708 INFO ProgressMeter - Starting traversal; 09:28:39.708 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; =========This is the indexed chromosome ===================; 09:28:41.968 INFO ProgressMeter - **chr7B_part2:295520629** 0.0 925631 24596040.7. 09:28:41.968 INFO ProgressMeter - Traversal complete. Processed 925631 total records in 0.0 minutes.; 09:28:42.006 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190327/data/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 09:28:42.006 INFO IndexFeatureFile - Shutting down engine; [May 6, 2019 9:28:42 CST AM] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 0.05 minutes.; Runtime.totalMemory()=2434269184). ```; This is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5917:6084,Usability,simpl,simply,6084," is my command line:; `java -DGATK_STACKTRACE_ON_USER_EXCEPTION=true -jar ${gatk4_jar} IndexFeatureFile --feature-file ${gvcf} --output ${gvcf}.idx 2>${LOGDIR}/index_candidates.log`. I tried this exact command line with another genome, which worked just fine with output progress report as following for a comparison of the multiple chromosomes processed:; ```; 12:50:38.871 INFO ProgressMeter - Starting traversal; 12:50:38.873 INFO ProgressMeter - Current Locus Elapsed Minutes Records Processed Records/Minute; 12:50:48.876 INFO ProgressMeter - N1:21408210 0.2 5669000 34010598.9; 12:50:58.876 INFO ProgressMeter - N2:13383863 0.3 11960000 35874618.8. ...... 12:55:58.884 INFO ProgressMeter - N19:50063133 5.3 208660000 39122405.2; 12:56:02.409 INFO ProgressMeter - N19:55994806 5.4 210940859 39119265.4; 12:56:02.409 INFO ProgressMeter - Traversal complete. Processed 210940859 total records in 5.4 minutes.; 12:56:02.429 INFO IndexFeatureFile - Successfully wrote index to /storage/ppl/yifang/20190225/data3/samtools_sorted_out/SNPs_candidates.g.vcf.idx; 12:56:02.429 INFO IndexFeatureFile - Shutting down engine; [April 25, 2019 12:56:02 PM CST] org.broadinstitute.hellbender.tools.IndexFeatureFile done. Elapsed time: 5.42 minutes.; Runtime.totalMemory()=5618270208; ```; Althought no warning/error messages was issued for the indexing of this big genome, I have tried to debug on 3 things I could think of:. 1. The chromosome and the coordinate are sorted ascendandly, although the chromosome names are not simply numeric continuous because of the A/B subgroup for each chromosome.; 2. The genome size difference, for which no clue was aboserved about the chromosome length limits. ; 3. The chromosome names for this big genome is quite long, but I tried the shorter names as A11 for chr1A_part1, A12 for chr1A_part2, ... B72 for chr7B_part2 (42 chromosomes in total), and the problem stayed exactly the same. Not sure what I may have missed. I appreciate any insight of this problem.; Yifang",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5917
https://github.com/broadinstitute/gatk/issues/5918:383,Availability,avail,available,383,"A user reported that when running GetPileupSummaries on gnomad vcf, the tool runs out of java heap memory. Xmx value was set to `-Xmx30G` and the machine has >200G RAM. **User Report**: I'm trying to run the cross sample contamination check on my samples, but GetPileupSummaries (4.1.1.0) keeps running out of memory, even when running a single sample on a VM that has >200GB of RAM available. <pre>; 14:35:16.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:35:17.116 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.117 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.1.1.0; 14:35:17.117 INFO GetPileupSummaries - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:35:17.118 INFO GetPileupSummaries - Executing as root@c64bec8aea6f on Linux v4.15.0-47-generic amd64; 14:35:17.118 INFO GetPileupSummaries - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 14:35:17.118 INFO GetPileupSummaries - Start Date/Time: April 24, 2019 2:35:16 PM UTC; 14:35:17.118 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.119 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.119 INFO GetPileupSummaries - HTSJDK Version: 2.19.0; 14:35:17.119 INFO GetPileupSummaries - Picard Version: 2.19.0; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:17.120 INFO GetPileupSummaries - Deflater: IntelDeflater; 14:35:17.120 INFO GetPileupSummaries - Inflater: IntelInflater;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918
https://github.com/broadinstitute/gatk/issues/5918:3150,Availability,down,down,3150,"tPileupSummaries -. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. Warning: GetPileupSummaries is a BETA tool and is not yet ready for use in production. !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!. 14:35:17.121 INFO GetPileupSummaries - Initializing engine; 14:35:17.456 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/data/gnomad/vcf/genomes/liftover_grch38/gnomad.b38.biallelic_only.concat.sorted.filtered.vcf.gz; 14:35:17.586 INFO FeatureManager - Using codec VCFCodec to read file file:///gatk/data/gnomad/vcf/genomes/liftover_grch38/gnomad.b38.biallelic_only.concat.sorted.filtered.vcf.gz; 16:39:08.359 INFO IntervalArgumentCollection - Processing 236373212 bp from intervals; 16:41:01.520 INFO GetPileupSummaries - Done initializing engine; 16:41:01.521 INFO ProgressMeter - Starting traversal; 16:41:01.521 INFO ProgressMeter - Current Locus Elapsed Minutes Loci Processed Loci/Minute; 02:44:42.116 INFO GetPileupSummaries - Shutting down engine; [April 25, 2019 2:44:42 AM UTC] org.broadinstitute.hellbender.tools.walkers.contamination.GetPileupSummaries done. Elapsed time: 729.42 minutes.; Runtime.totalMemory()=23243784192; Exception in thread ""main"" java.lang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.grow(ArrayList.java:265); at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:239); at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:231); at java.util.ArrayList.add(ArrayList.java:462); at htsjdk.samtools.BinningIndexContent.getChunksOverlapping(BinningIndexContent.java:131); at htsjdk.samtools.CachingBAMFileIndex.getSpanOverlapping(CachingBAMFileIndex.java:75); at htsjdk.samtools.BAMFileReader.getFileSpan(BAMFileReader.java:935); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:952); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918
https://github.com/broadinstitute/gatk/issues/5918:441,Performance,Load,Loading,441,"A user reported that when running GetPileupSummaries on gnomad vcf, the tool runs out of java heap memory. Xmx value was set to `-Xmx30G` and the machine has >200G RAM. **User Report**: I'm trying to run the cross sample contamination check on my samples, but GetPileupSummaries (4.1.1.0) keeps running out of memory, even when running a single sample on a VM that has >200GB of RAM available. <pre>; 14:35:16.874 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/gatk/gatk-package-4.1.1.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 14:35:17.116 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.117 INFO GetPileupSummaries - The Genome Analysis Toolkit (GATK) v4.1.1.0; 14:35:17.117 INFO GetPileupSummaries - For support and documentation go to https://software.broadinstitute.org/gatk/; 14:35:17.118 INFO GetPileupSummaries - Executing as root@c64bec8aea6f on Linux v4.15.0-47-generic amd64; 14:35:17.118 INFO GetPileupSummaries - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_191-8u191-b12-0ubuntu0.16.04.1-b12; 14:35:17.118 INFO GetPileupSummaries - Start Date/Time: April 24, 2019 2:35:16 PM UTC; 14:35:17.118 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.119 INFO GetPileupSummaries - ------------------------------------------------------------; 14:35:17.119 INFO GetPileupSummaries - HTSJDK Version: 2.19.0; 14:35:17.119 INFO GetPileupSummaries - Picard Version: 2.19.0; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 14:35:17.120 INFO GetPileupSummaries - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 14:35:17.120 INFO GetPileupSummaries - Deflater: IntelDeflater; 14:35:17.120 INFO GetPileupSummaries - Inflater: IntelInflater;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918
https://github.com/broadinstitute/gatk/issues/5918:4356,Performance,load,loadNextIterator,4356,ang.OutOfMemoryError: Java heap space; at java.util.Arrays.copyOf(Arrays.java:3181); at java.util.ArrayList.grow(ArrayList.java:265); at java.util.ArrayList.ensureExplicitCapacity(ArrayList.java:239); at java.util.ArrayList.ensureCapacityInternal(ArrayList.java:231); at java.util.ArrayList.add(ArrayList.java:462); at htsjdk.samtools.BinningIndexContent.getChunksOverlapping(BinningIndexContent.java:131); at htsjdk.samtools.CachingBAMFileIndex.getSpanOverlapping(CachingBAMFileIndex.java:75); at htsjdk.samtools.BAMFileReader.getFileSpan(BAMFileReader.java:935); at htsjdk.samtools.BAMFileReader.createIndexIterator(BAMFileReader.java:952); at htsjdk.samtools.BAMFileReader.query(BAMFileReader.java:612); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.query(SamReader.java:533); at htsjdk.samtools.SamReader$PrimitiveSamReaderToSamReaderAdapter.queryOverlapping(SamReader.java:405); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.loadNextIterator(SamReaderQueryingIterator.java:125); at org.broadinstitute.hellbender.utils.iterators.SamReaderQueryingIterator.<init>(SamReaderQueryingIterator.java:66); at org.broadinstitute.hellbender.engine.ReadsDataSource.prepareIteratorsForTraversal(ReadsDataSource.java:404); at org.broadinstitute.hellbender.engine.ReadsDataSource.iterator(ReadsDataSource.java:330); at java.lang.Iterable.spliterator(Iterable.java:101); at org.broadinstitute.hellbender.utils.Utils.stream(Utils.java:1098); at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:321); at org.broadinstitute.hellbender.engine.LocusWalker.traverse(LocusWalker.java:159); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:984); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLinePr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5918
https://github.com/broadinstitute/gatk/issues/5919:68,Availability,error,error,68,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:183,Availability,error,error,183,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:231,Availability,error,error,231,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:784,Availability,error,error,784,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:4122,Availability,error,error-with-gatk-,4122,nder.engine.spark.GATKSparkTool.writeReads(GATKSparkTool.java:349); at org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark.runTool(ApplyBQSRSpark.java:90); at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.io.IOException: Stream closed; at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170); at java.io.BufferedInputStream.read(BufferedInputStream.java:336); at java.io.DataInputStream.read(DataInputStream.java:149); at java.io.DataInputStream.read(DataInputStream.java:149); at org.disq_bio.disq.impl.file.HadoopFileSystemWrapper$SeekableHadoopStream.read(HadoopFileSystemWrapper.java:232); at java.io.BufferedInputStream.fill(BufferedInputStream.java:246); at java.io.BufferedInputStream.read1(BufferedInputStream.java:286); at java.io.BufferedInputStream.read(BufferedInputStream.java:345); at htsjdk.samtools.seekablestream.SeekableBufferedStream.read(SeekableBufferedStream.java:133); at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:21); ... 22 more; 19/04/28 10:11:25 INFO ShutdownHookManager: Shutdown hook called. Could you please help me to resolve this issue. Thanks In Advance; Fazulur Rehaman. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/23954/stream-closed-error-with-gatk-4-1-1-0/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:136,Deployability,upgrade,upgraded,136,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:263,Deployability,pipeline,pipeline,263,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:727,Safety,timeout,timeout,727,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:450,Testability,test,test-sort,450,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:473,Testability,test,test-sort-recal,473,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/issues/5919:511,Testability,test,test-sort-recal,511,"**Summary**: ; A user reported `java.io.IOException: Stream closed` error with ApplyBQSRSpark. GATK 4.0.9.0 runs fine but when the user upgraded to gatk 4.1.1.0 version, they see his error. **User Report**:; I am getting the below error when running gatk-variant pipeline of bcbio. Bcbio using gatk 4.1.1.0 version. ; When I run ApplyBQSRSpark using GATK 4.0.9.0, it runs fine without any issues. Here is the command; **; gatk ApplyBQSRSpark --input test-sort.bam --output test-sort-recal.bam --bqsr-recal-file test-sort-recal.grp --static-quantized-quals 10 --static-quantized-quals 20 --static-quantized-quals 30 --spark-master local[8] --conf spark.local.dir=scratch/ --conf spark.driver.host=localhost --conf spark.network.timeout=800 --jdk-deflater --jdk-inflater**. Here is the error. [April 28, 2019 10:11:25 AM AST] org.broadinstitute.hellbender.tools.spark.ApplyBQSRSpark done. Elapsed time: 0.15 minutes.; Runtime.totalMemory()=874512384; **htsjdk.samtools.util.RuntimeIOException: java.io.IOException: Stream closed**; at htsjdk.samtools.IndexStreamBuffer.readFully(IndexStreamBuffer.java:23); at htsjdk.samtools.IndexStreamBuffer.readLong(IndexStreamBuffer.java:62); at htsjdk.samtools.AbstractBAMFileIndex.readLong(AbstractBAMFileIndex.java:436); at htsjdk.samtools.AbstractBAMFileIndex.query(AbstractBAMFileIndex.java:311); at htsjdk.samtools.CachingBAMFileIndex.getQueryResults(CachingBAMFileIndex.java:159); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:43); at htsjdk.samtools.BAMIndexMerger.processIndex(BAMIndexMerger.java:16); at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); at org.disq_bio.disq.impl.formats.bam.BamSink.save(BamSink.java:132); at org.disq_bio.disq.HtsjdkReadsRddStorage.write(HtsjdkReadsRddStorage.java:225); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(ReadsSparkSink.java:155); at org.broadinstitute.hellbender.engine.spark.datasources.ReadsSparkSink.writeReads(Rea",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5919
https://github.com/broadinstitute/gatk/pull/5920:19,Safety,avoid,avoid,19,also trim reads to avoid calling past the end of short fragments,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5920
https://github.com/broadinstitute/gatk/issues/5922:506,Testability,test,tests,506,"As part of the effort to use linked de Bruijn graphs in assembly, we should eliminate the hacks that we use to mitigate the problems with cycles. Eventually this should become the only thing we ever do, but for now let's create a command line option in which we assemble a true de Bruijn graph where each kmer corresponds to exactly one vertex. This ought not to break anything although until we finish the linked de Bruijn epic it will hurt precision a bit. A lot of work for this ticket will be new unit tests.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5922
https://github.com/broadinstitute/gatk/issues/5923:132,Availability,error,errors,132,"Implement a `SeqGraph` version of the junction trees described in Kiran's paper. For now we can do something naive about reads with errors corresponding to pruned edges, such as skipping the remainder of the read. In addition to involving a minimal change to the current code, using `SeqGraph`s will make handling read errors a bit simpler and is a much more natural way to handle dangling ends.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5923
https://github.com/broadinstitute/gatk/issues/5923:319,Availability,error,errors,319,"Implement a `SeqGraph` version of the junction trees described in Kiran's paper. For now we can do something naive about reads with errors corresponding to pruned edges, such as skipping the remainder of the read. In addition to involving a minimal change to the current code, using `SeqGraph`s will make handling read errors a bit simpler and is a much more natural way to handle dangling ends.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5923
https://github.com/broadinstitute/gatk/issues/5923:332,Usability,simpl,simpler,332,"Implement a `SeqGraph` version of the junction trees described in Kiran's paper. For now we can do something naive about reads with errors corresponding to pruned edges, such as skipping the remainder of the read. In addition to involving a minimal change to the current code, using `SeqGraph`s will make handling read errors a bit simpler and is a much more natural way to handle dangling ends.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5923
https://github.com/broadinstitute/gatk/issues/5924:303,Usability,simpl,simply,303,"Once we have implement junction trees on `SeqGraph`s we will need a reasonable heuristic for handling bases in reads that have been pruned from the graph. For example, upon finding a mismatch between a read and a `SeqVertex` we might scan for the next kmer in the read that matches some `SeqVertex` and simply proceed from there. Or we might only do so if the matching vertex is either the current `SeqVertex` or one of its edge targets etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5924
https://github.com/broadinstitute/gatk/issues/5925:488,Availability,down,downstream,488,"Once we have built junction trees for linked de Bruijn graphs we can use them to find phased haplotypes that handle repeats / cycles. This essentially amounts to running the current Dijkstra's algorithm on junction trees (which are, after all, DAGs) instead of `SeqGraph`s. That is, the edge weights can remain log branching ratios. The complication is that while using the the ""oldest"" junction tree for edge weights we must also traverse younger junction trees in order to rely on them downstream. We may or may not want to work out a rule for when to use a younger junction tree with much higher coverage than an older one, or to combine multiple trees into a single vote.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5925
https://github.com/broadinstitute/gatk/issues/5925:311,Testability,log,log,311,"Once we have built junction trees for linked de Bruijn graphs we can use them to find phased haplotypes that handle repeats / cycles. This essentially amounts to running the current Dijkstra's algorithm on junction trees (which are, after all, DAGs) instead of `SeqGraph`s. That is, the edge weights can remain log branching ratios. The complication is that while using the the ""oldest"" junction tree for edge weights we must also traverse younger junction trees in order to rely on them downstream. We may or may not want to work out a rule for when to use a younger junction tree with much higher coverage than an older one, or to combine multiple trees into a single vote.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5925
https://github.com/broadinstitute/gatk/issues/5927:239,Testability,test,tests,239,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); `master`. ### Description ; For example, the following two snippets are equivalent, but due to random ordering, the last two are switched and automated tests are failing, even though the code is untouched.; ```; .../MRPL55_ENST00000430433.1_SPLICE_SITE/C1orf35_ENST00000272139.4_FIVE_PRIME_FLANK/MRPL55_ENST00000465397.1_SPLICE_SITE/C1orf35_ENST00000472617.1_FIVE_PRIME_FLANK....; .../MRPL55_ENST00000430433.1_SPLICE_SITE/C1orf35_ENST00000272139.4_FIVE_PRIME_FLANK/C1orf35_ENST00000472617.1_FIVE_PRIME_FLANK/MRPL55_ENST00000465397.1_SPLICE_SITE...; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5927
https://github.com/broadinstitute/gatk/pull/5928:23,Integrability,depend,dependency,23,"Add BigQuery as a GATK dependency. In order to add this dependency, we have to move; to an unshaded version of google-cloud-java, as the shaded version causes breakage; in BigQuery, as well as newer versions of Spark/Hadoop/Guava. This also includes basic utilities for working with BigQuery (`BigQueryUtils`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5928
https://github.com/broadinstitute/gatk/pull/5928:56,Integrability,depend,dependency,56,"Add BigQuery as a GATK dependency. In order to add this dependency, we have to move; to an unshaded version of google-cloud-java, as the shaded version causes breakage; in BigQuery, as well as newer versions of Spark/Hadoop/Guava. This also includes basic utilities for working with BigQuery (`BigQueryUtils`)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5928
https://github.com/broadinstitute/gatk/issues/5929:691,Availability,ERROR,ERROR,691,"Hi,. First I filter snp.raw.vcf with `VariantFiltration` command below: ; `gatk-4.1.2.0/gatk --java-options ""-Djava.io.tmpdir=/tmp/"" VariantFiltration -R genome.fa -V snp_rmnan.raw.vcf --filter-expression ""QUAL < 30.0 || QD < 2.0 || FS > 60.0 || MQ < 40.0 || SOR > 4.0 || ReadPosRankSum < -8.0"" --filter-name ""my_snp_filter"" --missing-values-evaluate-as-failing true -O snp_rmnan.raw.vcf.tmp.vcf `. This command runs successfully. But when I'm using `SelectVariants` command to extact the filtered site:; `gatk-4.1.2.0/gatk --java-options ""-Djava.io.tmpdir=/tmp/"" SelectVariants -R genome.fa -V snp_rmnan.raw.vcf.tmp.vcf --exclude-filtered -O snp_rmnan.raw.vcf.filter.vcf `. I get this java ERROR below, even without the wrong line number and do not know how to deal with it......... o(╥﹏╥)o，Thank you very much!. ~~~; 11:15:52.195 INFO ProgressMeter - Chr01:15144308 19.9 541000 27161.2; 11:16:04.187 INFO ProgressMeter - Chr01:15388212 20.1 547000 27189.6; 11:16:12.515 INFO SelectVariants - Shutting down engine; [May 9, 2019 11:16:12 AM CST] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 20.37 minutes.; Runtime.totalMemory()=2814377984; java.lang.NumberFormatException: For input string: ""1,0""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:734); at htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132); at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); at htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148); at htsjdk.variant.variantcontext.GenotypesContext.iterator(GenotypesContext.java:465); at org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants.initalizeAlleleAnyploidIndicesCache(Sele",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5929
https://github.com/broadinstitute/gatk/issues/5929:1003,Availability,down,down,1003,"t I filter snp.raw.vcf with `VariantFiltration` command below: ; `gatk-4.1.2.0/gatk --java-options ""-Djava.io.tmpdir=/tmp/"" VariantFiltration -R genome.fa -V snp_rmnan.raw.vcf --filter-expression ""QUAL < 30.0 || QD < 2.0 || FS > 60.0 || MQ < 40.0 || SOR > 4.0 || ReadPosRankSum < -8.0"" --filter-name ""my_snp_filter"" --missing-values-evaluate-as-failing true -O snp_rmnan.raw.vcf.tmp.vcf `. This command runs successfully. But when I'm using `SelectVariants` command to extact the filtered site:; `gatk-4.1.2.0/gatk --java-options ""-Djava.io.tmpdir=/tmp/"" SelectVariants -R genome.fa -V snp_rmnan.raw.vcf.tmp.vcf --exclude-filtered -O snp_rmnan.raw.vcf.filter.vcf `. I get this java ERROR below, even without the wrong line number and do not know how to deal with it......... o(╥﹏╥)o，Thank you very much!. ~~~; 11:15:52.195 INFO ProgressMeter - Chr01:15144308 19.9 541000 27161.2; 11:16:04.187 INFO ProgressMeter - Chr01:15388212 20.1 547000 27189.6; 11:16:12.515 INFO SelectVariants - Shutting down engine; [May 9, 2019 11:16:12 AM CST] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 20.37 minutes.; Runtime.totalMemory()=2814377984; java.lang.NumberFormatException: For input string: ""1,0""; at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65); at java.lang.Integer.parseInt(Integer.java:580); at java.lang.Integer.parseInt(Integer.java:615); at htsjdk.variant.vcf.AbstractVCFCodec.createGenotypeMap(AbstractVCFCodec.java:734); at htsjdk.variant.vcf.AbstractVCFCodec$LazyVCFGenotypesParser.parse(AbstractVCFCodec.java:132); at htsjdk.variant.variantcontext.LazyGenotypesContext.decode(LazyGenotypesContext.java:158); at htsjdk.variant.variantcontext.LazyGenotypesContext.getGenotypes(LazyGenotypesContext.java:148); at htsjdk.variant.variantcontext.GenotypesContext.iterator(GenotypesContext.java:465); at org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants.initalizeAlleleAnyploidIndicesCache(SelectVarian",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5929
https://github.com/broadinstitute/gatk/issues/5929:2894,Integrability,wrap,wrapAndCopyInto,2894,llbender.tools.walkers.variantutils.SelectVariants.initalizeAlleleAnyploidIndicesCache(SelectVariants.java:674); at org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants.apply(SelectVariants.java:580); at org.broadinstitute.hellbender.engine.VariantWalker.lambda$traverse$0(VariantWalker.java:106); at org.broadinstitute.hellbender.engine.VariantWalker$$Lambda$73/388489274.accept(Unknown Source); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:183); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:512); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:502); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:150); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:173); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.VariantWalker.traverse(VariantWalker.java:104); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5929
https://github.com/broadinstitute/gatk/pull/5930:255,Testability,test,tests,255,This is what you asked for @droazen. This is one approach to table writing and importantly I found this exponentially less bothersome than the restrictions placed upon me by `TableWriter` when handling the table outputs in #5913. This probably needs more tests and I can add some more comprehensive ones if you would like. Let me know your thoughts,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5930
https://github.com/broadinstitute/gatk/issues/5931:306,Availability,error,error,306,"## Bug Report. ### Affected tool(s) or class(es); CollectMultipleMetrics. ### Affected version(s); 4.1.2.0. ### Description ; CollectMultipleMetrics performs Percent-encoding of input paths. When running this tool as a step of a packed CWL workflow with Cromwell, this causes a `No such file or directory` error. The input file; ```; /cromwell-executions/transform_pack.cwl#main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; becomes; ```; file:///cromwell-executions/transform_pack.cwl%23main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; and can not be found. #### Expected behavior; The tool should collect metrics without error. #### Actual behavior; `CollectMultipleMetrics`; ```; Job main.metrics.metrics.cwl.gatk_collectmultiplemetrics:NA:1 exited with return code 3 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /mnt/scratch/runpack/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/execution/stderr.; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/tmp.a2640a46; 20:19:59.771 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/bin/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu May 09 20:20:00 UTC 2019] CollectMultipleMetrics --INPUT /cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931
https://github.com/broadinstitute/gatk/issues/5931:769,Availability,error,error,769,"## Bug Report. ### Affected tool(s) or class(es); CollectMultipleMetrics. ### Affected version(s); 4.1.2.0. ### Description ; CollectMultipleMetrics performs Percent-encoding of input paths. When running this tool as a step of a packed CWL workflow with Cromwell, this causes a `No such file or directory` error. The input file; ```; /cromwell-executions/transform_pack.cwl#main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; becomes; ```; file:///cromwell-executions/transform_pack.cwl%23main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; and can not be found. #### Expected behavior; The tool should collect metrics without error. #### Actual behavior; `CollectMultipleMetrics`; ```; Job main.metrics.metrics.cwl.gatk_collectmultiplemetrics:NA:1 exited with return code 3 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /mnt/scratch/runpack/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/execution/stderr.; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/tmp.a2640a46; 20:19:59.771 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/bin/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu May 09 20:20:00 UTC 2019] CollectMultipleMetrics --INPUT /cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931
https://github.com/broadinstitute/gatk/issues/5931:3647,Availability,avail,available,3647,"-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz --TMP_DIR . --REFERENCE_SEQUENCE /cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/inputs/-733038737/GRCh38.d1.vd1.fa --ASSUME_SORTED true --STOP_AFTER 0 --INCLUDE_UNPAIRED false --VERBOSITY INFO --QUIET false --VALIDATION_STRINGENCY STRICT --COMPRESSION_LEVEL 2 --MAX_RECORDS_IN_RAM 500000 --CREATE_INDEX false --CREATE_MD5_FILE false --GA4GH_CLIENT_SECRETS client_secrets.json --help false --version false --showHidden false --USE_JDK_DEFLATER false --USE_JDK_INFLATER false; [Thu May 09 20:20:00 UTC 2019] Executing as root@6b3fc2da5b97 on Linux 4.15.0-48-generic amd64; OpenJDK 64-Bit Server VM 1.8.0_191-8u191-b12-2ubuntu0.18.04.1-b12; Deflater: Intel; Inflater: Intel; Provider GCS is available; Picard version: Version:4.1.2.0; WARNING 2019-05-09 20:20:00 CollectMultipleMetrics The CollectBaseDistributionByCycle program does not support a metric accumulation level, but METRIC_ACCUMULATION_LEVEL was overridden in the command line. CollectBaseDistributionByCycle will be run against the entire input.; WARNING 2019-05-09 20:20:00 CollectMultipleMetrics The MeanQualityByCycle program does not support a metric accumulation level, but METRIC_ACCUMULATION_LEVEL was overridden in the command line. MeanQualityByCycle will be run against the entire input.; WARNING 2019-05-09 20:20:00 CollectMultipleMetrics The QualityScoreDistribution program does not support a metric accumulation level, but METRIC_ACCUMULATION_LEVEL was overridden in the command line. QualityScoreDistribution will be run against the entire input.; WARNING 2019-05-09 20:20:00 CollectMultipleMetrics The CollectQualityYieldMetrics program does not support a metric accumulation level, but METRIC_ACCUMUL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931
https://github.com/broadinstitute/gatk/issues/5931:149,Performance,perform,performs,149,"## Bug Report. ### Affected tool(s) or class(es); CollectMultipleMetrics. ### Affected version(s); 4.1.2.0. ### Description ; CollectMultipleMetrics performs Percent-encoding of input paths. When running this tool as a step of a packed CWL workflow with Cromwell, this causes a `No such file or directory` error. The input file; ```; /cromwell-executions/transform_pack.cwl#main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; becomes; ```; file:///cromwell-executions/transform_pack.cwl%23main/0cd8a732-b482-4b8e-ba6e-34d244620ded/call-picard_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; and can not be found. #### Expected behavior; The tool should collect metrics without error. #### Actual behavior; `CollectMultipleMetrics`; ```; Job main.metrics.metrics.cwl.gatk_collectmultiplemetrics:NA:1 exited with return code 3 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /mnt/scratch/runpack/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/execution/stderr.; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/tmp.a2640a46; 20:19:59.771 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/bin/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu May 09 20:20:00 UTC 2019] CollectMultipleMetrics --INPUT /cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_colle",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931
https://github.com/broadinstitute/gatk/issues/5931:1626,Performance,Load,Loading,1626,tiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; ```; and can not be found. #### Expected behavior; The tool should collect metrics without error. #### Actual behavior; `CollectMultipleMetrics`; ```; Job main.metrics.metrics.cwl.gatk_collectmultiplemetrics:NA:1 exited with return code 3 which has not been declared as a valid return code. See 'continueOnReturnCode' runtime attribute for more details.; Check the content of stderr for potential additional information: /mnt/scratch/runpack/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/execution/stderr.; Picked up _JAVA_OPTIONS: -Djava.io.tmpdir=/cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/tmp.a2640a46; 20:19:59.771 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/usr/local/bin/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; [Thu May 09 20:20:00 UTC 2019] CollectMultipleMetrics --INPUT /cromwell-executions/transform_pack.cwl#main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl#metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/inputs/-1966356616/A68634_2_lanes_dupsFlagged_gdc_realn.bam --OUTPUT A68634_2_lanes_dupsFlagged_gdc_realn --METRIC_ACCUMULATION_LEVEL LIBRARY --METRIC_ACCUMULATION_LEVEL ALL_READS --METRIC_ACCUMULATION_LEVEL READ_GROUP --METRIC_ACCUMULATION_LEVEL SAMPLE --PROGRAM CollectAlignmentSummaryMetrics --PROGRAM CollectBaseDistributionByCycle --PROGRAM CollectInsertSizeMetrics --PROGRAM MeanQualityByCycle --PROGRAM QualityScoreDistribution --PROGRAM CollectGcBiasMetrics --PROGRAM CollectQualityYieldMetrics --PROGRAM CollectSequencingArtifactMetrics --DB_SNP /cromwell-executions/transform_pack.cwl#main/8f580,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931
https://github.com/broadinstitute/gatk/issues/5931:5972,Performance,load,loadVcf,5972,"Metrics will be run against the entire input.; [Thu May 09 20:21:31 UTC 2019] picard.analysis.CollectMultipleMetrics done. Elapsed time: 1.53 minutes.; Runtime.totalMemory()=2486697984; To get help, see http://broadinstitute.github.io/picard/index.html#GettingHelp; htsjdk.tribble.TribbleException$MalformedFeatureFile: Unable to create BasicFeatureReader using feature file , for input source: file:///cromwell-executions/transform_pack.cwl%23main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl%23metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmultiplemetrics/inputs/-733038737/dbsnp_144.hg38.vcf.gz; at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:123); at htsjdk.tribble.AbstractFeatureReader.getFeatureReader(AbstractFeatureReader.java:80); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:140); at htsjdk.variant.vcf.VCFFileReader.<init>(VCFFileReader.java:92); at picard.util.DbSnpBitSetUtil.loadVcf(DbSnpBitSetUtil.java:143); at picard.util.DbSnpBitSetUtil.<init>(DbSnpBitSetUtil.java:99); at picard.analysis.artifacts.CollectSequencingArtifactMetrics.setup(CollectSequencingArtifactMetrics.java:213); at picard.analysis.SinglePassSamProgram.makeItSo(SinglePassSamProgram.java:129); at picard.analysis.CollectMultipleMetrics.doWork(CollectMultipleMetrics.java:563); at picard.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:295); at org.broadinstitute.hellbender.cmdline.PicardCommandLineProgramExecutor.instanceMain(PicardCommandLineProgramExecutor.java:25); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.io.FileNotFoundException: /cromwell-executions/transform_pack.cwl%23main/8f58079f-1b94-40a9-873f-41e8d765644d/call-metrics/transform_pack.cwl%23metrics.cwl/2a15d912-9a75-44dc-a723-b9f2dba439b3/call-gatk_collectmu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5931
https://github.com/broadinstitute/gatk/issues/5932:152,Modifiability,refactor,refactoring,152,"# Feature request; ### Tool(s) or class(es) involved; Funcotator. ### Description; Currently, the GencodeFuncotationFactory is doing a lot. A suggested refactoring would be to have an AbstractGencodeFuncotationFactory that encapsulates the shared functionality b/w segment and small mutation annotation. Then two concrete classes for annotating segments and small mutations.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5932
https://github.com/broadinstitute/gatk/issues/5934:153,Availability,error,error,153,"```; :compileJava/usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:436,Availability,error,error,436,"```; :compileJava/usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:719,Availability,error,error,719,"```; :compileJava/usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:1002,Availability,error,error,1002,"/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:1285,Availability,error,error,1285,"/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:1568,Availability,error,error,1568,"/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:1840,Availability,error,error,1840,"alkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:2177,Availability,error,error,2177,"ppable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/annotator/StrandOddsRatio.java:106: error: unmappable character for encoding ASCII; * SOR = ln(5.7284) + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:2514,Availability,error,error,2514,") + ln(0.2385) ??? ln(0.7559) = 1.7454427755 + (-1.433) ??? (-0.2798) = 0.592; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</n",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:2851,Availability,error,error,2851,"src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ```. FreeBSD 11.2 amd64",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:3188,Availability,error,error,3188,"src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ```. FreeBSD 11.2 amd64",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5934:3525,Availability,error,error,3525,"src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ^; /usr/ports/biology/gatk/work/gatk-4.1.2.0/src/main/java/org/broadinstitute/hellbender/tools/walkers/mutect/Mutect2.java:137: error: unmappable character for encoding ASCII; * Specifically, the mode sets <nobr>???-initial-tumor-lod</nobr> to 0, <nobr>???-tumor-lod-to-emit</nobr> to 0, <nobr>--af-of-alleles-not-in-resource</nobr> to; ```. FreeBSD 11.2 amd64",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5934
https://github.com/broadinstitute/gatk/issues/5935:136,Deployability,release,release,136,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:334,Deployability,release,release,334,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:429,Deployability,release,release,429,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:744,Deployability,update,updated,744,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:1246,Deployability,UPDATE,UPDATE,1246,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:671,Testability,log,logs,671,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:824,Testability,test,test,824,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:951,Testability,test,test,951,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:1097,Testability,log,logs,1097,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:1128,Testability,test,test,1128,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5935:1213,Testability,test,test,1213,"## Bug Report. ### Affected tool(s) or class(es); `ParallelCopyGCSDirectoryIntoHDFSSpark`. ### Affected version(s); - [x] Latest public release version [version?]; - [x] Latest master branch as of [2019-05-13]. ### Description . `ParallelCopyGCSDirectoryIntoHDFSSpark` behaves in the following strange way:. * under __master/latest__ release, it __fails__ to copy a GCS ""directory"" containing __BAMs__; * under __master/latest__ release, it __successfully__ copies a GCS ""directory"" containing __reference__; * changing the nio lib version from 81 to 66 in `build.gradle`, it __successfully__ copies GCS ""directories"" containing __reference__ or __BAMs__; * see attached logs. #### Steps to reproduce. Both scripts referred to below need to be updated accordingly, but trivially. * from the master branch, run the attached `test.nio.ver.81.sh`. * branch out from master, change the literal `81` to `66` on line 69 in `build.gradle`, run the attached `test.nio.ver.66.sh`. #### Expected behavior. Files in the ""directories"" given in the gs path copied successfully. #### Actual behavior; Fail. See logs attached. -------------; [test.nio.paraCopyHDFSSpark.zip](https://github.com/broadinstitute/gatk/files/3174143/test.nio.paraCopyHDFSSpark.zip). UPDATE:; reuploaded attachment",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5935
https://github.com/broadinstitute/gatk/issues/5937:795,Availability,error,errors,795,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); All. ### Description ; Since we expect users to write config files manually, we need to have some enforcement of naming rules. At the least, to disallow spaces in the name and version fields. There are many places throughout the code where we assume that there will be no spaces. Additionally, I hear from users that they want any Funcotator tsv outputs to never have spaces (or tabs or other special characters -- ""_"", ""-"" are obviously okay). . We can solicit users about which special characters are okay, but definitely disallow spaces and tabs. #### Steps to reproduce; Add a space to the Gencode datasource config (name or version field) and try to funcotate a segment file. #### Expected behavior; No errors and no spaces in the field names. #### Actual behavior; Exception in gene list output renderer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5937
https://github.com/broadinstitute/gatk/issues/5937:141,Modifiability,config,config,141,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); All. ### Description ; Since we expect users to write config files manually, we need to have some enforcement of naming rules. At the least, to disallow spaces in the name and version fields. There are many places throughout the code where we assume that there will be no spaces. Additionally, I hear from users that they want any Funcotator tsv outputs to never have spaces (or tabs or other special characters -- ""_"", ""-"" are obviously okay). . We can solicit users about which special characters are okay, but definitely disallow spaces and tabs. #### Steps to reproduce; Add a space to the Gencode datasource config (name or version field) and try to funcotate a segment file. #### Expected behavior; No errors and no spaces in the field names. #### Actual behavior; Exception in gene list output renderer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5937
https://github.com/broadinstitute/gatk/issues/5937:700,Modifiability,config,config,700,"## Bug Report. ### Affected tool(s) or class(es); Funcotator. ### Affected version(s); All. ### Description ; Since we expect users to write config files manually, we need to have some enforcement of naming rules. At the least, to disallow spaces in the name and version fields. There are many places throughout the code where we assume that there will be no spaces. Additionally, I hear from users that they want any Funcotator tsv outputs to never have spaces (or tabs or other special characters -- ""_"", ""-"" are obviously okay). . We can solicit users about which special characters are okay, but definitely disallow spaces and tabs. #### Steps to reproduce; Add a space to the Gencode datasource config (name or version field) and try to funcotate a segment file. #### Expected behavior; No errors and no spaces in the field names. #### Actual behavior; Exception in gene list output renderer.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5937
https://github.com/broadinstitute/gatk/issues/5939:917,Availability,error,error,917,"I read the [CNNScoreVariants documentation](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_vqsr_CNNScoreVariants.php#--info-annotation-keys) and ran the ```--help``` option, and both state the following. ```; --info-annotation-keys,-info-annotation-keys:String; 		The VCF info fields to send to python. This argument may be specified 0 or more times.; 		Default value: [MQ, DP, SOR, FS, QD, MQRankSum, ReadPosRankSum]. ; ``` . I successfully executed the CNNScoreVariants command with the default value of the ```--info-annotation-keys``` argument in the following way. ```; --info-annotation-keys '[MQ, DP, SOR, FS, QD, MQRankSum, ReadPosRankSum]' ; ```. However, when I try to change the number of fields, for example like. ```; --info-annotation-keys '[MQ, DP, SOR, FS, QD, MQRankSum]' ; ```. or anything more or less than seven fields I get an error like the following one. ```; Traceback (most recent call last):; 		File ""<stdin>"", line 1, in <module>; 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 127, in score_and_write_batch; 		batch_size=python_batch_size); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training.py"", line 1152, in predict; 		x, _, _ = self._standardize_user_data(x); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training.py"", line 754, in _standardize_user_data; 		exception_prefix='input'); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training_utils.py"", line 136, in standardize_input_data; 		str(data_shape)); 	ValueError: Error when checking input: expected annotations to have shape (7,) but got array with shape (6,); ```. According to the documentation, I should be able to use the argument with an arbitrary number of fields. Is this a bug, or am I using the ```--info-annotation-keys``` argument incorrectly?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5939
https://github.com/broadinstitute/gatk/issues/5939:1663,Availability,Error,Error,1663,"I read the [CNNScoreVariants documentation](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_vqsr_CNNScoreVariants.php#--info-annotation-keys) and ran the ```--help``` option, and both state the following. ```; --info-annotation-keys,-info-annotation-keys:String; 		The VCF info fields to send to python. This argument may be specified 0 or more times.; 		Default value: [MQ, DP, SOR, FS, QD, MQRankSum, ReadPosRankSum]. ; ``` . I successfully executed the CNNScoreVariants command with the default value of the ```--info-annotation-keys``` argument in the following way. ```; --info-annotation-keys '[MQ, DP, SOR, FS, QD, MQRankSum, ReadPosRankSum]' ; ```. However, when I try to change the number of fields, for example like. ```; --info-annotation-keys '[MQ, DP, SOR, FS, QD, MQRankSum]' ; ```. or anything more or less than seven fields I get an error like the following one. ```; Traceback (most recent call last):; 		File ""<stdin>"", line 1, in <module>; 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 127, in score_and_write_batch; 		batch_size=python_batch_size); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training.py"", line 1152, in predict; 		x, _, _ = self._standardize_user_data(x); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training.py"", line 754, in _standardize_user_data; 		exception_prefix='input'); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training_utils.py"", line 136, in standardize_input_data; 		str(data_shape)); 	ValueError: Error when checking input: expected annotations to have shape (7,) but got array with shape (6,); ```. According to the documentation, I should be able to use the argument with an arbitrary number of fields. Is this a bug, or am I using the ```--info-annotation-keys``` argument incorrectly?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5939
https://github.com/broadinstitute/gatk/issues/5939:1292,Safety,predict,predict,1292,"I read the [CNNScoreVariants documentation](https://software.broadinstitute.org/gatk/documentation/tooldocs/current/org_broadinstitute_hellbender_tools_walkers_vqsr_CNNScoreVariants.php#--info-annotation-keys) and ran the ```--help``` option, and both state the following. ```; --info-annotation-keys,-info-annotation-keys:String; 		The VCF info fields to send to python. This argument may be specified 0 or more times.; 		Default value: [MQ, DP, SOR, FS, QD, MQRankSum, ReadPosRankSum]. ; ``` . I successfully executed the CNNScoreVariants command with the default value of the ```--info-annotation-keys``` argument in the following way. ```; --info-annotation-keys '[MQ, DP, SOR, FS, QD, MQRankSum, ReadPosRankSum]' ; ```. However, when I try to change the number of fields, for example like. ```; --info-annotation-keys '[MQ, DP, SOR, FS, QD, MQRankSum]' ; ```. or anything more or less than seven fields I get an error like the following one. ```; Traceback (most recent call last):; 		File ""<stdin>"", line 1, in <module>; 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/vqsr_cnn/vqsr_cnn/inference.py"", line 127, in score_and_write_batch; 		batch_size=python_batch_size); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training.py"", line 1152, in predict; 		x, _, _ = self._standardize_user_data(x); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training.py"", line 754, in _standardize_user_data; 		exception_prefix='input'); 		File ""/opt/miniconda/envs/gatk/lib/python3.6/site-packages/keras/engine/training_utils.py"", line 136, in standardize_input_data; 		str(data_shape)); 	ValueError: Error when checking input: expected annotations to have shape (7,) but got array with shape (6,); ```. According to the documentation, I should be able to use the argument with an arbitrary number of fields. Is this a bug, or am I using the ```--info-annotation-keys``` argument incorrectly?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5939
https://github.com/broadinstitute/gatk/issues/5940:312,Deployability,pipeline,pipeline,312,"For allele-specific ranksums, an allele may not have a rank sum value if it didn't have any hets. Right now this is output as ""nul"", but should use the VCF . for missing. . This will a pretty noticeable change with respect to older versions, though we never condone using different versions together in the same pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5940
https://github.com/broadinstitute/gatk/pull/5941:2354,Deployability,configurat,configuration,2354,"output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:777,Modifiability,refactor,refactoring,777,"Adds the `FuncotateSegments` tool. . *`FuncotateSegments` does not support VCF input!*. This tool will create two output files from a GATK seg file:; - A simple TSV which has each segment of the input file funcotated with all the genes it overlaps and which gene/exon covers each breakpoint. The output format is meant to (closely) match Oncotator. ; - A gene list which has every gene, covered by a segment, listed with the segment that covers it. A gene can appear more than once if a segment breakpoint overlaps the gene (i.e. more than one segment overlaps the gene). The output format is meant to (closely) match Oncotator.; - Output formats may change.; - Input format is only seg files such as those generated from `ModelSegments`. . Dev and reviewer notes:; - Includes refactoring to drive much of the GencodeFuncotation data solely from the transcript. As opposed to a mix of the transcript and gene. This does cause some changes to sorting of the GencodeFuncotations (easily seen in the other transcripts field). It turns out that the transcript type field has different values for each transcript. This causes many transcripts to no longer be categorized as protein coding. Therefore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:2094,Modifiability,flexible,flexible,2094,"ses many transcripts to no longer be categorized as protein coding. Therefore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. T",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:2199,Modifiability,config,config,2199,"ore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Fu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:2354,Modifiability,config,configuration,2354,"output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instr",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:3141,Testability,test,test,3141," writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instructions for developers is present as comments in the code. This is a bit fragile, especially for developers that are using GATK as a library; - See #5921 for tracking `FuncotateSegments` WDL development. Closes #4609 . Output formats may change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:3237,Testability,test,test,3237," writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instructions for developers is present as comments in the code. This is a bit fragile, especially for developers that are using GATK as a library; - See #5921 for tracking `FuncotateSegments` WDL development. Closes #4609 . Output formats may change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:3355,Testability,test,test,3355," writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instructions for developers is present as comments in the code. This is a bit fragile, especially for developers that are using GATK as a library; - See #5921 for tracking `FuncotateSegments` WDL development. Closes #4609 . Output formats may change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:3360,Testability,test,tests,3360," writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for FuncotationMap based on Kryo. Also, added the necessary Kryo registrations. This induced a new unit test to enforce any concrete implementations of `Funcotation` to be Kryo serializable. The unit test does a recursive search of the funcotator package. For all concrete implementations, it tracks whether this unit test tests the serialization. If not, it fails. Instructions for developers is present as comments in the code. This is a bit fragile, especially for developers that are using GATK as a library; - See #5921 for tracking `FuncotateSegments` WDL development. Closes #4609 . Output formats may change.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:154,Usability,simpl,simple,154,"Adds the `FuncotateSegments` tool. . *`FuncotateSegments` does not support VCF input!*. This tool will create two output files from a GATK seg file:; - A simple TSV which has each segment of the input file funcotated with all the genes it overlaps and which gene/exon covers each breakpoint. The output format is meant to (closely) match Oncotator. ; - A gene list which has every gene, covered by a segment, listed with the segment that covers it. A gene can appear more than once if a segment breakpoint overlaps the gene (i.e. more than one segment overlaps the gene). The output format is meant to (closely) match Oncotator.; - Output formats may change.; - Input format is only seg files such as those generated from `ModelSegments`. . Dev and reviewer notes:; - Includes refactoring to drive much of the GencodeFuncotation data solely from the transcript. As opposed to a mix of the transcript and gene. This does cause some changes to sorting of the GencodeFuncotations (easily seen in the other transcripts field). It turns out that the transcript type field has different values for each transcript. This causes many transcripts to no longer be categorized as protein coding. Therefore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do t",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:1957,Usability,Simpl,SimpleTsvOutputRenderer,1957,"odeFuncotations (easily seen in the other transcripts field). It turns out that the transcript type field has different values for each transcript. This causes many transcripts to no longer be categorized as protein coding. Therefore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs p",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/pull/5941:2039,Usability,Simpl,SimpleTsvOutputRenderer,2039,"the transcript type field has different values for each transcript. This causes many transcripts to no longer be categorized as protein coding. Therefore, the ground truth (mostly/totally in `FuncotatorIntegrationTest`) had to be modified. *Please carefully review the ground truth changes*.; - Introduces the `CompsiteOutputRenderer`, which is composed of multiple output renderers. This is used when output type is `SEG`, so that it can write both output files simultaneously.; - Introduces the `GeneListOutputRenderer`. This does not write anything to disk until the entire input file is processed. The actual writing happens during the `close()` command. This is necessary since it cannot actually render its output until all segments have been seen. This output renderer also relies heavily on specific funcotation fields being in the input `FuncotationMap`. Internally, the gene list output renderer uses the `SimpleTsvOutputRenderer` (see below) to do the actual writing.; - Introduces the `SimpleTsvOutputRenderer`. This output renderer is very flexible and renders a tab-separated text file based on several output rules. Formats are driven through config files. And developers can limit the output columns to ignore extraneous funcotation fields. Note that excluded fields are honored, regardless. If a configuration + parameter combination would result in this class producing an empty file, an exception is thrown. More notes are in the javadocs of the class.; - Currently, only the `GencodeFuncotationFactory` can actually funcotate segments. ; - Code base currently enforces only small mutations when running `Funcotator` (segs are funcotated as CANNOT_DETERMINE) and only segments when running `FuncotateSegments` (small mutations produce exception). This is enforced with flags in the code. The backend does not disallow a mixture for future use. This may prove important when funcotating CNVs from VCFs produced by tools other than `ModelSegments`.; - Added copy creation method for F",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5941
https://github.com/broadinstitute/gatk/issues/5942:881,Availability,down,down,881,"## Bug Report. ### Affected tool(s) or class(es); StructuralVariationDiscoveryPipelineSpark . ### Affected version(s); GATK 4.1.2.0. ### Description . At end of run on a Hadoop cluster, the job aborts.... services=List(),; started=false); 2019-05-14 17:07:05 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-05-14 17:07:05 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-05-14 17:07:05 INFO MemoryStore:54 - MemoryStore cleared; 2019-05-14 17:07:05 INFO BlockManager:54 - BlockManager stopped; 2019-05-14 17:07:05 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-05-14 17:07:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-05-14 17:07:05 INFO SparkContext:54 - Successfully stopped SparkContext; 17:07:05.631 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 14, 2019 5:07:05 PM EDT] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 41.02 minutes.; Runtime.totalMemory()=23321378816; java.lang.IllegalArgumentException: Wrong FS: hdfs://scc:-1/project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file.sam, expected: hdfs://scc; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337); at org.apache.hadoop.fs.FileSystem.create",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:6102,Availability,heartbeat,heartbeatInterval,6102,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:6282,Availability,error,error,6282,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:4422,Deployability,deploy,deploy,4422,"ine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineS",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:4503,Deployability,deploy,deploy,4503,"e.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:4540,Deployability,deploy,deploy,4540,"gram.doWork(SparkCommandLineProgram.java:31); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_km",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:4612,Deployability,deploy,deploy,4612,"ute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/s",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:4688,Deployability,deploy,deploy,4688,"9); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///proj",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:4759,Deployability,deploy,deploy,4759,"eMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPAR",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:4828,Deployability,deploy,deploy,4828,"e.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); at java.lang.reflect.Method.invoke(Method.java:498); at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\;",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:5787,Deployability,deploy,deploy-mode,5787,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:6288,Integrability,message,message,6288,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:194,Safety,abort,aborts,194,"## Bug Report. ### Affected tool(s) or class(es); StructuralVariationDiscoveryPipelineSpark . ### Affected version(s); GATK 4.1.2.0. ### Description . At end of run on a Hadoop cluster, the job aborts.... services=List(),; started=false); 2019-05-14 17:07:05 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-05-14 17:07:05 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-05-14 17:07:05 INFO MemoryStore:54 - MemoryStore cleared; 2019-05-14 17:07:05 INFO BlockManager:54 - BlockManager stopped; 2019-05-14 17:07:05 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-05-14 17:07:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-05-14 17:07:05 INFO SparkContext:54 - Successfully stopped SparkContext; 17:07:05.631 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 14, 2019 5:07:05 PM EDT] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 41.02 minutes.; Runtime.totalMemory()=23321378816; java.lang.IllegalArgumentException: Wrong FS: hdfs://scc:-1/project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file.sam, expected: hdfs://scc; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337); at org.apache.hadoop.fs.FileSystem.create",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:6065,Safety,timeout,timeout,6065,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:6217,Safety,abort,aborts,6217,"vaMainApplication.start(SparkApplication.scala:52); at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:879); at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:197); at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:227); at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:136); at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Shutdown hook called; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-45f7a9f3-b94f-4040-bf32-0dbfe44f8f68; 2019-05-14 17:07:05 INFO ShutdownHookManager:54 - Deleting directory /restricted/projectnb/casa/wgs.hg38/sv/gatk.sv/tmp/spark-70db8953-5dec-4eb8-910d-f0abd7e1c42b. real 41m12.118s; user 83m41.069s; sys 10m15.403s. #### Steps to reproduce; atk --java-options ""-Djava.io.tmpdir=tmp"" StructuralVariationDiscoveryPipelineSpark \; -R $REF \; --aligner-index-image GRCh38_full_analysis_set_plus_decoy_hla.fa.img \; --kmers-to-ignore GRCh38_ignored_kmers.txt \; --contig-sam-file hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.contig-sam-file.sam\; -I $CRAM_DIR/$SAMPLE.cram \; -O hdfs:///project/casa/gcad/$CENTER/sv/$SAMPLE.sv.vcf.gz \; -- \; --spark-runner SPARK --spark-master yarn --deploy-mode client \; --executor-memory 85G\; --driver-memory 30g\; --num-executors 40\; --executor-cores 4\; --conf spark.yarn.submit.waitAppCompletion=false\; --name ""$SAMPLE"" \; --files $REF.img,$KMER \; --conf spark.yarn.executor.memoryOverhead=5000 \; --conf spark.network.timeout=600 \; --conf spark.executor.heartbeatInterval=120; #### Expected behavior. Should complete and write output files. . #### Actual behavior; Job aborts after running 45 min and no output files are written. The error message refers to filename that is not actually passed as a parameter to the tool: hdfs://scc:-1/. Not sure where the -1 is coming from. . ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:1472,Security,access,access,1472,"5 INFO BlockManager:54 - BlockManager stopped; 2019-05-14 17:07:05 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-05-14 17:07:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-05-14 17:07:05 INFO SparkContext:54 - Successfully stopped SparkContext; 17:07:05.631 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 14, 2019 5:07:05 PM EDT] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 41.02 minutes.; Runtime.totalMemory()=23321378816; java.lang.IllegalArgumentException: Wrong FS: hdfs://scc:-1/project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file.sam, expected: hdfs://scc; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:908); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:889); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:786); at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:775); at hdfs.jsr203.HadoopFileSystem.newOutputStream(HadoopFileSystem.java:554); at hdfs.jsr203.HadoopFileSystem.newByteChannel(HadoopFileSystem.java:395); at hdfs.jsr203.HadoopPath.newByteChannel(HadoopPath.java:558); at hdfs.jsr203.HadoopFileSystemProvider.newByteChannel(Hado",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/issues/5942:461,Usability,clear,cleared,461,"## Bug Report. ### Affected tool(s) or class(es); StructuralVariationDiscoveryPipelineSpark . ### Affected version(s); GATK 4.1.2.0. ### Description . At end of run on a Hadoop cluster, the job aborts.... services=List(),; started=false); 2019-05-14 17:07:05 INFO YarnClientSchedulerBackend:54 - Stopped; 2019-05-14 17:07:05 INFO MapOutputTrackerMasterEndpoint:54 - MapOutputTrackerMasterEndpoint stopped!; 2019-05-14 17:07:05 INFO MemoryStore:54 - MemoryStore cleared; 2019-05-14 17:07:05 INFO BlockManager:54 - BlockManager stopped; 2019-05-14 17:07:05 INFO BlockManagerMaster:54 - BlockManagerMaster stopped; 2019-05-14 17:07:05 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint:54 - OutputCommitCoordinator stopped!; 2019-05-14 17:07:05 INFO SparkContext:54 - Successfully stopped SparkContext; 17:07:05.631 INFO StructuralVariationDiscoveryPipelineSpark - Shutting down engine; [May 14, 2019 5:07:05 PM EDT] org.broadinstitute.hellbender.tools.spark.sv.StructuralVariationDiscoveryPipelineSpark done. Elapsed time: 41.02 minutes.; Runtime.totalMemory()=23321378816; java.lang.IllegalArgumentException: Wrong FS: hdfs://scc:-1/project/casa/gcad/adsp.cc/sv/A-ACT-AC000014-BL-NCR-15AD78694.hg38.realign.bqsr.contig-sam-file.sam, expected: hdfs://scc; at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645); at org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:193); at org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:105); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:397); at org.apache.hadoop.hdfs.DistributedFileSystem$6.doCall(DistributedFileSystem.java:393); at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:393); at org.apache.hadoop.hdfs.DistributedFileSystem.create(DistributedFileSystem.java:337); at org.apache.hadoop.fs.FileSystem.create",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5942
https://github.com/broadinstitute/gatk/pull/5943:156,Deployability,update,updated,156,"I don't think this is true anymore. We've been using it successfully with gatk4's `HaplotypeCaller`. I'd love some confirmation, and then the docs could be updated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5943
https://github.com/broadinstitute/gatk/issues/5944:6,Availability,down,downloaded,6,"Hi, I downloaded the newest version of GATK [""https://github.com/broadinstitute/gatk.git""] and I combined g.vcf files into vcf with the command; <pre>; gatk GenomicsDBImport --sample-name-map /work7_P1/GATK_RegionCall/Variant_call/cohort.sample_map --reader-threads 5 --batch-size 50 --genomicsdb-workspace-path /work7_P1/GATK_RegionCall/DBimport/chr01_directory --intervals chr01; </pre> . From the perspective of site 1660261, we have a missing C from 1660261 to 1660272. However, from site 1660263, we have a long deletion of ""CTCTCTCTC"", which is a conflict with information from 1660261. As I was trying to construct a personalized genome, tools I know can only deal with the first variant while ignoring overlapping variant. A good way to deal this might be isolating specific sample and trim the same tailing bases, for example, converting GTAAC->GAAC to GT->G. <pre>; chr01 <b>1660261</b> . TCTCTCTCTCTC TTCTCTCTCTC,T,* 93054.98 . AC=382,4,2;AF=0.070,7.294e-04,3.647e-04;AN=5484;BaseQRankSum=0.947;DP=21527;ExcessHet=-0.0000;FS=0.000;InbreedingCoeff=0.6279;MLEAC=463,3,1;MLEAF=0.084,5.470e-04,1.823e-04;MQ=58.57;MQRankSum=0.00;QD=27.98;ReadPosRankSum=0.00;SOR=0.533 GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:4,14,0,0:18:99:<b>0|1:1660261_TC_T</b>:507,0,114,519,156,675,519,156,675,675:1660261; chr01 1660262 . C *,T 6047.70 . AC=388,26;AF=0.070,4.712e-03;AN=5518;BaseQRankSum=1.38;DP=21571;ExcessHet=-0.0000;FS=0.000;InbreedingCoeff=0.5827;MLEAC=466,19;MLEAF=0.084,3.443e-03;MQ=59.84;MQRankSum=0.00;QD=2.47;ReadPosRankSum=0.431;SOR=0.510 GT:AD:DP:GQ:PGT:PID:PL:PS 0|1:4,14,0:18:99:<b>0|1:1660261_TC_T</b>:507,0,114,519,156,675:1660261; chr01 <b>1660263</b> . TCTCTCTCTC TTCTCTCTC,T,* 120442.78 . AC=102,382,6;AF=0.018,0.069,1.085e-03;AN=5530;BaseQRankSum=0.916;DP=21553;ExcessHet=-0.0000;FS=0.000;InbreedingCoeff=0.6783;MLEAC=119,455,4;MLEAF=0.022,0.082,7.233e-04;MQ=58.72;MQRankSum=0.00;QD=32.35;ReadPosRankSum=0.00;SOR=0.482 GT:AD:DP:GQ:PGT:PID:PL:PS <b>0|2</b>:4,0,14,0:18:99:<b>0|1:1660261_TC_T</b>:50",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5944
https://github.com/broadinstitute/gatk/pull/5946:222,Availability,error,error,222,"I tested this using the following sketchy procedure: I temporarily reverted https://github.com/broadinstitute/gatk/pull/5936 on this branch, thereby re-introducing non-ASCII characters into the source. That builds without error, as it should. Then I temporarily changed the newly added encoding declarations included this PR in build.gradle from ""UTF-8"" to ""US-ASCII"", after which I was able to reproduce exactly the same errors as reported in https://github.com/broadinstitute/gatk/issues/5934, for both compile and gatkDoc tasks. So I think these changes achieve the desired result (accept UTF-8 in source).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5946
https://github.com/broadinstitute/gatk/pull/5946:422,Availability,error,errors,422,"I tested this using the following sketchy procedure: I temporarily reverted https://github.com/broadinstitute/gatk/pull/5936 on this branch, thereby re-introducing non-ASCII characters into the source. That builds without error, as it should. Then I temporarily changed the newly added encoding declarations included this PR in build.gradle from ""UTF-8"" to ""US-ASCII"", after which I was able to reproduce exactly the same errors as reported in https://github.com/broadinstitute/gatk/issues/5934, for both compile and gatkDoc tasks. So I think these changes achieve the desired result (accept UTF-8 in source).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5946
https://github.com/broadinstitute/gatk/pull/5946:2,Testability,test,tested,2,"I tested this using the following sketchy procedure: I temporarily reverted https://github.com/broadinstitute/gatk/pull/5936 on this branch, thereby re-introducing non-ASCII characters into the source. That builds without error, as it should. Then I temporarily changed the newly added encoding declarations included this PR in build.gradle from ""UTF-8"" to ""US-ASCII"", after which I was able to reproduce exactly the same errors as reported in https://github.com/broadinstitute/gatk/issues/5934, for both compile and gatkDoc tasks. So I think these changes achieve the desired result (accept UTF-8 in source).",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5946
https://github.com/broadinstitute/gatk/issues/5947:368,Availability,echo,echo,368,"Hi,; This is a part of my script and I get the mistakes after I run it:; for sample in $samples ; do ; sample_gvcfs=${sample_gvcfs}"" --variant /data/users/zhanglei/species/Medicago/result/${sample}.HC.g.vcf.gz ""; done; time gatk CombineGVCFs \; -R /data/users/zhanglei/species/Medicago/Medicago.fa \; ${sample_gvcfs} \; -O $outdir/population/${outname}.HC.g.vcf.gz && echo ""** ${outname}.HC.g.vcf.gz done **"" &&. 18:08:13.704 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/software/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:08:13.872 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.873 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.0.3.0; 18:08:13.873 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:08:13.873 INFO CombineGVCFs - Executing as zhanglei@GenEngine on Linux v3.10.0-327.el7.x86_64 amd64; 18:08:13.873 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_201-b09; 18:08:13.874 INFO CombineGVCFs - Start Date/Time: May 18, 2019 6:08:13 PM CST; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - HTSJDK Version: 2.14.3; 18:08:13.874 INFO CombineGVCFs - Picard Version: 2.17.2; 18:08:13.874 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:08:13.875 INFO CombineGVCFs - Deflater: IntelDeflater; 18:08:13.875 INFO CombineGVCFs - Inflater: IntelInflater; 18:08:13.875 INFO CombineGVCFs - GCS max retries/reopens: 20; 18:08:13.875 I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947
https://github.com/broadinstitute/gatk/issues/5947:3988,Availability,down,down,3988,"o read file file:///data/users/zhanglei/species/Medicago/result/SRR340097.HC.g.vcf.gz; 18:08:15.044 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340098.HC.g.vcf.gz; 18:08:15.098 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340099.HC.g.vcf.gz; 18:08:15.142 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340100.HC.g.vcf.gz; 18:08:15.183 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340101.HC.g.vcf.gz; 18:08:15.217 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340102.HC.g.vcf.gz; 18:08:15.250 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340103.HC.g.vcf.gz; 18:08:15.277 INFO CombineGVCFs - Shutting down engine; [May 18, 2019 6:08:15 PM CST] org.broadinstitute.hellbender.tools.walkers.CombineGVCFs done. Elapsed time: 0.03 minutes.; Runtime.totalMemory()=2372403200; java.lang.IllegalArgumentException: Feature inputs must be unique: /data/users/zhanglei/species/Medicago/result/SRR340103.HC.g.vcf.gz; 	at org.broadinstitute.hellbender.engine.MultiVariantWalker.lambda$initializeDrivingVariants$0(MultiVariantWalker.java:60); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.ReferencePipeline$Head.forEach(ReferencePipeline.java:580); 	at org.broadinstitute.hellbender.engine.MultiVariantWalker.initializeDrivingVariants(MultiVariantWalker.java:56); 	at org.broadinstitute.hellbender.engine.VariantWalkerBase.initializeFeatures(VariantWalkerBase.java:47); 	at org.broadinstitute.hellbender.engine.GATKTool.onStartup(GATKTool.java:558); 	at org.broadinstitute.hellbender.engine.MultiVariantWalker.onStartup(MultiVariantWalker.java:48); 	at org.broadi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947
https://github.com/broadinstitute/gatk/issues/5947:2044,Deployability,patch,patch,2044," 64-Bit Server VM v1.8.0_201-b09; 18:08:13.874 INFO CombineGVCFs - Start Date/Time: May 18, 2019 6:08:13 PM CST; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - HTSJDK Version: 2.14.3; 18:08:13.874 INFO CombineGVCFs - Picard Version: 2.17.2; 18:08:13.874 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:08:13.875 INFO CombineGVCFs - Deflater: IntelDeflater; 18:08:13.875 INFO CombineGVCFs - Inflater: IntelInflater; 18:08:13.875 INFO CombineGVCFs - GCS max retries/reopens: 20; 18:08:13.875 INFO CombineGVCFs - Using google-cloud-java patch 6d11bef1c81f885c26b2b56c8616b7a705171e4f from https://github.com/droazen/google-cloud-java/tree/dr_all_nio_fixes; 18:08:13.875 INFO CombineGVCFs - Initializing engine; 18:08:14.718 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340092.HC.g.vcf.gz; 18:08:14.807 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340093.HC.g.vcf.gz; 18:08:14.852 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340094.HC.g.vcf.gz; 18:08:14.897 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340095.HC.g.vcf.gz; 18:08:14.958 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/result/SRR340096.HC.g.vcf.gz; 18:08:15.003 INFO FeatureManager - Using codec VCFCodec to read file file:///data/users/zhanglei/species/Medicago/re",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947
https://github.com/broadinstitute/gatk/issues/5947:453,Performance,Load,Loading,453,"Hi,; This is a part of my script and I get the mistakes after I run it:; for sample in $samples ; do ; sample_gvcfs=${sample_gvcfs}"" --variant /data/users/zhanglei/species/Medicago/result/${sample}.HC.g.vcf.gz ""; done; time gatk CombineGVCFs \; -R /data/users/zhanglei/species/Medicago/Medicago.fa \; ${sample_gvcfs} \; -O $outdir/population/${outname}.HC.g.vcf.gz && echo ""** ${outname}.HC.g.vcf.gz done **"" &&. 18:08:13.704 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/home/software/gatk-4.0.3.0/gatk-package-4.0.3.0-local.jar!/com/intel/gkl/native/libgkl_compression.so; 18:08:13.872 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.873 INFO CombineGVCFs - The Genome Analysis Toolkit (GATK) v4.0.3.0; 18:08:13.873 INFO CombineGVCFs - For support and documentation go to https://software.broadinstitute.org/gatk/; 18:08:13.873 INFO CombineGVCFs - Executing as zhanglei@GenEngine on Linux v3.10.0-327.el7.x86_64 amd64; 18:08:13.873 INFO CombineGVCFs - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_201-b09; 18:08:13.874 INFO CombineGVCFs - Start Date/Time: May 18, 2019 6:08:13 PM CST; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - ------------------------------------------------------------; 18:08:13.874 INFO CombineGVCFs - HTSJDK Version: 2.14.3; 18:08:13.874 INFO CombineGVCFs - Picard Version: 2.17.2; 18:08:13.874 INFO CombineGVCFs - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : false; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 18:08:13.875 INFO CombineGVCFs - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 18:08:13.875 INFO CombineGVCFs - Deflater: IntelDeflater; 18:08:13.875 INFO CombineGVCFs - Inflater: IntelInflater; 18:08:13.875 INFO CombineGVCFs - GCS max retries/reopens: 20; 18:08:13.875 I",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5947
https://github.com/broadinstitute/gatk/issues/5948:88,Availability,Error,Error,88,"Hello, I use gatk-4.1.1.0. The `ModelSegments` command always throw `OutOfMemoryError`. Error message is long, I paste a few line of it.; ```bash; [May 20, 2019 4:43:37 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 357.17 minutes.; Runtime.totalMemory()=28631367680; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68); at java.lang.StringBuilder.<init>(StringBuilder.java:101); ```; I don't think this is caused by memory size. I set max memory to 500G, my `denoised_copy_ratios` input file size is `5.7M` and `AllelicCounts` inpute file size is `3.2G`. ; After some search, [this website](https://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#par_gc.oom) gives an explanation. ; > The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended period of time while making little or no progress because the heap is too small. If necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.; > ; This means some code bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948
https://github.com/broadinstitute/gatk/issues/5948:1054,Availability,recover,recovered,1054,"Hello, I use gatk-4.1.1.0. The `ModelSegments` command always throw `OutOfMemoryError`. Error message is long, I paste a few line of it.; ```bash; [May 20, 2019 4:43:37 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 357.17 minutes.; Runtime.totalMemory()=28631367680; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68); at java.lang.StringBuilder.<init>(StringBuilder.java:101); ```; I don't think this is caused by memory size. I set max memory to 500G, my `denoised_copy_ratios` input file size is `5.7M` and `AllelicCounts` inpute file size is `3.2G`. ; After some search, [this website](https://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#par_gc.oom) gives an explanation. ; > The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended period of time while making little or no progress because the heap is too small. If necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.; > ; This means some code bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948
https://github.com/broadinstitute/gatk/issues/5948:94,Integrability,message,message,94,"Hello, I use gatk-4.1.1.0. The `ModelSegments` command always throw `OutOfMemoryError`. Error message is long, I paste a few line of it.; ```bash; [May 20, 2019 4:43:37 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 357.17 minutes.; Runtime.totalMemory()=28631367680; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68); at java.lang.StringBuilder.<init>(StringBuilder.java:101); ```; I don't think this is caused by memory size. I set max memory to 500G, my `denoised_copy_ratios` input file size is `5.7M` and `AllelicCounts` inpute file size is `3.2G`. ; After some search, [this website](https://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#par_gc.oom) gives an explanation. ; > The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended period of time while making little or no progress because the heap is too small. If necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.; > ; This means some code bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948
https://github.com/broadinstitute/gatk/issues/5948:1170,Modifiability,extend,extended,1170,"Hello, I use gatk-4.1.1.0. The `ModelSegments` command always throw `OutOfMemoryError`. Error message is long, I paste a few line of it.; ```bash; [May 20, 2019 4:43:37 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 357.17 minutes.; Runtime.totalMemory()=28631367680; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68); at java.lang.StringBuilder.<init>(StringBuilder.java:101); ```; I don't think this is caused by memory size. I set max memory to 500G, my `denoised_copy_ratios` input file size is `5.7M` and `AllelicCounts` inpute file size is `3.2G`. ; After some search, [this website](https://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#par_gc.oom) gives an explanation. ; > The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended period of time while making little or no progress because the heap is too small. If necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.; > ; This means some code bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948
https://github.com/broadinstitute/gatk/issues/5948:1054,Safety,recover,recovered,1054,"Hello, I use gatk-4.1.1.0. The `ModelSegments` command always throw `OutOfMemoryError`. Error message is long, I paste a few line of it.; ```bash; [May 20, 2019 4:43:37 AM CST] org.broadinstitute.hellbender.tools.copynumber.ModelSegments done. Elapsed time: 357.17 minutes.; Runtime.totalMemory()=28631367680; Exception in thread ""main"" java.lang.OutOfMemoryError: GC overhead limit exceeded; at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:68); at java.lang.StringBuilder.<init>(StringBuilder.java:101); ```; I don't think this is caused by memory size. I set max memory to 500G, my `denoised_copy_ratios` input file size is `5.7M` and `AllelicCounts` inpute file size is `3.2G`. ; After some search, [this website](https://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html#par_gc.oom) gives an explanation. ; > The parallel collector will throw an OutOfMemoryError if too much time is being spent in garbage collection: if more than 98% of the total time is spent in garbage collection and less than 2% of the heap is recovered, an OutOfMemoryError will be thrown. This feature is designed to prevent applications from running for an extended period of time while making little or no progress because the heap is too small. If necessary, this feature can be disabled by adding the option -XX:-UseGCOverheadLimit to the command line.; > ; This means some code bug?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5948
https://github.com/broadinstitute/gatk/pull/5950:0,Safety,Avoid,Avoid,0,"Avoid case where the reference was being serialized and sent to the Spark executors, causing OOM in some cases.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5950
https://github.com/broadinstitute/gatk/pull/5952:300,Availability,down,down,300,@davidbenjamin Was familiarizing myself with `KBestHaplotypeFinder` and decided to take a crack at this issue. . I have no idea how much of a performance hit this will end up being at extreme sites. At worst it involves adding more paths into the priority queues than existed before which could slow down the whole search algorithm. . Fixes #5907,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5952
https://github.com/broadinstitute/gatk/pull/5952:142,Performance,perform,performance,142,@davidbenjamin Was familiarizing myself with `KBestHaplotypeFinder` and decided to take a crack at this issue. . I have no idea how much of a performance hit this will end up being at extreme sites. At worst it involves adding more paths into the priority queues than existed before which could slow down the whole search algorithm. . Fixes #5907,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5952
https://github.com/broadinstitute/gatk/pull/5952:256,Performance,queue,queues,256,@davidbenjamin Was familiarizing myself with `KBestHaplotypeFinder` and decided to take a crack at this issue. . I have no idea how much of a performance hit this will end up being at extreme sites. At worst it involves adding more paths into the priority queues than existed before which could slow down the whole search algorithm. . Fixes #5907,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5952
https://github.com/broadinstitute/gatk/issues/5954:479,Performance,perform,performance,479,"In the process of working to generify the `KBestHaplotypeFinder` it has come to my attention that the method `KbestHaplotypeFinder.removeCyclesAndVerticesThatDontLeadToSinks()` seems inefficient. Specifically it calls out to `findGuiltyVerticesAndEdgesToRemoveCycles()` which is a recursive method that attempts to search every child path through the graph and determine which ones either don't lead to sinks or loop back on themselves, adding rule breaking edges to a list. The performance cost seems like it would relate to the size of the graph which would have a bigger impact on the non-seq graph generated after #5922. Currently no information is preserved between recursive calls meaning that much work will be repeated (for instance a dynamic approach could save the work of revisiting already visited edges). This may be complicated somewhat by the pruning mechanism this method uses to generate parent vertexes. . This is low priority since this method will likely be entirely replaced by a new approach in #5923 later anyway.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5954
https://github.com/broadinstitute/gatk/issues/5959:311,Energy Efficiency,reduce,reduce,311,"Apologies for re-opening, this is becoming an increasing issue for those looking to run GATK via Docker or singularity in a multi-tenant environment. Currently:; Docker creation and images provided run with a default user root within the container. Dropping privileges within the instance to a gatk user, would reduce the risk of inadvertent data access or harm when run in a multi-user environment. A possible solution:; Add something like the following within the Dockerfile:; RUN useradd -ms /bin/bash dev; WORKDIR /home/dev; USER dev. Providing:; Making changes like the above would bring the GATK docker container into line with best practice and greatly assist sites which are also looking to apply minimum standards enforcable through 3rd party applications, i.e. Aqua etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5959
https://github.com/broadinstitute/gatk/issues/5959:322,Safety,risk,risk,322,"Apologies for re-opening, this is becoming an increasing issue for those looking to run GATK via Docker or singularity in a multi-tenant environment. Currently:; Docker creation and images provided run with a default user root within the container. Dropping privileges within the instance to a gatk user, would reduce the risk of inadvertent data access or harm when run in a multi-user environment. A possible solution:; Add something like the following within the Dockerfile:; RUN useradd -ms /bin/bash dev; WORKDIR /home/dev; USER dev. Providing:; Making changes like the above would bring the GATK docker container into line with best practice and greatly assist sites which are also looking to apply minimum standards enforcable through 3rd party applications, i.e. Aqua etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5959
https://github.com/broadinstitute/gatk/issues/5959:347,Security,access,access,347,"Apologies for re-opening, this is becoming an increasing issue for those looking to run GATK via Docker or singularity in a multi-tenant environment. Currently:; Docker creation and images provided run with a default user root within the container. Dropping privileges within the instance to a gatk user, would reduce the risk of inadvertent data access or harm when run in a multi-user environment. A possible solution:; Add something like the following within the Dockerfile:; RUN useradd -ms /bin/bash dev; WORKDIR /home/dev; USER dev. Providing:; Making changes like the above would bring the GATK docker container into line with best practice and greatly assist sites which are also looking to apply minimum standards enforcable through 3rd party applications, i.e. Aqua etc.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5959
https://github.com/broadinstitute/gatk/pull/5960:109,Testability,test,test,109,"@meganshand This fixes the Sarah's recent bug, and just makes sense in general. I am not adding a regression test because our work on linked de Bruijn graphs is going to moot stuff like this soon.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5960
https://github.com/broadinstitute/gatk/issues/5961:117,Modifiability,refactor,refactor,117,"## Investigation. ### Tool(s) or class(es) involved; Funcotator. ### Description; We should determine whether we can refactor the datasources for both segment and small mutations. Perhaps separate the two into distinct class hierarchies. This will make initialization more complicated, since we may have to initialize more than one FuncotationFactory per datasource directory. We should also eventually support mixed inputs (since a VCF can have both)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5961
https://github.com/broadinstitute/gatk/issues/5962:224,Availability,avail,available,224,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description; Currently, the location of config files that specify the formats for SEG file output are hardcoded in the FuncotatorEngine. These should be available to to override via parameters to the FuncotatorEngine during initialization.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5962
https://github.com/broadinstitute/gatk/issues/5962:111,Modifiability,config,config,111,"## Feature request. ### Tool(s) or class(es) involved; Funcotator. ### Description; Currently, the location of config files that specify the formats for SEG file output are hardcoded in the FuncotatorEngine. These should be available to to override via parameters to the FuncotatorEngine during initialization.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5962
https://github.com/broadinstitute/gatk/issues/5963:63,Deployability,configurat,configuration,63,"`FuncotateSegments` currently uses `org.apache.commons:commons-configuration` for its configuration file(s). It should ideally be migrated to use Owner like the rest of the GATK. One way this could be done: have the user specify columns and their aliases uses a List of specially-formatted Strings, such as:. ```; Col1(Alias1, Alias2),Col2(Alias1),Col3(....etc.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963
https://github.com/broadinstitute/gatk/issues/5963:86,Deployability,configurat,configuration,86,"`FuncotateSegments` currently uses `org.apache.commons:commons-configuration` for its configuration file(s). It should ideally be migrated to use Owner like the rest of the GATK. One way this could be done: have the user specify columns and their aliases uses a List of specially-formatted Strings, such as:. ```; Col1(Alias1, Alias2),Col2(Alias1),Col3(....etc.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963
https://github.com/broadinstitute/gatk/issues/5963:63,Modifiability,config,configuration,63,"`FuncotateSegments` currently uses `org.apache.commons:commons-configuration` for its configuration file(s). It should ideally be migrated to use Owner like the rest of the GATK. One way this could be done: have the user specify columns and their aliases uses a List of specially-formatted Strings, such as:. ```; Col1(Alias1, Alias2),Col2(Alias1),Col3(....etc.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963
https://github.com/broadinstitute/gatk/issues/5963:86,Modifiability,config,configuration,86,"`FuncotateSegments` currently uses `org.apache.commons:commons-configuration` for its configuration file(s). It should ideally be migrated to use Owner like the rest of the GATK. One way this could be done: have the user specify columns and their aliases uses a List of specially-formatted Strings, such as:. ```; Col1(Alias1, Alias2),Col2(Alias1),Col3(....etc.; ```",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5963
https://github.com/broadinstitute/gatk/issues/5965:95,Usability,clear,clearly,95,"Hello,. Mutect2 [version 4.0.11.0] doesn't seem to be filtering out strand artifacts when they clearly are. This is in some data from the NovaSeq and I am looking specifically at the dinucleotide changes `TT>GG or AA>CC`. . Most of them are PASS even when they are clearly in the same strand when looking at IGV (and also the `F1R2 and F2R1` tags in the VCF). Only 1 out of hundreds FAILS with the `strand_artifact` filter. PASS example; ```; 1 213663521 . TT GG . PASS DP=112;ECNT=1;NLOD=14.45;N_ART_LOD=-1.698e+00;POP_AF=3.125e-05;P_CONTAM=0.00;P_GERMLINE=-1.875e+01;TLOD=23.33 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:OBAM:OBAMRC:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB 0/1:49,10:0.177:59:35,0:14,10:30,30:331,317:60:35:false:false:0:0.172,0.00,0.169:1.573e-03,0.670,0.329 0/0:48,0:0.020:48:27,0:21,0:30,0:337,0:0:0:false:false:0; 2 186475118 . TT GG . PASS DP=101;ECNT=1;NLOD=11.44;N_ART_LOD=-1.596e+00;POP_AF=3.125e-05;P_CONTAM=0.00;P_GERMLINE=-1.949e+01;TLOD=8.12 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:OBAM:OBAMRC:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB 0/1:55,6:0.103:61:35,0:20,6:30,30:353,293:60:52:false:false:0:0.101,0.00,0.098:2.429e-03,0.382,0.615 0/0:38,0:0.013:38:22,0:16,0:30,0:341,0:0:0:false:false:0; ```. FAIL example; ```; 8 134390574 . TT GG . strand_artifact DP=172;ECNT=1;NLOD=11.14;N_ART_LOD=-1.299e+00;POP_AF=3.125e-05;P_CONTAM=0.00;P_GERMLINE=-2.386e+01;TLOD=18.05 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:OBAM:OBAMRC:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB 0/1:109,20:0.130:129:63,0:46,20:30,25:346,379:60:50:false:false:0:0.152,0.00,0.155:5.139e-06,0.998,2.050e-03 0/0:37,0:0.026:37:23,0:14,0:30,0:339,0:0:0:false:false:0; ```. I would appreciate any input!. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5965
https://github.com/broadinstitute/gatk/issues/5965:265,Usability,clear,clearly,265,"Hello,. Mutect2 [version 4.0.11.0] doesn't seem to be filtering out strand artifacts when they clearly are. This is in some data from the NovaSeq and I am looking specifically at the dinucleotide changes `TT>GG or AA>CC`. . Most of them are PASS even when they are clearly in the same strand when looking at IGV (and also the `F1R2 and F2R1` tags in the VCF). Only 1 out of hundreds FAILS with the `strand_artifact` filter. PASS example; ```; 1 213663521 . TT GG . PASS DP=112;ECNT=1;NLOD=14.45;N_ART_LOD=-1.698e+00;POP_AF=3.125e-05;P_CONTAM=0.00;P_GERMLINE=-1.875e+01;TLOD=23.33 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:OBAM:OBAMRC:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB 0/1:49,10:0.177:59:35,0:14,10:30,30:331,317:60:35:false:false:0:0.172,0.00,0.169:1.573e-03,0.670,0.329 0/0:48,0:0.020:48:27,0:21,0:30,0:337,0:0:0:false:false:0; 2 186475118 . TT GG . PASS DP=101;ECNT=1;NLOD=11.44;N_ART_LOD=-1.596e+00;POP_AF=3.125e-05;P_CONTAM=0.00;P_GERMLINE=-1.949e+01;TLOD=8.12 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:OBAM:OBAMRC:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB 0/1:55,6:0.103:61:35,0:20,6:30,30:353,293:60:52:false:false:0:0.101,0.00,0.098:2.429e-03,0.382,0.615 0/0:38,0:0.013:38:22,0:16,0:30,0:341,0:0:0:false:false:0; ```. FAIL example; ```; 8 134390574 . TT GG . strand_artifact DP=172;ECNT=1;NLOD=11.14;N_ART_LOD=-1.299e+00;POP_AF=3.125e-05;P_CONTAM=0.00;P_GERMLINE=-2.386e+01;TLOD=18.05 GT:AD:AF:DP:F1R2:F2R1:MBQ:MFRL:MMQ:MPOS:OBAM:OBAMRC:ORIGINAL_CONTIG_MISMATCH:SA_MAP_AF:SA_POST_PROB 0/1:109,20:0.130:129:63,0:46,20:30,25:346,379:60:50:false:false:0:0.152,0.00,0.155:5.139e-06,0.998,2.050e-03 0/0:37,0:0.026:37:23,0:14,0:30,0:339,0:0:0:false:false:0; ```. I would appreciate any input!. Thanks.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5965
https://github.com/broadinstitute/gatk/pull/5967:108,Availability,failure,failures,108,Adding `FuncotateSegments` as an option to the Somatic CNV WDL.; - Added automated WDL test to find obvious failures.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5967
https://github.com/broadinstitute/gatk/pull/5967:87,Testability,test,test,87,Adding `FuncotateSegments` as an option to the Somatic CNV WDL.; - Added automated WDL test to find obvious failures.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5967
https://github.com/broadinstitute/gatk/issues/5968:84,Availability,error,error,84,"I run the BaseRecalibrator,and at fisrt it can good running,after a time,I got this error：; htsjdk.samtools.SAMFormatException: Invalid GZIP header; This is the log:; Using GATK jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx20G -Djava.io.tmpdir=./; -jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta -I M1.bam --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz -O M1_recal.table17:55:54.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compre; ssion.soMay 24, 2019 5:55:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:55:56.095 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:56.096 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.2.0; 17:55:56.096 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:55:56.096 INFO BaseRecalibrator - Executing as wuly@localhost.localdomain on Linux v3.10.0-957.10.1.el7.x86_64 amd64; 17:55:56.096 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 17:55:56.096 INFO BaseRecalibrator - Start Date/Time: May 24, 2019 5:55:54 PM EDT; 17:55:56.096 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:7745,Availability,down,down,7745,"18:02:08.659 INFO ProgressMeter - chr1:189989846 6.2 11368000 1838773.1; 18:02:18.685 INFO ProgressMeter - chr1:196788442 6.3 11527000 1815428.1; 18:02:28.690 INFO ProgressMeter - chr1:201317675 6.5 11862000 1820376.8; 18:02:38.693 INFO ProgressMeter - chr1:204176575 6.7 12290000 1839012.8; 18:02:48.701 INFO ProgressMeter - chr1:207325661 6.8 12708000 1855250.2; 18:02:58.737 INFO ProgressMeter - chr1:211941783 7.0 13001000 1852781.7; 18:03:08.789 INFO ProgressMeter - chr1:217052843 7.2 13270000 1847019.0; 18:03:18.840 INFO ProgressMeter - chr1:222942848 7.4 13509000 1837446.7; 18:03:28.843 INFO ProgressMeter - chr1:227016956 7.5 13856000 1842855.4; 18:03:38.858 INFO ProgressMeter - chr1:230704130 7.7 14213000 1849294.6; 18:03:48.900 INFO ProgressMeter - chr1:235326795 7.9 14501000 1846539.8; 18:03:58.915 INFO ProgressMeter - chr1:239911899 8.0 14790000 1844143.5; 18:04:08.930 INFO ProgressMeter - chr1:246522306 8.2 15003000 1832561.8; 18:04:17.556 INFO BaseRecalibrator - Shutting down engine; [May 24, 2019 6:04:17 PM EDT] org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator done. Elapsed time: 8.39 minutes.; Runtime.totalMemory()=4407164928; htsjdk.samtools.SAMFormatException: Invalid GZIP header; 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:121); 	at htsjdk.samtools.util.BlockGunzipper.unzipBlock(BlockGunzipper.java:96); 	at htsjdk.samtools.util.BlockCompressedInputStream.inflateBlock(BlockCompressedInputStream.java:550); 	at htsjdk.samtools.util.BlockCompressedInputStream.processNextBlock(BlockCompressedInputStream.java:532); 	at htsjdk.samtools.util.BlockCompressedInputStream.nextBlock(BlockCompressedInputStream.java:468); 	at htsjdk.samtools.util.BlockCompressedInputStream.seek(BlockCompressedInputStream.java:380); 	at htsjdk.tribble.readers.TabixReader$IteratorImpl.next(TabixReader.java:427); 	at htsjdk.tribble.readers.TabixIteratorLineReader.readLine(TabixIteratorLineReader.java:46); 	at htsjdk.tribble.TabixFeatureReader$",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:12139,Availability,error,errors,12139,"peline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291). My GATK version is :GATK4.1.2.0; My command is:; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta \; -I M1.bam \; --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz \; --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz \; --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \; --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; -O M1_recal.table; Then I run the ValidateSamFile to check the BAM file,this is the command : ; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" ValidateSamFile -I M1.bam. And the result is: No errors found; I also tried to use the BAM file before I merge them to run BaseRecalibrator and ValidateSamFile, but I got the same result.Can anybody tell me how solve this problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:10327,Integrability,wrap,wrapAndCopyInto,10327,es(FeatureContext.java:172); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:124); 	at org.broadinstitute.hellbender.engine.FeatureContext.getValues(FeatureContext.java:262); 	at org.broadinstitute.hellbender.tools.walkers.bqsr.BaseRecalibrator.apply(BaseRecalibrator.java:185); 	at org.broadinstitute.hellbender.engine.ReadWalker.lambda$traverse$0(ReadWalker.java:96); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellb,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:957,Performance,Load,Loading,957,"I run the BaseRecalibrator,and at fisrt it can good running,after a time,I got this error：; htsjdk.samtools.SAMFormatException: Invalid GZIP header; This is the log:; Using GATK jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx20G -Djava.io.tmpdir=./; -jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta -I M1.bam --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz -O M1_recal.table17:55:54.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compre; ssion.soMay 24, 2019 5:55:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:55:56.095 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:56.096 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.2.0; 17:55:56.096 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:55:56.096 INFO BaseRecalibrator - Executing as wuly@localhost.localdomain on Linux v3.10.0-957.10.1.el7.x86_64 amd64; 17:55:56.096 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 17:55:56.096 INFO BaseRecalibrator - Start Date/Time: May 24, 2019 5:55:54 PM EDT; 17:55:56.096 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:1246,Safety,detect,detect,1246,"0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx20G -Djava.io.tmpdir=./; -jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta -I M1.bam --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz -O M1_recal.table17:55:54.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compre; ssion.soMay 24, 2019 5:55:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:55:56.095 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:56.096 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.2.0; 17:55:56.096 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:55:56.096 INFO BaseRecalibrator - Executing as wuly@localhost.localdomain on Linux v3.10.0-957.10.1.el7.x86_64 amd64; 17:55:56.096 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 17:55:56.096 INFO BaseRecalibrator - Start Date/Time: May 24, 2019 5:55:54 PM EDT; 17:55:56.096 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:56.096 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:56.096 INFO BaseRecalibrator - HTSJDK Version: 2.19.0; 17:55:56.096 INFO BaseRecalibrator - Picard Version: 2.19.0; 17:55:56.096 INFO BaseRec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:11938,Security,Validat,ValidateSamFile,11938,"peline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291). My GATK version is :GATK4.1.2.0; My command is:; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta \; -I M1.bam \; --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz \; --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz \; --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \; --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; -O M1_recal.table; Then I run the ValidateSamFile to check the BAM file,this is the command : ; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" ValidateSamFile -I M1.bam. And the result is: No errors found; I also tried to use the BAM file before I merge them to run BaseRecalibrator and ValidateSamFile, but I got the same result.Can anybody tell me how solve this problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:12090,Security,Validat,ValidateSamFile,12090,"peline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291). My GATK version is :GATK4.1.2.0; My command is:; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta \; -I M1.bam \; --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz \; --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz \; --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \; --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; -O M1_recal.table; Then I run the ValidateSamFile to check the BAM file,this is the command : ; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" ValidateSamFile -I M1.bam. And the result is: No errors found; I also tried to use the BAM file before I merge them to run BaseRecalibrator and ValidateSamFile, but I got the same result.Can anybody tell me how solve this problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:12234,Security,Validat,ValidateSamFile,12234,"peline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:94); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291). My GATK version is :GATK4.1.2.0; My command is:; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta \; -I M1.bam \; --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz \; --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz \; --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz \; --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz \; -O M1_recal.table; Then I run the ValidateSamFile to check the BAM file,this is the command : ; /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk --java-options ""-Xmx20G -Djava.io.tmpdir=./"" ValidateSamFile -I M1.bam. And the result is: No errors found; I also tried to use the BAM file before I merge them to run BaseRecalibrator and ValidateSamFile, but I got the same result.Can anybody tell me how solve this problem?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/issues/5968:161,Testability,log,log,161,"I run the BaseRecalibrator,and at fisrt it can good running,after a time,I got this error：; htsjdk.samtools.SAMFormatException: Invalid GZIP header; This is the log:; Using GATK jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx20G -Djava.io.tmpdir=./; -jar /data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar BaseRecalibrator -R /data/home/wuly/source/Homo_sapiens_assembly38.fasta -I M1.bam --known-sites /data/home/wuly/source/dbsnp_146.hg38.vcf.gz --known-sites /data/home/wuly/source/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz --known-sites /data/home/wuly/source/1000G_phase1.snps.high_confidence.hg38.vcf.gz --known-sites /data/home/wuly/source/hapmap_3.3.hg38.vcf.gz -O M1_recal.table17:55:54.326 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/data/home/wuly/soft/GATK4/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_compre; ssion.soMay 24, 2019 5:55:56 PM shaded.cloud_nio.com.google.auth.oauth2.ComputeEngineCredentials runningOnComputeEngine; INFO: Failed to detect whether we are running on Google Compute Engine.; 17:55:56.095 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:56.096 INFO BaseRecalibrator - The Genome Analysis Toolkit (GATK) v4.1.2.0; 17:55:56.096 INFO BaseRecalibrator - For support and documentation go to https://software.broadinstitute.org/gatk/; 17:55:56.096 INFO BaseRecalibrator - Executing as wuly@localhost.localdomain on Linux v3.10.0-957.10.1.el7.x86_64 amd64; 17:55:56.096 INFO BaseRecalibrator - Java runtime: OpenJDK 64-Bit Server VM v1.8.0_192-b01; 17:55:56.096 INFO BaseRecalibrator - Start Date/Time: May 24, 2019 5:55:54 PM EDT; 17:55:56.096 INFO BaseRecalibrator - ------------------------------------------------------------; 17:55:",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5968
https://github.com/broadinstitute/gatk/pull/5970:465,Availability,failure,failure,465,"Support for incrementally adding samples to existing genomicsdb workspaces. I've added these comments to the docs, but just wanted to call out again that we recommend making a backup of the existing workspace before trying to update the workspace. Otherwise, if the incremental update fails the workspace may be in a corrupted/inconsistent state. . If the user chooses not to backup (or can't), there is a (somewhat painful, manual) way to restore the workspace on failure IFF the --consolidate option has not been used. The tool will output a backup callset file (suffixed .inc.backup) and a file suffixed .fragmentlist with a list of all the original fragments. In order to roll back to a consistent workspace, the user must; - replace the callset file in the workspace with the one suffixed .inc.backup. That is, something like:; > mv _workspace_/callset.json.inc.backup _workspace_/callset.json; - delete all the directories in the workspace not named genomicsdb_meta_dir or included the file suffixed .fragmentlist",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970
https://github.com/broadinstitute/gatk/pull/5970:226,Deployability,update,update,226,"Support for incrementally adding samples to existing genomicsdb workspaces. I've added these comments to the docs, but just wanted to call out again that we recommend making a backup of the existing workspace before trying to update the workspace. Otherwise, if the incremental update fails the workspace may be in a corrupted/inconsistent state. . If the user chooses not to backup (or can't), there is a (somewhat painful, manual) way to restore the workspace on failure IFF the --consolidate option has not been used. The tool will output a backup callset file (suffixed .inc.backup) and a file suffixed .fragmentlist with a list of all the original fragments. In order to roll back to a consistent workspace, the user must; - replace the callset file in the workspace with the one suffixed .inc.backup. That is, something like:; > mv _workspace_/callset.json.inc.backup _workspace_/callset.json; - delete all the directories in the workspace not named genomicsdb_meta_dir or included the file suffixed .fragmentlist",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970
https://github.com/broadinstitute/gatk/pull/5970:278,Deployability,update,update,278,"Support for incrementally adding samples to existing genomicsdb workspaces. I've added these comments to the docs, but just wanted to call out again that we recommend making a backup of the existing workspace before trying to update the workspace. Otherwise, if the incremental update fails the workspace may be in a corrupted/inconsistent state. . If the user chooses not to backup (or can't), there is a (somewhat painful, manual) way to restore the workspace on failure IFF the --consolidate option has not been used. The tool will output a backup callset file (suffixed .inc.backup) and a file suffixed .fragmentlist with a list of all the original fragments. In order to roll back to a consistent workspace, the user must; - replace the callset file in the workspace with the one suffixed .inc.backup. That is, something like:; > mv _workspace_/callset.json.inc.backup _workspace_/callset.json; - delete all the directories in the workspace not named genomicsdb_meta_dir or included the file suffixed .fragmentlist",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5970
https://github.com/broadinstitute/gatk/issues/5973:86,Usability,clear,clear,86,In the process of getting MarkDuplicatesSpark working for the 1000x bam it has become clear that multi-input bams are necessary. Unfortunately we have set a hard restriction that all multi-input bams must be query-group sorted to be valid. This creates issues if there are reasons for the input files to not be sorted consistently we want to be able to handle the problem more gracefully. I propose we add an opt-in argument that disables the check and allows mixed file bams.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5973
https://github.com/broadinstitute/gatk/pull/5974:92,Performance,perform,performance,92,An advanced argument to skip multiple inputs. We still want to discourage this behavior for performance reasons so its not the default behavior. Fixes #5973,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5974
https://github.com/broadinstitute/gatk/issues/5975:5950,Availability,down,download,5950,"dPosRankSum=0.433	GT:AD:DP:GQ:PL:SB	0/2:414,2,357,0:773:99:14672,11361,50781,0,41338,45124,13972,52387,44158,56529:206,208,177,182; chr13	32944608	.	T	A,*,<NON_REF>	0	.	BaseQRankSum=5.453;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6143,Availability,down,download,6143,"7;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6358,Availability,down,download,6358,"210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6551,Availability,down,download,6551,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6766,Availability,down,download,6766,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6959,Availability,down,download,6959,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:142,Deployability,release,release,142,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [ ] 4.0.8.1; - [x] 4.0.9.0; - [x] Latest public release version [4.1.2.0]. ### Description ; This maybe a series of mistakes.; I guess it will happen when a new variant should be detected within a spanning deletion.; Below is an example:. in 4.0.8.1, a NMP ( CTTT>CAAAA ) was detected as two variants( CTTT>C + T>TAAAA ).; *vcf of 4.0.8.1*; ```vcf of 4.0.8.1; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944606	.	CTTT	C	9210.73	.	AC=1;AF=0.500;AN=2;BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;FS=0.518;MLEAC=1;MLEAF=0.500;MQ=60.03;MQRankSum=0.000;QD=11.81;ReadPosRankSum=0.295;SOR=0.728	GT:AD:DP:GQ:PL	0/1:423,357:780:99:9248,0,45245; chr13	32944609	.	T	TAAAA	14802.73	.	AC=1;AF=0.500;AN=2;BaseQRankSum=4.179;DP=787;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=18.93;ReadPosRankSum=0.241;SOR=0.689	GT:AD:DP:GQ:PL	0/1:411,371:782:99:14840,0,45112; ```; *gvcf of 4.0.8.1*; ```gvcf of 4.0.8.1; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944440	.	T	<NON_REF>	.	.	END=32944605	GT:DP:GQ:MIN_DP:PL	0/0:592:99:352:0,120,1800; chr13	32944606	.	CTTT	C,<NON_REF>	9210.73	.	BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQ=2929400.00;ReadPosRankSum=0.295	GT:AD:DP:GQ:PL:SB	0/1:423,357,0:780:99:9248,0,45245,10522,46330,56852:212,211,175,182; chr13	32944609	.	T	A,TAAAA,<NON_REF>	14802.73	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQ=2833200.00;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0:770:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. but from version 4.0.9.0 which `support for genotyping spanning deletions and a fix to the reference confidence calculation around indels`,; the second variants got filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:3907,Deployability,release,release,3907,".77	.	BaseQRankSum=4.433;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQ=2871800.00;ReadPosRankSum=0.433	GT:AD:DP:GQ:PL:SB	0/2:414,2,357,0:773:99:14672,11361,50781,0,41338,45124,13972,52387,44158,56529:206,208,177,182; chr13	32944608	.	T	A,*,<NON_REF>	14811.77	.	BaseQRankSum=5.453;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQ=2869200.00;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	14802.73	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQ=2833200.00;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. And in the latest public release version [4.1.2.0], variant A>TAAAA was filtered as the QUAL is zero I guessed.; *vcf of 4.1.2.0*; ```vcf of 4.1.2.0; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944606	.	CTTT	C	9240.60	.	AC=1;AF=0.500;AN=2;BaseQRankSum=1.37;DP=813;ExcessHet=3.0103;FS=0.518;MLEAC=1;MLEAF=0.500;MQ=60.03;MQRankSum=0.00;QD=11.85;ReadPosRankSum=0.295;SOR=0.728	GT:AD:DP:GQ:PL	0/1:423,357:780:99:9248,0,45245; ```; *gvcf of 4.1.2.0*; ```gvcf of 4.1.2.0; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944440	.	T	<NON_REF>	.	.	END=32944605	GT:DP:GQ:MIN_DP:PL	0/0:592:99:352:0,120,1800; chr13	32944606	.	CTTT	C,<NON_REF>	9240.60	.	BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2929400,813;ReadPosRankSum=0.295	GT:AD:DP:GQ:PL:SB	0/1:423,357,0:780:99:9248,0,45245,10522,46330,56852:212,211,175,182; chr13	32944607	.	T	A,*,<NON_REF>	0	.	BaseQRankSum=4.433;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSu",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:5839,Deployability,pipeline,pipeline,5839,".433;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2871800,797;ReadPosRankSum=0.433	GT:AD:DP:GQ:PL:SB	0/2:414,2,357,0:773:99:14672,11361,50781,0,41338,45124,13972,52387,44158,56529:206,208,177,182; chr13	32944608	.	T	A,*,<NON_REF>	0	.	BaseQRankSum=5.453;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I targe",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:5941,Deployability,release,releases,5941,"dPosRankSum=0.433	GT:AD:DP:GQ:PL:SB	0/2:414,2,357,0:773:99:14672,11361,50781,0,41338,45124,13972,52387,44158,56529:206,208,177,182; chr13	32944608	.	T	A,*,<NON_REF>	0	.	BaseQRankSum=5.453;DP=797;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinst",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6134,Deployability,release,releases,6134,"7;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQandDP=2869200,797;ReadPosRankSum=0.386	GT:AD:DP:GQ:PL:SB	0/2:413,2,357,0:772:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:203,210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6349,Deployability,release,releases,6349,"210,177,182; chr13	32944609	.	T	A,*,TAAAA,<NON_REF>	0	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0,0;MLEAF=0.00,0.500,0.00,0.00;MQRankSum=0.000;RAW_MQandDP=2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or w",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6542,Deployability,release,releases,6542,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6757,Deployability,release,releases,6757,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:6950,Deployability,release,releases,6950,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:273,Safety,detect,detected,273,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [ ] 4.0.8.1; - [x] 4.0.9.0; - [x] Latest public release version [4.1.2.0]. ### Description ; This maybe a series of mistakes.; I guess it will happen when a new variant should be detected within a spanning deletion.; Below is an example:. in 4.0.8.1, a NMP ( CTTT>CAAAA ) was detected as two variants( CTTT>C + T>TAAAA ).; *vcf of 4.0.8.1*; ```vcf of 4.0.8.1; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944606	.	CTTT	C	9210.73	.	AC=1;AF=0.500;AN=2;BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;FS=0.518;MLEAC=1;MLEAF=0.500;MQ=60.03;MQRankSum=0.000;QD=11.81;ReadPosRankSum=0.295;SOR=0.728	GT:AD:DP:GQ:PL	0/1:423,357:780:99:9248,0,45245; chr13	32944609	.	T	TAAAA	14802.73	.	AC=1;AF=0.500;AN=2;BaseQRankSum=4.179;DP=787;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=18.93;ReadPosRankSum=0.241;SOR=0.689	GT:AD:DP:GQ:PL	0/1:411,371:782:99:14840,0,45112; ```; *gvcf of 4.0.8.1*; ```gvcf of 4.0.8.1; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944440	.	T	<NON_REF>	.	.	END=32944605	GT:DP:GQ:MIN_DP:PL	0/0:592:99:352:0,120,1800; chr13	32944606	.	CTTT	C,<NON_REF>	9210.73	.	BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQ=2929400.00;ReadPosRankSum=0.295	GT:AD:DP:GQ:PL:SB	0/1:423,357,0:780:99:9248,0,45245,10522,46330,56852:212,211,175,182; chr13	32944609	.	T	A,TAAAA,<NON_REF>	14802.73	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQ=2833200.00;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0:770:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. but from version 4.0.9.0 which `support for genotyping spanning deletions and a fix to the reference confidence calculation around indels`,; the second variants got filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:370,Safety,detect,detected,370,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller. ### Affected version(s); - [ ] 4.0.8.1; - [x] 4.0.9.0; - [x] Latest public release version [4.1.2.0]. ### Description ; This maybe a series of mistakes.; I guess it will happen when a new variant should be detected within a spanning deletion.; Below is an example:. in 4.0.8.1, a NMP ( CTTT>CAAAA ) was detected as two variants( CTTT>C + T>TAAAA ).; *vcf of 4.0.8.1*; ```vcf of 4.0.8.1; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944606	.	CTTT	C	9210.73	.	AC=1;AF=0.500;AN=2;BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;FS=0.518;MLEAC=1;MLEAF=0.500;MQ=60.03;MQRankSum=0.000;QD=11.81;ReadPosRankSum=0.295;SOR=0.728	GT:AD:DP:GQ:PL	0/1:423,357:780:99:9248,0,45245; chr13	32944609	.	T	TAAAA	14802.73	.	AC=1;AF=0.500;AN=2;BaseQRankSum=4.179;DP=787;ExcessHet=3.0103;FS=0.000;MLEAC=1;MLEAF=0.500;MQ=60.00;MQRankSum=0.000;QD=18.93;ReadPosRankSum=0.241;SOR=0.689	GT:AD:DP:GQ:PL	0/1:411,371:782:99:14840,0,45112; ```; *gvcf of 4.0.8.1*; ```gvcf of 4.0.8.1; #CHROM	POS	ID	REF	ALT	QUAL	FILTER	INFO	FORMAT	19B0117493; chr13	32944440	.	T	<NON_REF>	.	.	END=32944605	GT:DP:GQ:MIN_DP:PL	0/0:592:99:352:0,120,1800; chr13	32944606	.	CTTT	C,<NON_REF>	9210.73	.	BaseQRankSum=1.374;DP=813;ExcessHet=3.0103;MLEAC=1,0;MLEAF=0.500,0.00;MQRankSum=0.000;RAW_MQ=2929400.00;ReadPosRankSum=0.295	GT:AD:DP:GQ:PL:SB	0/1:423,357,0:780:99:9248,0,45245,10522,46330,56852:212,211,175,182; chr13	32944609	.	T	A,TAAAA,<NON_REF>	14802.73	.	BaseQRankSum=4.278;DP=787;ExcessHet=3.0103;MLEAC=0,1,0;MLEAF=0.00,0.500,0.00;MQRankSum=0.000;RAW_MQ=2833200.00;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0:770:99:14840,11462,50871,0,41338,45112,14111,52486,44158,56658:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. but from version 4.0.9.0 which `support for genotyping spanning deletions and a fix to the reference confidence calculation around indels`,; the second variants got filt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:7406,Safety,detect,detect,7406,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/issues/5975:7132,Security,expose,expose,7132,"2833200,787;ReadPosRankSum=0.252	GT:AD:DP:GQ:PL:SB	0/2:411,2,357,0,0:770:99:14840,11462,50871,0,41338,45112,17297,53328,47568,2147483647,16108,52933,46273,64838,62381:201,210,177,182; chr13	32944610	.	T	<NON_REF>	.	.	END=32944794	GT:DP:GQ:MIN_DP:PL	0/0:627:99:265:0,120,1800; ```. #### Steps to reproduce; * init; ```; hg19=pipeline/hg19/hg19_chM_male_mask.fa; ```; * reproduce of 4.0.8.1; ```; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.gvcf -ERC GVCF && tail target.4.0.8.1.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.8.1/gatk-4.0.8.1/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.8.1.vcf && tail target.4.0.8.1.vcf; ```; * reproduce of 4.0.9.0; ```; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.gvcf -ERC GVCF && tail target.4.0.9.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.0.9.0/gatk-4.0.9.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.0.9.0.vcf && tail target.4.0.9.0.vcf; ```; * reproduce of 4.1.2.0; ```; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.gvcf -ERC GVCF && tail target.4.1.2.0.gvcf; github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0/gatk HaplotypeCaller -R $hg19 -I target.1k.bam -L target.bed -O target.4.1.2.0.vcf && tail target.4.1.2.0.vcf; ```. #### Expected behavior; 1. expose the rule that the second variant (T>TAAAA) filtered (especially for version 4.0.9.0).; 2. give the right QUAL of the second variant; 3. then this type of variant can be retain in VCF as default operation or with some addition parameters.; 4. can GATK have ability to detect the `real` variant such as TTT>AAAA. #### Actual behavior; ~~_Tell us what happens instead_~~; unknown",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5975
https://github.com/broadinstitute/gatk/pull/5976:1015,Availability,down,downstream,1015,"Also changed LegacySegmentCollection and CalledLegacySegmentCollection writers to overwrite instead of append. (This was fixed for AbstractRecordCollection writers in an earlier PR but was missed in these collections that override the default writer.). See https://gatkforums.broadinstitute.org/gatk/discussion/24048/a-few-things-need-help-about-output-of-command-modelsegments#latest for some context. In principle, IGV should be able to handle linear scaling, but it scales automatically for seg files and I'm not sure if you can get around it without some additional steps. Seems easier to just output log2 copy ratios. (Perhaps we initially output linear copy ratios to maintain backwards compatibility with previous versions of GATK CNV? If so, the *LegacySegmentCollections seem to be pulling double duty, since we use them to provide IGV compatibility. In any case, we are currently inconsistent between ModelSegments and CallCopyRatioSegments, as I noted in the forum post.) . @LeeTL1220 any objections? If downstream scripts are consuming the IGV output of ModelSegments, they should be adjusted if necessary.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5976
https://github.com/broadinstitute/gatk/issues/5978:5,Availability,error,error,5,"This error was reported by a user: . Dear GATK Team, . I am writing you to discuss a an error while using FilterMutectCalls program and it seems a potential bug. ; I wanted to change the default value of **--normal-p-value-threshold **. It looks like the tool doesn't accept this parameter at all. I tried using it with the default value of 0.001 as well. . The program works fine when this parameter is removed. Moreover, the error message also states that **BUG: couldn't set field value** . please see the attached command and the error message. . ** gatk FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz**. Using GATK jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:88,Availability,error,error,88,"This error was reported by a user: . Dear GATK Team, . I am writing you to discuss a an error while using FilterMutectCalls program and it seems a potential bug. ; I wanted to change the default value of **--normal-p-value-threshold **. It looks like the tool doesn't accept this parameter at all. I tried using it with the default value of 0.001 as well. . The program works fine when this parameter is removed. Moreover, the error message also states that **BUG: couldn't set field value** . please see the attached command and the error message. . ** gatk FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz**. Using GATK jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:427,Availability,error,error,427,"This error was reported by a user: . Dear GATK Team, . I am writing you to discuss a an error while using FilterMutectCalls program and it seems a potential bug. ; I wanted to change the default value of **--normal-p-value-threshold **. It looks like the tool doesn't accept this parameter at all. I tried using it with the default value of 0.001 as well. . The program works fine when this parameter is removed. Moreover, the error message also states that **BUG: couldn't set field value** . please see the attached command and the error message. . ** gatk FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz**. Using GATK jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:534,Availability,error,error,534,"This error was reported by a user: . Dear GATK Team, . I am writing you to discuss a an error while using FilterMutectCalls program and it seems a potential bug. ; I wanted to change the default value of **--normal-p-value-threshold **. It looks like the tool doesn't accept this parameter at all. I tried using it with the default value of 0.001 as well. . The program works fine when this parameter is removed. Moreover, the error message also states that **BUG: couldn't set field value** . please see the attached command and the error message. . ** gatk FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz**. Using GATK jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:433,Integrability,message,message,433,"This error was reported by a user: . Dear GATK Team, . I am writing you to discuss a an error while using FilterMutectCalls program and it seems a potential bug. ; I wanted to change the default value of **--normal-p-value-threshold **. It looks like the tool doesn't accept this parameter at all. I tried using it with the default value of 0.001 as well. . The program works fine when this parameter is removed. Moreover, the error message also states that **BUG: couldn't set field value** . please see the attached command and the error message. . ** gatk FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz**. Using GATK jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:540,Integrability,message,message,540,"This error was reported by a user: . Dear GATK Team, . I am writing you to discuss a an error while using FilterMutectCalls program and it seems a potential bug. ; I wanted to change the default value of **--normal-p-value-threshold **. It looks like the tool doesn't accept this parameter at all. I tried using it with the default value of 0.001 as well. . The program works fine when this parameter is removed. Moreover, the error message also states that **BUG: couldn't set field value** . please see the attached command and the error message. . ** gatk FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz**. Using GATK jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /mnt/gpfs1/lmod/apps/gatk/4.1.1.0/gatk-package-4.1.1.0-local.jar FilterMutectCalls -V TAR-158_unfiltered.vcf.gz --normal-p-value-threshold 0.0001 -R ../data/hg_ref/genome.fa --contamination-table TAR-158_tumor_calculatecontamination.table -O TAR-158_artifact_0.01.vcf.gz; org.broadinstitute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at o",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:2640,Safety,Unsafe,UnsafeFieldAccessorImpl,2640,titute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:232); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:206); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.lang.IllegalAccessException: Can not set static final double field org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection.normalPileupPValueThreshold to java.lang.Double; 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); 	at sun.reflect.UnsafeQualifiedStaticDoubleFieldAccessorImpl.set(UnsafeQualifiedStaticDoubleFieldAccessorImpl.java:77); 	at java.lang.reflect.Field.set(Field.java:764); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1245); 	... 7 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24071/filtermutectcalls-bug/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:2702,Safety,Unsafe,UnsafeFieldAccessorImpl,2702,titute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:232); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:206); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.lang.IllegalAccessException: Can not set static final double field org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection.normalPileupPValueThreshold to java.lang.Double; 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); 	at sun.reflect.UnsafeQualifiedStaticDoubleFieldAccessorImpl.set(UnsafeQualifiedStaticDoubleFieldAccessorImpl.java:77); 	at java.lang.reflect.Field.set(Field.java:764); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1245); 	... 7 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24071/filtermutectcalls-bug/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:2752,Safety,Unsafe,UnsafeFieldAccessorImpl,2752,titute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:232); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:206); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.lang.IllegalAccessException: Can not set static final double field org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection.normalPileupPValueThreshold to java.lang.Double; 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); 	at sun.reflect.UnsafeQualifiedStaticDoubleFieldAccessorImpl.set(UnsafeQualifiedStaticDoubleFieldAccessorImpl.java:77); 	at java.lang.reflect.Field.set(Field.java:764); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1245); 	... 7 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24071/filtermutectcalls-bug/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:2814,Safety,Unsafe,UnsafeFieldAccessorImpl,2814,titute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:232); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:206); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.lang.IllegalAccessException: Can not set static final double field org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection.normalPileupPValueThreshold to java.lang.Double; 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); 	at sun.reflect.UnsafeQualifiedStaticDoubleFieldAccessorImpl.set(UnsafeQualifiedStaticDoubleFieldAccessorImpl.java:77); 	at java.lang.reflect.Field.set(Field.java:764); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1245); 	... 7 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24071/filtermutectcalls-bug/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:2864,Safety,Unsafe,UnsafeQualifiedStaticDoubleFieldAccessorImpl,2864,titute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:232); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:206); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.lang.IllegalAccessException: Can not set static final double field org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection.normalPileupPValueThreshold to java.lang.Double; 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); 	at sun.reflect.UnsafeQualifiedStaticDoubleFieldAccessorImpl.set(UnsafeQualifiedStaticDoubleFieldAccessorImpl.java:77); 	at java.lang.reflect.Field.set(Field.java:764); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1245); 	... 7 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24071/filtermutectcalls-bug/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/issues/5978:2913,Safety,Unsafe,UnsafeQualifiedStaticDoubleFieldAccessorImpl,2913,titute.barclay.argparser.CommandLineException$ShouldNeverReachHereException: **BUG: couldn't set field value. For normalPileupPValueThreshold in org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection@69d45cca with value 1.0E-4 This shouldn't happen since we setAccessible(true)**; 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1248); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.setArgument(CommandLineArgumentParser.java:710); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser.parseArguments(CommandLineArgumentParser.java:427); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.parseArgs(CommandLineProgram.java:232); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:206); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Caused by: java.lang.IllegalAccessException: Can not set static final double field org.broadinstitute.hellbender.tools.walkers.mutect.filtering.M2FiltersArgumentCollection.normalPileupPValueThreshold to java.lang.Double; 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:76); 	at sun.reflect.UnsafeFieldAccessorImpl.throwFinalFieldIllegalAccessException(UnsafeFieldAccessorImpl.java:80); 	at sun.reflect.UnsafeQualifiedStaticDoubleFieldAccessorImpl.set(UnsafeQualifiedStaticDoubleFieldAccessorImpl.java:77); 	at java.lang.reflect.Field.set(Field.java:764); 	at org.broadinstitute.barclay.argparser.CommandLineArgumentParser$ArgumentDefinition.setFieldValue(CommandLineArgumentParser.java:1245); 	... 7 more. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24071/filtermutectcalls-bug/p1,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5978
https://github.com/broadinstitute/gatk/pull/5979:76,Performance,load,load,76,The problem is that Spark executors can't rely on the system classloader to load resources. This change falls back to the current classloader if the resource can't be loaded from the system classloader. I've tested it successfully on a cluster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979
https://github.com/broadinstitute/gatk/pull/5979:167,Performance,load,loaded,167,The problem is that Spark executors can't rely on the system classloader to load resources. This change falls back to the current classloader if the resource can't be loaded from the system classloader. I've tested it successfully on a cluster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979
https://github.com/broadinstitute/gatk/pull/5979:208,Testability,test,tested,208,The problem is that Spark executors can't rely on the system classloader to load resources. This change falls back to the current classloader if the resource can't be loaded from the system classloader. I've tested it successfully on a cluster.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5979
https://github.com/broadinstitute/gatk/pull/5980:150,Availability,checkpoint,checkpoint,150,"And split out a common superclass for ReadThreadingGraph and ExperimentalReadThreadingGraph. Perhaps these should have better names? This is merely a checkpoint of progress intended to aid the review process for this work. . I will probably shortly add to this branch the first attempt at dangling tail merging, it was causing me some problems... Fixes #5923",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5980
https://github.com/broadinstitute/gatk/pull/5981:108,Availability,error,errors,108,Bug fix release that fixes #5919 and https://github.com/disq-bio/disq/pull/101 (the latter caused BAM count errors).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5981
https://github.com/broadinstitute/gatk/pull/5981:8,Deployability,release,release,8,Bug fix release that fixes #5919 and https://github.com/disq-bio/disq/pull/101 (the latter caused BAM count errors).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5981
https://github.com/broadinstitute/gatk/pull/5982:18,Availability,error,error,18,@takutosato Silly error on my part. Made the argument parser crash whenever the argument was set to a non-default value. @Closes #5978.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5982
https://github.com/broadinstitute/gatk/pull/5984:124,Integrability,message,messages,124,"In validateVariants tool, made the default case behave so that it does the validations that can be done, and issues warning messages for the validations that cannot be done (ie, required external files)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5984
https://github.com/broadinstitute/gatk/pull/5984:3,Security,validat,validateVariants,3,"In validateVariants tool, made the default case behave so that it does the validations that can be done, and issues warning messages for the validations that cannot be done (ie, required external files)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5984
https://github.com/broadinstitute/gatk/pull/5984:75,Security,validat,validations,75,"In validateVariants tool, made the default case behave so that it does the validations that can be done, and issues warning messages for the validations that cannot be done (ie, required external files)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5984
https://github.com/broadinstitute/gatk/pull/5984:141,Security,validat,validations,141,"In validateVariants tool, made the default case behave so that it does the validations that can be done, and issues warning messages for the validations that cannot be done (ie, required external files)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5984
https://github.com/broadinstitute/gatk/pull/5987:91,Modifiability,variab,variables,91,* SparkContextFactory no longer always emits a warning about the GCS connector environment variables.; This should now only occur when running tests and missing the necessary environment variables. I should have fixed this one a long time ago...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5987
https://github.com/broadinstitute/gatk/pull/5987:187,Modifiability,variab,variables,187,* SparkContextFactory no longer always emits a warning about the GCS connector environment variables.; This should now only occur when running tests and missing the necessary environment variables. I should have fixed this one a long time ago...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5987
https://github.com/broadinstitute/gatk/pull/5987:143,Testability,test,tests,143,* SparkContextFactory no longer always emits a warning about the GCS connector environment variables.; This should now only occur when running tests and missing the necessary environment variables. I should have fixed this one a long time ago...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5987
https://github.com/broadinstitute/gatk/pull/5991:19,Testability,test,tests,19,"Now that Disq runs tests on 'large' BAM and VCF files (https://github.com/disq-bio/disq/pull/103), we can remove the Beta label from some more of the Spark tools.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5991
https://github.com/broadinstitute/gatk/issues/5992:174,Availability,error,error,174,"Recently I was setting up GATK to run in a VM and I had forgotten to install Java8 onto the machine. When I tried to run GATK from the launch script I ran into the following error: ; ```; Using GATK jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar -help; Traceback (most recent call last):; File ""./gatk"", line 479, in <module>; main(sys.argv[1:]); File ""./gatk"", line 152, in main; runGATK(sparkRunner, sparkSubmitCommand, dryRun, gatkArgs, sparkArgs, javaOptions); File ""./gatk"", line 328, in runGATK; runCommand(cmd, dryrun); File ""./gatk"", line 384, in runCommand; check_call(cmd, env=gatk_env); File ""/usr/lib/python2.7/subprocess.py"", line 181, in check_call; retcode = call(*popenargs, **kwargs); File ""/usr/lib/python2.7/subprocess.py"", line 168, in call; return Popen(*popenargs, **kwargs).wait(); File ""/usr/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ```; This should perhaps be made a little bit clearer for users as this isn't particularly helpful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5992
https://github.com/broadinstitute/gatk/issues/5992:69,Deployability,install,install,69,"Recently I was setting up GATK to run in a VM and I had forgotten to install Java8 onto the machine. When I tried to run GATK from the launch script I ran into the following error: ; ```; Using GATK jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar -help; Traceback (most recent call last):; File ""./gatk"", line 479, in <module>; main(sys.argv[1:]); File ""./gatk"", line 152, in main; runGATK(sparkRunner, sparkSubmitCommand, dryRun, gatkArgs, sparkArgs, javaOptions); File ""./gatk"", line 328, in runGATK; runCommand(cmd, dryrun); File ""./gatk"", line 384, in runCommand; check_call(cmd, env=gatk_env); File ""/usr/lib/python2.7/subprocess.py"", line 181, in check_call; retcode = call(*popenargs, **kwargs); File ""/usr/lib/python2.7/subprocess.py"", line 168, in call; return Popen(*popenargs, **kwargs).wait(); File ""/usr/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ```; This should perhaps be made a little bit clearer for users as this isn't particularly helpful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5992
https://github.com/broadinstitute/gatk/issues/5992:1322,Usability,clear,clearer,1322,"Recently I was setting up GATK to run in a VM and I had forgotten to install Java8 onto the machine. When I tried to run GATK from the launch script I ran into the following error: ; ```; Using GATK jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /home/emeryj/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar -help; Traceback (most recent call last):; File ""./gatk"", line 479, in <module>; main(sys.argv[1:]); File ""./gatk"", line 152, in main; runGATK(sparkRunner, sparkSubmitCommand, dryRun, gatkArgs, sparkArgs, javaOptions); File ""./gatk"", line 328, in runGATK; runCommand(cmd, dryrun); File ""./gatk"", line 384, in runCommand; check_call(cmd, env=gatk_env); File ""/usr/lib/python2.7/subprocess.py"", line 181, in check_call; retcode = call(*popenargs, **kwargs); File ""/usr/lib/python2.7/subprocess.py"", line 168, in call; return Popen(*popenargs, **kwargs).wait(); File ""/usr/lib/python2.7/subprocess.py"", line 390, in __init__; errread, errwrite); File ""/usr/lib/python2.7/subprocess.py"", line 1024, in _execute_child; raise child_exception; OSError: [Errno 2] No such file or directory; ```; This should perhaps be made a little bit clearer for users as this isn't particularly helpful.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5992
https://github.com/broadinstitute/gatk/issues/5993:60,Availability,error,error,60,It should detect that java is missing and exit with a clear error message instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5993
https://github.com/broadinstitute/gatk/issues/5993:66,Integrability,message,message,66,It should detect that java is missing and exit with a clear error message instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5993
https://github.com/broadinstitute/gatk/issues/5993:10,Safety,detect,detect,10,It should detect that java is missing and exit with a clear error message instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5993
https://github.com/broadinstitute/gatk/issues/5993:54,Usability,clear,clear,54,It should detect that java is missing and exit with a clear error message instead.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5993
https://github.com/broadinstitute/gatk/issues/5996:317,Deployability,configurat,configuration,317,"When running Spark tools with GCS files on Dataproc the [GCS connector](https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs) is set up and configured for you, but this isn't the case when running with local Spark, even on a GCP VM. We should make the experience easier through documentation and/or configuration improvements.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996
https://github.com/broadinstitute/gatk/issues/5996:158,Modifiability,config,configured,158,"When running Spark tools with GCS files on Dataproc the [GCS connector](https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs) is set up and configured for you, but this isn't the case when running with local Spark, even on a GCP VM. We should make the experience easier through documentation and/or configuration improvements.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996
https://github.com/broadinstitute/gatk/issues/5996:317,Modifiability,config,configuration,317,"When running Spark tools with GCS files on Dataproc the [GCS connector](https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs) is set up and configured for you, but this isn't the case when running with local Spark, even on a GCP VM. We should make the experience easier through documentation and/or configuration improvements.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5996
https://github.com/broadinstitute/gatk/issues/5997:1148,Deployability,pipeline,pipelines,1148,", [chr1] and [chrM, chr1].; 	at htsjdk.tribble.index.tabix.TabixIndexMerger.processIndex(TabixIndexMerger.java:47); 	at htsjdk.tribble.index.tabix.TabixIndexMerger.processIndex(TabixIndexMerger.java:19); 	at org.disq_bio.disq.impl.file.IndexFileMerger.mergeParts(IndexFileMerger.java:90); 	at org.disq_bio.disq.impl.formats.vcf.VcfSink.save(VcfSink.java:120); 	at org.disq_bio.disq.HtsjdkVariantsRddStorage.write(HtsjdkVariantsRddStorage.java:150); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariantsSingle(VariantsSparkSink.java:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delega",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997
https://github.com/broadinstitute/gatk/issues/5997:2261,Deployability,deploy,deploy,2261,va:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997
https://github.com/broadinstitute/gatk/issues/5997:2343,Deployability,deploy,deploy,2343,va:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997
https://github.com/broadinstitute/gatk/issues/5997:2380,Deployability,deploy,deploy,2380,va:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997
https://github.com/broadinstitute/gatk/issues/5997:2453,Deployability,deploy,deploy,2453,va:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997
https://github.com/broadinstitute/gatk/issues/5997:2530,Deployability,deploy,deploy,2530,va:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997
https://github.com/broadinstitute/gatk/issues/5997:2602,Deployability,deploy,deploy,2602,va:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997
https://github.com/broadinstitute/gatk/issues/5997:2672,Deployability,deploy,deploy,2672,va:103); 	at org.broadinstitute.hellbender.engine.spark.datasources.VariantsSparkSink.writeVariants(VariantsSparkSink.java:79); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.processAssemblyRegions(HaplotypeCallerSpark.java:189); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:308); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:528); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:30); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:138); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala). ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/5997
https://github.com/broadinstitute/gatk/pull/5998:338,Energy Efficiency,adapt,adapt,338,"We have a tool, VariantQC, that extends VariantEval. This PR is a minor refactor to expose the code that creates the list of VariantStratifier and VariantEvaluator objects as protected methods, so subclasses could modify them. This should have no functional difference on VariantEval itself. We're hoping to use these changes in order to adapt our tool in response to reviewers, so if there is any way to push these changes we would appreciate it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998
https://github.com/broadinstitute/gatk/pull/5998:32,Modifiability,extend,extends,32,"We have a tool, VariantQC, that extends VariantEval. This PR is a minor refactor to expose the code that creates the list of VariantStratifier and VariantEvaluator objects as protected methods, so subclasses could modify them. This should have no functional difference on VariantEval itself. We're hoping to use these changes in order to adapt our tool in response to reviewers, so if there is any way to push these changes we would appreciate it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998
https://github.com/broadinstitute/gatk/pull/5998:72,Modifiability,refactor,refactor,72,"We have a tool, VariantQC, that extends VariantEval. This PR is a minor refactor to expose the code that creates the list of VariantStratifier and VariantEvaluator objects as protected methods, so subclasses could modify them. This should have no functional difference on VariantEval itself. We're hoping to use these changes in order to adapt our tool in response to reviewers, so if there is any way to push these changes we would appreciate it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998
https://github.com/broadinstitute/gatk/pull/5998:338,Modifiability,adapt,adapt,338,"We have a tool, VariantQC, that extends VariantEval. This PR is a minor refactor to expose the code that creates the list of VariantStratifier and VariantEvaluator objects as protected methods, so subclasses could modify them. This should have no functional difference on VariantEval itself. We're hoping to use these changes in order to adapt our tool in response to reviewers, so if there is any way to push these changes we would appreciate it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998
https://github.com/broadinstitute/gatk/pull/5998:84,Security,expose,expose,84,"We have a tool, VariantQC, that extends VariantEval. This PR is a minor refactor to expose the code that creates the list of VariantStratifier and VariantEvaluator objects as protected methods, so subclasses could modify them. This should have no functional difference on VariantEval itself. We're hoping to use these changes in order to adapt our tool in response to reviewers, so if there is any way to push these changes we would appreciate it.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/5998
https://github.com/broadinstitute/gatk/issues/6001:35,Deployability,update,updated,35,"Hi, ; I have a suggestion that the updated verisons of Mutect2 method [pdfs](https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf) should be kept. ; Of course we can look up from the git history log for what have changed in each edition, but it will be much more straightforward to obtain information from pdf file. Such as, [this picture](https://us.v-cdn.net/5019796/uploads/editor/8d/1rt7qtu6ohp2.png) ; in (this post)[https://gatkforums.broadinstitute.org/gatk/discussion/comment/56644#Comment_56644].; I know it comes from the Mutect2 method old version, but it was no longer here. Xiucz.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6001
https://github.com/broadinstitute/gatk/issues/6001:216,Testability,log,log,216,"Hi, ; I have a suggestion that the updated verisons of Mutect2 method [pdfs](https://github.com/broadinstitute/gatk/blob/master/docs/mutect/mutect.pdf) should be kept. ; Of course we can look up from the git history log for what have changed in each edition, but it will be much more straightforward to obtain information from pdf file. Such as, [this picture](https://us.v-cdn.net/5019796/uploads/editor/8d/1rt7qtu6ohp2.png) ; in (this post)[https://gatkforums.broadinstitute.org/gatk/discussion/comment/56644#Comment_56644].; I know it comes from the Mutect2 method old version, but it was no longer here. Xiucz.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6001
https://github.com/broadinstitute/gatk/issues/6003:380,Integrability,depend,dependant,380,"----. ## Bug Report. ### Affected tool(s) or class(es); Mutect2, Haplotypecaller. ### Affected version(s); 4.1.0. ### Description ; It is not an actual accuracy bug but I wanted to share it with you in case you are not aware of it. If you look at BaseGraph.java. There are 4 places that you use ""Collectors.toSet()"". This converts a stream to an unordered set. The final order is dependant on the memory address that each element occupies so the order of result unordered set is not deterministic and can differ from run to run. . This behavior is not problematic as long as we do not traverse the set in order. But let's look at MergeDiamonds.java as an example; ` final Set<SeqVertex> middles = getGraph().outgoingVerticesOf(top);; `; ` for ( final SeqVertex mi : middles ) {; `; First, we make unordered set ""middles"" and then traverse it. Merging diamonds is actually really sensitive which diamond you merge first and can result in different graphs if you change the order. This was not that important before but it seems that with the new chain pruning functionality that you put in mutect2. Sensitivity went up. and now we can actually see different results every time we run the mutect2. I was wondering if it makes sense to change this? The way that we fixed it for ourselves was to change it to linkedHashSet to preserve the order. _Tool/class name(s), parameters?_. ### Description ; _Describe what needs to be added or modified._. ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6003
https://github.com/broadinstitute/gatk/pull/6007:51,Deployability,release,release,51,We are two major versions behind the latest gradle release. A later version is needed for upgrading Spark (#5990) and using BigQuery (#5928).,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6007
https://github.com/broadinstitute/gatk/issues/6012:225,Availability,avail,available,225,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:306,Availability,fault,fault,306,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1799,Availability,Avail,Available,1799,"_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:59,Deployability,install,installation,59,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:152,Deployability,install,installed,152,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:453,Deployability,install,installing,453,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:780,Deployability,release,release,780,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:859,Deployability,release,released,859,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:948,Deployability,install,install,948,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:971,Deployability,Install,Installing,971,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1037,Deployability,install,install,1037,"added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO Nat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1070,Deployability,install,installation,1070,"added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO Nat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1180,Deployability,install,install,1180,"e AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:353,Integrability,depend,dependency,353,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1130,Performance,load,load,1130,"added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO Nat",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1277,Performance,Load,Loading,1277,"ower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1453,Performance,Load,Loading,1453,"t installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory)",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1926,Performance,multi-thread,multi-threaded,1926," do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils348417925139",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:2102,Performance,Load,Loading,2102,"hich allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:3",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:2288,Performance,load,load,2288," Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.390 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING imple",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:2508,Performance,load,loaded,2508,"bgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.390 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:2553,Performance,multi-thread,multi-threaded,2553,"bgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.390 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:2674,Performance,Load,Loading,2674,"bgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.390 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:2860,Performance,load,load,2860,"bgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.390 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:3080,Performance,load,loaded,3080,"bgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.389 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils5391341743604217103.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.389 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.389 INFO PairHMM - OpenMP multi-threaded AVX-accelerated native PairHMM implementation is not supported; > 19:43:36.389 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 19:43:36.390 WARN NativeLibraryLoader - Unable to load libgkl_utils.so from native/libgkl_utils.so (/tmp/libgkl_utils3484179251394006588.so: libgomp.so.1: cannot open shared object file: No such file or directory); > 19:43:36.390 WARN IntelPairHmm - Intel GKL Utils not loaded; > 19:43:36.390 WARN PairHMM - ***WARNING: Machine does not have the AVX instruction set support needed for the accelerated AVX PairHmm. Falling back to the MUCH slower LOGLESS_CACHING implementation!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:770,Testability,test,tested,770,"## Documentation request. ### Description ; I propose that installation of gcc be added to the instructions on the GATK Github README.md. If gcc is not installed, HaplotypeCaller complains that the AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6012:1223,Testability,test,test,1223,"e AVX instruction set is not available, even when it is. It falls back to slower LOGLESS_CACHING PairHMM. The fault is missing libgomp1, which is a required dependency of gcc. Since this documentation request is related to a ""bug"" that comes about from not installing necessary libraries, I'll include the bug report format below, in case someone else searches for solutions to this problem, as suggested by @lbergelson. ### Affected tool(s) or class(es); _HaplotypeCaller_, or any other tool that uses _PairHMM_. ### Affected version(s); -I think all as of _2019-06-20_. I tested on release version _4.1.2.0_. #### Steps to reproduce; Run HaplotypeCaller from a released jar on an Ubuntu VM that supports the AVX instruction set. Critically, do *NOT* install gcc on the VM. Installing gcc fixes this problem. #### Expected behavior; If you install gcc, that results in the installation of libgomp1, which allows the Intel library to load and use AVX acceleration. You could probably install libgomp1 on its own, but I did not test that.; > 14:51:01.013 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_utils.so; > 14:51:01.015 INFO NativeLibraryLoader - Loading libgkl_pairhmm_omp.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!/com/intel/gkl/native/libgkl_pairhmm_omp.so; > 14:51:01.053 INFO IntelPairHmm - Using CPU-supported AVX-512 instructions; > 14:51:01.053 INFO IntelPairHmm - Flush-to-zero (FTZ) is enabled when running PairHMM; > 14:51:01.054 INFO IntelPairHmm - Available threads: 16; > 14:51:01.054 INFO IntelPairHmm - Requested threads: 8; > 14:51:01.054 INFO PairHMM - Using the OpenMP multi-threaded AVX-accelerated native PairHMM implementation. #### Actual behavior; Without libgomp1, AVX acceleration doesn't work:; > 19:43:36.387 INFO NativeLibraryLoader - Loading libgkl_utils.so from jar:file:/home/ubuntu/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6012
https://github.com/broadinstitute/gatk/issues/6013:63,Testability,test,testing,63,"## Feature request. ### Tool(s) or class(es) involved; All WDL testing. ### Description; Currently, our WDL tests are not instrumented to look inside the outputs of a cromwell run. This stifles some WDL testing of differing parameters, since (for example) we cannot easily see if a file was created, such as a bamout.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6013
https://github.com/broadinstitute/gatk/issues/6013:108,Testability,test,tests,108,"## Feature request. ### Tool(s) or class(es) involved; All WDL testing. ### Description; Currently, our WDL tests are not instrumented to look inside the outputs of a cromwell run. This stifles some WDL testing of differing parameters, since (for example) we cannot easily see if a file was created, such as a bamout.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6013
https://github.com/broadinstitute/gatk/issues/6013:203,Testability,test,testing,203,"## Feature request. ### Tool(s) or class(es) involved; All WDL testing. ### Description; Currently, our WDL tests are not instrumented to look inside the outputs of a cromwell run. This stifles some WDL testing of differing parameters, since (for example) we cannot easily see if a file was created, such as a bamout.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6013
https://github.com/broadinstitute/gatk/issues/6014:530,Availability,down,down,530,"As per our discussion with @davidbenjamin, we think the code in the `SmithWatermanAligner` classes that is used to avoid having to actually call out to SmithWaterman can be improved. For instance we suspect the current heuristic of searching for an exact substring match in the reference from the read could be improved (for instance we could look for mismatches of exactly 1 base etc...). We want to both develop the tools to quantify the amount of time we currently spend in smith waterman code and come up with ways of cutting down on SmithWaterman calls in all parts of the HaplotypeCaller/Mutect Engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6014
https://github.com/broadinstitute/gatk/issues/6014:115,Safety,avoid,avoid,115,"As per our discussion with @davidbenjamin, we think the code in the `SmithWatermanAligner` classes that is used to avoid having to actually call out to SmithWaterman can be improved. For instance we suspect the current heuristic of searching for an exact substring match in the reference from the read could be improved (for instance we could look for mismatches of exactly 1 base etc...). We want to both develop the tools to quantify the amount of time we currently spend in smith waterman code and come up with ways of cutting down on SmithWaterman calls in all parts of the HaplotypeCaller/Mutect Engine.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6014
https://github.com/broadinstitute/gatk/pull/6015:168,Testability,test,tests,168,added counts for Smith-Waterman and non-Smith_Waterman calls; implemented oneMismatch heuristic that aligns reads to haplotypes given a read that only has 1 SNP; added tests for the oneMismatch heuristic,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6015
https://github.com/broadinstitute/gatk/pull/6017:29,Performance,perform,perform,29,"This adds two new tasks that perform QC checks on gCNV output, namely:; - Check that number of events in segments VCFs does not exceed preset value; - Check that not all PCs are used by the model (even if one of the shards fails, the QC status will be negative). Both tasks output *qc_status.txt file, for each sample and model respectively, and the file will contain string ""PASS"", or a string describing the fail condition",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6017
https://github.com/broadinstitute/gatk/issues/6018:928,Deployability,pipeline,pipeline,928,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:1159,Deployability,pipeline,pipeline,1159,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:1234,Security,validat,validation,1234,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:1323,Security,validat,validation,1323,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:32,Testability,test,test,32,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:160,Testability,test,test,160,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:339,Testability,test,test,339,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:411,Testability,test,test,411,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:524,Testability,test,test,524,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:622,Testability,test,test,622,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:730,Testability,test,test,730,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:848,Testability,test,test,848,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:952,Testability,test,test,952,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:1078,Testability,test,test,1078,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:1183,Testability,test,test,1183,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:1272,Testability,test,test,1272,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:1447,Testability,test,test,1447,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6018:1545,Testability,test,test,1545,"There are quite a few v2.1 CRAM test files being used in GATK that should probably be regenerated and replaced with v3.0 files. There are also quite a few CRAM test files floating around in both htsjdk and GATK that have external blocks with content ID=0 (not valid per the spec) and some of those blocks have no actual content:. gatk/src/test/resources/large/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/expected.clippingReadsTestCRAM.QT_10.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/ClipReads/clippingReadsTestCRAM.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/print_reads.sorted.queryname.htsjdk-2.1.0.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.NA12878.20.21.10m-10m100.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.bqsr.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/CEUTrio.HiSeq.WGS.b37.ch20.1m-1m1k.NA12878.noMD.noBQSR.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/BQSR/expected.MultiSite.reads.pipeline.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/single.read.cram; gatk/src/test/resources/org/broadinstitute/hellbender/tools/validation/another.single.read.cram. These have external blocks with ID=0, but the blocks have no actual content:. gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_crai_index.cram (0 bytes); gatk/src/test/resources/org/broadinstitute/hellbender/engine/cram_with_bai_index.cram (0 bytes). We should regenerate and replace with v3.0 CRAM files.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6018
https://github.com/broadinstitute/gatk/issues/6019:115,Availability,error,error,115,"Following the instructions to build from the main readme, `./gradlew bundle` and `./gradlew clean` ends with these error messages:; ```; ...; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 105. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.; ```. The build is being attempted in a docker container in a 64-bit ubuntu VM on a Win10 host. What am I doing wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6019
https://github.com/broadinstitute/gatk/issues/6019:142,Availability,Down,Download,142,"Following the instructions to build from the main readme, `./gradlew bundle` and `./gradlew clean` ends with these error messages:; ```; ...; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 105. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.; ```. The build is being attempted in a docker container in a 64-bit ubuntu VM on a Win10 host. What am I doing wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6019
https://github.com/broadinstitute/gatk/issues/6019:297,Availability,FAILURE,FAILURE,297,"Following the instructions to build from the main readme, `./gradlew bundle` and `./gradlew clean` ends with these error messages:; ```; ...; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 105. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.; ```. The build is being attempted in a docker container in a 64-bit ubuntu VM on a Win10 host. What am I doing wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6019
https://github.com/broadinstitute/gatk/issues/6019:603,Deployability,install,installed,603,"Following the instructions to build from the main readme, `./gradlew bundle` and `./gradlew clean` ends with these error messages:; ```; ...; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 105. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.; ```. The build is being attempted in a docker container in a 64-bit ubuntu VM on a Win10 host. What am I doing wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6019
https://github.com/broadinstitute/gatk/issues/6019:121,Integrability,message,messages,121,"Following the instructions to build from the main readme, `./gradlew bundle` and `./gradlew clean` ends with these error messages:; ```; ...; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 105. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.; ```. The build is being attempted in a docker container in a 64-bit ubuntu VM on a Win10 host. What am I doing wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6019
https://github.com/broadinstitute/gatk/issues/6019:811,Testability,log,log,811,"Following the instructions to build from the main readme, `./gradlew bundle` and `./gradlew clean` ends with these error messages:; ```; ...; Download https://repo1.maven.org/maven2/commons-codec/commons-codec/1.6/commons-codec-1.6.jar; Executing: git lfs pull --include src/main/resources/large. FAILURE: Build failed with an exception. * Where:; Build file '/gatk/build.gradle' line: 105. * What went wrong:; A problem occurred evaluating root project 'gatk'.; > Execution of ""git lfs pull --include src/main/resources/large"" failed with exit code: 1. git-lfs is required to build GATK but may not be installed. See https://github.com/broadinstitute/gatk#building for information on how to build GATK. * Try:; Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.; ```. The build is being attempted in a docker container in a 64-bit ubuntu VM on a Win10 host. What am I doing wrong?",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6019
https://github.com/broadinstitute/gatk/issues/6020:692,Deployability,update,update,692,"## Feature request. ### Tool(s) or class(es) involved; Docker container environment for FireCloud / GATK. ### Description. I started a FireCloud instance with a GATK example. It looks like although conda does seem to be correctly pulling TensorFlow from the Anaconda repository, TensorFlow is still not being enabled with Intel MKL-DNN (which would make TensorFlow much faster on CPU). To test this, you can start a notebook or python session and do:. `import tensorflow; print(tensorflow.pywrap_tensorflow.IsMklEnabled())`. If True, then MKL-DNN is enabled in TensorFlow. Currently, this is showing up as false. I'm wondering whether I could work with someone (possibly Sam @lucidtronix) to update the conda environment. Also, the compute instances on FireCloud are using older CPU hardware (AVX-2). Is there any way to update this to a Skylake or Cascade Lake instance (AVX-512/VNNI)?. Thanks.; -Tony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6020
https://github.com/broadinstitute/gatk/issues/6020:821,Deployability,update,update,821,"## Feature request. ### Tool(s) or class(es) involved; Docker container environment for FireCloud / GATK. ### Description. I started a FireCloud instance with a GATK example. It looks like although conda does seem to be correctly pulling TensorFlow from the Anaconda repository, TensorFlow is still not being enabled with Intel MKL-DNN (which would make TensorFlow much faster on CPU). To test this, you can start a notebook or python session and do:. `import tensorflow; print(tensorflow.pywrap_tensorflow.IsMklEnabled())`. If True, then MKL-DNN is enabled in TensorFlow. Currently, this is showing up as false. I'm wondering whether I could work with someone (possibly Sam @lucidtronix) to update the conda environment. Also, the compute instances on FireCloud are using older CPU hardware (AVX-2). Is there any way to update this to a Skylake or Cascade Lake instance (AVX-512/VNNI)?. Thanks.; -Tony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6020
https://github.com/broadinstitute/gatk/issues/6020:389,Testability,test,test,389,"## Feature request. ### Tool(s) or class(es) involved; Docker container environment for FireCloud / GATK. ### Description. I started a FireCloud instance with a GATK example. It looks like although conda does seem to be correctly pulling TensorFlow from the Anaconda repository, TensorFlow is still not being enabled with Intel MKL-DNN (which would make TensorFlow much faster on CPU). To test this, you can start a notebook or python session and do:. `import tensorflow; print(tensorflow.pywrap_tensorflow.IsMklEnabled())`. If True, then MKL-DNN is enabled in TensorFlow. Currently, this is showing up as false. I'm wondering whether I could work with someone (possibly Sam @lucidtronix) to update the conda environment. Also, the compute instances on FireCloud are using older CPU hardware (AVX-2). Is there any way to update this to a Skylake or Cascade Lake instance (AVX-512/VNNI)?. Thanks.; -Tony",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6020
https://github.com/broadinstitute/gatk/issues/6021:21,Availability,error,error,21,"Hi GATK team,. I had error message as following with GATK4.1.0.0 on our local cluster: ; `; Using GATK jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java1.8 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5; g -jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar SelectVariants -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembl; y38.fasta -L chr1 -V /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joi; nt525_c1.SNP.VQSR.g.vcf.gz; 09:15:49.372 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 09:15:51.131 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.132 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:15:51.132 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:15:51.132 INFO SelectVariants - Executing as xhong@blade5-4-11.dsg.wustl.edu on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 09:15:51.133 INFO SelectVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 09:15:51.133 INFO SelectVariants - Start Date/Time: June 27, 2019 9:15:49 AM CDT; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.134 INFO SelectVariants - HTSJDK Version: 2.18.2; 09:15:51.134 INFO SelectVariants - Picard Version: 2.18.25; 09:15:51.134 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:15:51.135 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:2998,Availability,down,down,2998,"09:15:51.135 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_SAMTOOLS : true; 09:15:51.135 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_WRITE_FOR_TRIBBLE : false; 09:15:51.135 INFO SelectVariants - Deflater: IntelDeflater; 09:15:51.135 INFO SelectVariants - Inflater: IntelInflater; 09:15:51.135 INFO SelectVariants - GCS max retries/reopens: 20; 09:15:51.135 INFO SelectVariants - Requester pays: disabled; 09:15:51.136 INFO SelectVariants - Initializing engine; 09:15:52.547 INFO FeatureManager - Using codec VCFCodec to read file file:///dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR/ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz; 09:15:53.171 INFO IntervalArgumentCollection - Processing 248956422 bp from intervals; 09:15:53.221 INFO SelectVariants - Done initializing engine; 09:15:53.390 INFO ProgressMeter - Starting traversal; 09:15:53.390 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:15:53.479 INFO SelectVariants - Shutting down engine; [June 27, 2019 9:15:53 AM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2131755008; htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 3433: The VCF specification does not allow for whitespace in the INFO field; . Offending field value was ""AC=1;AF=9.671e-04;AN=1034;AS_BaseQRankSum=-1.550;AS_FS=8.334;AS_InbreedingCoeff=-0.3147;AS_MQ=31.69;AS_MQRankSum=-0.200;AS_QD=28.73;AS_ReadPosR; ankSum=nul;AS_SOR=2.235;BaseQRankSum=-1.381e+00;DP=40368;ExcessHet=160.0000;FS=8.334;InbreedingCoeff=-0.3147;MLEAC=7;MLEAF=6.770e-03;MQ=37.13;MQRankSum=0.126;QD=2.46;SOR=2.; 235 GT:AD:DP:GQ:PGT:PID:PL:PS 0/0:75,0:75:0:.:.:0,0,1525; `. However, from the error message I cannot see any whitespace in the INFO field. The /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR/ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz is the output of following command:. `gatk4.1.0.0 --java-options '-X",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:3784,Availability,error,error,3784,"5:53.390 INFO ProgressMeter - Starting traversal; 09:15:53.390 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:15:53.479 INFO SelectVariants - Shutting down engine; [June 27, 2019 9:15:53 AM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2131755008; htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 3433: The VCF specification does not allow for whitespace in the INFO field; . Offending field value was ""AC=1;AF=9.671e-04;AN=1034;AS_BaseQRankSum=-1.550;AS_FS=8.334;AS_InbreedingCoeff=-0.3147;AS_MQ=31.69;AS_MQRankSum=-0.200;AS_QD=28.73;AS_ReadPosR; ankSum=nul;AS_SOR=2.235;BaseQRankSum=-1.381e+00;DP=40368;ExcessHet=160.0000;FS=8.334;InbreedingCoeff=-0.3147;MLEAC=7;MLEAF=6.770e-03;MQ=37.13;MQRankSum=0.126;QD=2.46;SOR=2.; 235 GT:AD:DP:GQ:PGT:PID:PL:PS 0/0:75,0:75:0:.:.:0,0,1525; `. However, from the error message I cannot see any whitespace in the INFO field. The /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR/ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz is the output of following command:. `gatk4.1.0.0 --java-options '-Xmx100g -Xmx100g' ApplyVQSR \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz \; -V ${SNPPath}/joint525_chr2_ExcessHet_filter.SNP.g.vcf.gz \; ....; -V ${SNPPath}/joint525_chr22_ExcessHet_filter.SNP.g.vcf.gz \; -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.g; z \; --truth-sensitivity-filter-level 97 \; --tranches-file /dsgmnt/seq5_llfs/work/xhong/v4100/VQSR//ExcessHet_joint525_c1_22.snp.tranches \; --recal-file /dsgmnt/seq5_llfs/work/xho; ng/v4100/VQSR//ExcessHet_joint525_c1_22.snp.recal \; -mode SNP`. There is no error or warning in the standard error and standard output of this step. I have tried to apply VQSR SNP model to ${SNPPath}/joint525_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:4672,Availability,error,error,4672,"27, 2019 9:15:53 AM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2131755008; htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 3433: The VCF specification does not allow for whitespace in the INFO field; . Offending field value was ""AC=1;AF=9.671e-04;AN=1034;AS_BaseQRankSum=-1.550;AS_FS=8.334;AS_InbreedingCoeff=-0.3147;AS_MQ=31.69;AS_MQRankSum=-0.200;AS_QD=28.73;AS_ReadPosR; ankSum=nul;AS_SOR=2.235;BaseQRankSum=-1.381e+00;DP=40368;ExcessHet=160.0000;FS=8.334;InbreedingCoeff=-0.3147;MLEAC=7;MLEAF=6.770e-03;MQ=37.13;MQRankSum=0.126;QD=2.46;SOR=2.; 235 GT:AD:DP:GQ:PGT:PID:PL:PS 0/0:75,0:75:0:.:.:0,0,1525; `. However, from the error message I cannot see any whitespace in the INFO field. The /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR/ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz is the output of following command:. `gatk4.1.0.0 --java-options '-Xmx100g -Xmx100g' ApplyVQSR \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz \; -V ${SNPPath}/joint525_chr2_ExcessHet_filter.SNP.g.vcf.gz \; ....; -V ${SNPPath}/joint525_chr22_ExcessHet_filter.SNP.g.vcf.gz \; -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.g; z \; --truth-sensitivity-filter-level 97 \; --tranches-file /dsgmnt/seq5_llfs/work/xhong/v4100/VQSR//ExcessHet_joint525_c1_22.snp.tranches \; --recal-file /dsgmnt/seq5_llfs/work/xho; ng/v4100/VQSR//ExcessHet_joint525_c1_22.snp.recal \; -mode SNP`. There is no error or warning in the standard error and standard output of this step. I have tried to apply VQSR SNP model to ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz. It works well. When I select BISNPs from the output, I could not repeat the error. . I would like to get suggestion on how to narrow down the problem. Any input is appreciated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:4705,Availability,error,error,4705,"27, 2019 9:15:53 AM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2131755008; htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 3433: The VCF specification does not allow for whitespace in the INFO field; . Offending field value was ""AC=1;AF=9.671e-04;AN=1034;AS_BaseQRankSum=-1.550;AS_FS=8.334;AS_InbreedingCoeff=-0.3147;AS_MQ=31.69;AS_MQRankSum=-0.200;AS_QD=28.73;AS_ReadPosR; ankSum=nul;AS_SOR=2.235;BaseQRankSum=-1.381e+00;DP=40368;ExcessHet=160.0000;FS=8.334;InbreedingCoeff=-0.3147;MLEAC=7;MLEAF=6.770e-03;MQ=37.13;MQRankSum=0.126;QD=2.46;SOR=2.; 235 GT:AD:DP:GQ:PGT:PID:PL:PS 0/0:75,0:75:0:.:.:0,0,1525; `. However, from the error message I cannot see any whitespace in the INFO field. The /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR/ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz is the output of following command:. `gatk4.1.0.0 --java-options '-Xmx100g -Xmx100g' ApplyVQSR \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz \; -V ${SNPPath}/joint525_chr2_ExcessHet_filter.SNP.g.vcf.gz \; ....; -V ${SNPPath}/joint525_chr22_ExcessHet_filter.SNP.g.vcf.gz \; -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.g; z \; --truth-sensitivity-filter-level 97 \; --tranches-file /dsgmnt/seq5_llfs/work/xhong/v4100/VQSR//ExcessHet_joint525_c1_22.snp.tranches \; --recal-file /dsgmnt/seq5_llfs/work/xho; ng/v4100/VQSR//ExcessHet_joint525_c1_22.snp.recal \; -mode SNP`. There is no error or warning in the standard error and standard output of this step. I have tried to apply VQSR SNP model to ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz. It works well. When I select BISNPs from the output, I could not repeat the error. . I would like to get suggestion on how to narrow down the problem. Any input is appreciated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:4917,Availability,error,error,4917,"27, 2019 9:15:53 AM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2131755008; htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 3433: The VCF specification does not allow for whitespace in the INFO field; . Offending field value was ""AC=1;AF=9.671e-04;AN=1034;AS_BaseQRankSum=-1.550;AS_FS=8.334;AS_InbreedingCoeff=-0.3147;AS_MQ=31.69;AS_MQRankSum=-0.200;AS_QD=28.73;AS_ReadPosR; ankSum=nul;AS_SOR=2.235;BaseQRankSum=-1.381e+00;DP=40368;ExcessHet=160.0000;FS=8.334;InbreedingCoeff=-0.3147;MLEAC=7;MLEAF=6.770e-03;MQ=37.13;MQRankSum=0.126;QD=2.46;SOR=2.; 235 GT:AD:DP:GQ:PGT:PID:PL:PS 0/0:75,0:75:0:.:.:0,0,1525; `. However, from the error message I cannot see any whitespace in the INFO field. The /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR/ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz is the output of following command:. `gatk4.1.0.0 --java-options '-Xmx100g -Xmx100g' ApplyVQSR \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz \; -V ${SNPPath}/joint525_chr2_ExcessHet_filter.SNP.g.vcf.gz \; ....; -V ${SNPPath}/joint525_chr22_ExcessHet_filter.SNP.g.vcf.gz \; -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.g; z \; --truth-sensitivity-filter-level 97 \; --tranches-file /dsgmnt/seq5_llfs/work/xhong/v4100/VQSR//ExcessHet_joint525_c1_22.snp.tranches \; --recal-file /dsgmnt/seq5_llfs/work/xho; ng/v4100/VQSR//ExcessHet_joint525_c1_22.snp.recal \; -mode SNP`. There is no error or warning in the standard error and standard output of this step. I have tried to apply VQSR SNP model to ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz. It works well. When I select BISNPs from the output, I could not repeat the error. . I would like to get suggestion on how to narrow down the problem. Any input is appreciated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:4974,Availability,down,down,4974,"27, 2019 9:15:53 AM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2131755008; htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 3433: The VCF specification does not allow for whitespace in the INFO field; . Offending field value was ""AC=1;AF=9.671e-04;AN=1034;AS_BaseQRankSum=-1.550;AS_FS=8.334;AS_InbreedingCoeff=-0.3147;AS_MQ=31.69;AS_MQRankSum=-0.200;AS_QD=28.73;AS_ReadPosR; ankSum=nul;AS_SOR=2.235;BaseQRankSum=-1.381e+00;DP=40368;ExcessHet=160.0000;FS=8.334;InbreedingCoeff=-0.3147;MLEAC=7;MLEAF=6.770e-03;MQ=37.13;MQRankSum=0.126;QD=2.46;SOR=2.; 235 GT:AD:DP:GQ:PGT:PID:PL:PS 0/0:75,0:75:0:.:.:0,0,1525; `. However, from the error message I cannot see any whitespace in the INFO field. The /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR/ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz is the output of following command:. `gatk4.1.0.0 --java-options '-Xmx100g -Xmx100g' ApplyVQSR \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz \; -V ${SNPPath}/joint525_chr2_ExcessHet_filter.SNP.g.vcf.gz \; ....; -V ${SNPPath}/joint525_chr22_ExcessHet_filter.SNP.g.vcf.gz \; -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.g; z \; --truth-sensitivity-filter-level 97 \; --tranches-file /dsgmnt/seq5_llfs/work/xhong/v4100/VQSR//ExcessHet_joint525_c1_22.snp.tranches \; --recal-file /dsgmnt/seq5_llfs/work/xho; ng/v4100/VQSR//ExcessHet_joint525_c1_22.snp.recal \; -mode SNP`. There is no error or warning in the standard error and standard output of this step. I have tried to apply VQSR SNP model to ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz. It works well. When I select BISNPs from the output, I could not repeat the error. . I would like to get suggestion on how to narrow down the problem. Any input is appreciated.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:27,Integrability,message,message,27,"Hi GATK team,. I had error message as following with GATK4.1.0.0 on our local cluster: ; `; Using GATK jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java1.8 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5; g -jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar SelectVariants -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembl; y38.fasta -L chr1 -V /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joi; nt525_c1.SNP.VQSR.g.vcf.gz; 09:15:49.372 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 09:15:51.131 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.132 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:15:51.132 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:15:51.132 INFO SelectVariants - Executing as xhong@blade5-4-11.dsg.wustl.edu on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 09:15:51.133 INFO SelectVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 09:15:51.133 INFO SelectVariants - Start Date/Time: June 27, 2019 9:15:49 AM CDT; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.134 INFO SelectVariants - HTSJDK Version: 2.18.2; 09:15:51.134 INFO SelectVariants - Picard Version: 2.18.25; 09:15:51.134 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:15:51.135 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:3790,Integrability,message,message,3790,"5:53.390 INFO ProgressMeter - Starting traversal; 09:15:53.390 INFO ProgressMeter - Current Locus Elapsed Minutes Variants Processed Variants/Minute; 09:15:53.479 INFO SelectVariants - Shutting down engine; [June 27, 2019 9:15:53 AM CDT] org.broadinstitute.hellbender.tools.walkers.variantutils.SelectVariants done. Elapsed time: 0.07 minutes.; Runtime.totalMemory()=2131755008; htsjdk.tribble.TribbleException: The provided VCF file is malformed at approximately line number 3433: The VCF specification does not allow for whitespace in the INFO field; . Offending field value was ""AC=1;AF=9.671e-04;AN=1034;AS_BaseQRankSum=-1.550;AS_FS=8.334;AS_InbreedingCoeff=-0.3147;AS_MQ=31.69;AS_MQRankSum=-0.200;AS_QD=28.73;AS_ReadPosR; ankSum=nul;AS_SOR=2.235;BaseQRankSum=-1.381e+00;DP=40368;ExcessHet=160.0000;FS=8.334;InbreedingCoeff=-0.3147;MLEAC=7;MLEAF=6.770e-03;MQ=37.13;MQRankSum=0.126;QD=2.46;SOR=2.; 235 GT:AD:DP:GQ:PGT:PID:PL:PS 0/0:75,0:75:0:.:.:0,0,1525; `. However, from the error message I cannot see any whitespace in the INFO field. The /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR/ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz is the output of following command:. `gatk4.1.0.0 --java-options '-Xmx100g -Xmx100g' ApplyVQSR \; -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembly38.fasta \; -V ${SNPPath}/joint525_chr1_ExcessHet_filter.SNP.g.vcf.gz \; -V ${SNPPath}/joint525_chr2_ExcessHet_filter.SNP.g.vcf.gz \; ....; -V ${SNPPath}/joint525_chr22_ExcessHet_filter.SNP.g.vcf.gz \; -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.g; z \; --truth-sensitivity-filter-level 97 \; --tranches-file /dsgmnt/seq5_llfs/work/xhong/v4100/VQSR//ExcessHet_joint525_c1_22.snp.tranches \; --recal-file /dsgmnt/seq5_llfs/work/xho; ng/v4100/VQSR//ExcessHet_joint525_c1_22.snp.recal \; -mode SNP`. There is no error or warning in the standard error and standard output of this step. I have tried to apply VQSR SNP model to ${SNPPath}/joint525_",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6021:772,Performance,Load,Loading,772,"Hi GATK team,. I had error message as following with GATK4.1.0.0 on our local cluster: ; `; Using GATK jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar; Running:; java1.8 -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx5; g -jar /dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar SelectVariants -R /dsgmnt/llfs2/masterdata/geno/hg38/resources_broad_hg38_v0_Homo_sapiens_assembl; y38.fasta -L chr1 -V /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joint525_c1_22.SNP.VQSR.g.vcf.gz -O /dsgmnt/seq5_llfs/work/xhong/v4100/ApplyVQSR//ExcessHet_joi; nt525_c1.SNP.VQSR.g.vcf.gz; 09:15:49.372 INFO NativeLibraryLoader - Loading libgkl_compression.so from jar:file:/dsg_cent/packages/GATK/gatk-4.1.0.0/gatk-package-4.1.0.0-local.jar!/com/intel/gkl/nati; ve/libgkl_compression.so; 09:15:51.131 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.132 INFO SelectVariants - The Genome Analysis Toolkit (GATK) v4.1.0.0; 09:15:51.132 INFO SelectVariants - For support and documentation go to https://software.broadinstitute.org/gatk/; 09:15:51.132 INFO SelectVariants - Executing as xhong@blade5-4-11.dsg.wustl.edu on Linux v2.6.32-573.12.1.el6.x86_64 amd64; 09:15:51.133 INFO SelectVariants - Java runtime: Java HotSpot(TM) 64-Bit Server VM v1.8.0_31-b13; 09:15:51.133 INFO SelectVariants - Start Date/Time: June 27, 2019 9:15:49 AM CDT; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.133 INFO SelectVariants - ------------------------------------------------------------; 09:15:51.134 INFO SelectVariants - HTSJDK Version: 2.18.2; 09:15:51.134 INFO SelectVariants - Picard Version: 2.18.25; 09:15:51.134 INFO SelectVariants - HTSJDK Defaults.COMPRESSION_LEVEL : 2; 09:15:51.135 INFO SelectVariants - HTSJDK Defaults.USE_ASYNC_IO_READ_FOR_SAMTOOLS : fals",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6021
https://github.com/broadinstitute/gatk/issues/6023:42,Availability,error,error,42,"User report:. ValidateVariants causes the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i -gvcf; ```; And it causes the following error for all my files:; ```; ***********************************************************************. A USER ERROR has occurred: In a GVCF all records must ordered. Record: [VC Unknown @ Super-Scaffold_2:1-4 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=4} filters= covers a position previously traversed. ***********************************************************************; ```. This doesn't cause the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i; ```. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/59104#Comment_59104",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023
https://github.com/broadinstitute/gatk/issues/6023:242,Availability,error,error,242,"User report:. ValidateVariants causes the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i -gvcf; ```; And it causes the following error for all my files:; ```; ***********************************************************************. A USER ERROR has occurred: In a GVCF all records must ordered. Record: [VC Unknown @ Super-Scaffold_2:1-4 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=4} filters= covers a position previously traversed. ***********************************************************************; ```. This doesn't cause the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i; ```. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/59104#Comment_59104",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023
https://github.com/broadinstitute/gatk/issues/6023:352,Availability,ERROR,ERROR,352,"User report:. ValidateVariants causes the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i -gvcf; ```; And it causes the following error for all my files:; ```; ***********************************************************************. A USER ERROR has occurred: In a GVCF all records must ordered. Record: [VC Unknown @ Super-Scaffold_2:1-4 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=4} filters= covers a position previously traversed. ***********************************************************************; ```. This doesn't cause the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i; ```. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/59104#Comment_59104",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023
https://github.com/broadinstitute/gatk/issues/6023:658,Availability,error,error,658,"User report:. ValidateVariants causes the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i -gvcf; ```; And it causes the following error for all my files:; ```; ***********************************************************************. A USER ERROR has occurred: In a GVCF all records must ordered. Record: [VC Unknown @ Super-Scaffold_2:1-4 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=4} filters= covers a position previously traversed. ***********************************************************************; ```. This doesn't cause the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i; ```. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/59104#Comment_59104",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023
https://github.com/broadinstitute/gatk/issues/6023:14,Security,Validat,ValidateVariants,14,"User report:. ValidateVariants causes the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i -gvcf; ```; And it causes the following error for all my files:; ```; ***********************************************************************. A USER ERROR has occurred: In a GVCF all records must ordered. Record: [VC Unknown @ Super-Scaffold_2:1-4 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=4} filters= covers a position previously traversed. ***********************************************************************; ```. This doesn't cause the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i; ```. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/59104#Comment_59104",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023
https://github.com/broadinstitute/gatk/issues/6023:148,Security,Validat,ValidateVariants,148,"User report:. ValidateVariants causes the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i -gvcf; ```; And it causes the following error for all my files:; ```; ***********************************************************************. A USER ERROR has occurred: In a GVCF all records must ordered. Record: [VC Unknown @ Super-Scaffold_2:1-4 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=4} filters= covers a position previously traversed. ***********************************************************************; ```. This doesn't cause the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i; ```. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/59104#Comment_59104",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023
https://github.com/broadinstitute/gatk/issues/6023:764,Security,Validat,ValidateVariants,764,"User report:. ValidateVariants causes the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i -gvcf; ```; And it causes the following error for all my files:; ```; ***********************************************************************. A USER ERROR has occurred: In a GVCF all records must ordered. Record: [VC Unknown @ Super-Scaffold_2:1-4 Q. of type=SYMBOLIC alleles=[G*, <NON_REF>] attr={END=4} filters= covers a position previously traversed. ***********************************************************************; ```. This doesn't cause the error:; ```; java -Xms32G -Xmx32G -jar /data/biosoftware/GATK/gatk-4.1.2.0/gatk-package-4.1.2.0-local.jar ValidateVariants -R ~/reference/reference.fasta -V $i; ```. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/comment/59104#Comment_59104",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6023
https://github.com/broadinstitute/gatk/issues/6024:119,Deployability,Install,Installing,119,"Java 8 is now old enough that it's not the default on new machines, (java 12 seems to be what comes on a new macbook.) Installing java 8 has become more of a hassle because oracle now requires you to login in order to get it. We should update the readme with information about how to get java 8, probably pointing people to https://adoptopenjdk.net/. I think comms probably needs to update/add some documentation about this as well because it's definitely a friction point. We might also want to advise using jenv for people with multiple java installations to manage them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6024
https://github.com/broadinstitute/gatk/issues/6024:236,Deployability,update,update,236,"Java 8 is now old enough that it's not the default on new machines, (java 12 seems to be what comes on a new macbook.) Installing java 8 has become more of a hassle because oracle now requires you to login in order to get it. We should update the readme with information about how to get java 8, probably pointing people to https://adoptopenjdk.net/. I think comms probably needs to update/add some documentation about this as well because it's definitely a friction point. We might also want to advise using jenv for people with multiple java installations to manage them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6024
https://github.com/broadinstitute/gatk/issues/6024:383,Deployability,update,update,383,"Java 8 is now old enough that it's not the default on new machines, (java 12 seems to be what comes on a new macbook.) Installing java 8 has become more of a hassle because oracle now requires you to login in order to get it. We should update the readme with information about how to get java 8, probably pointing people to https://adoptopenjdk.net/. I think comms probably needs to update/add some documentation about this as well because it's definitely a friction point. We might also want to advise using jenv for people with multiple java installations to manage them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6024
https://github.com/broadinstitute/gatk/issues/6024:544,Deployability,install,installations,544,"Java 8 is now old enough that it's not the default on new machines, (java 12 seems to be what comes on a new macbook.) Installing java 8 has become more of a hassle because oracle now requires you to login in order to get it. We should update the readme with information about how to get java 8, probably pointing people to https://adoptopenjdk.net/. I think comms probably needs to update/add some documentation about this as well because it's definitely a friction point. We might also want to advise using jenv for people with multiple java installations to manage them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6024
https://github.com/broadinstitute/gatk/issues/6024:200,Testability,log,login,200,"Java 8 is now old enough that it's not the default on new machines, (java 12 seems to be what comes on a new macbook.) Installing java 8 has become more of a hassle because oracle now requires you to login in order to get it. We should update the readme with information about how to get java 8, probably pointing people to https://adoptopenjdk.net/. I think comms probably needs to update/add some documentation about this as well because it's definitely a friction point. We might also want to advise using jenv for people with multiple java installations to manage them.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6024
https://github.com/broadinstitute/gatk/pull/6025:103,Availability,down,download,103,* Java 8 doesn't come by default on many machines now. Updating the; README with advice about where to download a java8 jdk.; * Part of https://github.com/broadinstitute/gatk/issues/6024,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6025
https://github.com/broadinstitute/gatk/pull/6028:168,Deployability,continuous,continuous,168,- Extracted the order validation for GVCF files into a separate method and included; a check to reset the counter when a new contig is found. Contigs have to; occur in continuous blocks; validation for files in which contigs occur; alternatingly is not supported.; - Added a set of integration tests for GVCF files with two and three contigs. Fixes #6023,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6028
https://github.com/broadinstitute/gatk/pull/6028:282,Deployability,integrat,integration,282,- Extracted the order validation for GVCF files into a separate method and included; a check to reset the counter when a new contig is found. Contigs have to; occur in continuous blocks; validation for files in which contigs occur; alternatingly is not supported.; - Added a set of integration tests for GVCF files with two and three contigs. Fixes #6023,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6028
https://github.com/broadinstitute/gatk/pull/6028:282,Integrability,integrat,integration,282,- Extracted the order validation for GVCF files into a separate method and included; a check to reset the counter when a new contig is found. Contigs have to; occur in continuous blocks; validation for files in which contigs occur; alternatingly is not supported.; - Added a set of integration tests for GVCF files with two and three contigs. Fixes #6023,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6028
https://github.com/broadinstitute/gatk/pull/6028:22,Security,validat,validation,22,- Extracted the order validation for GVCF files into a separate method and included; a check to reset the counter when a new contig is found. Contigs have to; occur in continuous blocks; validation for files in which contigs occur; alternatingly is not supported.; - Added a set of integration tests for GVCF files with two and three contigs. Fixes #6023,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6028
https://github.com/broadinstitute/gatk/pull/6028:187,Security,validat,validation,187,- Extracted the order validation for GVCF files into a separate method and included; a check to reset the counter when a new contig is found. Contigs have to; occur in continuous blocks; validation for files in which contigs occur; alternatingly is not supported.; - Added a set of integration tests for GVCF files with two and three contigs. Fixes #6023,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6028
https://github.com/broadinstitute/gatk/pull/6028:294,Testability,test,tests,294,- Extracted the order validation for GVCF files into a separate method and included; a check to reset the counter when a new contig is found. Contigs have to; occur in continuous blocks; validation for files in which contigs occur; alternatingly is not supported.; - Added a set of integration tests for GVCF files with two and three contigs. Fixes #6023,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6028
https://github.com/broadinstitute/gatk/issues/6029:147,Deployability,release,release,147,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs with --keep-combined-raw-annotations. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of (not tested). ### Description ; @ldgauthier was kind enough to introduce the `--keep-combined-raw-annotations` option for us after the discussion in issue #5698, and we've been using it extensively. We recently noticed a problem that affects a small fraction of variants though. We're noticing this with `AS_SB_TABLE` but it probably applies to all annotations that are per-allele or per-alt allele. The problem is that when GenotypeGVCFs runs it may chose to output only a subset of the alleles present in the gVCF. When it does this it does not appear to update the annotations to remove the values for the removed alleles. This results in annotations with more values than there are alleles, and no safe/predictable way to interpret those annotations since you don't know the original ordering of alleles and which ones were removed when looking at the resulting VCF. This is happening, in my case, primarily at homopolymer sites and occasionally at STRs with larger repeat units. I've attached a zip file - [AS_SB_TABLE_bug.zip](https://github.com/broadinstitute/gatk/files/3357101/AS_SB_TABLE_bug.zip) - which contains a one-record gVCF, the command to generate the VCF and the resulting VCF, which should be sufficient to demonstrate the problem and reproduce it. Here's what an offending variant looks like:. ```; chr1 100366446 . GTT G 562.64 . AC=1;AF=0.500;AN=2;AS_SB_TABLE=19,6|16,6|4,0|2,2|1,1;...;REF_BASES=ATGTTTTTTTGTTTTTTTTTT;RPA=13,11;RU=T;ReadPosRankSum=-1.296e+00;SOR=0.534;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:25,22:57:19,16:4,4:99:570,0,819; ```. #### Steps to reproduce; See attached zip file. #### Expected behavior; All per-allele and per-alt-allele annotations should be subsetted to only the values for the alleles that are output in the resulting VCF. #### Actual behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6029
https://github.com/broadinstitute/gatk/issues/6029:765,Deployability,update,update,765,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs with --keep-combined-raw-annotations. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of (not tested). ### Description ; @ldgauthier was kind enough to introduce the `--keep-combined-raw-annotations` option for us after the discussion in issue #5698, and we've been using it extensively. We recently noticed a problem that affects a small fraction of variants though. We're noticing this with `AS_SB_TABLE` but it probably applies to all annotations that are per-allele or per-alt allele. The problem is that when GenotypeGVCFs runs it may chose to output only a subset of the alleles present in the gVCF. When it does this it does not appear to update the annotations to remove the values for the removed alleles. This results in annotations with more values than there are alleles, and no safe/predictable way to interpret those annotations since you don't know the original ordering of alleles and which ones were removed when looking at the resulting VCF. This is happening, in my case, primarily at homopolymer sites and occasionally at STRs with larger repeat units. I've attached a zip file - [AS_SB_TABLE_bug.zip](https://github.com/broadinstitute/gatk/files/3357101/AS_SB_TABLE_bug.zip) - which contains a one-record gVCF, the command to generate the VCF and the resulting VCF, which should be sufficient to demonstrate the problem and reproduce it. Here's what an offending variant looks like:. ```; chr1 100366446 . GTT G 562.64 . AC=1;AF=0.500;AN=2;AS_SB_TABLE=19,6|16,6|4,0|2,2|1,1;...;REF_BASES=ATGTTTTTTTGTTTTTTTTTT;RPA=13,11;RU=T;ReadPosRankSum=-1.296e+00;SOR=0.534;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:25,22:57:19,16:4,4:99:570,0,819; ```. #### Steps to reproduce; See attached zip file. #### Expected behavior; All per-allele and per-alt-allele annotations should be subsetted to only the values for the alleles that are output in the resulting VCF. #### Actual behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6029
https://github.com/broadinstitute/gatk/issues/6029:910,Safety,safe,safe,910,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs with --keep-combined-raw-annotations. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of (not tested). ### Description ; @ldgauthier was kind enough to introduce the `--keep-combined-raw-annotations` option for us after the discussion in issue #5698, and we've been using it extensively. We recently noticed a problem that affects a small fraction of variants though. We're noticing this with `AS_SB_TABLE` but it probably applies to all annotations that are per-allele or per-alt allele. The problem is that when GenotypeGVCFs runs it may chose to output only a subset of the alleles present in the gVCF. When it does this it does not appear to update the annotations to remove the values for the removed alleles. This results in annotations with more values than there are alleles, and no safe/predictable way to interpret those annotations since you don't know the original ordering of alleles and which ones were removed when looking at the resulting VCF. This is happening, in my case, primarily at homopolymer sites and occasionally at STRs with larger repeat units. I've attached a zip file - [AS_SB_TABLE_bug.zip](https://github.com/broadinstitute/gatk/files/3357101/AS_SB_TABLE_bug.zip) - which contains a one-record gVCF, the command to generate the VCF and the resulting VCF, which should be sufficient to demonstrate the problem and reproduce it. Here's what an offending variant looks like:. ```; chr1 100366446 . GTT G 562.64 . AC=1;AF=0.500;AN=2;AS_SB_TABLE=19,6|16,6|4,0|2,2|1,1;...;REF_BASES=ATGTTTTTTTGTTTTTTTTTT;RPA=13,11;RU=T;ReadPosRankSum=-1.296e+00;SOR=0.534;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:25,22:57:19,16:4,4:99:570,0,819; ```. #### Steps to reproduce; See attached zip file. #### Expected behavior; All per-allele and per-alt-allele annotations should be subsetted to only the values for the alleles that are output in the resulting VCF. #### Actual behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6029
https://github.com/broadinstitute/gatk/issues/6029:915,Safety,predict,predictable,915,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs with --keep-combined-raw-annotations. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of (not tested). ### Description ; @ldgauthier was kind enough to introduce the `--keep-combined-raw-annotations` option for us after the discussion in issue #5698, and we've been using it extensively. We recently noticed a problem that affects a small fraction of variants though. We're noticing this with `AS_SB_TABLE` but it probably applies to all annotations that are per-allele or per-alt allele. The problem is that when GenotypeGVCFs runs it may chose to output only a subset of the alleles present in the gVCF. When it does this it does not appear to update the annotations to remove the values for the removed alleles. This results in annotations with more values than there are alleles, and no safe/predictable way to interpret those annotations since you don't know the original ordering of alleles and which ones were removed when looking at the resulting VCF. This is happening, in my case, primarily at homopolymer sites and occasionally at STRs with larger repeat units. I've attached a zip file - [AS_SB_TABLE_bug.zip](https://github.com/broadinstitute/gatk/files/3357101/AS_SB_TABLE_bug.zip) - which contains a one-record gVCF, the command to generate the VCF and the resulting VCF, which should be sufficient to demonstrate the problem and reproduce it. Here's what an offending variant looks like:. ```; chr1 100366446 . GTT G 562.64 . AC=1;AF=0.500;AN=2;AS_SB_TABLE=19,6|16,6|4,0|2,2|1,1;...;REF_BASES=ATGTTTTTTTGTTTTTTTTTT;RPA=13,11;RU=T;ReadPosRankSum=-1.296e+00;SOR=0.534;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:25,22:57:19,16:4,4:99:570,0,819; ```. #### Steps to reproduce; See attached zip file. #### Expected behavior; All per-allele and per-alt-allele annotations should be subsetted to only the values for the alleles that are output in the resulting VCF. #### Actual behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6029
https://github.com/broadinstitute/gatk/issues/6029:213,Testability,test,tested,213,"## Bug Report. ### Affected tool(s) or class(es); GenotypeGVCFs with --keep-combined-raw-annotations. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of (not tested). ### Description ; @ldgauthier was kind enough to introduce the `--keep-combined-raw-annotations` option for us after the discussion in issue #5698, and we've been using it extensively. We recently noticed a problem that affects a small fraction of variants though. We're noticing this with `AS_SB_TABLE` but it probably applies to all annotations that are per-allele or per-alt allele. The problem is that when GenotypeGVCFs runs it may chose to output only a subset of the alleles present in the gVCF. When it does this it does not appear to update the annotations to remove the values for the removed alleles. This results in annotations with more values than there are alleles, and no safe/predictable way to interpret those annotations since you don't know the original ordering of alleles and which ones were removed when looking at the resulting VCF. This is happening, in my case, primarily at homopolymer sites and occasionally at STRs with larger repeat units. I've attached a zip file - [AS_SB_TABLE_bug.zip](https://github.com/broadinstitute/gatk/files/3357101/AS_SB_TABLE_bug.zip) - which contains a one-record gVCF, the command to generate the VCF and the resulting VCF, which should be sufficient to demonstrate the problem and reproduce it. Here's what an offending variant looks like:. ```; chr1 100366446 . GTT G 562.64 . AC=1;AF=0.500;AN=2;AS_SB_TABLE=19,6|16,6|4,0|2,2|1,1;...;REF_BASES=ATGTTTTTTTGTTTTTTTTTT;RPA=13,11;RU=T;ReadPosRankSum=-1.296e+00;SOR=0.534;STR GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:25,22:57:19,16:4,4:99:570,0,819; ```. #### Steps to reproduce; See attached zip file. #### Expected behavior; All per-allele and per-alt-allele annotations should be subsetted to only the values for the alleles that are output in the resulting VCF. #### Actual behavi",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6029
https://github.com/broadinstitute/gatk/issues/6031:759,Deployability,release,release,759,"## Bug Report. We're finding that in rare instances that `GenotypeGVCFs` can emit a variant with a spanned allele (`*`), and a genotype that references the spanned allele, but fail to emit the upstream spanning variant. This seems like a bug to me - either the spanning variant should be emitted _or_ the spanned allele should revert to a reference call. FWIW I have a sneaking suspicion that this is related to setting a non-zero value for `-stand-call-conf` (see #5793). My guess is that in one part of the code it determines the upstream variant _will_ be emitted so retains the allele as spanned, but then somewhere later the upstream variant is filtered out. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [not tested]. ### Description ; Here's the example from the VCF in the attached zip file:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test_sample; chr17 46806234 . TC T 148.64 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:208,25:239:116,16:90,8:99:156,0,6824; chr17 46806237 . TTCTCTCTCTCTC TTCTC,* 1528.04 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 1/2:3,60,33:174:1,29,20:1,21,11:99:3633,1088,2142,1538,0,3285; ```. You can see from this that a) the first variant does not have a spanned allele, implying that there cannot be a spanning event further upstream and b) the second variant has a spanned allele that is present in the `1/2` genotype. #### Steps to reproduce. The attached zip file contains a reduced test case with a 3-record gVCF and a 2-record VCF that exhibits the problem. To reproduce:. 1. Unzip the attached zip file; 2. Edit `command.sh` to put in the path to HG19; 3. Run `. command.sh` in the directory with the extracted files. #### Expected behavior; Either the spanning variant should be emitted, or the spanned allele should not be. #### Actual behavior; A spanned allele is emitted when there is no spanning variant!. ZIP file with test case: [spanned_allele_not_spanne",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6031
https://github.com/broadinstitute/gatk/issues/6031:1510,Energy Efficiency,reduce,reduced,1510," spanned allele (`*`), and a genotype that references the spanned allele, but fail to emit the upstream spanning variant. This seems like a bug to me - either the spanning variant should be emitted _or_ the spanned allele should revert to a reference call. FWIW I have a sneaking suspicion that this is related to setting a non-zero value for `-stand-call-conf` (see #5793). My guess is that in one part of the code it determines the upstream variant _will_ be emitted so retains the allele as spanned, but then somewhere later the upstream variant is filtered out. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [not tested]. ### Description ; Here's the example from the VCF in the attached zip file:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test_sample; chr17 46806234 . TC T 148.64 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:208,25:239:116,16:90,8:99:156,0,6824; chr17 46806237 . TTCTCTCTCTCTC TTCTC,* 1528.04 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 1/2:3,60,33:174:1,29,20:1,21,11:99:3633,1088,2142,1538,0,3285; ```. You can see from this that a) the first variant does not have a spanned allele, implying that there cannot be a spanning event further upstream and b) the second variant has a spanned allele that is present in the `1/2` genotype. #### Steps to reproduce. The attached zip file contains a reduced test case with a 3-record gVCF and a 2-record VCF that exhibits the problem. To reproduce:. 1. Unzip the attached zip file; 2. Edit `command.sh` to put in the path to HG19; 3. Run `. command.sh` in the directory with the extracted files. #### Expected behavior; Either the spanning variant should be emitted, or the spanned allele should not be. #### Actual behavior; A spanned allele is emitted when there is no spanning variant!. ZIP file with test case: [spanned_allele_not_spanned.zip](https://github.com/broadinstitute/gatk/files/3374898/spanned_allele_not_spanned.zip). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6031
https://github.com/broadinstitute/gatk/issues/6031:824,Testability,test,tested,824,"## Bug Report. We're finding that in rare instances that `GenotypeGVCFs` can emit a variant with a spanned allele (`*`), and a genotype that references the spanned allele, but fail to emit the upstream spanning variant. This seems like a bug to me - either the spanning variant should be emitted _or_ the spanned allele should revert to a reference call. FWIW I have a sneaking suspicion that this is related to setting a non-zero value for `-stand-call-conf` (see #5793). My guess is that in one part of the code it determines the upstream variant _will_ be emitted so retains the allele as spanned, but then somewhere later the upstream variant is filtered out. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [not tested]. ### Description ; Here's the example from the VCF in the attached zip file:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test_sample; chr17 46806234 . TC T 148.64 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:208,25:239:116,16:90,8:99:156,0,6824; chr17 46806237 . TTCTCTCTCTCTC TTCTC,* 1528.04 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 1/2:3,60,33:174:1,29,20:1,21,11:99:3633,1088,2142,1538,0,3285; ```. You can see from this that a) the first variant does not have a spanned allele, implying that there cannot be a spanning event further upstream and b) the second variant has a spanned allele that is present in the `1/2` genotype. #### Steps to reproduce. The attached zip file contains a reduced test case with a 3-record gVCF and a 2-record VCF that exhibits the problem. To reproduce:. 1. Unzip the attached zip file; 2. Edit `command.sh` to put in the path to HG19; 3. Run `. command.sh` in the directory with the extracted files. #### Expected behavior; Either the spanning variant should be emitted, or the spanned allele should not be. #### Actual behavior; A spanned allele is emitted when there is no spanning variant!. ZIP file with test case: [spanned_allele_not_spanne",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6031
https://github.com/broadinstitute/gatk/issues/6031:1518,Testability,test,test,1518," spanned allele (`*`), and a genotype that references the spanned allele, but fail to emit the upstream spanning variant. This seems like a bug to me - either the spanning variant should be emitted _or_ the spanned allele should revert to a reference call. FWIW I have a sneaking suspicion that this is related to setting a non-zero value for `-stand-call-conf` (see #5793). My guess is that in one part of the code it determines the upstream variant _will_ be emitted so retains the allele as spanned, but then somewhere later the upstream variant is filtered out. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [not tested]. ### Description ; Here's the example from the VCF in the attached zip file:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test_sample; chr17 46806234 . TC T 148.64 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:208,25:239:116,16:90,8:99:156,0,6824; chr17 46806237 . TTCTCTCTCTCTC TTCTC,* 1528.04 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 1/2:3,60,33:174:1,29,20:1,21,11:99:3633,1088,2142,1538,0,3285; ```. You can see from this that a) the first variant does not have a spanned allele, implying that there cannot be a spanning event further upstream and b) the second variant has a spanned allele that is present in the `1/2` genotype. #### Steps to reproduce. The attached zip file contains a reduced test case with a 3-record gVCF and a 2-record VCF that exhibits the problem. To reproduce:. 1. Unzip the attached zip file; 2. Edit `command.sh` to put in the path to HG19; 3. Run `. command.sh` in the directory with the extracted files. #### Expected behavior; Either the spanning variant should be emitted, or the spanned allele should not be. #### Actual behavior; A spanned allele is emitted when there is no spanning variant!. ZIP file with test case: [spanned_allele_not_spanned.zip](https://github.com/broadinstitute/gatk/files/3374898/spanned_allele_not_spanned.zip). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6031
https://github.com/broadinstitute/gatk/issues/6031:1964,Testability,test,test,1964," spanned allele (`*`), and a genotype that references the spanned allele, but fail to emit the upstream spanning variant. This seems like a bug to me - either the spanning variant should be emitted _or_ the spanned allele should revert to a reference call. FWIW I have a sneaking suspicion that this is related to setting a non-zero value for `-stand-call-conf` (see #5793). My guess is that in one part of the code it determines the upstream variant _will_ be emitted so retains the allele as spanned, but then somewhere later the upstream variant is filtered out. ### Affected tool(s) or class(es); GenotypeGVCFs. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [not tested]. ### Description ; Here's the example from the VCF in the attached zip file:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT test_sample; chr17 46806234 . TC T 148.64 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 0/1:208,25:239:116,16:90,8:99:156,0,6824; chr17 46806237 . TTCTCTCTCTCTC TTCTC,* 1528.04 . ... GT:AD:DP:F1R2:F2R1:GQ:PL 1/2:3,60,33:174:1,29,20:1,21,11:99:3633,1088,2142,1538,0,3285; ```. You can see from this that a) the first variant does not have a spanned allele, implying that there cannot be a spanning event further upstream and b) the second variant has a spanned allele that is present in the `1/2` genotype. #### Steps to reproduce. The attached zip file contains a reduced test case with a 3-record gVCF and a 2-record VCF that exhibits the problem. To reproduce:. 1. Unzip the attached zip file; 2. Edit `command.sh` to put in the path to HG19; 3. Run `. command.sh` in the directory with the extracted files. #### Expected behavior; Either the spanning variant should be emitted, or the spanned allele should not be. #### Actual behavior; A spanned allele is emitted when there is no spanning variant!. ZIP file with test case: [spanned_allele_not_spanned.zip](https://github.com/broadinstitute/gatk/files/3374898/spanned_allele_not_spanned.zip). ----",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6031
https://github.com/broadinstitute/gatk/pull/6033:125,Modifiability,inherit,inheriting,125,"- Created FuncotationFactory as new base class for DataSourceFuncotationFactory; - Created ComputedFuncotationFactory class, inheriting from FuncotationFactory and acting as a base class for GCContentFuncotationFactory and ReferenceContextFuncotationFactory; - Extracted GC content calculation and reference context annotations from previous classes; - Created two new arguments for reference window size and gc content window size; - Created unit tests for GCContent- and ReferenceContextFuncotationFactories; - Regenerated validation files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6033
https://github.com/broadinstitute/gatk/pull/6033:525,Security,validat,validation,525,"- Created FuncotationFactory as new base class for DataSourceFuncotationFactory; - Created ComputedFuncotationFactory class, inheriting from FuncotationFactory and acting as a base class for GCContentFuncotationFactory and ReferenceContextFuncotationFactory; - Extracted GC content calculation and reference context annotations from previous classes; - Created two new arguments for reference window size and gc content window size; - Created unit tests for GCContent- and ReferenceContextFuncotationFactories; - Regenerated validation files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6033
https://github.com/broadinstitute/gatk/pull/6033:448,Testability,test,tests,448,"- Created FuncotationFactory as new base class for DataSourceFuncotationFactory; - Created ComputedFuncotationFactory class, inheriting from FuncotationFactory and acting as a base class for GCContentFuncotationFactory and ReferenceContextFuncotationFactory; - Extracted GC content calculation and reference context annotations from previous classes; - Created two new arguments for reference window size and gc content window size; - Created unit tests for GCContent- and ReferenceContextFuncotationFactories; - Regenerated validation files",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6033
https://github.com/broadinstitute/gatk/pull/6034:524,Availability,recover,recovery,524,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034
https://github.com/broadinstitute/gatk/pull/6034:140,Deployability,toggle,toggle,140,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034
https://github.com/broadinstitute/gatk/pull/6034:524,Safety,recover,recovery,524,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034
https://github.com/broadinstitute/gatk/pull/6034:789,Security,validat,validate,789,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034
https://github.com/broadinstitute/gatk/pull/6034:882,Testability,Test,Tests,882,"This is a prototype of the basic infrastructure that must go in to make the junction tree based Haplotype finding work. I have pulled out a toggle for the HaplotypeCaller that that enables a separate ReadThreadingAssembler codepath for haplotype finding. Right now when this mode is enabled `ExperimentalReadThreadingAssembler` is used in conjunction with `JuncitonTreeKBestHalotypeFinder` to extract only haplotypes that show up in our junction trees with evidence of > 3 reads. This still poses problems with dangling end recovery as definitionally those branches never include complete junction tree data. . I will continue to work on this branch (as it is in a somewhat rough state still) but I would like to at least get some eyes on it before i get too deep in the weeds to at least validate the structural approach I have chosen. . Currently known issues in this branch: ; - Tests are failing due to resolution of non-unique reference sink vertexes, I would solicit help as to how best to resolve the case where junction trees point to both a reference stop allele and a continued path.; - There is at least one very degenerate edge case that might cause the code to hang, I would also ask after what is the best way to close out of looping assembly structures that never have reads to close them (i.e. a ""dangling end"" hom-var that happens to point to a non-unique reference base). ; - Probably after discussion the threshold for discarding junction trees will be changed to instead use paths from the discarded tree first. . Resolves #5925",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6034
https://github.com/broadinstitute/gatk/issues/6035:351,Availability,recover,recovery,351,A potential avenue of improvement to the Junciton Tree code would be to include mate information when generating junction trees. We likely would want to implement this feature in one form or another if we choose to expand the active region size for HaplotypeCaller. This would have the advantage of significantly improving our junction tree haplotype recovery range be ~ associated with insert size. Unfortunately this requires thought in order to figure out how to handle resolving the missing insert sequences. A solution to the insert problem will likely be closely linked to the solution to #5924.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6035
https://github.com/broadinstitute/gatk/issues/6035:351,Safety,recover,recovery,351,A potential avenue of improvement to the Junciton Tree code would be to include mate information when generating junction trees. We likely would want to implement this feature in one form or another if we choose to expand the active region size for HaplotypeCaller. This would have the advantage of significantly improving our junction tree haplotype recovery range be ~ associated with insert size. Unfortunately this requires thought in order to figure out how to handle resolving the missing insert sequences. A solution to the insert problem will likely be closely linked to the solution to #5924.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6035
https://github.com/broadinstitute/gatk/issues/6037:140,Availability,down,download,140,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:221,Availability,down,download,221,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:743,Availability,echo,echo,743,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:775,Availability,echo,echo,775,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:822,Availability,echo,echo,822,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:882,Availability,echo,echo,882,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:1249,Availability,echo,echo,1249,"nzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.addGivenAlleles(AssemblyBasedCallerUtils.java:350); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:291); 	at org.broadinstitute.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:1299,Availability,echo,echo,1299,"01405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.addGivenAlleles(AssemblyBasedCallerUtils.java:350); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:291); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:1340,Availability,echo,echo,1340,"01405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.addGivenAlleles(AssemblyBasedCallerUtils.java:350); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:291); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:1691,Availability,error,error,1691,"na \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.addGivenAlleles(AssemblyBasedCallerUtils.java:350); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:291); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:3479,Availability,error,error,3479,"_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.addGivenAlleles(AssemblyBasedCallerUtils.java:350); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.AssemblyBasedCallerUtils.assembleReads(AssemblyBasedCallerUtils.java:291); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCallerEngine.callRegion(HaplotypeCallerEngine.java:542); 	at org.broadinstitute.hellbender.tools.walkers.haplotypecaller.HaplotypeCaller.apply(HaplotypeCaller.java:240); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. Somehow the HaplotypeCaller seems to have some bug in GENOTYPE_GIVEN_ALLELES mode and when the VCF file for the given alleles contains a very large indel it ends up giving a cryptic error, regardless of what is contained in the bam file.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:131,Deployability,release,releases,131,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6037:212,Deployability,release,releases,212,"I obtain this reproducible issue with gatk 4.1.2.0:. Using the following code:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chrX\t1052617\t.\tC\tCAAAGGCTGCAATGTGAATGAATTTTTGGAAATAGCCCTAATGCTCATCTATGAAGGAGTGATAAACACAGCATCCTTTATCCATGCAATGGAATATTATGCAGTCTAGAAAAGGAATAAGGCTCTGACAAAAGACTGCAATATGTATGAATTTTGGAAACAGCCCTACTGCCCATCTATAAAGGAATGGATAAACACAGCATAGTTCATCTATACAATGCAATATTATAATGGAATATTATGCAGCCTGGAACAGGAACAAGGCTCTGAG\t.\t.\t."") | \; bgzip > input.vcf.gz; \; tabix -f input.vcf.gz. (echo -e ""@HD\tVN:1.6\tGO:none\tSO:coordinate""; \; echo -e ""@SQ\tSN:chrX\tLN:156040895""; \; echo -e ""@RG\tID:ID\tPL:ILLUMINA\tPU:ID\tLB:LIBRARY\tSM:SAMPLE"") | \; samtools view -Sb -o input.bam; \; samtools index input.bam. gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz; ```. I get the following error:. ```; java.lang.IllegalArgumentException: Cigar cannot be null; 	at org.broadinstitute.hellbender.utils.read.AlignmentUtils.consolidateCigar(AlignmentUtils.java:716); 	at org.broadinstitute.hellbender.utils.haplotype.Haplotype.setCigar(Haplotype.java:193); 	at org.broadinstitute.hellbender.tools.walker",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6037
https://github.com/broadinstitute/gatk/issues/6038:71,Usability,undo,undo,71,"When both panel of normals and sample are GC bias corrected, does that undo all the work you do for GC bias? As in if you apply the kernel to both normals and sample and then divide the sample by the normals, that should also divide out the kernel? It's hard to tell from going through the code.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6038
https://github.com/broadinstitute/gatk/pull/6039:233,Deployability,pipeline,pipeline,233,"This extends Variant Eval to compare AFs between variants in binned AF buckets based on Thousand Genomes VCF, between the expected AF from Thousand Genomes and the seen one in the actual VCF, to be used as a QC metric for our arrays pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6039
https://github.com/broadinstitute/gatk/pull/6039:5,Modifiability,extend,extends,5,"This extends Variant Eval to compare AFs between variants in binned AF buckets based on Thousand Genomes VCF, between the expected AF from Thousand Genomes and the seen one in the actual VCF, to be used as a QC metric for our arrays pipeline.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6039
https://github.com/broadinstitute/gatk/issues/6041:708,Integrability,rout,route,708,"## Feature request. ### Tool(s) or class(es) involved; HaplotypeCaller, BaseRecalibrator. ### Description; We'd find it useful for HaplotypeCaller to be able to apply a recalibration table file (as it was able to in v3 with `-BQSR`) rather than have to apply it to the input BAM as a separate step. ### Rationale; We like to provide CRAM files with the guarantee that they contain all the basecalls and qualities from the instrument. We'd also like to make it easy for users of the CRAM files to use them with HaplotypeCaller with the recommended base quality recalibration - so we'd value the return of the `-BQSR` option to HaplotypeCaller to make the easy and avoid extra IO. N.b. we'd not want to go the route of putting original qualities in the `OQ` auxtag for fear of bloating our files too much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6041
https://github.com/broadinstitute/gatk/issues/6041:663,Safety,avoid,avoid,663,"## Feature request. ### Tool(s) or class(es) involved; HaplotypeCaller, BaseRecalibrator. ### Description; We'd find it useful for HaplotypeCaller to be able to apply a recalibration table file (as it was able to in v3 with `-BQSR`) rather than have to apply it to the input BAM as a separate step. ### Rationale; We like to provide CRAM files with the guarantee that they contain all the basecalls and qualities from the instrument. We'd also like to make it easy for users of the CRAM files to use them with HaplotypeCaller with the recommended base quality recalibration - so we'd value the return of the `-BQSR` option to HaplotypeCaller to make the easy and avoid extra IO. N.b. we'd not want to go the route of putting original qualities in the `OQ` auxtag for fear of bloating our files too much.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6041
https://github.com/broadinstitute/gatk/pull/6044:7,Testability,test,tested,7,I have tested that this explicitly works on the users data. I decided it was simplest to just check for mis-trimming at the very last stage. I'm a little weary about the change of the locus for the ref context from being the culledVC to being the mergedVC. . Fixes #5994,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6044
https://github.com/broadinstitute/gatk/pull/6044:77,Usability,simpl,simplest,77,I have tested that this explicitly works on the users data. I decided it was simplest to just check for mis-trimming at the very last stage. I'm a little weary about the change of the locus for the ref context from being the culledVC to being the mergedVC. . Fixes #5994,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6044
https://github.com/broadinstitute/gatk/issues/6045:99,Availability,down,download,99,"I have managed to generate a minimal bam file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:342,Availability,down,download,342,"I have managed to generate a minimal bam file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:423,Availability,down,download,423,"I have managed to generate a minimal bam file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:945,Availability,echo,echo,945,"I have managed to generate a minimal bam file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:977,Availability,echo,echo,977," file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. When `--min-base-quality-score 12` is us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:1024,Availability,echo,echo,1024," file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. When `--min-base-quality-score 12` is us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:1084,Availability,echo,echo,1084," file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. When `--min-base-quality-score 12` is us",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:1131,Availability,echo,echo,1131," dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. When `--min-base-quality-score 12` is used, it is as if HaplotypeCaller invents some reference allele reads and then uses them to genotype th",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:2290,Availability,error,error,2290,"ller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. When `--min-base-quality-score 12` is used, it is as if HaplotypeCaller invents some reference allele reads and then uses them to genotype the variant as heterozygous. I have seen issues like this all over the genome. Interestingly, if I run the same code on my own Ubuntu laptop, I get an error instead:; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method lambda$identity$d67512bf$1(Lorg/broadinstitute/hellbender/utils/read/GATKRead;)Lorg/broadinstitute/hellbender/utils/read/GATKRead; at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:276); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandL",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:333,Deployability,release,releases,333,"I have managed to generate a minimal bam file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/issues/6045:414,Deployability,release,releases,414,"I have managed to generate a minimal bam file that reproduces the issue. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/xae79hanumpireu/AABKo1l4Y-z5G5YLBqSpylRva?dl=0. Then the following code will reproduce the issue:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.2.0/gatk-4.1.2.0.zip; unzip gatk-4.1.2.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict. (echo ""##fileformat=VCFv4.2""; \; echo ""##contig=<ID=chrX,length=156040895>""; \; echo -e ""#CHROM\tPOS\tID\tREF\tALT\tQUAL\tFILTER\tINFO""; \; echo -e ""chr1\t97329945\t.\tT\tA\t.\t.\t.""; \; echo -e ""chr1\t97329967\t.\tC\tT\t.\t.\t."") | bgzip > input.vcf.gz && \; tabix -f input.vcf.gz. for score in 11 12; do; gatk-4.1.2.0/gatk HaplotypeCaller \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.$score.vcf.gz \; --genotyping-mode GENOTYPE_GIVEN_ALLELES \; --alleles input.vcf.gz \; -L chr1:97329945-97329967 \; --min-base-quality-score $score && \; bcftools query \; -f ""[%CHROM\t%POS\t%REF\t%ALT\t%GT\t%AD\n]"" \; output.$score.vcf.gz \; -r chr1:97329945-97329967; done; ```. When the parameter `--min-base-quality-score 11` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	1/1	0,35; chr1	97329967	C	T	1/1	0,33; ```; When the parameter `--min-base-quality-score 12` is used, the GT/AD output is this:; ```; chr1	97329945	T	A	0/1	9,10; chr1	97329967	C	T	0/1	6,11; ```; The first output is the output that makes sense. W",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6045
https://github.com/broadinstitute/gatk/pull/6046:32,Testability,test,tests,32,An experiment related to failed tests in #6042,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6046
https://github.com/broadinstitute/gatk/pull/6049:110,Availability,down,down,110,This branch (or some variation therin) will be necessary to keep track of how well #5980 is doing at bringing down the number of found haplotypes.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6049
https://github.com/broadinstitute/gatk/issues/6050:2145,Deployability,release,release,2145,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:780,Modifiability,Config,Config,780,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:803,Modifiability,Config,Config,803,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1231,Modifiability,Config,ConfigCache,1231,"/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1255,Modifiability,Config,ConfigCache,1255,"2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### S",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1299,Modifiability,Config,ConfigCache,1299,g.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1323,Modifiability,Config,ConfigCache,1323,: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will h,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1384,Modifiability,config,config,1384,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1391,Modifiability,Config,ConfigFactory,1391,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1417,Modifiability,Config,ConfigFactory,1417,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1481,Modifiability,config,config,1481,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1488,Modifiability,Config,ConfigFactory,1488,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1528,Modifiability,Config,ConfigFactory,1528,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1592,Modifiability,config,config,1592,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1599,Modifiability,Config,ConfigFactory,1599,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1657,Modifiability,Config,ConfigFactory,1657,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1721,Modifiability,config,config,1721,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1728,Modifiability,Config,ConfigFactory,1728,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1786,Modifiability,Config,ConfigFactory,1786,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:506,Performance,load,load,506,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:555,Performance,load,loaders,555,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:580,Performance,load,load,580,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:634,Performance,load,loaders,634,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:659,Performance,load,load,659,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:713,Performance,Load,LoadersManager,713,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:728,Performance,load,load,728,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:733,Performance,Load,LoadersManager,733,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:787,Performance,Load,LoadType,787,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:798,Performance,load,load,798,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:937,Performance,load,load,937,"## Bug Report. java.lang.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellben",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:1011,Performance,load,load,1011,"ng.OutOfMemoryError when 'java -jar /usr/hpc-bio/gatk/gatk-package-4.1.2.0-local.jar' sometimes, but there is a lot of memeory yet. And then all features can not be used. This is the call stack.; ```; java -jar gatk/gatk-package-4.1.2.0-local.jar; Exception in thread ""main"" java.lang.OutOfMemoryError: Requested array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Mai",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:2229,Testability,test,test,2229,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:2241,Testability,test,tested,2241,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/issues/6050:2282,Usability,clear,clear,2282,ed array size exceeds VM limit; at java.util.Properties$LineReader.readLine(Properties.java:485); at java.util.Properties.load0(Properties.java:353); at java.util.Properties.load(Properties.java:317); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:50); at org.aeonbits.owner.loaders.PropertiesLoader.load(PropertiesLoader.java:43); at org.aeonbits.owner.LoadersManager.load(LoadersManager.java:46); at org.aeonbits.owner.Config$LoadType$2.load(Config.java:129); at org.aeonbits.owner.PropertiesManager.doLoad(PropertiesManager.java:290); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:163); at org.aeonbits.owner.PropertiesManager.load(PropertiesManager.java:153); at org.aeonbits.owner.PropertiesInvocationHandler.<init>(PropertiesInvocationHandler.java:54); at org.aeonbits.owner.DefaultFactory.create(DefaultFactory.java:46); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:87); at org.aeonbits.owner.ConfigCache.getOrCreate(ConfigCache.java:40); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreate(ConfigFactory.java:268); at org.broadinstitute.hellbender.utils.config.ConfigFactory.getOrCreateConfigFromFile(ConfigFactory.java:454); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:439); at org.broadinstitute.hellbender.utils.config.ConfigFactory.initializeConfigurationsFromCommandLineArgs(ConfigFactory.java:414); at org.broadinstitute.hellbender.Main.parseArgsForConfigSetup(Main.java:121); at org.broadinstitute.hellbender.Main.setupConfigAndExtractProgram(Main.java:179); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:204); at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. ### Affected version(s); - [x] Latest public release version [version?]; Yes. 4.1.2.0. - [ ] Latest master branch as of [date of test?]; Not tested. #### Steps to reproduce; Yet not clear.; maybe the call stack above will help. ----,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6050
https://github.com/broadinstitute/gatk/pull/6051:0,Testability,test,testing,0,testing to see if codecov reports better if we disable push builds,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6051
https://github.com/broadinstitute/gatk/pull/6052:0,Testability,test,testing,0,testing if removing the codecov upload line in our yml disables the reporting on the pr,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6052
https://github.com/broadinstitute/gatk/issues/6053:33,Availability,avail,available,33,"## Bug Report; JDK8 is no longer available for the current stable Debian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/issues/6053:1840,Availability,error,error,1840,"ian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. #### Expected behavior; Program runs :). #### Actual behavior; Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/issues/6053:2064,Availability,Error,Error,2064,"ian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. #### Expected behavior; Program runs :). #### Actual behavior; Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/issues/6053:73,Deployability,release,release,73,"## Bug Report; JDK8 is no longer available for the current stable Debian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/issues/6053:128,Deployability,install,install,128,"## Bug Report; JDK8 is no longer available for the current stable Debian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/issues/6053:362,Deployability,release,release,362,"## Bug Report; JDK8 is no longer available for the current stable Debian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/issues/6053:1894,Deployability,install,install,1894,"ian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. #### Expected behavior; Program runs :). #### Actual behavior; Error",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/issues/6053:180,Integrability,depend,dependency,180,"## Bug Report; JDK8 is no longer available for the current stable Debian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/issues/6053:430,Testability,test,test,430,"## Bug Report; JDK8 is no longer available for the current stable Debian release (buster). Trying to run gatk with an OpenJDK11 install fails. I anticipate a WONTFIX since this is dependency related, but I figured it would be good to let people know. ### Affected tool(s) or class(es); GATKRead, probably others too. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [didn't test]. ### Description ; ```; Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Inconsistent constant pool data in classfile for class org/broadinstitute/hellbender/transformers/ReadTransformer. Method 'org.broadinstitute.hellbender.utils.read.GATKRead lambda$identity$d67512bf$1(org.broadinstitute.hellbender.utils.read.GATKRead)' at index 65 is CONSTANT_MethodRef and should be CONSTANT_InterfaceMethodRef; 	at org.broadinstitute.hellbender.transformers.ReadTransformer.identity(ReadTransformer.java:30); 	at org.broadinstitute.hellbender.engine.GATKTool.makePreReadFilterTransformer(GATKTool.java:345); 	at org.broadinstitute.hellbender.engine.GATKTool.getTransformedReadStream(GATKTool.java:374); 	at org.broadinstitute.hellbender.engine.ReadWalker.traverse(ReadWalker.java:93); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); ```. This error seems related to the JRE version. You can still install JDK8 manually but that's not ideal for many users. #### Steps to reproduce; Run GATK on OpenJDK11. ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6053
https://github.com/broadinstitute/gatk/pull/6054:48,Modifiability,inherit,inherit,48,This allows `PrintReads` and other classes that inherit from `ReadWalker` to split files over an arbitrary number of intervals and not get any repeated reads across all of the split data files. - Added reads-must-start-within-intervals flag to ReadWalker to allow for; splitting of files over an interval set without seeing repeats. - Added a new iterator type to filter reads by this interval.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6054
https://github.com/broadinstitute/gatk/issues/6056:383,Security,validat,validation-stringency,383,"HI, ; the following commands were included at my bash script .; after I ran them, I got a log file with ""Tool returned:; 6785087"", I am not sure why the return code is 6785087 , not 0 ???. Anything wrong with my commands ?; Thanks ! . gatk-4.1.2.0/gatk BaseRecalibrator -R $fasta -I $tumor_bam --known-sites a.vcf --known-sites b.vcf ; --intervals t.bed --interval-padding 50 --read-validation-stringency SILENT -O recal_data.table . gatk-4.1.2.0/gatk ApplyBQSR -R $fasta -I $tumor_bam --bqsr-recal-file recal_data.table ; --intervals t.bed --interval-padding 50 --read-validation-stringency SILENT -O recal_data.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6056
https://github.com/broadinstitute/gatk/issues/6056:570,Security,validat,validation-stringency,570,"HI, ; the following commands were included at my bash script .; after I ran them, I got a log file with ""Tool returned:; 6785087"", I am not sure why the return code is 6785087 , not 0 ???. Anything wrong with my commands ?; Thanks ! . gatk-4.1.2.0/gatk BaseRecalibrator -R $fasta -I $tumor_bam --known-sites a.vcf --known-sites b.vcf ; --intervals t.bed --interval-padding 50 --read-validation-stringency SILENT -O recal_data.table . gatk-4.1.2.0/gatk ApplyBQSR -R $fasta -I $tumor_bam --bqsr-recal-file recal_data.table ; --intervals t.bed --interval-padding 50 --read-validation-stringency SILENT -O recal_data.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6056
https://github.com/broadinstitute/gatk/issues/6056:90,Testability,log,log,90,"HI, ; the following commands were included at my bash script .; after I ran them, I got a log file with ""Tool returned:; 6785087"", I am not sure why the return code is 6785087 , not 0 ???. Anything wrong with my commands ?; Thanks ! . gatk-4.1.2.0/gatk BaseRecalibrator -R $fasta -I $tumor_bam --known-sites a.vcf --known-sites b.vcf ; --intervals t.bed --interval-padding 50 --read-validation-stringency SILENT -O recal_data.table . gatk-4.1.2.0/gatk ApplyBQSR -R $fasta -I $tumor_bam --bqsr-recal-file recal_data.table ; --intervals t.bed --interval-padding 50 --read-validation-stringency SILENT -O recal_data.bam",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6056
https://github.com/broadinstitute/gatk/issues/6058:80,Availability,error,errors,80,"I'm trying to run Mutect2 in tumor-only mode, for a small panel, and I get this errors at the FilterMutectCalls step. ```bash; [July 26, 2019 9:34:50 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2129657856; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:225); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:209); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.lambda$applyFiltersAndAccumulateOutputStats$13(Mutect2FilteringEngine.java:176); at java.util.Optional.ifPresent(Optional.java:159); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.applyFiltersAndAccumulateOutputStats(Mutect2FilteringEngine.java:174); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:142); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:342,Availability,error,errorRate,342,"I'm trying to run Mutect2 in tumor-only mode, for a small panel, and I get this errors at the FilterMutectCalls step. ```bash; [July 26, 2019 9:34:50 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2129657856; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:225); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:209); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.lambda$applyFiltersAndAccumulateOutputStats$13(Mutect2FilteringEngine.java:176); at java.util.Optional.ifPresent(Optional.java:159); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.applyFiltersAndAccumulateOutputStats(Mutect2FilteringEngine.java:174); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:142); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:516,Availability,error,errorProbToQual,516,"I'm trying to run Mutect2 in tumor-only mode, for a small panel, and I get this errors at the FilterMutectCalls step. ```bash; [July 26, 2019 9:34:50 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2129657856; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:225); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:209); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.lambda$applyFiltersAndAccumulateOutputStats$13(Mutect2FilteringEngine.java:176); at java.util.Optional.ifPresent(Optional.java:159); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.applyFiltersAndAccumulateOutputStats(Mutect2FilteringEngine.java:174); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:142); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:608,Availability,error,errorProbToQual,608,"I'm trying to run Mutect2 in tumor-only mode, for a small panel, and I get this errors at the FilterMutectCalls step. ```bash; [July 26, 2019 9:34:50 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2129657856; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:225); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:209); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.lambda$applyFiltersAndAccumulateOutputStats$13(Mutect2FilteringEngine.java:176); at java.util.Optional.ifPresent(Optional.java:159); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.applyFiltersAndAccumulateOutputStats(Mutect2FilteringEngine.java:174); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:142); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:1810,Integrability,wrap,wrapAndCopyInto,1810,il.Optional.ifPresent(Optional.java:159); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.applyFiltersAndAccumulateOutputStats(Mutect2FilteringEngine.java:174); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:142); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbend,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:435,Security,validat,validateArg,435,"I'm trying to run Mutect2 in tumor-only mode, for a small panel, and I get this errors at the FilterMutectCalls step. ```bash; [July 26, 2019 9:34:50 AM UTC] org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls done. Elapsed time: 0.06 minutes.; Runtime.totalMemory()=2129657856; java.lang.IllegalArgumentException: errorRate must be good probability but got NaN; at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:730); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:225); at org.broadinstitute.hellbender.utils.QualityUtils.errorProbToQual(QualityUtils.java:209); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.lambda$applyFiltersAndAccumulateOutputStats$13(Mutect2FilteringEngine.java:176); at java.util.Optional.ifPresent(Optional.java:159); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.applyFiltersAndAccumulateOutputStats(Mutect2FilteringEngine.java:174); at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:142); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3121,Testability,test,test,3121,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3166,Testability,test,test,3166,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3208,Testability,test,test-unfiltered,3208,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3269,Testability,test,test,3269,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3292,Testability,test,test,3292,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3374,Testability,test,test-unfiltered,3374,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3409,Testability,test,test,3409,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3450,Testability,test,test,3450,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6058:3237,Usability,Learn,LearnReadOrientationModel,3237,java:184); at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); at java.util.Iterator.forEachRemaining(Iterator.java:116); at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /gatk/gatk-package-4.1.2.0-local.jar. ```. Here is my commands:. ```bash. gatk \; Mutect2 \; -R hg19.fa \; -I test.bam \; -L myDesign.bed \; --f1r2-tar-gz test.f1r2.tar.gz \; $filterGermline \; -O test-unfiltered.vcf. gatk \; LearnReadOrientationModel \; -I test.f1r2.tar.gz \; -O test.read-orientation-model.tar.gz. gatk \; FilterMutectCalls \; -R hg19.fa \; -V test-unfiltered.vcf \; --ob-priors test.read-orientation-model.tar.gz \; -O test.vcf; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6058
https://github.com/broadinstitute/gatk/issues/6059:141,Deployability,release,release,141,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller --output-mode EMIT_ALL_SITES. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm trying to generate a VCF (not a gVCF) that contains calls spanning all the sites in my regions. Each region is small, and is more or less equivalent to a single variant. Ideally I'd use `GENOTYPE_GIVEN_ALLELES`, but I don't know the alleles, and in some cases the variant location is approximate (e.g. somewhere in _this_ 10bp window). I've been trying to use HaplotypeCaller to produce a VCF that contains calls covering my entire set of regions, but nothing seems to work. I started with just `--output-mode` and eventually ended up with:. ```; gatk HaplotypeCaller \; -R ref.fasta \; -L regions.interval_list \; --disable-optimizations \; --force-active \; --output-mode EMIT_ALL_SITES \; -I my.bam \; -O my.vcf.gz; ```. This does output considerably more records, including a lot of hom-ref records, but still nowhere near to the full set of bases within my regions. E.g. in one test this emits variants spanning 3,468bp which is way better than the ~120bp I get without those options, but nowhere near the 293,570bp with the regions I'm supplying. It would be great if `--output-mode EMIT_ALL_SITES` did as the documentation described, but if that's not possible, then perhaps that mode should simply be removed?. #### Steps to reproduce; Try calling a BAM file with HaplotypeCaller with a 100-1000bp region with `--output-mode EMIT_ALL_SITES`. #### Expected behavior; VCF should contain records spanning the entire input region. #### Actual behavior; VCF contains a minority of sites from the region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059
https://github.com/broadinstitute/gatk/issues/6059:866,Performance,optimiz,optimizations,866,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller --output-mode EMIT_ALL_SITES. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm trying to generate a VCF (not a gVCF) that contains calls spanning all the sites in my regions. Each region is small, and is more or less equivalent to a single variant. Ideally I'd use `GENOTYPE_GIVEN_ALLELES`, but I don't know the alleles, and in some cases the variant location is approximate (e.g. somewhere in _this_ 10bp window). I've been trying to use HaplotypeCaller to produce a VCF that contains calls covering my entire set of regions, but nothing seems to work. I started with just `--output-mode` and eventually ended up with:. ```; gatk HaplotypeCaller \; -R ref.fasta \; -L regions.interval_list \; --disable-optimizations \; --force-active \; --output-mode EMIT_ALL_SITES \; -I my.bam \; -O my.vcf.gz; ```. This does output considerably more records, including a lot of hom-ref records, but still nowhere near to the full set of bases within my regions. E.g. in one test this emits variants spanning 3,468bp which is way better than the ~120bp I get without those options, but nowhere near the 293,570bp with the regions I'm supplying. It would be great if `--output-mode EMIT_ALL_SITES` did as the documentation described, but if that's not possible, then perhaps that mode should simply be removed?. #### Steps to reproduce; Try calling a BAM file with HaplotypeCaller with a 100-1000bp region with `--output-mode EMIT_ALL_SITES`. #### Expected behavior; VCF should contain records spanning the entire input region. #### Actual behavior; VCF contains a minority of sites from the region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059
https://github.com/broadinstitute/gatk/issues/6059:211,Testability,test,test,211,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller --output-mode EMIT_ALL_SITES. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm trying to generate a VCF (not a gVCF) that contains calls spanning all the sites in my regions. Each region is small, and is more or less equivalent to a single variant. Ideally I'd use `GENOTYPE_GIVEN_ALLELES`, but I don't know the alleles, and in some cases the variant location is approximate (e.g. somewhere in _this_ 10bp window). I've been trying to use HaplotypeCaller to produce a VCF that contains calls covering my entire set of regions, but nothing seems to work. I started with just `--output-mode` and eventually ended up with:. ```; gatk HaplotypeCaller \; -R ref.fasta \; -L regions.interval_list \; --disable-optimizations \; --force-active \; --output-mode EMIT_ALL_SITES \; -I my.bam \; -O my.vcf.gz; ```. This does output considerably more records, including a lot of hom-ref records, but still nowhere near to the full set of bases within my regions. E.g. in one test this emits variants spanning 3,468bp which is way better than the ~120bp I get without those options, but nowhere near the 293,570bp with the regions I'm supplying. It would be great if `--output-mode EMIT_ALL_SITES` did as the documentation described, but if that's not possible, then perhaps that mode should simply be removed?. #### Steps to reproduce; Try calling a BAM file with HaplotypeCaller with a 100-1000bp region with `--output-mode EMIT_ALL_SITES`. #### Expected behavior; VCF should contain records spanning the entire input region. #### Actual behavior; VCF contains a minority of sites from the region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059
https://github.com/broadinstitute/gatk/issues/6059:1124,Testability,test,test,1124,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller --output-mode EMIT_ALL_SITES. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm trying to generate a VCF (not a gVCF) that contains calls spanning all the sites in my regions. Each region is small, and is more or less equivalent to a single variant. Ideally I'd use `GENOTYPE_GIVEN_ALLELES`, but I don't know the alleles, and in some cases the variant location is approximate (e.g. somewhere in _this_ 10bp window). I've been trying to use HaplotypeCaller to produce a VCF that contains calls covering my entire set of regions, but nothing seems to work. I started with just `--output-mode` and eventually ended up with:. ```; gatk HaplotypeCaller \; -R ref.fasta \; -L regions.interval_list \; --disable-optimizations \; --force-active \; --output-mode EMIT_ALL_SITES \; -I my.bam \; -O my.vcf.gz; ```. This does output considerably more records, including a lot of hom-ref records, but still nowhere near to the full set of bases within my regions. E.g. in one test this emits variants spanning 3,468bp which is way better than the ~120bp I get without those options, but nowhere near the 293,570bp with the regions I'm supplying. It would be great if `--output-mode EMIT_ALL_SITES` did as the documentation described, but if that's not possible, then perhaps that mode should simply be removed?. #### Steps to reproduce; Try calling a BAM file with HaplotypeCaller with a 100-1000bp region with `--output-mode EMIT_ALL_SITES`. #### Expected behavior; VCF should contain records spanning the entire input region. #### Actual behavior; VCF contains a minority of sites from the region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059
https://github.com/broadinstitute/gatk/issues/6059:1440,Usability,simpl,simply,1440,"## Bug Report. ### Affected tool(s) or class(es); HaplotypeCaller --output-mode EMIT_ALL_SITES. ### Affected version(s); - [x] Latest public release version [version?]; - [ ] Latest master branch as of [date of test?]. ### Description ; I'm trying to generate a VCF (not a gVCF) that contains calls spanning all the sites in my regions. Each region is small, and is more or less equivalent to a single variant. Ideally I'd use `GENOTYPE_GIVEN_ALLELES`, but I don't know the alleles, and in some cases the variant location is approximate (e.g. somewhere in _this_ 10bp window). I've been trying to use HaplotypeCaller to produce a VCF that contains calls covering my entire set of regions, but nothing seems to work. I started with just `--output-mode` and eventually ended up with:. ```; gatk HaplotypeCaller \; -R ref.fasta \; -L regions.interval_list \; --disable-optimizations \; --force-active \; --output-mode EMIT_ALL_SITES \; -I my.bam \; -O my.vcf.gz; ```. This does output considerably more records, including a lot of hom-ref records, but still nowhere near to the full set of bases within my regions. E.g. in one test this emits variants spanning 3,468bp which is way better than the ~120bp I get without those options, but nowhere near the 293,570bp with the regions I'm supplying. It would be great if `--output-mode EMIT_ALL_SITES` did as the documentation described, but if that's not possible, then perhaps that mode should simply be removed?. #### Steps to reproduce; Try calling a BAM file with HaplotypeCaller with a 100-1000bp region with `--output-mode EMIT_ALL_SITES`. #### Expected behavior; VCF should contain records spanning the entire input region. #### Actual behavior; VCF contains a minority of sites from the region.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6059
https://github.com/broadinstitute/gatk/pull/6060:458,Deployability,upgrade,upgrade,458,"After spending some time to resolve this users issue, https://gatkforums.broadinstitute.org/gatk/discussion/24134/gatk4-rmsmappingquality-results-differ-between-v4-0-0-0-and-v4-1-1-0/p1, it became clear that the issue was that the user simply mismatched her versions of gatk, which seems to have caused their MQ annotations to tank. The user didn't notice the warnings of this fact until we had already nearly found the issue by debugging. I propose that we upgrade the warning to an exception with explicit override to make it harder for this issue to slip past people in the future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6060
https://github.com/broadinstitute/gatk/pull/6060:197,Usability,clear,clear,197,"After spending some time to resolve this users issue, https://gatkforums.broadinstitute.org/gatk/discussion/24134/gatk4-rmsmappingquality-results-differ-between-v4-0-0-0-and-v4-1-1-0/p1, it became clear that the issue was that the user simply mismatched her versions of gatk, which seems to have caused their MQ annotations to tank. The user didn't notice the warnings of this fact until we had already nearly found the issue by debugging. I propose that we upgrade the warning to an exception with explicit override to make it harder for this issue to slip past people in the future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6060
https://github.com/broadinstitute/gatk/pull/6060:236,Usability,simpl,simply,236,"After spending some time to resolve this users issue, https://gatkforums.broadinstitute.org/gatk/discussion/24134/gatk4-rmsmappingquality-results-differ-between-v4-0-0-0-and-v4-1-1-0/p1, it became clear that the issue was that the user simply mismatched her versions of gatk, which seems to have caused their MQ annotations to tank. The user didn't notice the warnings of this fact until we had already nearly found the issue by debugging. I propose that we upgrade the warning to an exception with explicit override to make it harder for this issue to slip past people in the future.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6060
https://github.com/broadinstitute/gatk/issues/6063:1084,Testability,test,test,1084,"nk you for your great support and constant development of GATK! I was very pleased to see that the output mode options EMIT_ALL_CONFIDENT_SITES and EMIT_ALL_SITES were reintroduced recently in HaplotypeCaller (GitHub Issue 2865, I am not allowed to post links yet). However, I think these options or not yet working. When I am using the --output-mode options EMIT_ALL_CONFIDENT_SITES and EMIT_ALL_SITES, only some seemingly random monomorphic sites are reported with site quality QUAL=""Infinity"", instead of all sites (monomorphic sites and variants). This, despite many, many high depth covered monomorphic sites in my dataset. I get the same buggy result for single-sample calling and for multi-sample calling with HaplotypeCaller. I am using GATK version v4.1.2.0, java 1.8.0_101-b13. Here is the commands I used to get all confident sites for a two samples:; gatk --java-options ""-Xmx10G -Xms1G"" HaplotypeCaller --tmp-dir ./temp -R ref.fasta -I 22363.subset.bam -I 22365.subset.bam -L scaffold_7232:1-1000 -O test.allconf.vcf.gz --pcr-indel-model NONE --output-mode EMIT_ALL_CONFIDENT_SITES. The resulting VCF file look like this (barring all except the last header lines):; ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT 22363 22365; scaffold_7232 49 . C T 539.88 . AC=1;AF=0.250;AN=4;BaseQRankSum=-0.334;DP=132;ExcessHet=3.0103;FS=0.857;MLEAC=1;MLEAF=0.250;MQ=42.22;MQRankSum=-0.781;QD=5.57;ReadPosRankSum=-1.285;SOR=0.578 GT:AD:DP:GQ:PL 0/1:74,23:97:99:548,0,2548 0/0:33,0:33:99:0,99,1296; scaffold_7232 241 . A . Infinity . AN=4;DP=332;MQ=43.51 GT:AD:DP 0/0:232:232 0/0:100:100; scaffold_7232 276 . A C 1514.46 . AC=2;AF=0.500;AN=4;BaseQRankSum=2.611;DP=318;ExcessHet=4.7712;FS=2.430;MLEAC=2;MLEAF=0.500;MQ=42.98;MQRankSum=-0.010;QD=4.76;ReadPosRankSum=1.388;SOR=0.544 GT:AD:DP:GQ:PL 0/1:172,41:213:99:1039,0,5920 0/1:85,20:105:99:485,0,2913; scaffold_7232 333 . T . Infinity . AN=4;DP=311;MQ=41.99 GT:AD:DP 0/0:202:202 0/0:109:109; scaffold_7232 343 . C T 4427.46 . AC=2;AF=0.500;AN=4;Ba",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6063
https://github.com/broadinstitute/gatk/issues/6064:273,Availability,error,errors,273,"when the cluster is created. The default for this value is `false`, per [here](https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml), which is the version of Hadoop used in Dataproc image version 1.3. If left as `false`, one keeps getting errors like below when requesting reference bases localized to the HDFS attached to the dataproc cluster, regardless if using *.fasta.gz or *.fasta.; ```; 19/07/26 20:15:43 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 20.5 in stage 50.0 (TID 45798, shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-w-2.c.broad-dsde-methods.internal, executor 44): htsjdk.samtools.SAMException: Unable to load chr14(100526932, 100526932) from /reference/Homo_sapiens_assembly38.fasta; 	at htsjdk.samtools.reference.AbstractIndexedFastaSequenceFile.getSubsequenceAt(AbstractIndexedFastaSequenceFile.java:207); 	at htsjdk.samtools.reference.IndexedFastaSequenceFile.getSubsequenceAt(IndexedFastaSequenceFile.java:49); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceHadoopSparkSource.getReferenceBases(ReferenceHadoopSparkSource.java:31); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType.extractRefBases(SvType.java:161); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SimpleSVType$DuplicationTandem.<init>(SimpleSVType.java:190); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.inferSimpleTypeFromNovelAdjacency(ContigChimericAlignmentIterativeInterpreter.java:229); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.lambda$discoverVariantsFromChimeras$610a78cb$1(ContigChimericAlignmentIterativeInterpreter.java:84); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6064:468,Energy Efficiency,schedul,scheduler,468,"when the cluster is created. The default for this value is `false`, per [here](https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml), which is the version of Hadoop used in Dataproc image version 1.3. If left as `false`, one keeps getting errors like below when requesting reference bases localized to the HDFS attached to the dataproc cluster, regardless if using *.fasta.gz or *.fasta.; ```; 19/07/26 20:15:43 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 20.5 in stage 50.0 (TID 45798, shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-w-2.c.broad-dsde-methods.internal, executor 44): htsjdk.samtools.SAMException: Unable to load chr14(100526932, 100526932) from /reference/Homo_sapiens_assembly38.fasta; 	at htsjdk.samtools.reference.AbstractIndexedFastaSequenceFile.getSubsequenceAt(AbstractIndexedFastaSequenceFile.java:207); 	at htsjdk.samtools.reference.IndexedFastaSequenceFile.getSubsequenceAt(IndexedFastaSequenceFile.java:49); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceHadoopSparkSource.getReferenceBases(ReferenceHadoopSparkSource.java:31); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType.extractRefBases(SvType.java:161); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SimpleSVType$DuplicationTandem.<init>(SimpleSVType.java:190); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.inferSimpleTypeFromNovelAdjacency(ContigChimericAlignmentIterativeInterpreter.java:229); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.lambda$discoverVariantsFromChimeras$610a78cb$1(ContigChimericAlignmentIterativeInterpreter.java:84); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6064:3387,Energy Efficiency,schedul,scheduler,3387,ic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	a,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6064:3459,Energy Efficiency,schedul,scheduler,3459,.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104); 	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1004); ,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6064:671,Performance,load,load,671,"when the cluster is created. The default for this value is `false`, per [here](https://hadoop.apache.org/docs/r2.9.2/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml), which is the version of Hadoop used in Dataproc image version 1.3. If left as `false`, one keeps getting errors like below when requesting reference bases localized to the HDFS attached to the dataproc cluster, regardless if using *.fasta.gz or *.fasta.; ```; 19/07/26 20:15:43 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 20.5 in stage 50.0 (TID 45798, shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-w-2.c.broad-dsde-methods.internal, executor 44): htsjdk.samtools.SAMException: Unable to load chr14(100526932, 100526932) from /reference/Homo_sapiens_assembly38.fasta; 	at htsjdk.samtools.reference.AbstractIndexedFastaSequenceFile.getSubsequenceAt(AbstractIndexedFastaSequenceFile.java:207); 	at htsjdk.samtools.reference.IndexedFastaSequenceFile.getSubsequenceAt(IndexedFastaSequenceFile.java:49); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceHadoopSparkSource.getReferenceBases(ReferenceHadoopSparkSource.java:31); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType.extractRefBases(SvType.java:161); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SimpleSVType$DuplicationTandem.<init>(SimpleSVType.java:190); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.inferSimpleTypeFromNovelAdjacency(ContigChimericAlignmentIterativeInterpreter.java:229); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.lambda$discoverVariantsFromChimeras$610a78cb$1(ContigChimericAlignmentIterativeInterpreter.java:84); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6064:3584,Performance,concurren,concurrent,3584,.scala:48); 	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310); 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1004); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channe,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6064:3669,Performance,concurren,concurrent,3669, 	at scala.collection.AbstractIterator.to(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302); 	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336); 	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289); 	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:945); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-1798951329-10.128.1.77-1564169124618:blk_1073741844_1020 file=/reference/Homo_sapiens_assembly38.fasta; 	at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1085); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1068); 	at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:1047); 	at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:655); 	at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:949); 	at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:1004); 	at java.io.DataInputStream.read(DataInputStream.java:149); 	at java.nio.channels.Channels$ReadableByteChannelImpl.read(Channels.java:385); 	at hdfs.jsr203.HadoopFileSystem$3.read(HadoopFileSystem.java:478); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6064:1421,Usability,Simpl,SimpleSVType,1421," 20:15:43 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 20.5 in stage 50.0 (TID 45798, shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-w-2.c.broad-dsde-methods.internal, executor 44): htsjdk.samtools.SAMException: Unable to load chr14(100526932, 100526932) from /reference/Homo_sapiens_assembly38.fasta; 	at htsjdk.samtools.reference.AbstractIndexedFastaSequenceFile.getSubsequenceAt(AbstractIndexedFastaSequenceFile.java:207); 	at htsjdk.samtools.reference.IndexedFastaSequenceFile.getSubsequenceAt(IndexedFastaSequenceFile.java:49); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceHadoopSparkSource.getReferenceBases(ReferenceHadoopSparkSource.java:31); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType.extractRefBases(SvType.java:161); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SimpleSVType$DuplicationTandem.<init>(SimpleSVType.java:190); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.inferSimpleTypeFromNovelAdjacency(ContigChimericAlignmentIterativeInterpreter.java:229); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.lambda$discoverVariantsFromChimeras$610a78cb$1(ContigChimericAlignmentIterativeInterpreter.java:84); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scal",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6064:1459,Usability,Simpl,SimpleSVType,1459,"spark.scheduler.TaskSetManager: Lost task 20.5 in stage 50.0 (TID 45798, shuang-g94794-chmi-chmi3-wgs1-cram-bam-feature-w-2.c.broad-dsde-methods.internal, executor 44): htsjdk.samtools.SAMException: Unable to load chr14(100526932, 100526932) from /reference/Homo_sapiens_assembly38.fasta; 	at htsjdk.samtools.reference.AbstractIndexedFastaSequenceFile.getSubsequenceAt(AbstractIndexedFastaSequenceFile.java:207); 	at htsjdk.samtools.reference.IndexedFastaSequenceFile.getSubsequenceAt(IndexedFastaSequenceFile.java:49); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceHadoopSparkSource.getReferenceBases(ReferenceHadoopSparkSource.java:31); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvType.extractRefBases(SvType.java:161); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SimpleSVType$DuplicationTandem.<init>(SimpleSVType.java:190); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.inferSimpleTypeFromNovelAdjacency(ContigChimericAlignmentIterativeInterpreter.java:229); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.ContigChimericAlignmentIterativeInterpreter.lambda$discoverVariantsFromChimeras$610a78cb$1(ContigChimericAlignmentIterativeInterpreter.java:84); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at scala.collection.Iterator$class.foreach(Iterator.scala:893); 	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336); 	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59); 	at scala.collecti",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6064
https://github.com/broadinstitute/gatk/issues/6065:161,Availability,down,downloaded,161,"## Bug Report. ### Affected tool(s) or class(es); PrintReads, and probably most other ROD-based tools. ### Affected version(s); GATK v4.1.0. ### Description ; I downloaded a .bam from SRA (https://www.ncbi.nlm.nih.gov/sra/SRX4114173[accn]); and ran gatk PrintReads to extract subregions based on a picard-style interval list. . The bug is that PrintReads ran without any warnings or errors and silently dropped some (though not all) reads that it should have included based on the interval list. It does include these reads if I run it with an interval list that just contains that one interval I'm interested in, but not if I include it among many other intervals. The interval list is sorted based on the .bam's sequence dictionary (by running ; `picard BedToIntervalList --SEQUENCE_DICTIONARY ../SRR7205167.1.bam --SORT -I GRCh38_intervals.bed -O GRCh38_intervals.sorted.list`). . The underlying issue as far as I can tell, is that the .bam reads are sorted, but not in the same order as its sequence dictionary. . This might be related to https://github.com/broadinstitute/gatk/issues/101. #### Expected behavior; I think GATK should fail with an error when this occurs. Otherwise it's easy for users to miss the data loss and end up with incorrect analyses. #### Actual behavior; Silently drops data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6065
https://github.com/broadinstitute/gatk/issues/6065:383,Availability,error,errors,383,"## Bug Report. ### Affected tool(s) or class(es); PrintReads, and probably most other ROD-based tools. ### Affected version(s); GATK v4.1.0. ### Description ; I downloaded a .bam from SRA (https://www.ncbi.nlm.nih.gov/sra/SRX4114173[accn]); and ran gatk PrintReads to extract subregions based on a picard-style interval list. . The bug is that PrintReads ran without any warnings or errors and silently dropped some (though not all) reads that it should have included based on the interval list. It does include these reads if I run it with an interval list that just contains that one interval I'm interested in, but not if I include it among many other intervals. The interval list is sorted based on the .bam's sequence dictionary (by running ; `picard BedToIntervalList --SEQUENCE_DICTIONARY ../SRR7205167.1.bam --SORT -I GRCh38_intervals.bed -O GRCh38_intervals.sorted.list`). . The underlying issue as far as I can tell, is that the .bam reads are sorted, but not in the same order as its sequence dictionary. . This might be related to https://github.com/broadinstitute/gatk/issues/101. #### Expected behavior; I think GATK should fail with an error when this occurs. Otherwise it's easy for users to miss the data loss and end up with incorrect analyses. #### Actual behavior; Silently drops data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6065
https://github.com/broadinstitute/gatk/issues/6065:1151,Availability,error,error,1151,"## Bug Report. ### Affected tool(s) or class(es); PrintReads, and probably most other ROD-based tools. ### Affected version(s); GATK v4.1.0. ### Description ; I downloaded a .bam from SRA (https://www.ncbi.nlm.nih.gov/sra/SRX4114173[accn]); and ran gatk PrintReads to extract subregions based on a picard-style interval list. . The bug is that PrintReads ran without any warnings or errors and silently dropped some (though not all) reads that it should have included based on the interval list. It does include these reads if I run it with an interval list that just contains that one interval I'm interested in, but not if I include it among many other intervals. The interval list is sorted based on the .bam's sequence dictionary (by running ; `picard BedToIntervalList --SEQUENCE_DICTIONARY ../SRR7205167.1.bam --SORT -I GRCh38_intervals.bed -O GRCh38_intervals.sorted.list`). . The underlying issue as far as I can tell, is that the .bam reads are sorted, but not in the same order as its sequence dictionary. . This might be related to https://github.com/broadinstitute/gatk/issues/101. #### Expected behavior; I think GATK should fail with an error when this occurs. Otherwise it's easy for users to miss the data loss and end up with incorrect analyses. #### Actual behavior; Silently drops data.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6065
https://github.com/broadinstitute/gatk/issues/6067:78,Performance,perform,perform,78,"Dear all, ; May I ask about the DP, AD, and AF information of Mutect2?. If we perform somatic mutation calling with the same pair of tumor samples, we can see that:; >chr2 227915762 . T C 240 PASS ; >STATUS=StrongSomatic;SAMPLE=yjhzy_T1;TYPE=SNV;DP=155;VD=69;AF=0.4452;SHIFT3=0;MSI=2.000;MSILEN=1;SSF=0;SOR=Inf;LSEQ=GAGCCTGGAGGGCCTGGGGG;RSEQ=CCAGGAGGCCCTGGCTGACC ; >GT:DP:VD:ALD:RD:AD:AF:BIAS:PMEAN:PSTD:QUAL:QSTD:SBF:ODDRATIO:MQ:SN:HIAF:ADJAF:NM; >0/1:155:69:34,35:45,41:86,69:0.4452:2,2:31.6:1:39.3:1:0.74799:1.12895:60:68:0.4416:0:1.4; >0/0:106:0:0,0:59,47:106,0:0:2,0:35.6:1:35.1:1:1:0:60:105:1:0:0.4. VarDict said the DP, AD, AF were 155, 69, and 0.4452. The control DP and AD were 106, 0, 0. >chr2 227915762 . T C . PASS ; >DP=262;MQ=60.00;MQ0=0;NT=ref;QSS=242;QSS_NT=3070;ReadPosRankSum=1.63;SGT=TT->CT;SNVSB=0.00;SOMATIC;SomaticEVS=19.98;TQSS=1;TQSS_NT=1 ; >DP:FDP:SDP:SUBDP:AU:CU:GU:TU; >106:0:0:0:0,0:0,0:0,0:106,106; >156:0:0:0:0,0:69,69:0,0:87,87. Strelka2 said the DP of tumor was 156. The control DP was 106, (The sequence was opposite with VarDict). However, Mutect2 said that; >chr2 227915762 . T C . PASS ; >CONTQ=93;DP=88;ECNT=1;GERMQ=118;MBQ=35,40;MFRL=205,190;MMQ=60,60;MPOS=38;NALOD=1.70;NLOD=14.70;POPAF=3.00;SAAF=0.364,0.333,0.378;SAPP=0.014,0.036,0.950;TLOD=50.64 ; >GT:AD:AF:DP:F1R2:F2R1 ; >0/1:23,14:0.385:37:11,7:12,7 ; >0/0:49,0:0.019:49:25,0:24,0; The DP, AD, AF were 37, 14, and 0.385. The control DP, AD and AF were 49, 0, 0.019. There are serveral things which are confusing:. 1. Why the DP reported in Mutect2 results are different in ##INFO and ##FORMAT fields? We can see: CONTQ=93;**DP=88**, vs. **DP=37**.; 2. Why the AF of Mutect2 is strange? **14 / ( 23 + 14) = 0.378 != 0.385**.; 3. Why in the control sample, we got AF = 0.019, but **0 / (0 + 49) = 0**?; 4. Why all the sites in the VCFs showed as above?. Any reply will be appreciated. Best regards!",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6067
https://github.com/broadinstitute/gatk/pull/6069:33,Testability,test,test,33,Additionally got rid of the unit test for `ExpandingArrayList`,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6069
https://github.com/broadinstitute/gatk/pull/6070:162,Availability,failure,failure,162,"a bug fix suggested by Louis.; @lbergelson I don't know how to add a test. Without this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:220,Availability,failure,failure,220,"a bug fix suggested by Louis.; @lbergelson I don't know how to add a test. Without this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:393,Deployability,install,installed,393,"a bug fix suggested by Louis.; @lbergelson I don't know how to add a test. Without this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:7543,Deployability,deploy,deploy,7543,"TKSparkTool.runPipeline(GATKSparkTool.java:534); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.b",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:7625,Deployability,deploy,deploy,7625,".engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseSt",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:7662,Deployability,deploy,deploy,7662,"SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.br",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:7735,Deployability,deploy,deploy,7735,"er.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantT",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:7812,Deployability,deploy,deploy,7812,"roadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbende",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:7884,Deployability,deploy,deploy,7884,"eArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:7954,Deployability,deploy,deploy,7954,"cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:8070,Deployability,install,installed,8070,"nder.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVaria",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3245,Energy Efficiency,schedul,scheduler,3245,emoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.sp,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3317,Energy Efficiency,schedul,scheduler,3317,n$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGSched,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3682,Energy Efficiency,schedul,scheduler,3682,ockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apa,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3722,Energy Efficiency,schedul,scheduler,3722,mpute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3821,Energy Efficiency,schedul,scheduler,3821,org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProces,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3919,Energy Efficiency,schedul,scheduler,3919,.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4173,Energy Efficiency,schedul,scheduler,4173,apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkConte,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4254,Energy Efficiency,schedul,scheduler,4254,ltTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(Spar,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4360,Energy Efficiency,schedul,scheduler,4360,he.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4510,Energy Efficiency,schedul,scheduler,4510,t java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:11,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4599,Energy Efficiency,schedul,scheduler,4599,va.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4697,Energy Efficiency,schedul,scheduler,4697,er.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:3,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4793,Energy Efficiency,schedul,scheduler,4793,; 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.br,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4958,Energy Efficiency,schedul,scheduler,4958,.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.makeInterpretation(SimpleNovelAdjacencyInterpreter.java:48); 	at org.broad,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:10922,Energy Efficiency,schedul,scheduler,10922,$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Full stacktrace [here](https://console.cloud.google.com/dataproc/jobs/333e650177dc48dd95474c37316a5bf2?organizationId=548622027621&project=broad-dsde-methods&region=global),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:10994,Energy Efficiency,schedul,scheduler,10994,$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Full stacktrace [here](https://console.cloud.google.com/dataproc/jobs/333e650177dc48dd95474c37316a5bf2?organizationId=548622027621&project=broad-dsde-methods&region=global),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3442,Performance,concurren,concurrent,3442,ockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3527,Performance,concurren,concurrent,3527,scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:11119,Performance,concurren,concurrent,11119,$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Full stacktrace [here](https://console.cloud.google.com/dataproc/jobs/333e650177dc48dd95474c37316a5bf2?organizationId=548622027621&project=broad-dsde-methods&region=global),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:11204,Performance,concurren,concurrent,11204,$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748); ```. Full stacktrace [here](https://console.cloud.google.com/dataproc/jobs/333e650177dc48dd95474c37316a5bf2?organizationId=548622027621&project=broad-dsde-methods&region=global),MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:141,Safety,abort,aborted,141,"a bug fix suggested by Louis.; @lbergelson I don't know how to add a test. Without this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3853,Safety,abort,abortStage,3853,apPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGSche,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:3951,Safety,abort,abortStage,3951,kpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49); 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.ap,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:4196,Safety,abort,abortStage,4196,RDD.scala:288); 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87); 	at org.apache.spark.scheduler.Task.run(Task.scala:109); 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345); 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149); 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624); 	at java.lang.Thread.run(Thread.java:748). Driver stacktrace:; 	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1661); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1649); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1648); 	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59); 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1648); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831); 	at scala.Option.foreach(Option.scala:257); 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1882); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1831); 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1820); 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:1066,Security,access,access,1066,"t this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:8743,Security,access,access,8743,"unMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:69,Testability,test,test,69,"a bug fix suggested by Louis.; @lbergelson I don't know how to add a test. Without this you'd see. ```; org.apache.spark.SparkException: Job aborted due to stage failure: Task 45 in stage 9.0 failed 8 times, most recent failure: Lost task 45.7 in stage 9.0 (TID 734, shuang-svdps-ceu-w-1.c.broad-dsde-methods.internal, executor 2): java.nio.file.FileSystemNotFoundException: Provider ""gs"" not installed; 	at java.nio.file.Paths.get(Paths.java:147); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferencePath(ReferenceFileSparkSource.java:53); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:1626,Usability,Simpl,SimpleNovelAdjacencyInterpreter,1626,s.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:1668,Usability,Simpl,SimpleNovelAdjacencyInterpreter,1668,ceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseU,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:1780,Usability,Simpl,SimpleNovelAdjacencyInterpreter,1780,parkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:1869,Usability,Simpl,SimpleNovelAdjacencyInterpreter,1869,89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartiti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:5857,Usability,Simpl,SimpleNovelAdjacencyInterpreter,5857,rg.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.makeInterpretation(SimpleNovelAdjacencyInterpreter.java:48); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvDiscoverFromLocalAssemblyContigAlignmentsSpark.extractSimpleVariants(SvDiscoverFromLocalAssemblyContigAlignmentsSpark.java:328); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvDiscoverFromLocalAssemblyContigAlignmentsSpark.dispatchJobs(SvDiscoverFromLocalAssemblyContigAlignmentsSpark.java:303); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvDiscoverFromLocalAssemblyContigAlignmentsSpark.runTool(SvDiscoverFromLocalAssemblyContigAlignmentsSpark.java:170); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:534); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instan,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:5908,Usability,Simpl,SimpleNovelAdjacencyInterpreter,5908,n(EventLoop.scala:48); 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074); 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099); 	at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:945); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151); 	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112); 	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363); 	at org.apache.spark.rdd.RDD.collect(RDD.scala:944); 	at org.apache.spark.api.java.JavaRDDLike$class.collect(JavaRDDLike.scala:361); 	at org.apache.spark.api.java.AbstractJavaRDDLike.collect(JavaRDDLike.scala:45); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.makeInterpretation(SimpleNovelAdjacencyInterpreter.java:48); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvDiscoverFromLocalAssemblyContigAlignmentsSpark.extractSimpleVariants(SvDiscoverFromLocalAssemblyContigAlignmentsSpark.java:328); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvDiscoverFromLocalAssemblyContigAlignmentsSpark.dispatchJobs(SvDiscoverFromLocalAssemblyContigAlignmentsSpark.java:303); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.SvDiscoverFromLocalAssemblyContigAlignmentsSpark.runTool(SvDiscoverFromLocalAssemblyContigAlignmentsSpark.java:170); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:534); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.jav,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:9303,Usability,Simpl,SimpleNovelAdjacencyInterpreter,9303,s.ReferenceFileSparkSource.getReferenceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:9345,Usability,Simpl,SimpleNovelAdjacencyInterpreter,9345,ceBases(ReferenceFileSparkSource.java:60); 	at org.broadinstitute.hellbender.engine.spark.datasources.ReferenceMultiSparkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseU,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:9457,Usability,Simpl,SimpleNovelAdjacencyInterpreter,9457,parkSource.getReferenceBases(ReferenceMultiSparkSource.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/pull/6070:9546,Usability,Simpl,SimpleNovelAdjacencyInterpreter,9546,89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.getRefBaseString(BreakEndVariantType.java:89); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType.access$200(BreakEndVariantType.java:20); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.<init>(BreakEndVariantType.java:253); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.BreakEndVariantType$InterChromosomeBreakend.getOrderedMates(BreakEndVariantType.java:261); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.NovelAdjacencyAndAltHaplotype.toSimpleOrBNDTypes(NovelAdjacencyAndAltHaplotype.java:246); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.inferType(SimpleNovelAdjacencyInterpreter.java:129); 	at org.broadinstitute.hellbender.tools.spark.sv.discovery.inference.SimpleNovelAdjacencyInterpreter.lambda$inferTypeFromSingleContigSimpleChimera$24ddc343$1(SimpleNovelAdjacencyInterpreter.java:107); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:1043); 	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409); 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:217); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094); 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020); 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085); 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811); 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335); 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286); 	at org.apache.spark.rdd.MapPartiti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6070
https://github.com/broadinstitute/gatk/issues/6071:68,Deployability,pipeline,pipeline,68,"Hi,; The HaplotypeCaller_GATK4_VCF task in the gatk4-exome-analysis-pipeline doesn't seem to add any interval padding. Shouldn't there be interval padding?. Unless the configured Broad intervals already have padding added, but it is not clear why that would be, since that same file is used for calculating HsMetrics, which should not have padding. The question is, for my own implementation of this pipeline, should I add on interval padding to the interval list file? And if so, what size padding? Or should I add the interval padding option to the HaplotypeCaller itself in the wdl script. Thanks for any advice on this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6071
https://github.com/broadinstitute/gatk/issues/6071:400,Deployability,pipeline,pipeline,400,"Hi,; The HaplotypeCaller_GATK4_VCF task in the gatk4-exome-analysis-pipeline doesn't seem to add any interval padding. Shouldn't there be interval padding?. Unless the configured Broad intervals already have padding added, but it is not clear why that would be, since that same file is used for calculating HsMetrics, which should not have padding. The question is, for my own implementation of this pipeline, should I add on interval padding to the interval list file? And if so, what size padding? Or should I add the interval padding option to the HaplotypeCaller itself in the wdl script. Thanks for any advice on this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6071
https://github.com/broadinstitute/gatk/issues/6071:168,Modifiability,config,configured,168,"Hi,; The HaplotypeCaller_GATK4_VCF task in the gatk4-exome-analysis-pipeline doesn't seem to add any interval padding. Shouldn't there be interval padding?. Unless the configured Broad intervals already have padding added, but it is not clear why that would be, since that same file is used for calculating HsMetrics, which should not have padding. The question is, for my own implementation of this pipeline, should I add on interval padding to the interval list file? And if so, what size padding? Or should I add the interval padding option to the HaplotypeCaller itself in the wdl script. Thanks for any advice on this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6071
https://github.com/broadinstitute/gatk/issues/6071:237,Usability,clear,clear,237,"Hi,; The HaplotypeCaller_GATK4_VCF task in the gatk4-exome-analysis-pipeline doesn't seem to add any interval padding. Shouldn't there be interval padding?. Unless the configured Broad intervals already have padding added, but it is not clear why that would be, since that same file is used for calculating HsMetrics, which should not have padding. The question is, for my own implementation of this pipeline, should I add on interval padding to the interval list file? And if so, what size padding? Or should I add the interval padding option to the HaplotypeCaller itself in the wdl script. Thanks for any advice on this.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6071
https://github.com/broadinstitute/gatk/issues/6072:754,Availability,Error,Error,754,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:824,Availability,avail,available,824,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:930,Availability,down,down,930,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:1119,Availability,down,down,1119,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:156,Deployability,Install,Installing,156,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:224,Deployability,Install,Installing,224,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:288,Deployability,Install,Installing,288,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:347,Deployability,Install,Installing,347,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:410,Deployability,Install,Installing,410,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:474,Deployability,Install,Installing,474,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:673,Deployability,Install,Installing,673,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:1070,Deployability,upgrade,upgrade,1070,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:84,Integrability,depend,dependency,84,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:786,Integrability,depend,dependencies,786,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:182,Modifiability,config,config,182,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:250,Modifiability,config,config,250,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:314,Modifiability,config,config,314,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:373,Modifiability,config,config,373,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:436,Modifiability,config,config,436,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:500,Modifiability,config,config,500,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:542,Modifiability,config,config,542,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:65,Performance,cache,cache,65,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/issues/6072:53,Usability,clear,cleared,53,"It looks like all of our builds are failing since we cleared the cache because of R dependency issues. ```; ... Setting up r-base-core (3.1.3-1trusty) ...; Installing new version of config file /etc/bash_completion.d/R ...; Installing new version of config file /etc/R/Renviron.site ...; Installing new version of config file /etc/R/Makeconf ...; Installing new version of config file /etc/R/repositories ...; Installing new version of config file /etc/R/Rprofile.site ...; Installing new version of config file /etc/R/ldpaths ...; Replacing config file /etc/R/Renviron with new version; W: --force-yes is deprecated, use one of the options starting with --allow instead.; Installing packages into ‘/home/travis/site-library’; (as ‘lib’ is unspecified); Error: (converted from warning) dependencies ‘rlang’, ‘vctrs’ are not available; Execution halted; ```. Both libraries now require R >= 3.2.; We could either try again to nail down the R versions exactly, which is almost certainly possible but not something we've ever figured out a good way to do, or we could just upgrade R and hope for the best, kicking the can down the road again.",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6072
https://github.com/broadinstitute/gatk/pull/6074:55,Deployability,pipeline,pipeline,55,Adds NIO to the SubsetBam task and converts all the MT pipeline WDLs to WDL 1.0. I ran this on one sample in Terra and it worked.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6074
https://github.com/broadinstitute/gatk/pull/6076:60,Availability,error,error,60,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076
https://github.com/broadinstitute/gatk/pull/6076:96,Availability,error,error,96,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076
https://github.com/broadinstitute/gatk/pull/6076:102,Integrability,message,message,102,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076
https://github.com/broadinstitute/gatk/pull/6076:263,Security,Validat,ValidateVariants,263,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076
https://github.com/broadinstitute/gatk/pull/6076:280,Security,validat,validationExampleBad,280,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076
https://github.com/broadinstitute/gatk/pull/6076:318,Security,validat,validation,318,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076
https://github.com/broadinstitute/gatk/pull/6076:10,Testability,test,test,10,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076
https://github.com/broadinstitute/gatk/pull/6076:204,Testability,test,test,204,"I added a test according to #4642 , but can't reproduce the error. The user also noted that the error message was badly formed, which is true because it ended with a colon. Now it looks like:; ""Input src/test/resources/org/broadinstitute/hellbender/tools/walkers/ValidateVariants/validationExampleBad.vcf fails strict validation of type CHR_COUNTS: the Allele Count (AC) tag is incorrect for the record at position 1:985447, 1 vs. 2""",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6076
https://github.com/broadinstitute/gatk/pull/6086:24,Availability,error,error,24,Avoids an out-of-bounds error when there are large numbers of alt alleles.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6086
https://github.com/broadinstitute/gatk/pull/6086:0,Safety,Avoid,Avoids,0,Avoids an out-of-bounds error when there are large numbers of alt alleles.,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6086
https://github.com/broadinstitute/gatk/pull/6087:22,Availability,failure,failure,22,maybe this will lower failure rates,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6087
https://github.com/broadinstitute/gatk/issues/6088:577,Availability,down,downsampling,577,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:939,Availability,down,downsampling,939,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:1165,Availability,down,downsampling,1165,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:1224,Availability,reliab,reliable,1224,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:1340,Availability,down,downsampling,1340,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:1814,Availability,down,downsampling,1814,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:137,Deployability,release,release,137,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:993,Energy Efficiency,reduce,reduce,993,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:1284,Performance,perform,performance,1284,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:248,Safety,detect,detect,248,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:467,Safety,detect,detected,467,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/issues/6088:206,Testability,test,test,206,"Report; ### Affected tool(s) or class(es); HaplotypeCaller --max-reads-per-alignment-start. ### Affected version(s); - [x] Latest public release version [4.1.2.0]; - [ ] Latest master branch as of [date of test?]. ### Description; We used GATK4 to detect a fairly large duplication (60bp) in a control sample. We did sequenced two replicates for this sample, one having significantly more coverage than the other.With default GATK4 parameter the duplication was only detected in the sample with the lowest coverage. After inspection of GATK4 parameter we found that it was the downsampling throught the --max-reads-per-alignment-start that was in cause.Indeed, all the reads that contains the duplications are softcliped (see IGV capture below) because the insertion/duplication event is too bigged to be correctly aligned by BWA. This causes all reads containing the duplication to have the same start position in the BAM file. Then, the downsampling based on start position must drastically reduce the signal and the variant is skipped. This explains why the variant was missed at high coverage level and not in the replicates with lower signal.We think that the downsampling should take Softclips into account to be more reliable, but maybe you have a better idea.Also we did some performance evaluation and GATK4 runned faster with the downsampling desactivated. Is it normal ?; ![duplication](https://user-images.githubusercontent.com/53903734/62783152-17f41180-babc-11e9-9ddb-bed3c3042d97.png). #### Steps to reproduce; Run GATK4 with default parameters on the BAM containing the duplication (we can provide a toy). Disable --max-reads-per-alignment-start by switching the value to 0 to enable the identification of the duplication. #### Expected behavior; The duplication should have been found because the downsampling on start position does not take into accout the reads softclips. #### Actual behavior; The duplication is missed at high coverage depth",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6088
https://github.com/broadinstitute/gatk/pull/6089:51,Testability,test,tests,51,Oversight leaving some hardcoded debug in with the tests...,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6089
https://github.com/broadinstitute/gatk/issues/6091:2879,Deployability,pipeline,pipelines,2879,t.scala:292); 	at org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:127); 	at org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:88); 	at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34); 	at org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62); 	at org.apache.spark.SparkContext.broadcast(SparkContext.scala:1482); 	at org.apache.spark.api.java.JavaSparkContext.broadcast(JavaSparkContext.scala:650); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.assemblyRegionEvaluatorSupplierBroadcastFunction(HaplotypeCallerSpark.java:265); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.assemblyRegionEvaluatorSupplierBroadcast(HaplotypeCallerSpark.java:245); 	at org.broadinstitute.hellbender.tools.HaplotypeCallerSpark.callVariantsWithHaplotypeCallerAndWriteOutput(HaplotypeCallerSpark.java:303); 	at org.broadinstitute.hellbender.tools.spark.pipelines.ReadsPipelineSpark.runTool(ReadsPipelineSpark.java:224); 	at org.broadinstitute.hellbender.engine.spark.GATKSparkTool.runPipeline(GATKSparkTool.java:533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Delega,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091
https://github.com/broadinstitute/gatk/issues/6091:3992,Deployability,deploy,deploy,3992,TKSparkTool.runPipeline(GATKSparkTool.java:533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(Clo,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091
https://github.com/broadinstitute/gatk/issues/6091:4074,Deployability,deploy,deploy,4074,533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:61); 	... 46 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091
https://github.com/broadinstitute/gatk/issues/6091:4111,Deployability,deploy,deploy,4111,533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:61); 	... 46 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091
https://github.com/broadinstitute/gatk/issues/6091:4184,Deployability,deploy,deploy,4184,533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:61); 	... 46 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091
https://github.com/broadinstitute/gatk/issues/6091:4261,Deployability,deploy,deploy,4261,533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:61); 	... 46 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091
https://github.com/broadinstitute/gatk/issues/6091:4333,Deployability,deploy,deploy,4333,533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:61); 	... 46 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091
https://github.com/broadinstitute/gatk/issues/6091:4403,Deployability,deploy,deploy,4403,533); 	at org.broadinstitute.hellbender.engine.spark.SparkCommandLineProgram.doWork(SparkCommandLineProgram.java:31); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method); 	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62); 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43); 	at java.lang.reflect.Method.invoke(Method.java:498); 	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52); 	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:894); 	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198); 	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228); 	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137); 	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala); Caused by: java.lang.RuntimeException: Could not serialize lambda; 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:70); 	at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:552); 	at com.esotericsoftware.kryo.serializers.ObjectField.write(ObjectField.java:80); 	... 44 more; Caused by: java.lang.NoSuchMethodException: htsjdk.samtools.reference.AbstractFastaSequenceFile$$Lambda$94/1029586776.writeReplace(); 	at java.lang.Class.getDeclaredMethod(Class.java:2130); 	at com.twitter.chill.java.ClosureSerializer.write(ClosureSerializer.java:61); 	... 46 more; ```,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6091
https://github.com/broadinstitute/gatk/pull/6093:34,Testability,test,test,34,* Adding forkEvery 5 to cause the test jvm to be restarted every few test classes.; * This might fix a theoretical memory leak causing our builds to fail often. This might help?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6093
https://github.com/broadinstitute/gatk/pull/6093:69,Testability,test,test,69,* Adding forkEvery 5 to cause the test jvm to be restarted every few test classes.; * This might fix a theoretical memory leak causing our builds to fail often. This might help?,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6093
https://github.com/broadinstitute/gatk/pull/6094:2,Deployability,Update,Update,2,* Update htsjdk 2.20.1 -> 2.20.2; * This release fixes https://github.com/broadinstitute/gatk/issues/6091,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6094
https://github.com/broadinstitute/gatk/pull/6094:41,Deployability,release,release,41,* Update htsjdk 2.20.1 -> 2.20.2; * This release fixes https://github.com/broadinstitute/gatk/issues/6091,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/pull/6094
https://github.com/broadinstitute/gatk/issues/6095:147,Deployability,release,release,147,"## Bug Report. ### Affected tool(s) or class(es); VariantAnnotator -A ReferenceBases -A TandemRepeat. ### Affected version(s); - [x] Latest public release version [4.1.3]; - [ ] Latest master branch as of [date of test?]. ### Description ; VariantAnnotator doesn't seem to correctly generate annotations that are based off the reference. I've tried on a few different VCFs and with different genome builds, and get the same result every time. For `ReferenceBases` it seems to generate a string for each variant that is the ref allele, followed by `20-len(ref) * N`. E.g.:. ```; 1 118617 rs372912307 T C 50 PASS REF_BASES=TNNNNNNNNNNNNNNNNNNNN; 1 567239 rs78150957 CG C 50 PASS REF_BASES=CGNNNNNNNNNNNNNNNNNNN; ```. The STR annotations get their header lines added to the header, but not a single variant is flagged as an STR. I've tried processing the GIAB VCFs for NA12878 and NA24385 with the same results - even obvious STR variants are not flagged. I suspect this is related to the fact that REF_BASES isn't compute properly. #### Steps to reproduce; Take any decent size VCF without the above annotations (e.g. GIAB VCFs) and run something like:. gatk VariantAnnotator -V NA12878.vcf.gz -O NA12878.ann.vcf.gz -A TandemRepeat -A ReferenceBases -R hg38.fa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6095
https://github.com/broadinstitute/gatk/issues/6095:214,Testability,test,test,214,"## Bug Report. ### Affected tool(s) or class(es); VariantAnnotator -A ReferenceBases -A TandemRepeat. ### Affected version(s); - [x] Latest public release version [4.1.3]; - [ ] Latest master branch as of [date of test?]. ### Description ; VariantAnnotator doesn't seem to correctly generate annotations that are based off the reference. I've tried on a few different VCFs and with different genome builds, and get the same result every time. For `ReferenceBases` it seems to generate a string for each variant that is the ref allele, followed by `20-len(ref) * N`. E.g.:. ```; 1 118617 rs372912307 T C 50 PASS REF_BASES=TNNNNNNNNNNNNNNNNNNNN; 1 567239 rs78150957 CG C 50 PASS REF_BASES=CGNNNNNNNNNNNNNNNNNNN; ```. The STR annotations get their header lines added to the header, but not a single variant is flagged as an STR. I've tried processing the GIAB VCFs for NA12878 and NA24385 with the same results - even obvious STR variants are not flagged. I suspect this is related to the fact that REF_BASES isn't compute properly. #### Steps to reproduce; Take any decent size VCF without the above annotations (e.g. GIAB VCFs) and run something like:. gatk VariantAnnotator -V NA12878.vcf.gz -O NA12878.ann.vcf.gz -A TandemRepeat -A ReferenceBases -R hg38.fa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6095
https://github.com/broadinstitute/gatk/issues/6096:79,Availability,down,download,79,"I obtain this reproducible issue with gatk 4.1.3.0:. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/78rz5wrhu9zkfzh/AACW9ZPhl4WnD-wmAkKcdHT3a?dl=0. Then setup a GATK working environment:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.3.0/gatk-4.1.3.0.zip; unzip gatk-4.1.3.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405\; .15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict; ```. Now if I run Mutect2:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. This will generate a VCF file with one variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:6,21:0.778:27:4,8:0,11:2,4,12,9; ```; With an allelic depth of six supporting the reference. However, there are only four fragments supporting the reference. If I remove those for fragments from the BAM file:; ```; samtools view -h input.bam | \; grep -v "":6112\|:10233\|:18618\|:20229"" | \; samtools view -Sb -o input2.bam && \; samtools index input2.bam; ```. And I run Mutect2 again:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input2.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. It will generate a VCF with the same variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:0,20:0.954:20:0,7:0,11:0,0,11,9; ```; With an allelic depth of zero supporting the reference. The same problem exists with the HaplotypeCaller. I believe this was not the intended ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096
https://github.com/broadinstitute/gatk/issues/6096:311,Availability,down,download,311,"I obtain this reproducible issue with gatk 4.1.3.0:. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/78rz5wrhu9zkfzh/AACW9ZPhl4WnD-wmAkKcdHT3a?dl=0. Then setup a GATK working environment:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.3.0/gatk-4.1.3.0.zip; unzip gatk-4.1.3.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405\; .15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict; ```. Now if I run Mutect2:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. This will generate a VCF file with one variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:6,21:0.778:27:4,8:0,11:2,4,12,9; ```; With an allelic depth of six supporting the reference. However, there are only four fragments supporting the reference. If I remove those for fragments from the BAM file:; ```; samtools view -h input.bam | \; grep -v "":6112\|:10233\|:18618\|:20229"" | \; samtools view -Sb -o input2.bam && \; samtools index input2.bam; ```. And I run Mutect2 again:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input2.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. It will generate a VCF with the same variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:0,20:0.954:20:0,7:0,11:0,0,11,9; ```; With an allelic depth of zero supporting the reference. The same problem exists with the HaplotypeCaller. I believe this was not the intended ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096
https://github.com/broadinstitute/gatk/issues/6096:392,Availability,down,download,392,"I obtain this reproducible issue with gatk 4.1.3.0:. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/78rz5wrhu9zkfzh/AACW9ZPhl4WnD-wmAkKcdHT3a?dl=0. Then setup a GATK working environment:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.3.0/gatk-4.1.3.0.zip; unzip gatk-4.1.3.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405\; .15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict; ```. Now if I run Mutect2:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. This will generate a VCF file with one variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:6,21:0.778:27:4,8:0,11:2,4,12,9; ```; With an allelic depth of six supporting the reference. However, there are only four fragments supporting the reference. If I remove those for fragments from the BAM file:; ```; samtools view -h input.bam | \; grep -v "":6112\|:10233\|:18618\|:20229"" | \; samtools view -Sb -o input2.bam && \; samtools index input2.bam; ```. And I run Mutect2 again:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input2.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. It will generate a VCF with the same variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:0,20:0.954:20:0,7:0,11:0,0,11,9; ```; With an allelic depth of zero supporting the reference. The same problem exists with the HaplotypeCaller. I believe this was not the intended ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096
https://github.com/broadinstitute/gatk/issues/6096:302,Deployability,release,releases,302,"I obtain this reproducible issue with gatk 4.1.3.0:. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/78rz5wrhu9zkfzh/AACW9ZPhl4WnD-wmAkKcdHT3a?dl=0. Then setup a GATK working environment:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.3.0/gatk-4.1.3.0.zip; unzip gatk-4.1.3.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405\; .15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict; ```. Now if I run Mutect2:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. This will generate a VCF file with one variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:6,21:0.778:27:4,8:0,11:2,4,12,9; ```; With an allelic depth of six supporting the reference. However, there are only four fragments supporting the reference. If I remove those for fragments from the BAM file:; ```; samtools view -h input.bam | \; grep -v "":6112\|:10233\|:18618\|:20229"" | \; samtools view -Sb -o input2.bam && \; samtools index input2.bam; ```. And I run Mutect2 again:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input2.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. It will generate a VCF with the same variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:0,20:0.954:20:0,7:0,11:0,0,11,9; ```; With an allelic depth of zero supporting the reference. The same problem exists with the HaplotypeCaller. I believe this was not the intended ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096
https://github.com/broadinstitute/gatk/issues/6096:383,Deployability,release,releases,383,"I obtain this reproducible issue with gatk 4.1.3.0:. First of all, you have to download the mini input.bam file from this dropbox link: https://www.dropbox.com/sh/78rz5wrhu9zkfzh/AACW9ZPhl4WnD-wmAkKcdHT3a?dl=0. Then setup a GATK working environment:; ```; wget https://github.com/broadinstitute/picard/releases/download/2.19.0/picard.jar. wget https://github.com/broadinstitute/gatk/releases/download/4.1.3.0/gatk-4.1.3.0.zip; unzip gatk-4.1.3.0.zip. wget -O- ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405\; .15_GRCh38_no_alt_analysis_set.fna.gz | \; gzip -d > GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. samtools faidx GCA_000001405.15_GRCh38_no_alt_analysis_set.fna. java -jar picard.jar \; CreateSequenceDictionary \; R=GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; O=GCA_000001405.15_GRCh38_no_alt_analysis_set.dict; ```. Now if I run Mutect2:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. This will generate a VCF file with one variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:6,21:0.778:27:4,8:0,11:2,4,12,9; ```; With an allelic depth of six supporting the reference. However, there are only four fragments supporting the reference. If I remove those for fragments from the BAM file:; ```; samtools view -h input.bam | \; grep -v "":6112\|:10233\|:18618\|:20229"" | \; samtools view -Sb -o input2.bam && \; samtools index input2.bam; ```. And I run Mutect2 again:; ```; gatk-4.1.3.0/gatk \; Mutect2 \; -R GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -I input2.bam \; -O output.vcf.gz \; -L chr1:233443225-233443225; ```. It will generate a VCF with the same variant:; ```; GT:AD:AF:DP:F1R2:F2R1:SB; 0/1:0,20:0.954:20:0,7:0,11:0,0,11,9; ```; With an allelic depth of zero supporting the reference. The same problem exists with the HaplotypeCaller. I believe this was not the intended ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6096
https://github.com/broadinstitute/gatk/issues/6097:74,Availability,down,downstream,74,See the log: https://people.freebsd.org/~pi/logs/gatk-4.1.2.0.43.log. The downstream bug report: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=239901,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6097
https://github.com/broadinstitute/gatk/issues/6097:8,Testability,log,log,8,See the log: https://people.freebsd.org/~pi/logs/gatk-4.1.2.0.43.log. The downstream bug report: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=239901,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6097
https://github.com/broadinstitute/gatk/issues/6097:44,Testability,log,logs,44,See the log: https://people.freebsd.org/~pi/logs/gatk-4.1.2.0.43.log. The downstream bug report: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=239901,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6097
https://github.com/broadinstitute/gatk/issues/6097:65,Testability,log,log,65,See the log: https://people.freebsd.org/~pi/logs/gatk-4.1.2.0.43.log. The downstream bug report: https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=239901,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6097
https://github.com/broadinstitute/gatk/issues/6098:416,Availability,error,error,416,"As discussed here is the information regarding the OutOfBoundsException. Please find below the user report:. I am running the Mutect2 pipeline on canine tumor samples in Terra, using WDL version 2.5 and GATK version 4.1.2.0. I was able to run the pipeline successfully without entering a germline resource file or VCF of common variants for contamination, however, when I did add these files in, I got the following error:. ```; java.lang.IndexOutOfBoundsException: Index: 5, Size: 5; 	at java.util.ArrayList.rangeCheck(ArrayList.java:657); 	at java.util.ArrayList.get(ArrayList.java:433); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$31(SomaticGenotypingEngine.java:350); 	at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:352); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:335); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:250); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(Assembl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:3915,Availability,down,downsampling-stride,3915,"ion_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -I gs://fc-8268e82b-ed61-4e04-a8c9-a95a05c0952e/bda6f5ba-8928-45bf-a6b0-9fe67d8dd9a4/PreProcessingForVariantDiscovery_GATK4/cccdda67-56e1-4363-aa6c-46ce53ef8afd/call-GatherBamFiles/attempt-2/Abrams_cell.bam -tumor Abrams_1 --germline-resource gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/canid_wgs_ref.1.0.no_samples.vcf.gz -pon gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/b92f3c35-5813-455b-94dc-3de3b54f5f98/Mutect2_Panel/c9f21d8a-384e-4d17-a6f8-79a502698827/call-MergeVCFs/1-Mutect2_PON_2019-07-25T22-08-49.vcf -L gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/f2138b33-3918-4f8a-9b87-1823a0084ac3/Mutect2/c4844164-ecad-4878-9e5d-cd134a7fb40d/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf --f1r2-tar-gz f1r2.tar.gz --af-of-alleles-not-in-resource 0.0007 --downsampling-stride 20 --max-reads-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6`; ```. The germline resource is a VCF of approximately 80 million SNPs and indels (including multi allelic sites) called from a large number of canine WGS. It is formatted as a VCF with no sample information:; ```; chr1 240 . TG T 464.40 PASS AC=4;AF=0.011;AN=332;BaseQRankSum=0.674;ClippingRankSum=0;DP=14798;ExcessHet=0.0026;FS=5.63;InbreedingCoeff=-0.005;MLEAC=14;MLEAF=0.017;MQ=7.49;MQRankSum=-0.967;QD=22.11;ReadPosRankSum=0.967;SOR=3.18; ```. The VCF for variants for contamination is a subset of this VCF, with only biallelic SNPs with AF between 0.01 and 0.2. Initially, it was formatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:4929,Availability,error,error,4929,"ds-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6`; ```. The germline resource is a VCF of approximately 80 million SNPs and indels (including multi allelic sites) called from a large number of canine WGS. It is formatted as a VCF with no sample information:; ```; chr1 240 . TG T 464.40 PASS AC=4;AF=0.011;AN=332;BaseQRankSum=0.674;ClippingRankSum=0;DP=14798;ExcessHet=0.0026;FS=5.63;InbreedingCoeff=-0.005;MLEAC=14;MLEAF=0.017;MQ=7.49;MQRankSum=-0.967;QD=22.11;ReadPosRankSum=0.967;SOR=3.18; ```. The VCF for variants for contamination is a subset of this VCF, with only biallelic SNPs with AF between 0.01 and 0.2. Initially, it was formatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:5002,Availability,error,error,5002,"ds-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6`; ```. The germline resource is a VCF of approximately 80 million SNPs and indels (including multi allelic sites) called from a large number of canine WGS. It is formatted as a VCF with no sample information:; ```; chr1 240 . TG T 464.40 PASS AC=4;AF=0.011;AN=332;BaseQRankSum=0.674;ClippingRankSum=0;DP=14798;ExcessHet=0.0026;FS=5.63;InbreedingCoeff=-0.005;MLEAC=14;MLEAF=0.017;MQ=7.49;MQRankSum=-0.967;QD=22.11;ReadPosRankSum=0.967;SOR=3.18; ```. The VCF for variants for contamination is a subset of this VCF, with only biallelic SNPs with AF between 0.01 and 0.2. Initially, it was formatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:5585,Availability,error,errorProbability,5585,"ormatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:5698,Availability,Error,ErrorProbabilities,5698,"eld of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutec",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:5730,Availability,Error,ErrorProbabilities,5730,"ntamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(Filte",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:6440,Availability,Error,ErrorProbabilities,6440,ateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractP,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:6466,Availability,Error,ErrorProbabilities,6466,taminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:10319,Availability,error,error-with-canine-germline-resource-and-variants-for-contamination-files,10319,"mmandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /root/gatk.jar FilterMutectCalls -V gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeVCFs/Abrams_cell-unfiltered.vcf -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -O Abrams_cell-filtered.vcf --contamination-table /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/contamination.table --tumor-segmentation /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/segments.table --ob-priors /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-LearnReadOrientationModel/artifact-priors.tar.gz -stats /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeStats/merged.stats --filtering-stats filtering.stats --min-median-read-position 10; ```. Both of these tests were run on an interval that included a single chromosome (approximately 24Mb). Thank you for your help!. Best,; Kate. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24345/m2-error-with-canine-germline-resource-and-variants-for-contamination-files/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:134,Deployability,pipeline,pipeline,134,"As discussed here is the information regarding the OutOfBoundsException. Please find below the user report:. I am running the Mutect2 pipeline on canine tumor samples in Terra, using WDL version 2.5 and GATK version 4.1.2.0. I was able to run the pipeline successfully without entering a germline resource file or VCF of common variants for contamination, however, when I did add these files in, I got the following error:. ```; java.lang.IndexOutOfBoundsException: Index: 5, Size: 5; 	at java.util.ArrayList.rangeCheck(ArrayList.java:657); 	at java.util.ArrayList.get(ArrayList.java:433); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$31(SomaticGenotypingEngine.java:350); 	at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:352); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:335); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:250); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(Assembl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:247,Deployability,pipeline,pipeline,247,"As discussed here is the information regarding the OutOfBoundsException. Please find below the user report:. I am running the Mutect2 pipeline on canine tumor samples in Terra, using WDL version 2.5 and GATK version 4.1.2.0. I was able to run the pipeline successfully without entering a germline resource file or VCF of common variants for contamination, however, when I did add these files in, I got the following error:. ```; java.lang.IndexOutOfBoundsException: Index: 5, Size: 5; 	at java.util.ArrayList.rangeCheck(ArrayList.java:657); 	at java.util.ArrayList.get(ArrayList.java:433); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$31(SomaticGenotypingEngine.java:350); 	at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:352); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:335); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:250); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(Assembl",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:5851,Energy Efficiency,Reduce,ReduceOps,5851,"or contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWa",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:5882,Energy Efficiency,Reduce,ReduceOps,5882,"e. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at or",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:6165,Energy Efficiency,Reduce,ReduceOps,6165, org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.uti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:6175,Energy Efficiency,Reduce,ReduceOp,6175, org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.uti,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:6203,Energy Efficiency,Reduce,ReduceOps,6203,bender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemain,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:1022,Integrability,wrap,wrapAndCopyInto,1022,"egarding the OutOfBoundsException. Please find below the user report:. I am running the Mutect2 pipeline on canine tumor samples in Terra, using WDL version 2.5 and GATK version 4.1.2.0. I was able to run the pipeline successfully without entering a germline resource file or VCF of common variants for contamination, however, when I did add these files in, I got the following error:. ```; java.lang.IndexOutOfBoundsException: Index: 5, Size: 5; 	at java.util.ArrayList.rangeCheck(ArrayList.java:657); 	at java.util.ArrayList.get(ArrayList.java:433); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.lambda$getGermlineAltAlleleFrequencies$31(SomaticGenotypingEngine.java:350); 	at java.util.stream.ReferencePipeline$6$1.accept(ReferencePipeline.java:244); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:545); 	at java.util.stream.AbstractPipeline.evaluateToArrayNode(AbstractPipeline.java:260); 	at java.util.stream.DoublePipeline.toArray(DoublePipeline.java:506); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getGermlineAltAlleleFrequencies(SomaticGenotypingEngine.java:352); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.getNegativeLogPopulationAFAnnotation(SomaticGenotypingEngine.java:335); 	at org.broadinstitute.hellbender.tools.walkers.mutect.SomaticGenotypingEngine.callMutations(SomaticGenotypingEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2Engine.callRegion(Mutect2Engine.java:250); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broad",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:6100,Integrability,wrap,wrapAndCopyInto,6100,te.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:499); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePip,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:7425,Integrability,wrap,wrapAndCopyInto,7425,rrorProbabilities.<init>(ErrorProbabilities.java:19); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2FilteringEngine.accumulateData(Mutect2FilteringEngine.java:141); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.FilterMutectCalls.nthPassApply(FilterMutectCalls.java:146); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverse$0(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.lambda$traverseVariants$1(MultiplePassVariantWalker.java:77); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184); 	at java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175); 	at java.util.Iterator.forEachRemaining(Iterator.java:116); 	at java.util.Spliterators$IteratorSpliterator.forEachRemaining(Spliterators.java:1801); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151); 	at java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174); 	at java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitu,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:2782,Modifiability,variab,variable,2782,llRegion(Mutect2Engine.java:250); 	at org.broadinstitute.hellbender.tools.walkers.mutect.Mutect2.apply(Mutect2.java:324); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.processReadShard(AssemblyRegionWalker.java:308); 	at org.broadinstitute.hellbender.engine.AssemblyRegionWalker.traverse(AssemblyRegionWalker.java:281); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1039); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx3000m -jar /root/gatk.jar Mutect2 -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -I gs://fc-8268e82b-ed61-4e04-a8c9-a95a05c0952e/bda6f5ba-8928-45bf-a6b0-9fe67d8dd9a4/PreProcessingForVariantDiscovery_GATK4/cccdda67-56e1-4363-aa6c-46ce53ef8afd/call-GatherBamFiles/attempt-2/Abrams_cell.bam -tumor Abrams_1 --germline-resource gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/canid_wgs_ref.1.0.no_samples.vcf.gz -pon gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/b92f3c35-5813-455b-94dc-3de3b54f5f98/Mutect2_Panel/c9f21d8a-384e-4d17-a6f8-79a502698827/call-MergeVCFs/1-Mutect2_PON_2019-07-25T22-08-49.vcf -L gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/f2138b33-3918-4f8a-9b87-1823a0084ac3/Mutect2/c4844164-ecad-4878-9e5d-cd134a7fb40d/call-SplitIntervals/glob-0fc990c5ca95eebc97c4c,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:8677,Modifiability,variab,variable,8677,AbstractPipeline.java:234); 	at java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:418); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverseVariants(MultiplePassVariantWalker.java:75); 	at org.broadinstitute.hellbender.engine.MultiplePassVariantWalker.traverse(MultiplePassVariantWalker.java:40); 	at org.broadinstitute.hellbender.engine.GATKTool.doWork(GATKTool.java:1048); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.runTool(CommandLineProgram.java:139); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMainPostParseArgs(CommandLineProgram.java:191); 	at org.broadinstitute.hellbender.cmdline.CommandLineProgram.instanceMain(CommandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /root/gatk.jar FilterMutectCalls -V gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeVCFs/Abrams_cell-unfiltered.vcf -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -O Abrams_cell-filtered.vcf --contamination-table /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/contamination.table --tumor-segmentation /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/segments.table --ob-priors /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e,MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:5142,Security,validat,validateArg,5142,"number of canine WGS. It is formatted as a VCF with no sample information:; ```; chr1 240 . TG T 464.40 PASS AC=4;AF=0.011;AN=332;BaseQRankSum=0.674;ClippingRankSum=0;DP=14798;ExcessHet=0.0026;FS=5.63;InbreedingCoeff=-0.005;MLEAC=14;MLEAF=0.017;MQ=7.49;MQRankSum=-0.967;QD=22.11;ReadPosRankSum=0.967;SOR=3.18; ```. The VCF for variants for contamination is a subset of this VCF, with only biallelic SNPs with AF between 0.01 and 0.2. Initially, it was formatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at java.util.stream.Collectors.lambda$toMap$58(Collectors.java:1321); 	at java.util.stream.ReduceOps$3ReducingSink.accept(ReduceOps.java:169); 	at java.util.ArrayList$ArrayListSpliterator.forEachRemaining(ArrayList.java:1382); 	at java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:481); 	at java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:471); 	at jav",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:10075,Testability,test,tests,10075,"mmandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /root/gatk.jar FilterMutectCalls -V gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeVCFs/Abrams_cell-unfiltered.vcf -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -O Abrams_cell-filtered.vcf --contamination-table /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/contamination.table --tumor-segmentation /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/segments.table --ob-priors /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-LearnReadOrientationModel/artifact-priors.tar.gz -stats /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeStats/merged.stats --filtering-stats filtering.stats --min-median-read-position 10; ```. Both of these tests were run on an interval that included a single chromosome (approximately 24Mb). Thank you for your help!. Best,; Kate. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24345/m2-error-with-canine-germline-resource-and-variants-for-contamination-files/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:4799,Usability,simpl,simplified,4799,"vals/glob-0fc990c5ca95eebc97c4c204e3e303e1/0000-scattered.interval_list -O output.vcf --f1r2-tar-gz f1r2.tar.gz --af-of-alleles-not-in-resource 0.0007 --downsampling-stride 20 --max-reads-per-alignment-start 6 --max-suspicious-reads-per-alignment-start 6`; ```. The germline resource is a VCF of approximately 80 million SNPs and indels (including multi allelic sites) called from a large number of canine WGS. It is formatted as a VCF with no sample information:; ```; chr1 240 . TG T 464.40 PASS AC=4;AF=0.011;AN=332;BaseQRankSum=0.674;ClippingRankSum=0;DP=14798;ExcessHet=0.0026;FS=5.63;InbreedingCoeff=-0.005;MLEAC=14;MLEAF=0.017;MQ=7.49;MQRankSum=-0.967;QD=22.11;ReadPosRankSum=0.967;SOR=3.18; ```. The VCF for variants for contamination is a subset of this VCF, with only biallelic SNPs with AF between 0.01 and 0.2. Initially, it was formatted the same as the above file. As part of debugging, I tried removing everything from the INFO field of the variants for contamination file, except allele frequency, and I tried using that simplified VCF both for the germline resource and the variants for contamination file. This seemed to fix the index out of bounds error, but the job then failed at the filtering step, with the following error:. ```; java.lang.IllegalArgumentException: log10p: Log10-probability must be 0 or less; 	at org.broadinstitute.hellbender.utils.Utils.validateArg(Utils.java:724); 	at org.broadinstitute.hellbender.utils.MathUtils.log10BinomialProbability(MathUtils.java:934); 	at org.broadinstitute.hellbender.utils.MathUtils.binomialProbability(MathUtils.java:927); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ContaminationFilter.calculateErrorProbability(ContaminationFilter.java:56); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.Mutect2VariantFilter.errorProbability(Mutect2VariantFilter.java:15); 	at org.broadinstitute.hellbender.tools.walkers.mutect.filtering.ErrorProbabilities.lambda$new$1(ErrorProbabilities.java:19); 	at ",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
https://github.com/broadinstitute/gatk/issues/6098:9769,Usability,Learn,LearnReadOrientationModel,9769,"mmandLineProgram.java:210); 	at org.broadinstitute.hellbender.Main.runCommandLineProgram(Main.java:162); 	at org.broadinstitute.hellbender.Main.mainEntry(Main.java:205); 	at org.broadinstitute.hellbender.Main.main(Main.java:291); Using GATK jar /root/gatk.jar defined in environment variable GATK_LOCAL_JAR; Running:; java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -Xmx6500m -jar /root/gatk.jar FilterMutectCalls -V gs://fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeVCFs/Abrams_cell-unfiltered.vcf -R gs://fc-0b0cb3ce-e2cb-4aef-a8b2-08e60d78e87c/Canis_lupus_familiaris_assembly3.fasta -O Abrams_cell-filtered.vcf --contamination-table /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/contamination.table --tumor-segmentation /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-CalculateContamination/segments.table --ob-priors /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-LearnReadOrientationModel/artifact-priors.tar.gz -stats /cromwell_root/fc-afa03a31-404c-4a93-9f6a-31b673db5c69/0bbb4e0e-7293-4ce5-b81f-d722fcec561a/Mutect2/223610c8-ec63-4439-b339-9503ceb80828/call-MergeStats/merged.stats --filtering-stats filtering.stats --min-median-read-position 10; ```. Both of these tests were run on an interval that included a single chromosome (approximately 24Mb). Thank you for your help!. Best,; Kate. This Issue was generated from your [forums] ; [forums]: https://gatkforums.broadinstitute.org/gatk/discussion/24345/m2-error-with-canine-germline-resource-and-variants-for-contamination-files/p1",MatchSource.ISSUE,broadinstitute,gatk,4.6.0.0,https://software.broadinstitute.org/gatk,https://github.com/broadinstitute/gatk/issues/6098
