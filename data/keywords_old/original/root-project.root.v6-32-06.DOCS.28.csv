id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:1796,Modifiability,variab,variable,1796,"e-types-functions-variables-and-enumerators-properly>`_; states:. Variable names should be nouns (as they represent state). The name should be; camel case, and start with an upper case letter (e.g. Leader or Boats). This rule is the same as that for type names. This is a problem because the; type name cannot be reused for a variable name [*]_. LLVM developers tend to; work around this by either prepending ``The`` to the type name::. Triple TheTriple;. ... or more commonly use an acronym, despite the coding standard stating ""Avoid; abbreviations unless they are well known""::. Triple T;. The proliferation of acronyms leads to hard-to-read code such as `this; <https://github.com/llvm/llvm-project/blob/0a8bc14ad7f3209fe702d18e250194cd90188596/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp#L7445>`_::. InnerLoopVectorizer LB(L, PSE, LI, DT, TLI, TTI, AC, ORE, VF.Width, IC,; &LVL, &CM);. Many other coding guidelines [LLDB]_ [Google]_ [WebKit]_ [Qt]_ [Rust]_ [Swift]_; [Python]_ require that variable names begin with a lower case letter in contrast; to class names which begin with a capital letter. This convention means that the; most readable variable name also requires the least thought::. Triple triple;. There is some agreement that the current rule is broken [LattnerAgree]_; [ArsenaultAgree]_ [RobinsonAgree]_ and that acronyms are an obstacle to reading; new code [MalyutinDistinguish]_ [CarruthAcronym]_ [PicusAcronym]_. There are; some opposing views [ParzyszekAcronym2]_ [RicciAcronyms]_. This work-in-progress proposal is to change the coding standard for variable; names to require that they start with a lower case letter. .. [*] In `some cases; <https://github.com/llvm/llvm-project/blob/8b72080d4d7b13072f371712eed333f987b7a18e/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp#L2727>`_; the type name *is* reused as a variable name, but this shadows the type name; and confuses many debuggers [DenisovCamelBack]_. Variable Names Coding Standard Options; =======================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:1951,Modifiability,variab,variable,1951,", and start with an upper case letter (e.g. Leader or Boats). This rule is the same as that for type names. This is a problem because the; type name cannot be reused for a variable name [*]_. LLVM developers tend to; work around this by either prepending ``The`` to the type name::. Triple TheTriple;. ... or more commonly use an acronym, despite the coding standard stating ""Avoid; abbreviations unless they are well known""::. Triple T;. The proliferation of acronyms leads to hard-to-read code such as `this; <https://github.com/llvm/llvm-project/blob/0a8bc14ad7f3209fe702d18e250194cd90188596/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp#L7445>`_::. InnerLoopVectorizer LB(L, PSE, LI, DT, TLI, TTI, AC, ORE, VF.Width, IC,; &LVL, &CM);. Many other coding guidelines [LLDB]_ [Google]_ [WebKit]_ [Qt]_ [Rust]_ [Swift]_; [Python]_ require that variable names begin with a lower case letter in contrast; to class names which begin with a capital letter. This convention means that the; most readable variable name also requires the least thought::. Triple triple;. There is some agreement that the current rule is broken [LattnerAgree]_; [ArsenaultAgree]_ [RobinsonAgree]_ and that acronyms are an obstacle to reading; new code [MalyutinDistinguish]_ [CarruthAcronym]_ [PicusAcronym]_. There are; some opposing views [ParzyszekAcronym2]_ [RicciAcronyms]_. This work-in-progress proposal is to change the coding standard for variable; names to require that they start with a lower case letter. .. [*] In `some cases; <https://github.com/llvm/llvm-project/blob/8b72080d4d7b13072f371712eed333f987b7a18e/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp#L2727>`_; the type name *is* reused as a variable name, but this shadows the type name; and confuses many debuggers [DenisovCamelBack]_. Variable Names Coding Standard Options; ======================================. There are two main options for variable names that begin with a lower case; letter: ``camelBack`` and ``lower_case``. (These are also kno",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:2375,Modifiability,variab,variable,2375,"""::. Triple T;. The proliferation of acronyms leads to hard-to-read code such as `this; <https://github.com/llvm/llvm-project/blob/0a8bc14ad7f3209fe702d18e250194cd90188596/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp#L7445>`_::. InnerLoopVectorizer LB(L, PSE, LI, DT, TLI, TTI, AC, ORE, VF.Width, IC,; &LVL, &CM);. Many other coding guidelines [LLDB]_ [Google]_ [WebKit]_ [Qt]_ [Rust]_ [Swift]_; [Python]_ require that variable names begin with a lower case letter in contrast; to class names which begin with a capital letter. This convention means that the; most readable variable name also requires the least thought::. Triple triple;. There is some agreement that the current rule is broken [LattnerAgree]_; [ArsenaultAgree]_ [RobinsonAgree]_ and that acronyms are an obstacle to reading; new code [MalyutinDistinguish]_ [CarruthAcronym]_ [PicusAcronym]_. There are; some opposing views [ParzyszekAcronym2]_ [RicciAcronyms]_. This work-in-progress proposal is to change the coding standard for variable; names to require that they start with a lower case letter. .. [*] In `some cases; <https://github.com/llvm/llvm-project/blob/8b72080d4d7b13072f371712eed333f987b7a18e/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp#L2727>`_; the type name *is* reused as a variable name, but this shadows the type name; and confuses many debuggers [DenisovCamelBack]_. Variable Names Coding Standard Options; ======================================. There are two main options for variable names that begin with a lower case; letter: ``camelBack`` and ``lower_case``. (These are also known by other names; but here we use the terminology from clang-tidy). ``camelBack`` is consistent with [WebKit]_, [Qt]_ and [Swift]_ while; ``lower_case`` is consistent with [LLDB]_, [Google]_, [Rust]_ and [Python]_. ``camelBack`` is already used for function names, which may be considered an; advantage [LattnerFunction]_ or a disadvantage [CarruthFunction]_. Approval for ``camelBack`` was expressed by [DenisovCamelBack]",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:2639,Modifiability,variab,variable,2639,"VF.Width, IC,; &LVL, &CM);. Many other coding guidelines [LLDB]_ [Google]_ [WebKit]_ [Qt]_ [Rust]_ [Swift]_; [Python]_ require that variable names begin with a lower case letter in contrast; to class names which begin with a capital letter. This convention means that the; most readable variable name also requires the least thought::. Triple triple;. There is some agreement that the current rule is broken [LattnerAgree]_; [ArsenaultAgree]_ [RobinsonAgree]_ and that acronyms are an obstacle to reading; new code [MalyutinDistinguish]_ [CarruthAcronym]_ [PicusAcronym]_. There are; some opposing views [ParzyszekAcronym2]_ [RicciAcronyms]_. This work-in-progress proposal is to change the coding standard for variable; names to require that they start with a lower case letter. .. [*] In `some cases; <https://github.com/llvm/llvm-project/blob/8b72080d4d7b13072f371712eed333f987b7a18e/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp#L2727>`_; the type name *is* reused as a variable name, but this shadows the type name; and confuses many debuggers [DenisovCamelBack]_. Variable Names Coding Standard Options; ======================================. There are two main options for variable names that begin with a lower case; letter: ``camelBack`` and ``lower_case``. (These are also known by other names; but here we use the terminology from clang-tidy). ``camelBack`` is consistent with [WebKit]_, [Qt]_ and [Swift]_ while; ``lower_case`` is consistent with [LLDB]_, [Google]_, [Rust]_ and [Python]_. ``camelBack`` is already used for function names, which may be considered an; advantage [LattnerFunction]_ or a disadvantage [CarruthFunction]_. Approval for ``camelBack`` was expressed by [DenisovCamelBack]_; [LattnerFunction]_ [IvanovicDistinguish]_.; Opposition to ``camelBack`` was expressed by [CarruthCamelBack]_; [TurnerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLow",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:2846,Modifiability,variab,variable,2846,"ch begin with a capital letter. This convention means that the; most readable variable name also requires the least thought::. Triple triple;. There is some agreement that the current rule is broken [LattnerAgree]_; [ArsenaultAgree]_ [RobinsonAgree]_ and that acronyms are an obstacle to reading; new code [MalyutinDistinguish]_ [CarruthAcronym]_ [PicusAcronym]_. There are; some opposing views [ParzyszekAcronym2]_ [RicciAcronyms]_. This work-in-progress proposal is to change the coding standard for variable; names to require that they start with a lower case letter. .. [*] In `some cases; <https://github.com/llvm/llvm-project/blob/8b72080d4d7b13072f371712eed333f987b7a18e/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp#L2727>`_; the type name *is* reused as a variable name, but this shadows the type name; and confuses many debuggers [DenisovCamelBack]_. Variable Names Coding Standard Options; ======================================. There are two main options for variable names that begin with a lower case; letter: ``camelBack`` and ``lower_case``. (These are also known by other names; but here we use the terminology from clang-tidy). ``camelBack`` is consistent with [WebKit]_, [Qt]_ and [Swift]_ while; ``lower_case`` is consistent with [LLDB]_, [Google]_, [Rust]_ and [Python]_. ``camelBack`` is already used for function names, which may be considered an; advantage [LattnerFunction]_ or a disadvantage [CarruthFunction]_. Approval for ``camelBack`` was expressed by [DenisovCamelBack]_; [LattnerFunction]_ [IvanovicDistinguish]_.; Opposition to ``camelBack`` was expressed by [CarruthCamelBack]_; [TurnerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLower]_. Differentiating variable kinds; ------------------------------. An additional requested change is to distinguish between different kinds of; variables [RobinsonDistinguish]_ [RobinsonDistinguish2]_ [Jone",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:3686,Modifiability,variab,variable,3686,"ny debuggers [DenisovCamelBack]_. Variable Names Coding Standard Options; ======================================. There are two main options for variable names that begin with a lower case; letter: ``camelBack`` and ``lower_case``. (These are also known by other names; but here we use the terminology from clang-tidy). ``camelBack`` is consistent with [WebKit]_, [Qt]_ and [Swift]_ while; ``lower_case`` is consistent with [LLDB]_, [Google]_, [Rust]_ and [Python]_. ``camelBack`` is already used for function names, which may be considered an; advantage [LattnerFunction]_ or a disadvantage [CarruthFunction]_. Approval for ``camelBack`` was expressed by [DenisovCamelBack]_; [LattnerFunction]_ [IvanovicDistinguish]_.; Opposition to ``camelBack`` was expressed by [CarruthCamelBack]_; [TurnerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLower]_. Differentiating variable kinds; ------------------------------. An additional requested change is to distinguish between different kinds of; variables [RobinsonDistinguish]_ [RobinsonDistinguish2]_ [JonesDistinguish]_; [IvanovicDistinguish]_ [CarruthDistinguish]_ [MalyutinDistinguish]_. Others oppose this idea [HähnleDistinguish]_ [GreeneDistinguish]_; [HendersonPrefix]_. A possibility is for member variables to be prefixed with ``m_`` and for global; variables to be prefixed with ``g_`` to distinguish them from local variables.; This is consistent with [LLDB]_. The ``m_`` prefix is consistent with [WebKit]_. A variation is for member variables to be prefixed with ``m``; [IvanovicDistinguish]_ [BeylsDistinguish]_. This is consistent with [Mozilla]_. Another option is for member variables to be suffixed with ``_`` which is; consistent with [Google]_ and similar to [Python]_. Opposed by; [ParzyszekDistinguish]_. Reducing the number of acronyms; ===============================. While switching coding standard will make",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:3811,Modifiability,variab,variables,3811," variable names that begin with a lower case; letter: ``camelBack`` and ``lower_case``. (These are also known by other names; but here we use the terminology from clang-tidy). ``camelBack`` is consistent with [WebKit]_, [Qt]_ and [Swift]_ while; ``lower_case`` is consistent with [LLDB]_, [Google]_, [Rust]_ and [Python]_. ``camelBack`` is already used for function names, which may be considered an; advantage [LattnerFunction]_ or a disadvantage [CarruthFunction]_. Approval for ``camelBack`` was expressed by [DenisovCamelBack]_; [LattnerFunction]_ [IvanovicDistinguish]_.; Opposition to ``camelBack`` was expressed by [CarruthCamelBack]_; [TurnerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLower]_. Differentiating variable kinds; ------------------------------. An additional requested change is to distinguish between different kinds of; variables [RobinsonDistinguish]_ [RobinsonDistinguish2]_ [JonesDistinguish]_; [IvanovicDistinguish]_ [CarruthDistinguish]_ [MalyutinDistinguish]_. Others oppose this idea [HähnleDistinguish]_ [GreeneDistinguish]_; [HendersonPrefix]_. A possibility is for member variables to be prefixed with ``m_`` and for global; variables to be prefixed with ``g_`` to distinguish them from local variables.; This is consistent with [LLDB]_. The ``m_`` prefix is consistent with [WebKit]_. A variation is for member variables to be prefixed with ``m``; [IvanovicDistinguish]_ [BeylsDistinguish]_. This is consistent with [Mozilla]_. Another option is for member variables to be suffixed with ``_`` which is; consistent with [Google]_ and similar to [Python]_. Opposed by; [ParzyszekDistinguish]_. Reducing the number of acronyms; ===============================. While switching coding standard will make it easier to use non-acronym names for; new code, it doesn't improve the existing large body of code that uses acronyms; extensively to the det",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:4073,Modifiability,variab,variables,4073," [LLDB]_, [Google]_, [Rust]_ and [Python]_. ``camelBack`` is already used for function names, which may be considered an; advantage [LattnerFunction]_ or a disadvantage [CarruthFunction]_. Approval for ``camelBack`` was expressed by [DenisovCamelBack]_; [LattnerFunction]_ [IvanovicDistinguish]_.; Opposition to ``camelBack`` was expressed by [CarruthCamelBack]_; [TurnerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLower]_. Differentiating variable kinds; ------------------------------. An additional requested change is to distinguish between different kinds of; variables [RobinsonDistinguish]_ [RobinsonDistinguish2]_ [JonesDistinguish]_; [IvanovicDistinguish]_ [CarruthDistinguish]_ [MalyutinDistinguish]_. Others oppose this idea [HähnleDistinguish]_ [GreeneDistinguish]_; [HendersonPrefix]_. A possibility is for member variables to be prefixed with ``m_`` and for global; variables to be prefixed with ``g_`` to distinguish them from local variables.; This is consistent with [LLDB]_. The ``m_`` prefix is consistent with [WebKit]_. A variation is for member variables to be prefixed with ``m``; [IvanovicDistinguish]_ [BeylsDistinguish]_. This is consistent with [Mozilla]_. Another option is for member variables to be suffixed with ``_`` which is; consistent with [Google]_ and similar to [Python]_. Opposed by; [ParzyszekDistinguish]_. Reducing the number of acronyms; ===============================. While switching coding standard will make it easier to use non-acronym names for; new code, it doesn't improve the existing large body of code that uses acronyms; extensively to the detriment of its readability. Further, it is natural and; generally encouraged that new code be written in the style of the surrounding; code. Therefore it is likely that much newly written code will also use; acronyms despite what the coding standard says, much as it is today. As ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:4126,Modifiability,variab,variables,4126," [LLDB]_, [Google]_, [Rust]_ and [Python]_. ``camelBack`` is already used for function names, which may be considered an; advantage [LattnerFunction]_ or a disadvantage [CarruthFunction]_. Approval for ``camelBack`` was expressed by [DenisovCamelBack]_; [LattnerFunction]_ [IvanovicDistinguish]_.; Opposition to ``camelBack`` was expressed by [CarruthCamelBack]_; [TurnerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLower]_. Differentiating variable kinds; ------------------------------. An additional requested change is to distinguish between different kinds of; variables [RobinsonDistinguish]_ [RobinsonDistinguish2]_ [JonesDistinguish]_; [IvanovicDistinguish]_ [CarruthDistinguish]_ [MalyutinDistinguish]_. Others oppose this idea [HähnleDistinguish]_ [GreeneDistinguish]_; [HendersonPrefix]_. A possibility is for member variables to be prefixed with ``m_`` and for global; variables to be prefixed with ``g_`` to distinguish them from local variables.; This is consistent with [LLDB]_. The ``m_`` prefix is consistent with [WebKit]_. A variation is for member variables to be prefixed with ``m``; [IvanovicDistinguish]_ [BeylsDistinguish]_. This is consistent with [Mozilla]_. Another option is for member variables to be suffixed with ``_`` which is; consistent with [Google]_ and similar to [Python]_. Opposed by; [ParzyszekDistinguish]_. Reducing the number of acronyms; ===============================. While switching coding standard will make it easier to use non-acronym names for; new code, it doesn't improve the existing large body of code that uses acronyms; extensively to the detriment of its readability. Further, it is natural and; generally encouraged that new code be written in the style of the surrounding; code. Therefore it is likely that much newly written code will also use; acronyms despite what the coding standard says, much as it is today. As ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:4194,Modifiability,variab,variables,4194," [LLDB]_, [Google]_, [Rust]_ and [Python]_. ``camelBack`` is already used for function names, which may be considered an; advantage [LattnerFunction]_ or a disadvantage [CarruthFunction]_. Approval for ``camelBack`` was expressed by [DenisovCamelBack]_; [LattnerFunction]_ [IvanovicDistinguish]_.; Opposition to ``camelBack`` was expressed by [CarruthCamelBack]_; [TurnerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLower]_. Differentiating variable kinds; ------------------------------. An additional requested change is to distinguish between different kinds of; variables [RobinsonDistinguish]_ [RobinsonDistinguish2]_ [JonesDistinguish]_; [IvanovicDistinguish]_ [CarruthDistinguish]_ [MalyutinDistinguish]_. Others oppose this idea [HähnleDistinguish]_ [GreeneDistinguish]_; [HendersonPrefix]_. A possibility is for member variables to be prefixed with ``m_`` and for global; variables to be prefixed with ``g_`` to distinguish them from local variables.; This is consistent with [LLDB]_. The ``m_`` prefix is consistent with [WebKit]_. A variation is for member variables to be prefixed with ``m``; [IvanovicDistinguish]_ [BeylsDistinguish]_. This is consistent with [Mozilla]_. Another option is for member variables to be suffixed with ``_`` which is; consistent with [Google]_ and similar to [Python]_. Opposed by; [ParzyszekDistinguish]_. Reducing the number of acronyms; ===============================. While switching coding standard will make it easier to use non-acronym names for; new code, it doesn't improve the existing large body of code that uses acronyms; extensively to the detriment of its readability. Further, it is natural and; generally encouraged that new code be written in the style of the surrounding; code. Therefore it is likely that much newly written code will also use; acronyms despite what the coding standard says, much as it is today. As ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:4313,Modifiability,variab,variables,4313," was expressed by [DenisovCamelBack]_; [LattnerFunction]_ [IvanovicDistinguish]_.; Opposition to ``camelBack`` was expressed by [CarruthCamelBack]_; [TurnerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLower]_. Differentiating variable kinds; ------------------------------. An additional requested change is to distinguish between different kinds of; variables [RobinsonDistinguish]_ [RobinsonDistinguish2]_ [JonesDistinguish]_; [IvanovicDistinguish]_ [CarruthDistinguish]_ [MalyutinDistinguish]_. Others oppose this idea [HähnleDistinguish]_ [GreeneDistinguish]_; [HendersonPrefix]_. A possibility is for member variables to be prefixed with ``m_`` and for global; variables to be prefixed with ``g_`` to distinguish them from local variables.; This is consistent with [LLDB]_. The ``m_`` prefix is consistent with [WebKit]_. A variation is for member variables to be prefixed with ``m``; [IvanovicDistinguish]_ [BeylsDistinguish]_. This is consistent with [Mozilla]_. Another option is for member variables to be suffixed with ``_`` which is; consistent with [Google]_ and similar to [Python]_. Opposed by; [ParzyszekDistinguish]_. Reducing the number of acronyms; ===============================. While switching coding standard will make it easier to use non-acronym names for; new code, it doesn't improve the existing large body of code that uses acronyms; extensively to the detriment of its readability. Further, it is natural and; generally encouraged that new code be written in the style of the surrounding; code. Therefore it is likely that much newly written code will also use; acronyms despite what the coding standard says, much as it is today. As well as changing the case of variable names, they could also be expanded to; their non-acronym form e.g. ``Triple T`` → ``Triple triple``. There is support for expanding many acronyms [CarruthAcronym]_ [PicusAcronym]",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:4459,Modifiability,variab,variables,4459,"nerCamelBack]_.; Approval for ``lower_case`` was expressed by [CarruthLower]_; [CarruthCamelBack]_ [TurnerLLDB]_.; Opposition to ``lower_case`` was expressed by [LattnerLower]_. Differentiating variable kinds; ------------------------------. An additional requested change is to distinguish between different kinds of; variables [RobinsonDistinguish]_ [RobinsonDistinguish2]_ [JonesDistinguish]_; [IvanovicDistinguish]_ [CarruthDistinguish]_ [MalyutinDistinguish]_. Others oppose this idea [HähnleDistinguish]_ [GreeneDistinguish]_; [HendersonPrefix]_. A possibility is for member variables to be prefixed with ``m_`` and for global; variables to be prefixed with ``g_`` to distinguish them from local variables.; This is consistent with [LLDB]_. The ``m_`` prefix is consistent with [WebKit]_. A variation is for member variables to be prefixed with ``m``; [IvanovicDistinguish]_ [BeylsDistinguish]_. This is consistent with [Mozilla]_. Another option is for member variables to be suffixed with ``_`` which is; consistent with [Google]_ and similar to [Python]_. Opposed by; [ParzyszekDistinguish]_. Reducing the number of acronyms; ===============================. While switching coding standard will make it easier to use non-acronym names for; new code, it doesn't improve the existing large body of code that uses acronyms; extensively to the detriment of its readability. Further, it is natural and; generally encouraged that new code be written in the style of the surrounding; code. Therefore it is likely that much newly written code will also use; acronyms despite what the coding standard says, much as it is today. As well as changing the case of variable names, they could also be expanded to; their non-acronym form e.g. ``Triple T`` → ``Triple triple``. There is support for expanding many acronyms [CarruthAcronym]_ [PicusAcronym]_; but there is a preference that expanding acronyms be deferred; [ParzyszekAcronym]_ [CarruthAcronym]_. The consensus within the community seems to be t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:5153,Modifiability,variab,variable,5153,"uish them from local variables.; This is consistent with [LLDB]_. The ``m_`` prefix is consistent with [WebKit]_. A variation is for member variables to be prefixed with ``m``; [IvanovicDistinguish]_ [BeylsDistinguish]_. This is consistent with [Mozilla]_. Another option is for member variables to be suffixed with ``_`` which is; consistent with [Google]_ and similar to [Python]_. Opposed by; [ParzyszekDistinguish]_. Reducing the number of acronyms; ===============================. While switching coding standard will make it easier to use non-acronym names for; new code, it doesn't improve the existing large body of code that uses acronyms; extensively to the detriment of its readability. Further, it is natural and; generally encouraged that new code be written in the style of the surrounding; code. Therefore it is likely that much newly written code will also use; acronyms despite what the coding standard says, much as it is today. As well as changing the case of variable names, they could also be expanded to; their non-acronym form e.g. ``Triple T`` → ``Triple triple``. There is support for expanding many acronyms [CarruthAcronym]_ [PicusAcronym]_; but there is a preference that expanding acronyms be deferred; [ParzyszekAcronym]_ [CarruthAcronym]_. The consensus within the community seems to be that at least some acronyms are; valuable [ParzyszekAcronym]_ [LattnerAcronym]_. The most commonly cited acronym; is ``TLI`` however that is used to refer to both ``TargetLowering`` and; ``TargetLibraryInfo`` [GreeneDistinguish]_. The following is a list of acronyms considered sufficiently useful that the; benefit of using them outweighs the cost of learning them. Acronyms that are; either not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInst",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:6450,Modifiability,variab,variable,6450,"yms are; valuable [ParzyszekAcronym]_ [LattnerAcronym]_. The most commonly cited acronym; is ``TLI`` however that is used to refer to both ``TargetLowering`` and; ``TargetLibraryInfo`` [GreeneDistinguish]_. The following is a list of acronyms considered sufficiently useful that the; benefit of using them outweighs the cost of learning them. Acronyms that are; either not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInstr mi; MachineRegisterInfo mri; ScalarEvolution se; TargetInstrInfo tii; TargetLibraryInfo tli; TargetRegisterInfo tri; ============================ =============. In some cases renaming acronyms to the full type name will result in overly; verbose code. Unlike most classes, a variable's scope is limited and therefore; some of its purpose can implied from that scope, meaning that fewer words are; necessary to give it a clear name. For example, in an optimization pass the reader; can assume that a variable's purpose relates to optimization and therefore an; ``OptimizationRemarkEmitter`` variable could be given the name ``remarkEmitter``; or even ``remarker``. The following is a list of longer class names and the associated shorter; variable name. ========================= =============; Class name Variable name; ========================= =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; -----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:6674,Modifiability,variab,variable,6674," The following is a list of acronyms considered sufficiently useful that the; benefit of using them outweighs the cost of learning them. Acronyms that are; either not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInstr mi; MachineRegisterInfo mri; ScalarEvolution se; TargetInstrInfo tii; TargetLibraryInfo tli; TargetRegisterInfo tri; ============================ =============. In some cases renaming acronyms to the full type name will result in overly; verbose code. Unlike most classes, a variable's scope is limited and therefore; some of its purpose can implied from that scope, meaning that fewer words are; necessary to give it a clear name. For example, in an optimization pass the reader; can assume that a variable's purpose relates to optimization and therefore an; ``OptimizationRemarkEmitter`` variable could be given the name ``remarkEmitter``; or even ``remarker``. The following is a list of longer class names and the associated shorter; variable name. ========================= =============; Class name Variable name; ========================= =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; --------------------------------. Proponents of keeping the current coding standard (i.e. not transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [Bradbur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:6765,Modifiability,variab,variable,6765," The following is a list of acronyms considered sufficiently useful that the; benefit of using them outweighs the cost of learning them. Acronyms that are; either not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInstr mi; MachineRegisterInfo mri; ScalarEvolution se; TargetInstrInfo tii; TargetLibraryInfo tli; TargetRegisterInfo tri; ============================ =============. In some cases renaming acronyms to the full type name will result in overly; verbose code. Unlike most classes, a variable's scope is limited and therefore; some of its purpose can implied from that scope, meaning that fewer words are; necessary to give it a clear name. For example, in an optimization pass the reader; can assume that a variable's purpose relates to optimization and therefore an; ``OptimizationRemarkEmitter`` variable could be given the name ``remarkEmitter``; or even ``remarker``. The following is a list of longer class names and the associated shorter; variable name. ========================= =============; Class name Variable name; ========================= =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; --------------------------------. Proponents of keeping the current coding standard (i.e. not transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [Bradbur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:6913,Modifiability,variab,variable,6913,"er not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInstr mi; MachineRegisterInfo mri; ScalarEvolution se; TargetInstrInfo tii; TargetLibraryInfo tli; TargetRegisterInfo tri; ============================ =============. In some cases renaming acronyms to the full type name will result in overly; verbose code. Unlike most classes, a variable's scope is limited and therefore; some of its purpose can implied from that scope, meaning that fewer words are; necessary to give it a clear name. For example, in an optimization pass the reader; can assume that a variable's purpose relates to optimization and therefore an; ``OptimizationRemarkEmitter`` variable could be given the name ``remarkEmitter``; or even ``remarker``. The following is a list of longer class names and the associated shorter; variable name. ========================= =============; Class name Variable name; ========================= =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; --------------------------------. Proponents of keeping the current coding standard (i.e. not transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [BradburyConcern]_.; The costs are that ``git blame`` will become less usable; and that merging the; changes will be costly for downstream maintainers. See `Big bang`_ f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:8021,Modifiability,variab,variable,8021,"====================== =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; --------------------------------. Proponents of keeping the current coding standard (i.e. not transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [BradburyConcern]_.; The costs are that ``git blame`` will become less usable; and that merging the; changes will be costly for downstream maintainers. See `Big bang`_ for potential; mitigations. Laissez faire; -------------. The coding standard could allow both ``CamelCase`` and ``camelBack`` styles for; variable names [LattnerTransition]_. A code review to implement this is at https://reviews.llvm.org/D57896. Advantages; **********. * Very easy to implement initially. Disadvantages; *************. * Leads to inconsistency [BradburyConcern]_ [AminiInconsistent]_.; * Inconsistency means it will be hard to know at a guess what name a variable; will have [DasInconsistent]_ [CarruthInconsistent]_.; * Some large-scale renaming may happen anyway, leading to its disadvantages; without any mitigations. Big bang; --------. With this approach, variables will be renamed by an automated script in a series; of large commits. The principle advantage of this approach is that it minimises the cost of; inconsistency [BradburyTransition]_ [RobinsonTransition]_. It goes against a policy of avoiding large-scale reformatting of existing code; [GreeneDistinguish]_. It has been suggested that LLD would be a good starter project for the renaming; [Ueyama]_. Keeping git blame usable; ***",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:8355,Modifiability,variab,variable,8355,"ere are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; --------------------------------. Proponents of keeping the current coding standard (i.e. not transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [BradburyConcern]_.; The costs are that ``git blame`` will become less usable; and that merging the; changes will be costly for downstream maintainers. See `Big bang`_ for potential; mitigations. Laissez faire; -------------. The coding standard could allow both ``CamelCase`` and ``camelBack`` styles for; variable names [LattnerTransition]_. A code review to implement this is at https://reviews.llvm.org/D57896. Advantages; **********. * Very easy to implement initially. Disadvantages; *************. * Leads to inconsistency [BradburyConcern]_ [AminiInconsistent]_.; * Inconsistency means it will be hard to know at a guess what name a variable; will have [DasInconsistent]_ [CarruthInconsistent]_.; * Some large-scale renaming may happen anyway, leading to its disadvantages; without any mitigations. Big bang; --------. With this approach, variables will be renamed by an automated script in a series; of large commits. The principle advantage of this approach is that it minimises the cost of; inconsistency [BradburyTransition]_ [RobinsonTransition]_. It goes against a policy of avoiding large-scale reformatting of existing code; [GreeneDistinguish]_. It has been suggested that LLD would be a good starter project for the renaming; [Ueyama]_. Keeping git blame usable; ************************. ``git blame`` (or ``git annotate``) permits quickly identifying the commit that; changed a given line in a file. After renaming variables, many lines will show; as being changed by that one commit, requiring a further invocation of ``git; blame`` to identify prior, more interesting commits [GreeneGitBlame]_; [RicciAcronyms]_. **",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:8561,Modifiability,variab,variables,8561,"ot transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [BradburyConcern]_.; The costs are that ``git blame`` will become less usable; and that merging the; changes will be costly for downstream maintainers. See `Big bang`_ for potential; mitigations. Laissez faire; -------------. The coding standard could allow both ``CamelCase`` and ``camelBack`` styles for; variable names [LattnerTransition]_. A code review to implement this is at https://reviews.llvm.org/D57896. Advantages; **********. * Very easy to implement initially. Disadvantages; *************. * Leads to inconsistency [BradburyConcern]_ [AminiInconsistent]_.; * Inconsistency means it will be hard to know at a guess what name a variable; will have [DasInconsistent]_ [CarruthInconsistent]_.; * Some large-scale renaming may happen anyway, leading to its disadvantages; without any mitigations. Big bang; --------. With this approach, variables will be renamed by an automated script in a series; of large commits. The principle advantage of this approach is that it minimises the cost of; inconsistency [BradburyTransition]_ [RobinsonTransition]_. It goes against a policy of avoiding large-scale reformatting of existing code; [GreeneDistinguish]_. It has been suggested that LLD would be a good starter project for the renaming; [Ueyama]_. Keeping git blame usable; ************************. ``git blame`` (or ``git annotate``) permits quickly identifying the commit that; changed a given line in a file. After renaming variables, many lines will show; as being changed by that one commit, requiring a further invocation of ``git; blame`` to identify prior, more interesting commits [GreeneGitBlame]_; [RicciAcronyms]_. **Mitigation**: `git-hyper-blame; <https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/git-hyper-blame.html>`_; can ignore or ""look through"" a given set of commits.; A ``.git-blame-ignore-revs`` file identif",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:9149,Modifiability,variab,variables,9149,"ncy [BradburyConcern]_ [AminiInconsistent]_.; * Inconsistency means it will be hard to know at a guess what name a variable; will have [DasInconsistent]_ [CarruthInconsistent]_.; * Some large-scale renaming may happen anyway, leading to its disadvantages; without any mitigations. Big bang; --------. With this approach, variables will be renamed by an automated script in a series; of large commits. The principle advantage of this approach is that it minimises the cost of; inconsistency [BradburyTransition]_ [RobinsonTransition]_. It goes against a policy of avoiding large-scale reformatting of existing code; [GreeneDistinguish]_. It has been suggested that LLD would be a good starter project for the renaming; [Ueyama]_. Keeping git blame usable; ************************. ``git blame`` (or ``git annotate``) permits quickly identifying the commit that; changed a given line in a file. After renaming variables, many lines will show; as being changed by that one commit, requiring a further invocation of ``git; blame`` to identify prior, more interesting commits [GreeneGitBlame]_; [RicciAcronyms]_. **Mitigation**: `git-hyper-blame; <https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/git-hyper-blame.html>`_; can ignore or ""look through"" a given set of commits.; A ``.git-blame-ignore-revs`` file identifying the variable renaming commits; could be added to the LLVM git repository root directory.; It is being `investigated; <https://public-inbox.org/git/20190324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could be added to ``git blame`` itself. Minimising cost of downstream merges; ************************************. There are many forks of LLVM with downstream changes. Merging a large-scale; renaming change could be difficult for the fork maintainers. **Mitigation**: A large-scale renaming would be automated. A fork maintainer can; merge from the commit immediately before the renaming, then apply the renaming; script ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:9599,Modifiability,variab,variable,9599,"ies; of large commits. The principle advantage of this approach is that it minimises the cost of; inconsistency [BradburyTransition]_ [RobinsonTransition]_. It goes against a policy of avoiding large-scale reformatting of existing code; [GreeneDistinguish]_. It has been suggested that LLD would be a good starter project for the renaming; [Ueyama]_. Keeping git blame usable; ************************. ``git blame`` (or ``git annotate``) permits quickly identifying the commit that; changed a given line in a file. After renaming variables, many lines will show; as being changed by that one commit, requiring a further invocation of ``git; blame`` to identify prior, more interesting commits [GreeneGitBlame]_; [RicciAcronyms]_. **Mitigation**: `git-hyper-blame; <https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/git-hyper-blame.html>`_; can ignore or ""look through"" a given set of commits.; A ``.git-blame-ignore-revs`` file identifying the variable renaming commits; could be added to the LLVM git repository root directory.; It is being `investigated; <https://public-inbox.org/git/20190324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could be added to ``git blame`` itself. Minimising cost of downstream merges; ************************************. There are many forks of LLVM with downstream changes. Merging a large-scale; renaming change could be difficult for the fork maintainers. **Mitigation**: A large-scale renaming would be automated. A fork maintainer can; merge from the commit immediately before the renaming, then apply the renaming; script to their own branch. They can then merge again from the renaming commit,; resolving all conflicts by choosing their own version. This could be tested on; the [SVE]_ fork. Provisional Plan; ================. This is a provisional plan for the `Big bang`_ approach. It has not been agreed. #. Investigate improving ``git blame``. The extent to which it can be made to; ""look throu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:10760,Modifiability,refactor,refactoring,10760,"0324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could be added to ``git blame`` itself. Minimising cost of downstream merges; ************************************. There are many forks of LLVM with downstream changes. Merging a large-scale; renaming change could be difficult for the fork maintainers. **Mitigation**: A large-scale renaming would be automated. A fork maintainer can; merge from the commit immediately before the renaming, then apply the renaming; script to their own branch. They can then merge again from the renaming commit,; resolving all conflicts by choosing their own version. This could be tested on; the [SVE]_ fork. Provisional Plan; ================. This is a provisional plan for the `Big bang`_ approach. It has not been agreed. #. Investigate improving ``git blame``. The extent to which it can be made to; ""look through"" commits may impact how big a change can be made. #. Write a script to expand acronyms. #. Experiment and perform dry runs of the various refactoring options.; Results can be published in forks of the LLVM Git repository. #. Consider the evidence and agree on the new policy. #. Agree & announce a date for the renaming of the starter project (LLD). #. Update the `policy page <../CodingStandards.html>`_. This will explain the; old and new rules and which projects each applies to. #. Refactor the starter project in two commits:. 1. Add or change the project's .clang-tidy to reflect the agreed rules.; (This is in a separate commit to enable the merging process described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy page.; 2. Apply ``clang-tidy`` to the project's files, with only the; ``readability-identifier-naming`` rules enabled. ``clang-tidy`` will also; reformat the affected lines according to the rules in ``.clang-format``.; It is anticipated that this will be a good dog-fooding opportunity for; clang-tidy, and bugs should be fixed in the process, likely includin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:11918,Modifiability,variab,variables,11918,"ce a date for the renaming of the starter project (LLD). #. Update the `policy page <../CodingStandards.html>`_. This will explain the; old and new rules and which projects each applies to. #. Refactor the starter project in two commits:. 1. Add or change the project's .clang-tidy to reflect the agreed rules.; (This is in a separate commit to enable the merging process described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy page.; 2. Apply ``clang-tidy`` to the project's files, with only the; ``readability-identifier-naming`` rules enabled. ``clang-tidy`` will also; reformat the affected lines according to the rules in ``.clang-format``.; It is anticipated that this will be a good dog-fooding opportunity for; clang-tidy, and bugs should be fixed in the process, likely including:. * `readability-identifier-naming incorrectly fixes lambda capture; <https://bugs.llvm.org/show_bug.cgi?id=41119>`_.; * `readability-identifier-naming incorrectly fixes variables which; become keywords <https://bugs.llvm.org/show_bug.cgi?id=41120>`_.; * `readability-identifier-naming misses fixing member variables in; destructor <https://bugs.llvm.org/show_bug.cgi?id=41122>`_. #. Gather feedback and refine the process as appropriate. #. Apply the process to the following projects, with a suitable delay between; each (at least 4 weeks after the first change, at least 2 weeks subsequently); to allow gathering further feedback.; This list should exclude projects that must adhere to an externally defined; standard e.g. libcxx.; The list is roughly in chronological order of renaming.; Some items may not make sense to rename individually - it is expected that; this list will change following experimentation:. * TableGen; * llvm/tools; * clang-tools-extra; * clang; * ARM backend; * AArch64 backend; * AMDGPU backend; * ARC backend; * AVR backend; * BPF backend; * Hexagon backend; * Lanai backend; * MIPS backend; * NVPTX backend; * PowerPC backend; * RISC-V",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:12055,Modifiability,variab,variables,12055,"the; old and new rules and which projects each applies to. #. Refactor the starter project in two commits:. 1. Add or change the project's .clang-tidy to reflect the agreed rules.; (This is in a separate commit to enable the merging process described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy page.; 2. Apply ``clang-tidy`` to the project's files, with only the; ``readability-identifier-naming`` rules enabled. ``clang-tidy`` will also; reformat the affected lines according to the rules in ``.clang-format``.; It is anticipated that this will be a good dog-fooding opportunity for; clang-tidy, and bugs should be fixed in the process, likely including:. * `readability-identifier-naming incorrectly fixes lambda capture; <https://bugs.llvm.org/show_bug.cgi?id=41119>`_.; * `readability-identifier-naming incorrectly fixes variables which; become keywords <https://bugs.llvm.org/show_bug.cgi?id=41120>`_.; * `readability-identifier-naming misses fixing member variables in; destructor <https://bugs.llvm.org/show_bug.cgi?id=41122>`_. #. Gather feedback and refine the process as appropriate. #. Apply the process to the following projects, with a suitable delay between; each (at least 4 weeks after the first change, at least 2 weeks subsequently); to allow gathering further feedback.; This list should exclude projects that must adhere to an externally defined; standard e.g. libcxx.; The list is roughly in chronological order of renaming.; Some items may not make sense to rename individually - it is expected that; this list will change following experimentation:. * TableGen; * llvm/tools; * clang-tools-extra; * clang; * ARM backend; * AArch64 backend; * AMDGPU backend; * ARC backend; * AVR backend; * BPF backend; * Hexagon backend; * Lanai backend; * MIPS backend; * NVPTX backend; * PowerPC backend; * RISC-V backend; * Sparc backend; * SystemZ backend; * WebAssembly backend; * X86 backend; * XCore backend; * libLTO; * Debug Information; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:13158,Modifiability,variab,variable,13158," as appropriate. #. Apply the process to the following projects, with a suitable delay between; each (at least 4 weeks after the first change, at least 2 weeks subsequently); to allow gathering further feedback.; This list should exclude projects that must adhere to an externally defined; standard e.g. libcxx.; The list is roughly in chronological order of renaming.; Some items may not make sense to rename individually - it is expected that; this list will change following experimentation:. * TableGen; * llvm/tools; * clang-tools-extra; * clang; * ARM backend; * AArch64 backend; * AMDGPU backend; * ARC backend; * AVR backend; * BPF backend; * Hexagon backend; * Lanai backend; * MIPS backend; * NVPTX backend; * PowerPC backend; * RISC-V backend; * Sparc backend; * SystemZ backend; * WebAssembly backend; * X86 backend; * XCore backend; * libLTO; * Debug Information; * Remainder of llvm; * compiler-rt; * libunwind; * openmp; * parallel-libs; * polly; * lldb. #. Remove the old variable name rule from the policy page. #. Repeat many of the steps in the sequence, using a script to expand acronyms. References; ==========. .. [LLDB] LLDB Coding Conventions https://llvm.org/svn/llvm-project/lldb/branches/release_39/www/lldb-coding-conventions.html; .. [Google] Google C++ Style Guide https://google.github.io/styleguide/cppguide.html#Variable_Names; .. [WebKit] WebKit Code Style Guidelines https://webkit.org/code-style-guidelines/#names; .. [Qt] Qt Coding Style https://wiki.qt.io/Qt_Coding_Style#Declaring_variables; .. [Rust] Rust naming conventions https://doc.rust-lang.org/1.0.0/style/style/naming/README.html; .. [Swift] Swift API Design Guidelines https://swift.org/documentation/api-design-guidelines/#general-conventions; .. [Python] Style Guide for Python Code https://www.python.org/dev/peps/pep-0008/#function-and-variable-names; .. [Mozilla] Mozilla Coding style: Prefixes https://firefox-source-docs.mozilla.org/tools/lint/coding-style/coding_style_cpp.html#prefixes; .. [S",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:14009,Modifiability,variab,variable-names,14009,"* XCore backend; * libLTO; * Debug Information; * Remainder of llvm; * compiler-rt; * libunwind; * openmp; * parallel-libs; * polly; * lldb. #. Remove the old variable name rule from the policy page. #. Repeat many of the steps in the sequence, using a script to expand acronyms. References; ==========. .. [LLDB] LLDB Coding Conventions https://llvm.org/svn/llvm-project/lldb/branches/release_39/www/lldb-coding-conventions.html; .. [Google] Google C++ Style Guide https://google.github.io/styleguide/cppguide.html#Variable_Names; .. [WebKit] WebKit Code Style Guidelines https://webkit.org/code-style-guidelines/#names; .. [Qt] Qt Coding Style https://wiki.qt.io/Qt_Coding_Style#Declaring_variables; .. [Rust] Rust naming conventions https://doc.rust-lang.org/1.0.0/style/style/naming/README.html; .. [Swift] Swift API Design Guidelines https://swift.org/documentation/api-design-guidelines/#general-conventions; .. [Python] Style Guide for Python Code https://www.python.org/dev/peps/pep-0008/#function-and-variable-names; .. [Mozilla] Mozilla Coding style: Prefixes https://firefox-source-docs.mozilla.org/tools/lint/coding-style/coding_style_cpp.html#prefixes; .. [SVE] LLVM with support for SVE https://github.com/ARM-software/LLVM-SVE; .. [AminiInconsistent] Mehdi Amini, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130329.html; .. [ArsenaultAgree] Matt Arsenault, http://lists.llvm.org/pipermail/llvm-dev/2019-February/129934.html; .. [BeylsDistinguish] Kristof Beyls, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130292.html; .. [BradburyConcern] Alex Bradbury, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130266.html; .. [BradburyTransition] Alex Bradbury, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130388.html; .. [CarruthAcronym] Chandler Carruth, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130313.html; .. [CarruthCamelBack] Chandler Carruth, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130214.html; .. [CarruthDistingui",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:6626,Performance,optimiz,optimization,6626," The following is a list of acronyms considered sufficiently useful that the; benefit of using them outweighs the cost of learning them. Acronyms that are; either not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInstr mi; MachineRegisterInfo mri; ScalarEvolution se; TargetInstrInfo tii; TargetLibraryInfo tli; TargetRegisterInfo tri; ============================ =============. In some cases renaming acronyms to the full type name will result in overly; verbose code. Unlike most classes, a variable's scope is limited and therefore; some of its purpose can implied from that scope, meaning that fewer words are; necessary to give it a clear name. For example, in an optimization pass the reader; can assume that a variable's purpose relates to optimization and therefore an; ``OptimizationRemarkEmitter`` variable could be given the name ``remarkEmitter``; or even ``remarker``. The following is a list of longer class names and the associated shorter; variable name. ========================= =============; Class name Variable name; ========================= =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; --------------------------------. Proponents of keeping the current coding standard (i.e. not transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [Bradbur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:6704,Performance,optimiz,optimization,6704," The following is a list of acronyms considered sufficiently useful that the; benefit of using them outweighs the cost of learning them. Acronyms that are; either not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInstr mi; MachineRegisterInfo mri; ScalarEvolution se; TargetInstrInfo tii; TargetLibraryInfo tli; TargetRegisterInfo tri; ============================ =============. In some cases renaming acronyms to the full type name will result in overly; verbose code. Unlike most classes, a variable's scope is limited and therefore; some of its purpose can implied from that scope, meaning that fewer words are; necessary to give it a clear name. For example, in an optimization pass the reader; can assume that a variable's purpose relates to optimization and therefore an; ``OptimizationRemarkEmitter`` variable could be given the name ``remarkEmitter``; or even ``remarker``. The following is a list of longer class names and the associated shorter; variable name. ========================= =============; Class name Variable name; ========================= =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; --------------------------------. Proponents of keeping the current coding standard (i.e. not transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [Bradbur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:10728,Performance,perform,perform,10728,"0324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could be added to ``git blame`` itself. Minimising cost of downstream merges; ************************************. There are many forks of LLVM with downstream changes. Merging a large-scale; renaming change could be difficult for the fork maintainers. **Mitigation**: A large-scale renaming would be automated. A fork maintainer can; merge from the commit immediately before the renaming, then apply the renaming; script to their own branch. They can then merge again from the renaming commit,; resolving all conflicts by choosing their own version. This could be tested on; the [SVE]_ fork. Provisional Plan; ================. This is a provisional plan for the `Big bang`_ approach. It has not been agreed. #. Investigate improving ``git blame``. The extent to which it can be made to; ""look through"" commits may impact how big a change can be made. #. Write a script to expand acronyms. #. Experiment and perform dry runs of the various refactoring options.; Results can be published in forks of the LLVM Git repository. #. Consider the evidence and agree on the new policy. #. Agree & announce a date for the renaming of the starter project (LLD). #. Update the `policy page <../CodingStandards.html>`_. This will explain the; old and new rules and which projects each applies to. #. Refactor the starter project in two commits:. 1. Add or change the project's .clang-tidy to reflect the agreed rules.; (This is in a separate commit to enable the merging process described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy page.; 2. Apply ``clang-tidy`` to the project's files, with only the; ``readability-identifier-naming`` rules enabled. ``clang-tidy`` will also; reformat the affected lines according to the rules in ``.clang-format``.; It is anticipated that this will be a good dog-fooding opportunity for; clang-tidy, and bugs should be fixed in the process, likely includin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:8803,Safety,avoid,avoiding,8803,"ll be costly for downstream maintainers. See `Big bang`_ for potential; mitigations. Laissez faire; -------------. The coding standard could allow both ``CamelCase`` and ``camelBack`` styles for; variable names [LattnerTransition]_. A code review to implement this is at https://reviews.llvm.org/D57896. Advantages; **********. * Very easy to implement initially. Disadvantages; *************. * Leads to inconsistency [BradburyConcern]_ [AminiInconsistent]_.; * Inconsistency means it will be hard to know at a guess what name a variable; will have [DasInconsistent]_ [CarruthInconsistent]_.; * Some large-scale renaming may happen anyway, leading to its disadvantages; without any mitigations. Big bang; --------. With this approach, variables will be renamed by an automated script in a series; of large commits. The principle advantage of this approach is that it minimises the cost of; inconsistency [BradburyTransition]_ [RobinsonTransition]_. It goes against a policy of avoiding large-scale reformatting of existing code; [GreeneDistinguish]_. It has been suggested that LLD would be a good starter project for the renaming; [Ueyama]_. Keeping git blame usable; ************************. ``git blame`` (or ``git annotate``) permits quickly identifying the commit that; changed a given line in a file. After renaming variables, many lines will show; as being changed by that one commit, requiring a further invocation of ``git; blame`` to identify prior, more interesting commits [GreeneGitBlame]_; [RicciAcronyms]_. **Mitigation**: `git-hyper-blame; <https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/git-hyper-blame.html>`_; can ignore or ""look through"" a given set of commits.; A ``.git-blame-ignore-revs`` file identifying the variable renaming commits; could be added to the LLVM git repository root directory.; It is being `investigated; <https://public-inbox.org/git/20190324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:10384,Testability,test,tested,10384,"//commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/git-hyper-blame.html>`_; can ignore or ""look through"" a given set of commits.; A ``.git-blame-ignore-revs`` file identifying the variable renaming commits; could be added to the LLVM git repository root directory.; It is being `investigated; <https://public-inbox.org/git/20190324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could be added to ``git blame`` itself. Minimising cost of downstream merges; ************************************. There are many forks of LLVM with downstream changes. Merging a large-scale; renaming change could be difficult for the fork maintainers. **Mitigation**: A large-scale renaming would be automated. A fork maintainer can; merge from the commit immediately before the renaming, then apply the renaming; script to their own branch. They can then merge again from the renaming commit,; resolving all conflicts by choosing their own version. This could be tested on; the [SVE]_ fork. Provisional Plan; ================. This is a provisional plan for the `Big bang`_ approach. It has not been agreed. #. Investigate improving ``git blame``. The extent to which it can be made to; ""look through"" commits may impact how big a change can be made. #. Write a script to expand acronyms. #. Experiment and perform dry runs of the various refactoring options.; Results can be published in forks of the LLVM Git repository. #. Consider the evidence and agree on the new policy. #. Agree & announce a date for the renaming of the starter project (LLD). #. Update the `policy page <../CodingStandards.html>`_. This will explain the; old and new rules and which projects each applies to. #. Refactor the starter project in two commits:. 1. Add or change the project's .clang-tidy to reflect the agreed rules.; (This is in a separate commit to enable the merging process described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy pag",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:1710,Usability,guid,guidelines,1710,"e-types-functions-variables-and-enumerators-properly>`_; states:. Variable names should be nouns (as they represent state). The name should be; camel case, and start with an upper case letter (e.g. Leader or Boats). This rule is the same as that for type names. This is a problem because the; type name cannot be reused for a variable name [*]_. LLVM developers tend to; work around this by either prepending ``The`` to the type name::. Triple TheTriple;. ... or more commonly use an acronym, despite the coding standard stating ""Avoid; abbreviations unless they are well known""::. Triple T;. The proliferation of acronyms leads to hard-to-read code such as `this; <https://github.com/llvm/llvm-project/blob/0a8bc14ad7f3209fe702d18e250194cd90188596/llvm/lib/Transforms/Vectorize/LoopVectorize.cpp#L7445>`_::. InnerLoopVectorizer LB(L, PSE, LI, DT, TLI, TTI, AC, ORE, VF.Width, IC,; &LVL, &CM);. Many other coding guidelines [LLDB]_ [Google]_ [WebKit]_ [Qt]_ [Rust]_ [Swift]_; [Python]_ require that variable names begin with a lower case letter in contrast; to class names which begin with a capital letter. This convention means that the; most readable variable name also requires the least thought::. Triple triple;. There is some agreement that the current rule is broken [LattnerAgree]_; [ArsenaultAgree]_ [RobinsonAgree]_ and that acronyms are an obstacle to reading; new code [MalyutinDistinguish]_ [CarruthAcronym]_ [PicusAcronym]_. There are; some opposing views [ParzyszekAcronym2]_ [RicciAcronyms]_. This work-in-progress proposal is to change the coding standard for variable; names to require that they start with a lower case letter. .. [*] In `some cases; <https://github.com/llvm/llvm-project/blob/8b72080d4d7b13072f371712eed333f987b7a18e/llvm/lib/CodeGen/SelectionDAG/SelectionDAG.cpp#L2727>`_; the type name *is* reused as a variable name, but this shadows the type name; and confuses many debuggers [DenisovCamelBack]_. Variable Names Coding Standard Options; =======================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:5844,Usability,learn,learning,5844,"body of code that uses acronyms; extensively to the detriment of its readability. Further, it is natural and; generally encouraged that new code be written in the style of the surrounding; code. Therefore it is likely that much newly written code will also use; acronyms despite what the coding standard says, much as it is today. As well as changing the case of variable names, they could also be expanded to; their non-acronym form e.g. ``Triple T`` → ``Triple triple``. There is support for expanding many acronyms [CarruthAcronym]_ [PicusAcronym]_; but there is a preference that expanding acronyms be deferred; [ParzyszekAcronym]_ [CarruthAcronym]_. The consensus within the community seems to be that at least some acronyms are; valuable [ParzyszekAcronym]_ [LattnerAcronym]_. The most commonly cited acronym; is ``TLI`` however that is used to refer to both ``TargetLowering`` and; ``TargetLibraryInfo`` [GreeneDistinguish]_. The following is a list of acronyms considered sufficiently useful that the; benefit of using them outweighs the cost of learning them. Acronyms that are; either not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInstr mi; MachineRegisterInfo mri; ScalarEvolution se; TargetInstrInfo tii; TargetLibraryInfo tli; TargetRegisterInfo tri; ============================ =============. In some cases renaming acronyms to the full type name will result in overly; verbose code. Unlike most classes, a variable's scope is limited and therefore; some of its purpose can implied from that scope, meaning that fewer words are; necessary to give it a clear name. For example, in an optimization pass the reader; can assume that a variable's purpose relates to optimization and therefore an; ``OptimizationRemarkEmitter`` variable could be given t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:6595,Usability,clear,clear,6595,"yms are; valuable [ParzyszekAcronym]_ [LattnerAcronym]_. The most commonly cited acronym; is ``TLI`` however that is used to refer to both ``TargetLowering`` and; ``TargetLibraryInfo`` [GreeneDistinguish]_. The following is a list of acronyms considered sufficiently useful that the; benefit of using them outweighs the cost of learning them. Acronyms that are; either not on the list or are used to refer to a different type should be; expanded. ============================ =============; Class name Variable name; ============================ =============; DeterministicFiniteAutomaton dfa; DominatorTree dt; LoopInfo li; MachineFunction mf; MachineInstr mi; MachineRegisterInfo mri; ScalarEvolution se; TargetInstrInfo tii; TargetLibraryInfo tli; TargetRegisterInfo tri; ============================ =============. In some cases renaming acronyms to the full type name will result in overly; verbose code. Unlike most classes, a variable's scope is limited and therefore; some of its purpose can implied from that scope, meaning that fewer words are; necessary to give it a clear name. For example, in an optimization pass the reader; can assume that a variable's purpose relates to optimization and therefore an; ``OptimizationRemarkEmitter`` variable could be given the name ``remarkEmitter``; or even ``remarker``. The following is a list of longer class names and the associated shorter; variable name. ========================= =============; Class name Variable name; ========================= =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; -----------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:7785,Usability,usab,usable,7785,"`remarkEmitter``; or even ``remarker``. The following is a list of longer class names and the associated shorter; variable name. ========================= =============; Class name Variable name; ========================= =============; BasicBlock block; ConstantExpr expr; ExecutionEngine engine; MachineOperand operand; OptimizationRemarkEmitter remarker; PreservedAnalyses analyses; PreservedAnalysesChecker checker; TargetLowering lowering; TargetMachine machine; ========================= =============. Transition Options; ==================. There are three main options for transitioning:. 1. Keep the current coding standard; 2. Laissez faire; 3. Big bang. Keep the current coding standard; --------------------------------. Proponents of keeping the current coding standard (i.e. not transitioning at; all) question whether the cost of transition outweighs the benefit; [EmersonConcern]_ [ReamesConcern]_ [BradburyConcern]_.; The costs are that ``git blame`` will become less usable; and that merging the; changes will be costly for downstream maintainers. See `Big bang`_ for potential; mitigations. Laissez faire; -------------. The coding standard could allow both ``CamelCase`` and ``camelBack`` styles for; variable names [LattnerTransition]_. A code review to implement this is at https://reviews.llvm.org/D57896. Advantages; **********. * Very easy to implement initially. Disadvantages; *************. * Leads to inconsistency [BradburyConcern]_ [AminiInconsistent]_.; * Inconsistency means it will be hard to know at a guess what name a variable; will have [DasInconsistent]_ [CarruthInconsistent]_.; * Some large-scale renaming may happen anyway, leading to its disadvantages; without any mitigations. Big bang; --------. With this approach, variables will be renamed by an automated script in a series; of large commits. The principle advantage of this approach is that it minimises the cost of; inconsistency [BradburyTransition]_ [RobinsonTransition]_. It goes against a policy",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:8987,Usability,usab,usable,8987," ``camelBack`` styles for; variable names [LattnerTransition]_. A code review to implement this is at https://reviews.llvm.org/D57896. Advantages; **********. * Very easy to implement initially. Disadvantages; *************. * Leads to inconsistency [BradburyConcern]_ [AminiInconsistent]_.; * Inconsistency means it will be hard to know at a guess what name a variable; will have [DasInconsistent]_ [CarruthInconsistent]_.; * Some large-scale renaming may happen anyway, leading to its disadvantages; without any mitigations. Big bang; --------. With this approach, variables will be renamed by an automated script in a series; of large commits. The principle advantage of this approach is that it minimises the cost of; inconsistency [BradburyTransition]_ [RobinsonTransition]_. It goes against a policy of avoiding large-scale reformatting of existing code; [GreeneDistinguish]_. It has been suggested that LLD would be a good starter project for the renaming; [Ueyama]_. Keeping git blame usable; ************************. ``git blame`` (or ``git annotate``) permits quickly identifying the commit that; changed a given line in a file. After renaming variables, many lines will show; as being changed by that one commit, requiring a further invocation of ``git; blame`` to identify prior, more interesting commits [GreeneGitBlame]_; [RicciAcronyms]_. **Mitigation**: `git-hyper-blame; <https://commondatastorage.googleapis.com/chrome-infra-docs/flat/depot_tools/docs/html/git-hyper-blame.html>`_; can ignore or ""look through"" a given set of commits.; A ``.git-blame-ignore-revs`` file identifying the variable renaming commits; could be added to the LLVM git repository root directory.; It is being `investigated; <https://public-inbox.org/git/20190324235020.49706-1-michael@platin.gs/>`_; whether similar functionality could be added to ``git blame`` itself. Minimising cost of downstream merges; ************************************. There are many forks of LLVM with downstream changes. Mergin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:12139,Usability,feedback,feedback,12139,"dd or change the project's .clang-tidy to reflect the agreed rules.; (This is in a separate commit to enable the merging process described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy page.; 2. Apply ``clang-tidy`` to the project's files, with only the; ``readability-identifier-naming`` rules enabled. ``clang-tidy`` will also; reformat the affected lines according to the rules in ``.clang-format``.; It is anticipated that this will be a good dog-fooding opportunity for; clang-tidy, and bugs should be fixed in the process, likely including:. * `readability-identifier-naming incorrectly fixes lambda capture; <https://bugs.llvm.org/show_bug.cgi?id=41119>`_.; * `readability-identifier-naming incorrectly fixes variables which; become keywords <https://bugs.llvm.org/show_bug.cgi?id=41120>`_.; * `readability-identifier-naming misses fixing member variables in; destructor <https://bugs.llvm.org/show_bug.cgi?id=41122>`_. #. Gather feedback and refine the process as appropriate. #. Apply the process to the following projects, with a suitable delay between; each (at least 4 weeks after the first change, at least 2 weeks subsequently); to allow gathering further feedback.; This list should exclude projects that must adhere to an externally defined; standard e.g. libcxx.; The list is roughly in chronological order of renaming.; Some items may not make sense to rename individually - it is expected that; this list will change following experimentation:. * TableGen; * llvm/tools; * clang-tools-extra; * clang; * ARM backend; * AArch64 backend; * AMDGPU backend; * ARC backend; * AVR backend; * BPF backend; * Hexagon backend; * Lanai backend; * MIPS backend; * NVPTX backend; * PowerPC backend; * RISC-V backend; * Sparc backend; * SystemZ backend; * WebAssembly backend; * X86 backend; * XCore backend; * libLTO; * Debug Information; * Remainder of llvm; * compiler-rt; * libunwind; * openmp; * parallel-libs; * polly; * lldb. #. Remove the old v",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:12372,Usability,feedback,feedback,12372,"s described in; `Minimising cost of downstream merges`_).; Also update the project list on the policy page.; 2. Apply ``clang-tidy`` to the project's files, with only the; ``readability-identifier-naming`` rules enabled. ``clang-tidy`` will also; reformat the affected lines according to the rules in ``.clang-format``.; It is anticipated that this will be a good dog-fooding opportunity for; clang-tidy, and bugs should be fixed in the process, likely including:. * `readability-identifier-naming incorrectly fixes lambda capture; <https://bugs.llvm.org/show_bug.cgi?id=41119>`_.; * `readability-identifier-naming incorrectly fixes variables which; become keywords <https://bugs.llvm.org/show_bug.cgi?id=41120>`_.; * `readability-identifier-naming misses fixing member variables in; destructor <https://bugs.llvm.org/show_bug.cgi?id=41122>`_. #. Gather feedback and refine the process as appropriate. #. Apply the process to the following projects, with a suitable delay between; each (at least 4 weeks after the first change, at least 2 weeks subsequently); to allow gathering further feedback.; This list should exclude projects that must adhere to an externally defined; standard e.g. libcxx.; The list is roughly in chronological order of renaming.; Some items may not make sense to rename individually - it is expected that; this list will change following experimentation:. * TableGen; * llvm/tools; * clang-tools-extra; * clang; * ARM backend; * AArch64 backend; * AMDGPU backend; * ARC backend; * AVR backend; * BPF backend; * Hexagon backend; * Lanai backend; * MIPS backend; * NVPTX backend; * PowerPC backend; * RISC-V backend; * Sparc backend; * SystemZ backend; * WebAssembly backend; * X86 backend; * XCore backend; * libLTO; * Debug Information; * Remainder of llvm; * compiler-rt; * libunwind; * openmp; * parallel-libs; * polly; * lldb. #. Remove the old variable name rule from the policy page. #. Repeat many of the steps in the sequence, using a script to expand acronyms. Refere",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:13602,Usability,guid,guidelines,13602,"ected that; this list will change following experimentation:. * TableGen; * llvm/tools; * clang-tools-extra; * clang; * ARM backend; * AArch64 backend; * AMDGPU backend; * ARC backend; * AVR backend; * BPF backend; * Hexagon backend; * Lanai backend; * MIPS backend; * NVPTX backend; * PowerPC backend; * RISC-V backend; * Sparc backend; * SystemZ backend; * WebAssembly backend; * X86 backend; * XCore backend; * libLTO; * Debug Information; * Remainder of llvm; * compiler-rt; * libunwind; * openmp; * parallel-libs; * polly; * lldb. #. Remove the old variable name rule from the policy page. #. Repeat many of the steps in the sequence, using a script to expand acronyms. References; ==========. .. [LLDB] LLDB Coding Conventions https://llvm.org/svn/llvm-project/lldb/branches/release_39/www/lldb-coding-conventions.html; .. [Google] Google C++ Style Guide https://google.github.io/styleguide/cppguide.html#Variable_Names; .. [WebKit] WebKit Code Style Guidelines https://webkit.org/code-style-guidelines/#names; .. [Qt] Qt Coding Style https://wiki.qt.io/Qt_Coding_Style#Declaring_variables; .. [Rust] Rust naming conventions https://doc.rust-lang.org/1.0.0/style/style/naming/README.html; .. [Swift] Swift API Design Guidelines https://swift.org/documentation/api-design-guidelines/#general-conventions; .. [Python] Style Guide for Python Code https://www.python.org/dev/peps/pep-0008/#function-and-variable-names; .. [Mozilla] Mozilla Coding style: Prefixes https://firefox-source-docs.mozilla.org/tools/lint/coding-style/coding_style_cpp.html#prefixes; .. [SVE] LLVM with support for SVE https://github.com/ARM-software/LLVM-SVE; .. [AminiInconsistent] Mehdi Amini, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130329.html; .. [ArsenaultAgree] Matt Arsenault, http://lists.llvm.org/pipermail/llvm-dev/2019-February/129934.html; .. [BeylsDistinguish] Kristof Beyls, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130292.html; .. [BradburyConcern] Alex Bradbury, http://lists.l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst:13881,Usability,guid,guidelines,13881,"end; * PowerPC backend; * RISC-V backend; * Sparc backend; * SystemZ backend; * WebAssembly backend; * X86 backend; * XCore backend; * libLTO; * Debug Information; * Remainder of llvm; * compiler-rt; * libunwind; * openmp; * parallel-libs; * polly; * lldb. #. Remove the old variable name rule from the policy page. #. Repeat many of the steps in the sequence, using a script to expand acronyms. References; ==========. .. [LLDB] LLDB Coding Conventions https://llvm.org/svn/llvm-project/lldb/branches/release_39/www/lldb-coding-conventions.html; .. [Google] Google C++ Style Guide https://google.github.io/styleguide/cppguide.html#Variable_Names; .. [WebKit] WebKit Code Style Guidelines https://webkit.org/code-style-guidelines/#names; .. [Qt] Qt Coding Style https://wiki.qt.io/Qt_Coding_Style#Declaring_variables; .. [Rust] Rust naming conventions https://doc.rust-lang.org/1.0.0/style/style/naming/README.html; .. [Swift] Swift API Design Guidelines https://swift.org/documentation/api-design-guidelines/#general-conventions; .. [Python] Style Guide for Python Code https://www.python.org/dev/peps/pep-0008/#function-and-variable-names; .. [Mozilla] Mozilla Coding style: Prefixes https://firefox-source-docs.mozilla.org/tools/lint/coding-style/coding_style_cpp.html#prefixes; .. [SVE] LLVM with support for SVE https://github.com/ARM-software/LLVM-SVE; .. [AminiInconsistent] Mehdi Amini, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130329.html; .. [ArsenaultAgree] Matt Arsenault, http://lists.llvm.org/pipermail/llvm-dev/2019-February/129934.html; .. [BeylsDistinguish] Kristof Beyls, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130292.html; .. [BradburyConcern] Alex Bradbury, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130266.html; .. [BradburyTransition] Alex Bradbury, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130388.html; .. [CarruthAcronym] Chandler Carruth, http://lists.llvm.org/pipermail/llvm-dev/2019-February/130313.html; .. [CarruthC",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VariableNames.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:281,Availability,mask,mask,281,"==========================; Vector Predication Roadmap; ==========================. .. contents:: Table of Contents; :depth: 3; :local:. Motivation; ==========. This proposal defines a roadmap towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/In",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:547,Availability,mask,masked,547,"==========================; Vector Predication Roadmap; ==========================. .. contents:: Table of Contents; :depth: 3; :local:. Motivation; ==========. This proposal defines a roadmap towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/In",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:576,Availability,avail,available,576,"==========================; Vector Predication Roadmap; ==========================. .. contents:: Table of Contents; :depth: 3; :local:. Motivation; ==========. This proposal defines a roadmap towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/In",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:1023,Availability,avail,available,1023,"===========; Vector Predication Roadmap; ==========================. .. contents:: Table of Contents; :depth: 3; :local:. Motivation; ==========. This proposal defines a roadmap towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:1436,Availability,mask,mask,1436,"2, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instruction",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2164,Availability,mask,masked,2164,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2277,Availability,mask,masked,2277,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2516,Availability,mask,mask,2516,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2916,Availability,mask,masked,2916,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2974,Availability,mask,masked-vector-load-and-store-intrinsics,2974,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:820,Deployability,patch,patch,820,"==========================; Vector Predication Roadmap; ==========================. .. contents:: Table of Contents; :depth: 3; :local:. Motivation; ==========. This proposal defines a roadmap towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/In",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:1219,Deployability,integrat,integration,1219,"towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2193,Energy Efficiency,reduce,reduce,2193,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2306,Energy Efficiency,reduce,reduce,2306,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2700,Energy Efficiency,reduce,reduce,2700,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:1219,Integrability,integrat,integration,1219,"towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:554,Performance,load,loads,554,"==========================; Vector Predication Roadmap; ==========================. .. contents:: Table of Contents; :depth: 3; :local:. Motivation; ==========. This proposal defines a roadmap towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/In",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:2988,Performance,load,load-and-store-intrinsics,2988,"torLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.reduce.*; -------------------------------------------------------. - Modernize llvm.masked.* / llvm.experimental.reduce* by translating to VP.; - DCE transitional APIs. Result: VP has superseded earlier vector intrinsics. 5. Predicated IR Instructions; -----------------------------. - Vector instructions have an optional mask and vector length parameter. These; lower to VP SDNodes (from Stage 2).; - Phase out VP intrinsics, only keeping those that are not equivalent to; vectorized scalar instructions (reduce, shuffles, ..); - InstCombine/InstSimplify expect predication in regular Instructions (Stage (3); has laid the groundwork). Result: Native vector predication in IR. References; ==========. .. [MaskedIR] `llvm.masked.*` intrinsics,; https://llvm.org/docs/LangRef.html#masked-vector-load-and-store-intrinsics. .. [VPRFC] RFC: Prototype & Roadmap for vector predication in LLVM,; https://reviews.llvm.org/D57504; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst:1145,Usability,usab,usable,1145,"towards native vector predication in LLVM,; specifically for vector instructions with a mask and/or an explicit vector; length. LLVM currently has no target-independent means to model predicated; vector instructions for modern SIMD ISAs such as AVX512, ARM SVE, the RISC-V V; extension and NEC SX-Aurora. Only some predicated vector operations, such as; masked loads and stores, are available through intrinsics [MaskedIR]_. The Vector Predication (VP) extensions is a concrete RFC and prototype; implementation to achieve native vector predication in LLVM. The VP prototype; and all related discussions can be found in the VP patch on Phabricator; [VPRFC]_. Roadmap; =======. 1. IR-level VP intrinsics; -------------------------. - There is a consensus on the semantics/instruction set of VP.; - VP intrinsics and attributes are available on IR level.; - TTI has capability flags for VP (``supportsVP()``?,; ``haveActiveVectorLength()``?). Result: VP usable for IR-level vectorizers (LV, VPlan, RegionVectorizer),; potential integration in Clang with builtins. 2. CodeGen support; ------------------. - VP intrinsics translate to first-class SDNodes; (eg ``llvm.vp.fdiv.* -> vp_fdiv``).; - VP legalization (legalize explicit vector length to mask (AVX512), legalize VP; SDNodes to pre-existing ones (SSE, NEON)). Result: Backend development based on VP SDNodes. 3. Lift InstSimplify/InstCombine/DAGCombiner to VP; --------------------------------------------------. - Introduce PredicatedInstruction, PredicatedBinaryOperator, .. helper classes; that match standard vector IR and VP intrinsics.; - Add a matcher context to PatternMatch and context-aware IR Builder APIs.; - Incrementally lift DAGCombiner to work on VP SDNodes as well as on regular; vector instructions.; - Incrementally lift InstCombine/InstSimplify to operate on VP as well as; regular IR instructions. Result: Optimization of VP intrinsics on par with standard vector instructions. 4. Deprecate llvm.masked.* / llvm.experimental.r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/Proposals/VectorPredication.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:464,Availability,error,error,464,"=================; TableGen BackEnds; =================. .. contents::; :local:. Introduction; ============. TableGen backends are at the core of TableGen's functionality. The source; files provide the classes and records that are parsed and end up as a; collection of record instances, but it's up to the backend to interpret and; print the records in a way that is meaningful to the user (normally a C++; include file or a textual list of warnings, options, and error messages). TableGen is used by both LLVM, Clang, and MLIR with very different goals.; LLVM uses it as a way to automate the generation of massive amounts of; information regarding instructions, schedules, cores, and architecture; features. Some backends generate output that is consumed by more than one; source file, so they need to be created in a way that makes it is easy for; preprocessor tricks to be used. Some backends can also print C++ code; structures, so that they can be directly included as-is. Clang, on the other hand, uses it mainly for diagnostic messages (errors,; warnings, tips) and attributes, so more on the textual end of the scale. MLIR uses TableGen to define operations, operation dialects, and operation; traits. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen, and the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for a guide to writing a new backend. LLVM BackEnds; =============. .. warning::; This portion is incomplete. Each section below needs three subsections:; description of its purpose with a list of users, output generated from; generic input, and finally why it needed a new backend (in case there's; something similar). Overall, each backend will take the same TableGen file type and transform into; similar output for different targets/uses. There is an implicit contract between; the TableGen files, the back-ends and their users. For instance, a global contract is that each back-end produces macro-guarded; sections. Bas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:1045,Availability,error,errors,1045,"===. .. contents::; :local:. Introduction; ============. TableGen backends are at the core of TableGen's functionality. The source; files provide the classes and records that are parsed and end up as a; collection of record instances, but it's up to the backend to interpret and; print the records in a way that is meaningful to the user (normally a C++; include file or a textual list of warnings, options, and error messages). TableGen is used by both LLVM, Clang, and MLIR with very different goals.; LLVM uses it as a way to automate the generation of massive amounts of; information regarding instructions, schedules, cores, and architecture; features. Some backends generate output that is consumed by more than one; source file, so they need to be created in a way that makes it is easy for; preprocessor tricks to be used. Some backends can also print C++ code; structures, so that they can be directly included as-is. Clang, on the other hand, uses it mainly for diagnostic messages (errors,; warnings, tips) and attributes, so more on the textual end of the scale. MLIR uses TableGen to define operations, operation dialects, and operation; traits. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen, and the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for a guide to writing a new backend. LLVM BackEnds; =============. .. warning::; This portion is incomplete. Each section below needs three subsections:; description of its purpose with a list of users, output generated from; generic input, and finally why it needed a new backend (in case there's; something similar). Overall, each backend will take the same TableGen file type and transform into; similar output for different targets/uses. There is an implicit contract between; the TableGen files, the back-ends and their users. For instance, a global contract is that each back-end produces macro-guarded; sections. Based on whether the file is included by a header or a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:4023,Availability,mask,masks,4023," executed on the root; TableGen file ``<Target>.td``, which should include all others. This guarantees; that all information needed is accessible, and that no duplication is needed; in the TableGen files. CodeEmitter; -----------. **Purpose**: CodeEmitterGen uses the descriptions of instructions and their fields to; construct an automated code emitter: a function that, given a MachineInstr,; returns the (currently, 32-bit unsigned) value of the instruction. **Output**: C++ code, implementing the target's CodeEmitter; class by overriding the virtual functions as ``<Target>CodeEmitter::function()``. **Usage**: Used to include directly at the end of ``<Target>MCCodeEmitter.cpp``. RegisterInfo; ------------. **Purpose**: This tablegen backend is responsible for emitting a description of a target; register file for a code generator. It uses instances of the Register,; RegisterAliases, and RegisterClass classes to gather this information. **Output**: C++ code with enums and structures representing the register mappings,; properties, masks, etc. **Usage**: Both on ``<Target>BaseRegisterInfo`` and ``<Target>MCTargetDesc`` (headers; and source files) with macros defining in which they are for declaration vs.; initialization issues. InstrInfo; ---------. **Purpose**: This tablegen backend is responsible for emitting a description of the target; instruction set for the code generator. (what are the differences from CodeEmitter?). **Output**: C++ code with enums and structures representing the instruction mappings,; properties, masks, etc. **Usage**: Both on ``<Target>BaseInstrInfo`` and ``<Target>MCTargetDesc`` (headers; and source files) with macros defining in which they are for declaration vs.; initialization issues. AsmWriter; ---------. **Purpose**: Emits an assembly printer for the current target. **Output**: Implementation of ``<Target>InstPrinter::printInstruction()``, among; other things. **Usage**: Included directly into ``InstPrinter/<Target>InstPrinter.cpp``. AsmMa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:4522,Availability,mask,masks,4522,"the target's CodeEmitter; class by overriding the virtual functions as ``<Target>CodeEmitter::function()``. **Usage**: Used to include directly at the end of ``<Target>MCCodeEmitter.cpp``. RegisterInfo; ------------. **Purpose**: This tablegen backend is responsible for emitting a description of a target; register file for a code generator. It uses instances of the Register,; RegisterAliases, and RegisterClass classes to gather this information. **Output**: C++ code with enums and structures representing the register mappings,; properties, masks, etc. **Usage**: Both on ``<Target>BaseRegisterInfo`` and ``<Target>MCTargetDesc`` (headers; and source files) with macros defining in which they are for declaration vs.; initialization issues. InstrInfo; ---------. **Purpose**: This tablegen backend is responsible for emitting a description of the target; instruction set for the code generator. (what are the differences from CodeEmitter?). **Output**: C++ code with enums and structures representing the instruction mappings,; properties, masks, etc. **Usage**: Both on ``<Target>BaseInstrInfo`` and ``<Target>MCTargetDesc`` (headers; and source files) with macros defining in which they are for declaration vs.; initialization issues. AsmWriter; ---------. **Purpose**: Emits an assembly printer for the current target. **Output**: Implementation of ``<Target>InstPrinter::printInstruction()``, among; other things. **Usage**: Included directly into ``InstPrinter/<Target>InstPrinter.cpp``. AsmMatcher; ----------. **Purpose**: Emits a target specifier matcher for; converting parsed assembly operands in the MCInst structures. It also; emits a matcher for custom operand parsing. Extensive documentation is; written on the ``AsmMatcherEmitter.cpp`` file. **Output**: Assembler parsers' matcher functions, declarations, etc. **Usage**: Used in back-ends' ``AsmParser/<Target>AsmParser.cpp`` for; building the AsmParser class. Disassembler; ------------. **Purpose**: Contains disassembler table",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:664,Energy Efficiency,schedul,schedules,664,"=================; TableGen BackEnds; =================. .. contents::; :local:. Introduction; ============. TableGen backends are at the core of TableGen's functionality. The source; files provide the classes and records that are parsed and end up as a; collection of record instances, but it's up to the backend to interpret and; print the records in a way that is meaningful to the user (normally a C++; include file or a textual list of warnings, options, and error messages). TableGen is used by both LLVM, Clang, and MLIR with very different goals.; LLVM uses it as a way to automate the generation of massive amounts of; information regarding instructions, schedules, cores, and architecture; features. Some backends generate output that is consumed by more than one; source file, so they need to be created in a way that makes it is easy for; preprocessor tricks to be used. Some backends can also print C++ code; structures, so that they can be directly included as-is. Clang, on the other hand, uses it mainly for diagnostic messages (errors,; warnings, tips) and attributes, so more on the textual end of the scale. MLIR uses TableGen to define operations, operation dialects, and operation; traits. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen, and the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for a guide to writing a new backend. LLVM BackEnds; =============. .. warning::; This portion is incomplete. Each section below needs three subsections:; description of its purpose with a list of users, output generated from; generic input, and finally why it needed a new backend (in case there's; something similar). Overall, each backend will take the same TableGen file type and transform into; similar output for different targets/uses. There is an implicit contract between; the TableGen files, the back-ends and their users. For instance, a global contract is that each back-end produces macro-guarded; sections. Bas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:13207,Energy Efficiency,efficient,efficient,13207,"rParsedAttrKinds; ------------------------. **Purpose**: Creates AttrParsedAttrKinds.inc, which is used to implement the; ``AttributeList::getKind`` function, mapping a string (and syntax) to a parsed; attribute ``AttributeList::Kind`` enumeration. ClangAttrDump; -------------. **Purpose**: Creates AttrDump.inc, which dumps information about an attribute.; It is used to implement ``ASTDumper::dumpAttr``. ClangDiagsDefs; --------------. Generate Clang diagnostics definitions. ClangDiagGroups; ---------------. Generate Clang diagnostic groups. ClangDiagsIndexName; -------------------. Generate Clang diagnostic name index. ClangCommentNodes; -----------------. Generate Clang AST comment nodes. ClangDeclNodes; --------------. Generate Clang AST declaration nodes. ClangStmtNodes; --------------. Generate Clang AST statement nodes. ClangSACheckers; ---------------. Generate Clang Static Analyzer checkers. ClangCommentHTMLTags; --------------------. Generate efficient matchers for HTML tag names that are used in documentation comments. ClangCommentHTMLTagsProperties; ------------------------------. Generate efficient matchers for HTML tag properties. ClangCommentHTMLNamedCharacterReferences; ----------------------------------------. Generate function to translate named character references to UTF-8 sequences. ClangCommentCommandInfo; -----------------------. Generate command properties for commands that are used in documentation comments. ClangCommentCommandList; -----------------------. Generate list of commands that are used in documentation comments. ArmNeon; -------. Generate arm_neon.h for clang. ArmNeonSema; -----------. Generate ARM NEON sema support for clang. ArmNeonTest; -----------. Generate ARM NEON tests for clang. AttrDocs; --------. **Purpose**: Creates ``AttributeReference.rst`` from ``AttrDocs.td``, and is; used for documenting user-facing attributes. General BackEnds; ================. Print Records; -------------. The TableGen command option ``--print-re",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:13359,Energy Efficiency,efficient,efficient,13359,"ist::getKind`` function, mapping a string (and syntax) to a parsed; attribute ``AttributeList::Kind`` enumeration. ClangAttrDump; -------------. **Purpose**: Creates AttrDump.inc, which dumps information about an attribute.; It is used to implement ``ASTDumper::dumpAttr``. ClangDiagsDefs; --------------. Generate Clang diagnostics definitions. ClangDiagGroups; ---------------. Generate Clang diagnostic groups. ClangDiagsIndexName; -------------------. Generate Clang diagnostic name index. ClangCommentNodes; -----------------. Generate Clang AST comment nodes. ClangDeclNodes; --------------. Generate Clang AST declaration nodes. ClangStmtNodes; --------------. Generate Clang AST statement nodes. ClangSACheckers; ---------------. Generate Clang Static Analyzer checkers. ClangCommentHTMLTags; --------------------. Generate efficient matchers for HTML tag names that are used in documentation comments. ClangCommentHTMLTagsProperties; ------------------------------. Generate efficient matchers for HTML tag properties. ClangCommentHTMLNamedCharacterReferences; ----------------------------------------. Generate function to translate named character references to UTF-8 sequences. ClangCommentCommandInfo; -----------------------. Generate command properties for commands that are used in documentation comments. ClangCommentCommandList; -----------------------. Generate list of commands that are used in documentation comments. ArmNeon; -------. Generate arm_neon.h for clang. ArmNeonSema; -----------. Generate ARM NEON sema support for clang. ArmNeonTest; -----------. Generate ARM NEON tests for clang. AttrDocs; --------. **Purpose**: Creates ``AttributeReference.rst`` from ``AttrDocs.td``, and is; used for documenting user-facing attributes. General BackEnds; ================. Print Records; -------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend opt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:470,Integrability,message,messages,470,"=================; TableGen BackEnds; =================. .. contents::; :local:. Introduction; ============. TableGen backends are at the core of TableGen's functionality. The source; files provide the classes and records that are parsed and end up as a; collection of record instances, but it's up to the backend to interpret and; print the records in a way that is meaningful to the user (normally a C++; include file or a textual list of warnings, options, and error messages). TableGen is used by both LLVM, Clang, and MLIR with very different goals.; LLVM uses it as a way to automate the generation of massive amounts of; information regarding instructions, schedules, cores, and architecture; features. Some backends generate output that is consumed by more than one; source file, so they need to be created in a way that makes it is easy for; preprocessor tricks to be used. Some backends can also print C++ code; structures, so that they can be directly included as-is. Clang, on the other hand, uses it mainly for diagnostic messages (errors,; warnings, tips) and attributes, so more on the textual end of the scale. MLIR uses TableGen to define operations, operation dialects, and operation; traits. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen, and the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for a guide to writing a new backend. LLVM BackEnds; =============. .. warning::; This portion is incomplete. Each section below needs three subsections:; description of its purpose with a list of users, output generated from; generic input, and finally why it needed a new backend (in case there's; something similar). Overall, each backend will take the same TableGen file type and transform into; similar output for different targets/uses. There is an implicit contract between; the TableGen files, the back-ends and their users. For instance, a global contract is that each back-end produces macro-guarded; sections. Bas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:1035,Integrability,message,messages,1035,"===. .. contents::; :local:. Introduction; ============. TableGen backends are at the core of TableGen's functionality. The source; files provide the classes and records that are parsed and end up as a; collection of record instances, but it's up to the backend to interpret and; print the records in a way that is meaningful to the user (normally a C++; include file or a textual list of warnings, options, and error messages). TableGen is used by both LLVM, Clang, and MLIR with very different goals.; LLVM uses it as a way to automate the generation of massive amounts of; information regarding instructions, schedules, cores, and architecture; features. Some backends generate output that is consumed by more than one; source file, so they need to be created in a way that makes it is easy for; preprocessor tricks to be used. Some backends can also print C++ code; structures, so that they can be directly included as-is. Clang, on the other hand, uses it mainly for diagnostic messages (errors,; warnings, tips) and attributes, so more on the textual end of the scale. MLIR uses TableGen to define operations, operation dialects, and operation; traits. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen, and the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for a guide to writing a new backend. LLVM BackEnds; =============. .. warning::; This portion is incomplete. Each section below needs three subsections:; description of its purpose with a list of users, output generated from; generic input, and finally why it needed a new backend (in case there's; something similar). Overall, each backend will take the same TableGen file type and transform into; similar output for different targets/uses. There is an implicit contract between; the TableGen files, the back-ends and their users. For instance, a global contract is that each back-end produces macro-guarded; sections. Based on whether the file is included by a header or a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:1841,Integrability,contract,contract,1841,"tricks to be used. Some backends can also print C++ code; structures, so that they can be directly included as-is. Clang, on the other hand, uses it mainly for diagnostic messages (errors,; warnings, tips) and attributes, so more on the textual end of the scale. MLIR uses TableGen to define operations, operation dialects, and operation; traits. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen, and the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for a guide to writing a new backend. LLVM BackEnds; =============. .. warning::; This portion is incomplete. Each section below needs three subsections:; description of its purpose with a list of users, output generated from; generic input, and finally why it needed a new backend (in case there's; something similar). Overall, each backend will take the same TableGen file type and transform into; similar output for different targets/uses. There is an implicit contract between; the TableGen files, the back-ends and their users. For instance, a global contract is that each back-end produces macro-guarded; sections. Based on whether the file is included by a header or a source file,; or even in which context of each file the include is being used, you have; todefine a macro just before including it, to get the right output:. .. code-block:: c++. #define GET_REGINFO_TARGET_DESC; #include ""ARMGenRegisterInfo.inc"". And just part of the generated file would be included. This is useful if; you need the same information in multiple formats (instantiation, initialization,; getter/setter functions, etc) from the same source TableGen file without having; to re-compile the TableGen file multiple times. Sometimes, multiple macros might be defined before the same include file to; output multiple blocks:. .. code-block:: c++. #define GET_REGISTER_MATCHER; #define GET_SUBTARGET_FEATURE_NAME; #define GET_MATCHER_IMPLEMENTATION; #include ""ARMGenAsmMatcher.inc"". The macros will be u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:1933,Integrability,contract,contract,1933," directly included as-is. Clang, on the other hand, uses it mainly for diagnostic messages (errors,; warnings, tips) and attributes, so more on the textual end of the scale. MLIR uses TableGen to define operations, operation dialects, and operation; traits. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen, and the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for a guide to writing a new backend. LLVM BackEnds; =============. .. warning::; This portion is incomplete. Each section below needs three subsections:; description of its purpose with a list of users, output generated from; generic input, and finally why it needed a new backend (in case there's; something similar). Overall, each backend will take the same TableGen file type and transform into; similar output for different targets/uses. There is an implicit contract between; the TableGen files, the back-ends and their users. For instance, a global contract is that each back-end produces macro-guarded; sections. Based on whether the file is included by a header or a source file,; or even in which context of each file the include is being used, you have; todefine a macro just before including it, to get the right output:. .. code-block:: c++. #define GET_REGINFO_TARGET_DESC; #include ""ARMGenRegisterInfo.inc"". And just part of the generated file would be included. This is useful if; you need the same information in multiple formats (instantiation, initialization,; getter/setter functions, etc) from the same source TableGen file without having; to re-compile the TableGen file multiple times. Sometimes, multiple macros might be defined before the same include file to; output multiple blocks:. .. code-block:: c++. #define GET_REGISTER_MATCHER; #define GET_SUBTARGET_FEATURE_NAME; #define GET_MATCHER_IMPLEMENTATION; #include ""ARMGenAsmMatcher.inc"". The macros will be undef'd automatically as they're used, in the include file. On all LLVM back-ends, the ``l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:8795,Integrability,interface,interface,8795,"merations. **Output**: Enums, globals, local tables for sub-target information. **Usage**: Populates ``<Target>Subtarget`` and; ``MCTargetDesc/<Target>MCTargetDesc`` files (both headers and source). Intrinsic; ---------. **Purpose**: Generate (target) intrinsic information. OptParserDefs; -------------. **Purpose**: Print enum values for a class. SearchableTables; ----------------. **Purpose**: Generate custom searchable tables. **Output**: Enums, global tables, and lookup helper functions. **Usage**: This backend allows generating free-form, target-specific tables; from TableGen records. The ARM and AArch64 targets use this backend to generate; tables of system registers; the AMDGPU target uses it to generate meta-data; about complex image and memory buffer instructions. See `SearchableTables Reference`_ for a detailed description. CTags; -----. **Purpose**: This tablegen backend emits an index of definitions in ctags(1); format. A helper script, utils/TableGen/tdtags, provides an easier-to-use; interface; run 'tdtags -H' for documentation. X86EVEX2VEX; -----------. **Purpose**: This X86 specific tablegen backend emits tables that map EVEX; encoded instructions to their VEX encoded identical instruction. Clang BackEnds; ==============. ClangAttrClasses; ----------------. **Purpose**: Creates Attrs.inc, which contains semantic attribute class; declarations for any attribute in ``Attr.td`` that has not set ``ASTNode = 0``.; This file is included as part of ``Attr.h``. ClangAttrParserStringSwitches; -----------------------------. **Purpose**: Creates AttrParserStringSwitches.inc, which contains; StringSwitch::Case statements for parser-related string switches. Each switch; is given its own macro (such as ``CLANG_ATTR_ARG_CONTEXT_LIST``, or; ``CLANG_ATTR_IDENTIFIER_ARG_LIST``), which is expected to be defined before; including AttrParserStringSwitches.inc, and undefined after. ClangAttrImpl; -------------. **Purpose**: Creates AttrImpl.inc, which contains semantic attri",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:10241,Modifiability,inherit,inheritable,10241,"e is included as part of ``Attr.h``. ClangAttrParserStringSwitches; -----------------------------. **Purpose**: Creates AttrParserStringSwitches.inc, which contains; StringSwitch::Case statements for parser-related string switches. Each switch; is given its own macro (such as ``CLANG_ATTR_ARG_CONTEXT_LIST``, or; ``CLANG_ATTR_IDENTIFIER_ARG_LIST``), which is expected to be defined before; including AttrParserStringSwitches.inc, and undefined after. ClangAttrImpl; -------------. **Purpose**: Creates AttrImpl.inc, which contains semantic attribute class; definitions for any attribute in ``Attr.td`` that has not set ``ASTNode = 0``.; This file is included as part of ``AttrImpl.cpp``. ClangAttrList; -------------. **Purpose**: Creates AttrList.inc, which is used when a list of semantic; attribute identifiers is required. For instance, ``AttrKinds.h`` includes this; file to generate the list of ``attr::Kind`` enumeration values. This list is; separated out into multiple categories: attributes, inheritable attributes, and; inheritable parameter attributes. This categorization happens automatically; based on information in ``Attr.td`` and is used to implement the ``classof``; functionality required for ``dyn_cast`` and similar APIs. ClangAttrPCHRead; ----------------. **Purpose**: Creates AttrPCHRead.inc, which is used to deserialize attributes; in the ``ASTReader::ReadAttributes`` function. ClangAttrPCHWrite; -----------------. **Purpose**: Creates AttrPCHWrite.inc, which is used to serialize attributes in; the ``ASTWriter::WriteAttributes`` function. ClangAttrSpellings; ---------------------. **Purpose**: Creates AttrSpellings.inc, which is used to implement the; ``__has_attribute`` feature test macro. ClangAttrSpellingListIndex; --------------------------. **Purpose**: Creates AttrSpellingListIndex.inc, which is used to map parsed; attribute spellings (including which syntax or scope was used) to an attribute; spelling list index. These spelling list index values are inte",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:10270,Modifiability,inherit,inheritable,10270,"e is included as part of ``Attr.h``. ClangAttrParserStringSwitches; -----------------------------. **Purpose**: Creates AttrParserStringSwitches.inc, which contains; StringSwitch::Case statements for parser-related string switches. Each switch; is given its own macro (such as ``CLANG_ATTR_ARG_CONTEXT_LIST``, or; ``CLANG_ATTR_IDENTIFIER_ARG_LIST``), which is expected to be defined before; including AttrParserStringSwitches.inc, and undefined after. ClangAttrImpl; -------------. **Purpose**: Creates AttrImpl.inc, which contains semantic attribute class; definitions for any attribute in ``Attr.td`` that has not set ``ASTNode = 0``.; This file is included as part of ``AttrImpl.cpp``. ClangAttrList; -------------. **Purpose**: Creates AttrList.inc, which is used when a list of semantic; attribute identifiers is required. For instance, ``AttrKinds.h`` includes this; file to generate the list of ``attr::Kind`` enumeration values. This list is; separated out into multiple categories: attributes, inheritable attributes, and; inheritable parameter attributes. This categorization happens automatically; based on information in ``Attr.td`` and is used to implement the ``classof``; functionality required for ``dyn_cast`` and similar APIs. ClangAttrPCHRead; ----------------. **Purpose**: Creates AttrPCHRead.inc, which is used to deserialize attributes; in the ``ASTReader::ReadAttributes`` function. ClangAttrPCHWrite; -----------------. **Purpose**: Creates AttrPCHWrite.inc, which is used to serialize attributes in; the ``ASTWriter::WriteAttributes`` function. ClangAttrSpellings; ---------------------. **Purpose**: Creates AttrSpellings.inc, which is used to implement the; ``__has_attribute`` feature test macro. ClangAttrSpellingListIndex; --------------------------. **Purpose**: Creates AttrSpellingListIndex.inc, which is used to map parsed; attribute spellings (including which syntax or scope was used) to an attribute; spelling list index. These spelling list index values are inte",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:14619,Modifiability,variab,variables,14619,"e command properties for commands that are used in documentation comments. ClangCommentCommandList; -----------------------. Generate list of commands that are used in documentation comments. ArmNeon; -------. Generate arm_neon.h for clang. ArmNeonSema; -----------. Generate ARM NEON sema support for clang. ArmNeonTest; -----------. Generate ARM NEON tests for clang. AttrDocs; --------. **Purpose**: Creates ``AttributeReference.rst`` from ``AttrDocs.td``, and is; used for documenting user-facing attributes. General BackEnds; ================. Print Records; -------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. See the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for more information. Print Detailed Records; ----------------------. The TableGen command option ``--print-detailed-records`` invokes a backend; that prints all the global variables, classes, and records defined in the; source files, with more detail than the default record printer. See the; :doc:`TableGen Backend Developer's Guide <./BackGuide>` for more; information. JSON Reference; --------------. **Purpose**: Output all the values in every ``def``, as a JSON data; structure that can be easily parsed by a variety of languages. Useful; for writing custom backends without having to modify TableGen itself,; or for performing auxiliary analysis on the same TableGen data passed; to a built-in backend. **Output**:. The root of the output file is a JSON object (i.e. dictionary),; containing the following fixed keys:. * ``!tablegen_json_version``: a numeric version field that will; increase if an incompatible change is ever made to the structure of; this data. The format described here corresponds to version 1. * ``!instanceof``: a dictionary whose keys are the class names defined; in the TableGen input. For each key, the corresponding value is an; array of strin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:16167,Modifiability,variab,variables,16167,"he root of the output file is a JSON object (i.e. dictionary),; containing the following fixed keys:. * ``!tablegen_json_version``: a numeric version field that will; increase if an incompatible change is ever made to the structure of; this data. The format described here corresponds to version 1. * ``!instanceof``: a dictionary whose keys are the class names defined; in the TableGen input. For each key, the corresponding value is an; array of strings giving the names of ``def`` records that derive; from that class. So ``root[""!instanceof""][""Instruction""]``, for; example, would list the names of all the records deriving from the; class ``Instruction``. For each ``def`` record, the root object also has a key for the record; name. The corresponding value is a subsidiary object containing the; following fixed keys:. * ``!superclasses``: an array of strings giving the names of all the; classes that this record derives from. * ``!fields``: an array of strings giving the names of all the variables; in this record that were defined with the ``field`` keyword. * ``!name``: a string giving the name of the record. This is always; identical to the key in the JSON root object corresponding to this; record's dictionary. (If the record is anonymous, the name is; arbitrary.). * ``!anonymous``: a boolean indicating whether the record's name was; specified by the TableGen input (if it is ``false``), or invented by; TableGen itself (if ``true``). For each variable defined in a record, the ``def`` object for that; record also has a key for the variable name. The corresponding value; is a translation into JSON of the variable's value, using the; conventions described below. Some TableGen data types are translated directly into the; corresponding JSON type:. * A completely undefined value (e.g. for a variable declared without; initializer in some superclass of this record, and never initialized; by the record itself or any other superclass) is emitted as the JSON; ``null`` value. * ``int",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:16632,Modifiability,variab,variable,16632,"m that class. So ``root[""!instanceof""][""Instruction""]``, for; example, would list the names of all the records deriving from the; class ``Instruction``. For each ``def`` record, the root object also has a key for the record; name. The corresponding value is a subsidiary object containing the; following fixed keys:. * ``!superclasses``: an array of strings giving the names of all the; classes that this record derives from. * ``!fields``: an array of strings giving the names of all the variables; in this record that were defined with the ``field`` keyword. * ``!name``: a string giving the name of the record. This is always; identical to the key in the JSON root object corresponding to this; record's dictionary. (If the record is anonymous, the name is; arbitrary.). * ``!anonymous``: a boolean indicating whether the record's name was; specified by the TableGen input (if it is ``false``), or invented by; TableGen itself (if ``true``). For each variable defined in a record, the ``def`` object for that; record also has a key for the variable name. The corresponding value; is a translation into JSON of the variable's value, using the; conventions described below. Some TableGen data types are translated directly into the; corresponding JSON type:. * A completely undefined value (e.g. for a variable declared without; initializer in some superclass of this record, and never initialized; by the record itself or any other superclass) is emitted as the JSON; ``null`` value. * ``int`` and ``bit`` values are emitted as numbers. Note that; TableGen ``int`` values are capable of holding integers too large to; be exactly representable in IEEE double precision. The integer; literal in the JSON output will show the full exact integer value.; So if you need to retrieve large integers with full precision, you; should use a JSON reader capable of translating such literals back; into 64-bit integers without losing precision, such as Python's; standard ``json`` module. * ``string`` and ``cod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:16721,Modifiability,variab,variable,16721,"m that class. So ``root[""!instanceof""][""Instruction""]``, for; example, would list the names of all the records deriving from the; class ``Instruction``. For each ``def`` record, the root object also has a key for the record; name. The corresponding value is a subsidiary object containing the; following fixed keys:. * ``!superclasses``: an array of strings giving the names of all the; classes that this record derives from. * ``!fields``: an array of strings giving the names of all the variables; in this record that were defined with the ``field`` keyword. * ``!name``: a string giving the name of the record. This is always; identical to the key in the JSON root object corresponding to this; record's dictionary. (If the record is anonymous, the name is; arbitrary.). * ``!anonymous``: a boolean indicating whether the record's name was; specified by the TableGen input (if it is ``false``), or invented by; TableGen itself (if ``true``). For each variable defined in a record, the ``def`` object for that; record also has a key for the variable name. The corresponding value; is a translation into JSON of the variable's value, using the; conventions described below. Some TableGen data types are translated directly into the; corresponding JSON type:. * A completely undefined value (e.g. for a variable declared without; initializer in some superclass of this record, and never initialized; by the record itself or any other superclass) is emitted as the JSON; ``null`` value. * ``int`` and ``bit`` values are emitted as numbers. Note that; TableGen ``int`` values are capable of holding integers too large to; be exactly representable in IEEE double precision. The integer; literal in the JSON output will show the full exact integer value.; So if you need to retrieve large integers with full precision, you; should use a JSON reader capable of translating such literals back; into 64-bit integers without losing precision, such as Python's; standard ``json`` module. * ``string`` and ``cod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:16795,Modifiability,variab,variable,16795,"ving from the; class ``Instruction``. For each ``def`` record, the root object also has a key for the record; name. The corresponding value is a subsidiary object containing the; following fixed keys:. * ``!superclasses``: an array of strings giving the names of all the; classes that this record derives from. * ``!fields``: an array of strings giving the names of all the variables; in this record that were defined with the ``field`` keyword. * ``!name``: a string giving the name of the record. This is always; identical to the key in the JSON root object corresponding to this; record's dictionary. (If the record is anonymous, the name is; arbitrary.). * ``!anonymous``: a boolean indicating whether the record's name was; specified by the TableGen input (if it is ``false``), or invented by; TableGen itself (if ``true``). For each variable defined in a record, the ``def`` object for that; record also has a key for the variable name. The corresponding value; is a translation into JSON of the variable's value, using the; conventions described below. Some TableGen data types are translated directly into the; corresponding JSON type:. * A completely undefined value (e.g. for a variable declared without; initializer in some superclass of this record, and never initialized; by the record itself or any other superclass) is emitted as the JSON; ``null`` value. * ``int`` and ``bit`` values are emitted as numbers. Note that; TableGen ``int`` values are capable of holding integers too large to; be exactly representable in IEEE double precision. The integer; literal in the JSON output will show the full exact integer value.; So if you need to retrieve large integers with full precision, you; should use a JSON reader capable of translating such literals back; into 64-bit integers without losing precision, such as Python's; standard ``json`` module. * ``string`` and ``code`` values are emitted as JSON strings. * ``list<T>`` values, for any element type ``T``, are emitted as JSON; arra",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:16981,Modifiability,variab,variable,16981,"sses that this record derives from. * ``!fields``: an array of strings giving the names of all the variables; in this record that were defined with the ``field`` keyword. * ``!name``: a string giving the name of the record. This is always; identical to the key in the JSON root object corresponding to this; record's dictionary. (If the record is anonymous, the name is; arbitrary.). * ``!anonymous``: a boolean indicating whether the record's name was; specified by the TableGen input (if it is ``false``), or invented by; TableGen itself (if ``true``). For each variable defined in a record, the ``def`` object for that; record also has a key for the variable name. The corresponding value; is a translation into JSON of the variable's value, using the; conventions described below. Some TableGen data types are translated directly into the; corresponding JSON type:. * A completely undefined value (e.g. for a variable declared without; initializer in some superclass of this record, and never initialized; by the record itself or any other superclass) is emitted as the JSON; ``null`` value. * ``int`` and ``bit`` values are emitted as numbers. Note that; TableGen ``int`` values are capable of holding integers too large to; be exactly representable in IEEE double precision. The integer; literal in the JSON output will show the full exact integer value.; So if you need to retrieve large integers with full precision, you; should use a JSON reader capable of translating such literals back; into 64-bit integers without losing precision, such as Python's; standard ``json`` module. * ``string`` and ``code`` values are emitted as JSON strings. * ``list<T>`` values, for any element type ``T``, are emitted as JSON; arrays. Each element of the array is represented in turn using these; same conventions. * ``bits`` values are also emitted as arrays. A ``bits`` array is; ordered from least-significant bit to most-significant. So the; element with index ``i`` corresponds to the bit described as",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:18758,Modifiability,variab,variable,18758,"nt of the array is represented in turn using these; same conventions. * ``bits`` values are also emitted as arrays. A ``bits`` array is; ordered from least-significant bit to most-significant. So the; element with index ``i`` corresponds to the bit described as; ``x{i}`` in TableGen source. However, note that this means that; scripting languages are likely to *display* the array in the; opposite order from the way it appears in the TableGen source or in; the diagnostic ``-print-records`` output. All other TableGen value types are emitted as a JSON object,; containing two standard fields: ``kind`` is a discriminator describing; which kind of value the object represents, and ``printable`` is a; string giving the same representation of the value that would appear; in ``-print-records``. * A reference to a ``def`` object has ``kind==""def""``, and has an; extra field ``def`` giving the name of the object referred to. * A reference to another variable in the same record has; ``kind==""var""``, and has an extra field ``var`` giving the name of; the variable referred to. * A reference to a specific bit of a ``bits``-typed variable in the; same record has ``kind==""varbit""``, and has two extra fields:; ``var`` gives the name of the variable referred to, and ``index``; gives the index of the bit. * A value of type ``dag`` has ``kind==""dag""``, and has two extra; fields. ``operator`` gives the initial value after the opening; parenthesis of the dag initializer; ``args`` is an array giving the; following arguments. The elements of ``args`` are arrays of length; 2, giving the value of each argument followed by its colon-suffixed; name (if any). For example, in the JSON representation of the dag; value ``(Op 22, ""hello"":$foo)`` (assuming that ``Op`` is the name of; a record defined elsewhere with a ``def`` statement):. * ``operator`` will be an object in which ``kind==""def""`` and; ``def==""Op""``. * ``args`` will be the array ``[[22, null], [""hello"", ""foo""]]``. * If any other kind of va",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:18863,Modifiability,variab,variable,18863,"nt of the array is represented in turn using these; same conventions. * ``bits`` values are also emitted as arrays. A ``bits`` array is; ordered from least-significant bit to most-significant. So the; element with index ``i`` corresponds to the bit described as; ``x{i}`` in TableGen source. However, note that this means that; scripting languages are likely to *display* the array in the; opposite order from the way it appears in the TableGen source or in; the diagnostic ``-print-records`` output. All other TableGen value types are emitted as a JSON object,; containing two standard fields: ``kind`` is a discriminator describing; which kind of value the object represents, and ``printable`` is a; string giving the same representation of the value that would appear; in ``-print-records``. * A reference to a ``def`` object has ``kind==""def""``, and has an; extra field ``def`` giving the name of the object referred to. * A reference to another variable in the same record has; ``kind==""var""``, and has an extra field ``var`` giving the name of; the variable referred to. * A reference to a specific bit of a ``bits``-typed variable in the; same record has ``kind==""varbit""``, and has two extra fields:; ``var`` gives the name of the variable referred to, and ``index``; gives the index of the bit. * A value of type ``dag`` has ``kind==""dag""``, and has two extra; fields. ``operator`` gives the initial value after the opening; parenthesis of the dag initializer; ``args`` is an array giving the; following arguments. The elements of ``args`` are arrays of length; 2, giving the value of each argument followed by its colon-suffixed; name (if any). For example, in the JSON representation of the dag; value ``(Op 22, ""hello"":$foo)`` (assuming that ``Op`` is the name of; a record defined elsewhere with a ``def`` statement):. * ``operator`` will be an object in which ``kind==""def""`` and; ``def==""Op""``. * ``args`` will be the array ``[[22, null], [""hello"", ""foo""]]``. * If any other kind of va",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:18937,Modifiability,variab,variable,18937,"nt. So the; element with index ``i`` corresponds to the bit described as; ``x{i}`` in TableGen source. However, note that this means that; scripting languages are likely to *display* the array in the; opposite order from the way it appears in the TableGen source or in; the diagnostic ``-print-records`` output. All other TableGen value types are emitted as a JSON object,; containing two standard fields: ``kind`` is a discriminator describing; which kind of value the object represents, and ``printable`` is a; string giving the same representation of the value that would appear; in ``-print-records``. * A reference to a ``def`` object has ``kind==""def""``, and has an; extra field ``def`` giving the name of the object referred to. * A reference to another variable in the same record has; ``kind==""var""``, and has an extra field ``var`` giving the name of; the variable referred to. * A reference to a specific bit of a ``bits``-typed variable in the; same record has ``kind==""varbit""``, and has two extra fields:; ``var`` gives the name of the variable referred to, and ``index``; gives the index of the bit. * A value of type ``dag`` has ``kind==""dag""``, and has two extra; fields. ``operator`` gives the initial value after the opening; parenthesis of the dag initializer; ``args`` is an array giving the; following arguments. The elements of ``args`` are arrays of length; 2, giving the value of each argument followed by its colon-suffixed; name (if any). For example, in the JSON representation of the dag; value ``(Op 22, ""hello"":$foo)`` (assuming that ``Op`` is the name of; a record defined elsewhere with a ``def`` statement):. * ``operator`` will be an object in which ``kind==""def""`` and; ``def==""Op""``. * ``args`` will be the array ``[[22, null], [""hello"", ""foo""]]``. * If any other kind of value or complicated expression appears in the; output, it will have ``kind==""complex""``, and no additional fields.; These values are not expected to be needed by backends. The standard; ``pri",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:19047,Modifiability,variab,variable,19047,"nt. So the; element with index ``i`` corresponds to the bit described as; ``x{i}`` in TableGen source. However, note that this means that; scripting languages are likely to *display* the array in the; opposite order from the way it appears in the TableGen source or in; the diagnostic ``-print-records`` output. All other TableGen value types are emitted as a JSON object,; containing two standard fields: ``kind`` is a discriminator describing; which kind of value the object represents, and ``printable`` is a; string giving the same representation of the value that would appear; in ``-print-records``. * A reference to a ``def`` object has ``kind==""def""``, and has an; extra field ``def`` giving the name of the object referred to. * A reference to another variable in the same record has; ``kind==""var""``, and has an extra field ``var`` giving the name of; the variable referred to. * A reference to a specific bit of a ``bits``-typed variable in the; same record has ``kind==""varbit""``, and has two extra fields:; ``var`` gives the name of the variable referred to, and ``index``; gives the index of the bit. * A value of type ``dag`` has ``kind==""dag""``, and has two extra; fields. ``operator`` gives the initial value after the opening; parenthesis of the dag initializer; ``args`` is an array giving the; following arguments. The elements of ``args`` are arrays of length; 2, giving the value of each argument followed by its colon-suffixed; name (if any). For example, in the JSON representation of the dag; value ``(Op 22, ""hello"":$foo)`` (assuming that ``Op`` is the name of; a record defined elsewhere with a ``def`` statement):. * ``operator`` will be an object in which ``kind==""def""`` and; ``def==""Op""``. * ``args`` will be the array ``[[22, null], [""hello"", ""foo""]]``. * If any other kind of value or complicated expression appears in the; output, it will have ``kind==""complex""``, and no additional fields.; These values are not expected to be needed by backends. The standard; ``pri",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:15069,Performance,perform,performing,15069,"ttrDocs.td``, and is; used for documenting user-facing attributes. General BackEnds; ================. Print Records; -------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. See the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for more information. Print Detailed Records; ----------------------. The TableGen command option ``--print-detailed-records`` invokes a backend; that prints all the global variables, classes, and records defined in the; source files, with more detail than the default record printer. See the; :doc:`TableGen Backend Developer's Guide <./BackGuide>` for more; information. JSON Reference; --------------. **Purpose**: Output all the values in every ``def``, as a JSON data; structure that can be easily parsed by a variety of languages. Useful; for writing custom backends without having to modify TableGen itself,; or for performing auxiliary analysis on the same TableGen data passed; to a built-in backend. **Output**:. The root of the output file is a JSON object (i.e. dictionary),; containing the following fixed keys:. * ``!tablegen_json_version``: a numeric version field that will; increase if an incompatible change is ever made to the structure of; this data. The format described here corresponds to version 1. * ``!instanceof``: a dictionary whose keys are the class names defined; in the TableGen input. For each key, the corresponding value is an; array of strings giving the names of ``def`` records that derive; from that class. So ``root[""!instanceof""][""Instruction""]``, for; example, would list the names of all the records deriving from the; class ``Instruction``. For each ``def`` record, the root object also has a key for the record; name. The corresponding value is a subsidiary object containing the; following fixed keys:. * ``!superclasses``: an array of strings giving the names of all the; clas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:24211,Performance,perform,performs,24211," fields. * ``string FilterClass``. The table will have one entry for each record; that derives from this class. * ``string FilterClassField``. This is an optional field of ``FilterClass``; which should be `bit` type. If specified, only those records with this field; being true will have corresponding entries in the table. This field won't be; included in generated C++ fields if it isn't included in ``Fields`` list. * ``string CppTypeName``. The name of the C++ struct/class type of the; table that holds the entries. If unspecified, the ``FilterClass`` name is; used. * ``list<string> Fields``. A list of the names of the fields *in the; collected records* that contain the data for the table entries. The order of; this list determines the order of the values in the C++ initializers. See; below for information about the types of these fields. * ``list<string> PrimaryKey``. The list of fields that make up the; primary key. * ``string PrimaryKeyName``. The name of the generated C++ function; that performs a lookup on the primary key. * ``bit PrimaryKeyEarlyOut``. See the third example below. TableGen attempts to deduce the type of each of the table fields so that it; can format the C++ initializers in the emitted table. It can deduce ``bit``,; ``bits<n>``, ``string``, ``Intrinsic``, and ``Instruction``. These can be; used in the primary key. Any other field types must be specified; explicitly; this is done as shown in the second example below. Such fields; cannot be used in the primary key. One special case of the field type has to do with code. Arbitrary code is; represented by a string, but has to be emitted as a C++ initializer without; quotes. If the code field was defined using a code literal (``[{...}]``),; then TableGen will know to emit it without quotes. However, if it was; defined using a string literal or complex string expression, then TableGen; will not know. In this case, you can force TableGen to treat the field as; code by including the following line in the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:27397,Performance,perform,performed,27397,"able[] = {; { ""Costa"", 0x2, 0x1 }, // 0; { ""Carol"", 0x2, 0x6 }, // 1; { ""Ted"", 0x4, 0x4 }, // 2; { ""Alice"", 0x4, 0x5 }, // 3; { ""Bob"", 0x5, 0x3 }, // 4; /* { ""Dale"", 0x2, 0x1 }, // 5 */ // We don't generate this line as `IsNeeded` is 0.; };. const AEntry *lookupATableByValues(uint8_t Val1, uint16_t Val2) {; struct KeyType {; uint8_t Val1;; uint16_t Val2;; };; KeyType Key = { Val1, Val2 };; auto Table = ArrayRef(ATable);; auto Idx = std::lower_bound(Table.begin(), Table.end(), Key,; [](const AEntry &LHS, const KeyType &RHS) {; if (LHS.Val1 < RHS.Val1); return true;; if (LHS.Val1 > RHS.Val1); return false;; if (LHS.Val2 < RHS.Val2); return true;; if (LHS.Val2 > RHS.Val2); return false;; return false;; });. if (Idx == Table.end() ||; Key.Val1 != Idx->Val1 ||; Key.Val2 != Idx->Val2); return nullptr;; return &*Idx;; }; #endif. The table entries in ``ATable`` are sorted in order by ``Val1``, and within; each of those values, by ``Val2``. This allows a binary search of the table,; which is performed in the lookup function by ``std::lower_bound``. The; lookup function returns a reference to the found table entry, or the null; pointer if no entry is found. This example includes a field whose type TableGen cannot deduce. The ``Kind``; field uses the enumerated type ``CEnum`` defined above. To inform TableGen; of the type, the record derived from ``GenericTable`` must include a string field; named ``TypeOf_``\ *field*, where *field* is the name of the field whose type; is required. .. code-block:: text. def CTable : GenericTable {; let FilterClass = ""CEntry"";; let Fields = [""Name"", ""Kind"", ""Encoding""];; string TypeOf_Kind = ""CEnum"";; let PrimaryKey = [""Encoding""];; let PrimaryKeyName = ""lookupCEntryByEncoding"";; }. class CEntry<string name, CEnum kind, int enc> {; string Name = name;; CEnum Kind = kind;; bits<16> Encoding = enc;; }. def : CEntry<""Apple"", CFoo, 10>;; def : CEntry<""Pear"", CBaz, 15>;; def : CEntry<""Apple"", CBar, 13>;. Here is the generated C++ code. .. code-block",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:29397,Performance,perform,performing,29397,". .. code-block:: text. #ifdef GET_CTable_DECL; const CEntry *lookupCEntryByEncoding(uint16_t Encoding);; #endif. #ifdef GET_CTable_IMPL; constexpr CEntry CTable[] = {; { ""Apple"", CFoo, 0xA }, // 0; { ""Apple"", CBar, 0xD }, // 1; { ""Pear"", CBaz, 0xF }, // 2; };. const CEntry *lookupCEntryByEncoding(uint16_t Encoding) {; struct KeyType {; uint16_t Encoding;; };; KeyType Key = { Encoding };; auto Table = ArrayRef(CTable);; auto Idx = std::lower_bound(Table.begin(), Table.end(), Key,; [](const CEntry &LHS, const KeyType &RHS) {; if (LHS.Encoding < RHS.Encoding); return true;; if (LHS.Encoding > RHS.Encoding); return false;; return false;; });. if (Idx == Table.end() ||; Key.Encoding != Idx->Encoding); return nullptr;; return &*Idx;; }. The ``PrimaryKeyEarlyOut`` field, when set to 1, modifies the lookup; function so that it tests the first field of the primary key to determine; whether it is within the range of the collected records' primary keys. If; not, the function returns the null pointer without performing the binary; search. This is useful for tables that provide data for only some of the; elements of a larger enum-based space. The first field of the primary key; must be an integral type; it cannot be a string. Adding ``let PrimaryKeyEarlyOut = 1`` to the ``ATable`` above:. .. code-block:: text. def ATable : GenericTable {; let FilterClass = ""AEntry"";; let Fields = [""Str"", ""Val1"", ""Val2""];; let PrimaryKey = [""Val1"", ""Val2""];; let PrimaryKeyName = ""lookupATableByValues"";; let PrimaryKeyEarlyOut = 1;; }. causes the lookup function to change as follows:. .. code-block:: text. const AEntry *lookupATableByValues(uint8_t Val1, uint16_t Val2) {; if ((Val1 < 0x2) ||; (Val1 > 0x5)); return nullptr;. struct KeyType {; ... We can construct two GenericTables with the same ``FilterClass``, so that they; select from the same overall set of records, but assign them with different; ``FilterClassField`` values so that they include different subsets of the; records of that class. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:3115,Security,access,accessible,3115," used, you have; todefine a macro just before including it, to get the right output:. .. code-block:: c++. #define GET_REGINFO_TARGET_DESC; #include ""ARMGenRegisterInfo.inc"". And just part of the generated file would be included. This is useful if; you need the same information in multiple formats (instantiation, initialization,; getter/setter functions, etc) from the same source TableGen file without having; to re-compile the TableGen file multiple times. Sometimes, multiple macros might be defined before the same include file to; output multiple blocks:. .. code-block:: c++. #define GET_REGISTER_MATCHER; #define GET_SUBTARGET_FEATURE_NAME; #define GET_MATCHER_IMPLEMENTATION; #include ""ARMGenAsmMatcher.inc"". The macros will be undef'd automatically as they're used, in the include file. On all LLVM back-ends, the ``llvm-tblgen`` binary will be executed on the root; TableGen file ``<Target>.td``, which should include all others. This guarantees; that all information needed is accessible, and that no duplication is needed; in the TableGen files. CodeEmitter; -----------. **Purpose**: CodeEmitterGen uses the descriptions of instructions and their fields to; construct an automated code emitter: a function that, given a MachineInstr,; returns the (currently, 32-bit unsigned) value of the instruction. **Output**: C++ code, implementing the target's CodeEmitter; class by overriding the virtual functions as ``<Target>CodeEmitter::function()``. **Usage**: Used to include directly at the end of ``<Target>MCCodeEmitter.cpp``. RegisterInfo; ------------. **Purpose**: This tablegen backend is responsible for emitting a description of a target; register file for a code generator. It uses instances of the Register,; RegisterAliases, and RegisterClass classes to gather this information. **Output**: C++ code with enums and structures representing the register mappings,; properties, masks, etc. **Usage**: Both on ``<Target>BaseRegisterInfo`` and ``<Target>MCTargetDesc`` (headers; and",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:11268,Security,expose,exposed,11268,", and; inheritable parameter attributes. This categorization happens automatically; based on information in ``Attr.td`` and is used to implement the ``classof``; functionality required for ``dyn_cast`` and similar APIs. ClangAttrPCHRead; ----------------. **Purpose**: Creates AttrPCHRead.inc, which is used to deserialize attributes; in the ``ASTReader::ReadAttributes`` function. ClangAttrPCHWrite; -----------------. **Purpose**: Creates AttrPCHWrite.inc, which is used to serialize attributes in; the ``ASTWriter::WriteAttributes`` function. ClangAttrSpellings; ---------------------. **Purpose**: Creates AttrSpellings.inc, which is used to implement the; ``__has_attribute`` feature test macro. ClangAttrSpellingListIndex; --------------------------. **Purpose**: Creates AttrSpellingListIndex.inc, which is used to map parsed; attribute spellings (including which syntax or scope was used) to an attribute; spelling list index. These spelling list index values are internal; implementation details exposed via; ``AttributeList::getAttributeSpellingListIndex``. ClangAttrVisitor; -------------------. **Purpose**: Creates AttrVisitor.inc, which is used when implementing; recursive AST visitors. ClangAttrTemplateInstantiate; ----------------------------. **Purpose**: Creates AttrTemplateInstantiate.inc, which implements the; ``instantiateTemplateAttribute`` function, used when instantiating a template; that requires an attribute to be cloned. ClangAttrParsedAttrList; -----------------------. **Purpose**: Creates AttrParsedAttrList.inc, which is used to generate the; ``AttributeList::Kind`` parsed attribute enumeration. ClangAttrParsedAttrImpl; -----------------------. **Purpose**: Creates AttrParsedAttrImpl.inc, which is used by; ``AttributeList.cpp`` to implement several functions on the ``AttributeList``; class. This functionality is implemented via the ``AttrInfoMap ParsedAttrInfo``; array, which contains one element per parsed attribute object. ClangAttrParsedAttrKinds; -----",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:10952,Testability,test,test,10952,". ClangAttrList; -------------. **Purpose**: Creates AttrList.inc, which is used when a list of semantic; attribute identifiers is required. For instance, ``AttrKinds.h`` includes this; file to generate the list of ``attr::Kind`` enumeration values. This list is; separated out into multiple categories: attributes, inheritable attributes, and; inheritable parameter attributes. This categorization happens automatically; based on information in ``Attr.td`` and is used to implement the ``classof``; functionality required for ``dyn_cast`` and similar APIs. ClangAttrPCHRead; ----------------. **Purpose**: Creates AttrPCHRead.inc, which is used to deserialize attributes; in the ``ASTReader::ReadAttributes`` function. ClangAttrPCHWrite; -----------------. **Purpose**: Creates AttrPCHWrite.inc, which is used to serialize attributes in; the ``ASTWriter::WriteAttributes`` function. ClangAttrSpellings; ---------------------. **Purpose**: Creates AttrSpellings.inc, which is used to implement the; ``__has_attribute`` feature test macro. ClangAttrSpellingListIndex; --------------------------. **Purpose**: Creates AttrSpellingListIndex.inc, which is used to map parsed; attribute spellings (including which syntax or scope was used) to an attribute; spelling list index. These spelling list index values are internal; implementation details exposed via; ``AttributeList::getAttributeSpellingListIndex``. ClangAttrVisitor; -------------------. **Purpose**: Creates AttrVisitor.inc, which is used when implementing; recursive AST visitors. ClangAttrTemplateInstantiate; ----------------------------. **Purpose**: Creates AttrTemplateInstantiate.inc, which implements the; ``instantiateTemplateAttribute`` function, used when instantiating a template; that requires an attribute to be cloned. ClangAttrParsedAttrList; -----------------------. **Purpose**: Creates AttrParsedAttrList.inc, which is used to generate the; ``AttributeList::Kind`` parsed attribute enumeration. ClangAttrParsedAttrImpl; ---",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:13975,Testability,test,tests,13975,"Generate Clang AST declaration nodes. ClangStmtNodes; --------------. Generate Clang AST statement nodes. ClangSACheckers; ---------------. Generate Clang Static Analyzer checkers. ClangCommentHTMLTags; --------------------. Generate efficient matchers for HTML tag names that are used in documentation comments. ClangCommentHTMLTagsProperties; ------------------------------. Generate efficient matchers for HTML tag properties. ClangCommentHTMLNamedCharacterReferences; ----------------------------------------. Generate function to translate named character references to UTF-8 sequences. ClangCommentCommandInfo; -----------------------. Generate command properties for commands that are used in documentation comments. ClangCommentCommandList; -----------------------. Generate list of commands that are used in documentation comments. ArmNeon; -------. Generate arm_neon.h for clang. ArmNeonSema; -----------. Generate ARM NEON sema support for clang. ArmNeonTest; -----------. Generate ARM NEON tests for clang. AttrDocs; --------. **Purpose**: Creates ``AttributeReference.rst`` from ``AttrDocs.td``, and is; used for documenting user-facing attributes. General BackEnds; ================. Print Records; -------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. See the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for more information. Print Detailed Records; ----------------------. The TableGen command option ``--print-detailed-records`` invokes a backend; that prints all the global variables, classes, and records defined in the; source files, with more detail than the default record printer. See the; :doc:`TableGen Backend Developer's Guide <./BackGuide>` for more; information. JSON Reference; --------------. **Purpose**: Output all the values in every ``def``, as a JSON data; structure that can be easily parsed by a variety of la",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:29216,Testability,test,tests,29216,"Encoding = enc;; }. def : CEntry<""Apple"", CFoo, 10>;; def : CEntry<""Pear"", CBaz, 15>;; def : CEntry<""Apple"", CBar, 13>;. Here is the generated C++ code. .. code-block:: text. #ifdef GET_CTable_DECL; const CEntry *lookupCEntryByEncoding(uint16_t Encoding);; #endif. #ifdef GET_CTable_IMPL; constexpr CEntry CTable[] = {; { ""Apple"", CFoo, 0xA }, // 0; { ""Apple"", CBar, 0xD }, // 1; { ""Pear"", CBaz, 0xF }, // 2; };. const CEntry *lookupCEntryByEncoding(uint16_t Encoding) {; struct KeyType {; uint16_t Encoding;; };; KeyType Key = { Encoding };; auto Table = ArrayRef(CTable);; auto Idx = std::lower_bound(Table.begin(), Table.end(), Key,; [](const CEntry &LHS, const KeyType &RHS) {; if (LHS.Encoding < RHS.Encoding); return true;; if (LHS.Encoding > RHS.Encoding); return false;; return false;; });. if (Idx == Table.end() ||; Key.Encoding != Idx->Encoding); return nullptr;; return &*Idx;; }. The ``PrimaryKeyEarlyOut`` field, when set to 1, modifies the lookup; function so that it tests the first field of the primary key to determine; whether it is within the range of the collected records' primary keys. If; not, the function returns the null pointer without performing the binary; search. This is useful for tables that provide data for only some of the; elements of a larger enum-based space. The first field of the primary key; must be an integral type; it cannot be a string. Adding ``let PrimaryKeyEarlyOut = 1`` to the ``ATable`` above:. .. code-block:: text. def ATable : GenericTable {; let FilterClass = ""AEntry"";; let Fields = [""Str"", ""Val1"", ""Val2""];; let PrimaryKey = [""Val1"", ""Val2""];; let PrimaryKeyName = ""lookupATableByValues"";; let PrimaryKeyEarlyOut = 1;; }. causes the lookup function to change as follows:. .. code-block:: text. const AEntry *lookupATableByValues(uint8_t Val1, uint16_t Val2) {; if ((Val1 < 0x2) ||; (Val1 > 0x5)); return nullptr;. struct KeyType {; ... We can construct two GenericTables with the same ``FilterClass``, so that they; select from the same ove",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:1383,Usability,guid,guide,1383," (normally a C++; include file or a textual list of warnings, options, and error messages). TableGen is used by both LLVM, Clang, and MLIR with very different goals.; LLVM uses it as a way to automate the generation of massive amounts of; information regarding instructions, schedules, cores, and architecture; features. Some backends generate output that is consumed by more than one; source file, so they need to be created in a way that makes it is easy for; preprocessor tricks to be used. Some backends can also print C++ code; structures, so that they can be directly included as-is. Clang, on the other hand, uses it mainly for diagnostic messages (errors,; warnings, tips) and attributes, so more on the textual end of the scale. MLIR uses TableGen to define operations, operation dialects, and operation; traits. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen, and the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for a guide to writing a new backend. LLVM BackEnds; =============. .. warning::; This portion is incomplete. Each section below needs three subsections:; description of its purpose with a list of users, output generated from; generic input, and finally why it needed a new backend (in case there's; something similar). Overall, each backend will take the same TableGen file type and transform into; similar output for different targets/uses. There is an implicit contract between; the TableGen files, the back-ends and their users. For instance, a global contract is that each back-end produces macro-guarded; sections. Based on whether the file is included by a header or a source file,; or even in which context of each file the include is being used, you have; todefine a macro just before including it, to get the right output:. .. code-block:: c++. #define GET_REGINFO_TARGET_DESC; #include ""ARMGenRegisterInfo.inc"". And just part of the generated file would be included. This is useful if; you need the sa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst:14259,Usability,simpl,simple,14259,"tion comments. ClangCommentHTMLTagsProperties; ------------------------------. Generate efficient matchers for HTML tag properties. ClangCommentHTMLNamedCharacterReferences; ----------------------------------------. Generate function to translate named character references to UTF-8 sequences. ClangCommentCommandInfo; -----------------------. Generate command properties for commands that are used in documentation comments. ClangCommentCommandList; -----------------------. Generate list of commands that are used in documentation comments. ArmNeon; -------. Generate arm_neon.h for clang. ArmNeonSema; -----------. Generate ARM NEON sema support for clang. ArmNeonTest; -----------. Generate ARM NEON tests for clang. AttrDocs; --------. **Purpose**: Creates ``AttributeReference.rst`` from ``AttrDocs.td``, and is; used for documenting user-facing attributes. General BackEnds; ================. Print Records; -------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. See the :doc:`TableGen Backend Developer's Guide; <./BackGuide>` for more information. Print Detailed Records; ----------------------. The TableGen command option ``--print-detailed-records`` invokes a backend; that prints all the global variables, classes, and records defined in the; source files, with more detail than the default record printer. See the; :doc:`TableGen Backend Developer's Guide <./BackGuide>` for more; information. JSON Reference; --------------. **Purpose**: Output all the values in every ``def``, as a JSON data; structure that can be easily parsed by a variety of languages. Useful; for writing custom backends without having to modify TableGen itself,; or for performing auxiliary analysis on the same TableGen data passed; to a built-in backend. **Output**:. The root of the output file is a JSON object (i.e. dictionary),; containing the following fixed keys:. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackEnds.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:6068,Availability,avail,available,6068,"instance contains the name of the field, stored in an ``Init``; instance. It also contains the value of the field, likewise stored in an; ``Init``. (A better name for this class might be ``RecordField``.). In addition to those primary members, the ``RecordVal`` has other data members. * The source file location of the field definition. * The type of the field, stored as an instance; of the ``RecTy`` class (see `RecTy`_). The ``RecordVal`` class provides some useful functions. * Functions to get the name of the field in various forms. * A function to get the type of the field. * A function to get the value of the field. * A function to get the source file location. Note that field values are more easily obtained directly from the ``Record``; instance (see `Record`_). A ``RecordVal`` instance can be printed to an output stream with the ``<<``; operator. ``RecTy``; ---------. The ``RecTy`` class is used to represent the types of field values. It is; the base class for a series of subclasses, one for each of the; available field types. The ``RecTy`` class has one data member that is an; enumerated type specifying the specific type of field value. (A better; name for this class might be ``FieldTy``.). The ``RecTy`` class provides a few useful functions. * A virtual function to get the type name as a string. * A virtual function to check whether all the values of this type can; be converted to another given type. * A virtual function to check whether this type is a subtype of; another given type. * A function to get the corresponding ``list``; type for lists with elements of this type. For example, the function; returns the ``list<int>`` type when called with the ``int`` type. The subclasses that inherit from ``RecTy`` are; ``BitRecTy``,; ``BitsRecTy``,; ``CodeRecTy``,; ``DagRecTy``,; ``IntRecTy``,; ``ListRecTy``,; ``RecordRecTy``, and; ``StringRecTy``.; Some of these classes have additional members that; are described in the following subsections. *All* of the classes de",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:9084,Availability,avail,available,9084,"the ``ListRecTy`` type corresponding to ``list<``\ *type*\ ``>``. ``RecordRecTy``; ~~~~~~~~~~~~~~~. This class includes data members that contain the list of parent classes of; this record. It also provides a function to obtain the array of classes and; two functions to get the iterator ``begin()`` and ``end()`` values. The; class defines a type for the return values of the latter two functions. .. code-block:: text. using const_record_iterator = Record * const *;. The ``get()`` function takes an ``ArrayRef`` of pointers to the ``Record``; instances of the *direct* superclasses of the record and returns the ``RecordRecTy``; corresponding to the record inheriting from those superclasses. ``Init``; --------. The ``Init`` class is used to represent TableGen values. The name derives; from *initialization value*. This class should not be confused with the; ``RecordVal`` class, which represents record fields, both their names and; values. The ``Init`` class is the base class for a series of subclasses, one; for each of the available value types. The primary data member of ``Init``; is an enumerated type that represents the specific type of the value. The ``Init`` class provides a few useful functions. * A function to get the type enumerator. * A boolean virtual function to determine whether a value is completely; specified; that is, has no uninitialized subvalues. * Virtual functions to get the value as a string. * Virtual functions to cast the value to other types, implement the bit; range feature of TableGen, and implement the list slice feature. * A virtual function to get a particular bit of the value. The subclasses that inherit directly from ``Init`` are; ``UnsetInit`` and ``TypedInit``. An ``Init`` instance can be printed to an output stream with the ``<<``; operator. .. warning::; It is not specified whether two separate initialization values with; the same underlying type and value (e.g., two strings with the value; ""Hello"") are represented by two ``Init``\ s or s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:20322,Availability,error,error,20322," = Records.getAllDerivedDefinitions(""Attribute"");; for (Record *AttrRec : AttrRecords) {; ...; }. Getting Record Names and Fields; ===============================. As described above (see `Record`_), there are multiple functions that; return the name of a record. One particularly useful one is; ``getNameInitAsString()``, which returns the name as a ``std::string``. There are also multiple functions that return the fields of a record. To; obtain and iterate over all the fields:. .. code-block:: text. for (const RecordVal &Field : SomeRec->getValues()) {; ...; }. You will recall that ``RecordVal`` is the class whose instances contain; information about the fields in records. The ``getValue()`` function returns the ``RecordVal`` instance for a field; specified by name. There are multiple overloaded functions, some taking a; ``StringRef`` and others taking a ``const Init *``. Some functions return a; ``RecordVal *`` and others return a ``const RecordVal *``. If the field does; not exist, a fatal error message is printed. More often than not, you are interested in the value of the field, not all; the information in the ``RecordVal``. There is a large set of functions that; take a field name in some form and return its value. One function,; ``getValueInit``, returns the value as an ``Init *``. Another function,; ``isValueUnset``, returns a boolean specifying whether the value is unset; (uninitialized). Most of the functions return the value in some more useful form. For; example:. .. code-block:: text. std::vector<int64_t> RegCosts =; SomeRec->getValueAsListOfInts(""RegCosts"");. The field ``RegCosts`` is assumed to be a list of integers. That list is; returned as a ``std::vector`` of 64-bit integers. If the field is not a list; of integers, a fatal error message is printed. Here is a function that returns a field value as a ``Record``, but returns; null if the field does not exist. .. code-block:: text. if (Record *BaseRec = SomeRec->getValueAsOptionalDef(BaseFieldName)) {",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:21087,Availability,error,error,21087,"pecified by name. There are multiple overloaded functions, some taking a; ``StringRef`` and others taking a ``const Init *``. Some functions return a; ``RecordVal *`` and others return a ``const RecordVal *``. If the field does; not exist, a fatal error message is printed. More often than not, you are interested in the value of the field, not all; the information in the ``RecordVal``. There is a large set of functions that; take a field name in some form and return its value. One function,; ``getValueInit``, returns the value as an ``Init *``. Another function,; ``isValueUnset``, returns a boolean specifying whether the value is unset; (uninitialized). Most of the functions return the value in some more useful form. For; example:. .. code-block:: text. std::vector<int64_t> RegCosts =; SomeRec->getValueAsListOfInts(""RegCosts"");. The field ``RegCosts`` is assumed to be a list of integers. That list is; returned as a ``std::vector`` of 64-bit integers. If the field is not a list; of integers, a fatal error message is printed. Here is a function that returns a field value as a ``Record``, but returns; null if the field does not exist. .. code-block:: text. if (Record *BaseRec = SomeRec->getValueAsOptionalDef(BaseFieldName)) {; ...; }. The field is assumed to have another record as its value. That record is returned; as a pointer to a ``Record``. If the field does not exist or is unset, the; functions returns null. Getting Record Superclasses; ===========================. The ``Record`` class provides a function to obtain the superclasses of a; record. It is named ``getSuperClasses`` and returns an ``ArrayRef`` of an; array of ``std::pair`` pairs. The superclasses are in post-order: the order; in which the superclasses were visited while copying their fields into the; record. Each pair consists of a pointer to the ``Record`` instance for a; superclass record and an instance of the ``SMRange`` class. The range; indicates the source file locations of the beginning and end o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:23744,Availability,error,error,23744,"n, this stream is saved in the emitter class member; named ``OS``, although some ``run`` functions are simple and just use the; stream without saving it. The output can be produced by writing values; directly to the output stream, or by using the ``std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:23805,Availability,error,error,23805,"d just use the; stream without saving it. The output can be produced by writing values; directly to the output stream, or by using the ``std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:23834,Availability,error,error,23834,"d just use the; stream without saving it. The output can be produced by writing values; directly to the output stream, or by using the ``std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:24005,Availability,error,error,24005,"`std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Usi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:24064,Availability,error,error,24064,": text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:24560,Availability,error,error,24560,"eGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error report; possible. Debugging Tools; ===============. TableGen provides some tools to aid in debugging backends. The ``PrintRecords`` Backend; ----------------------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. The format of the output is guaranteed to be; constant over time, so that the outp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:25045,Availability,error,error,25045,"alError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error report; possible. Debugging Tools; ===============. TableGen provides some tools to aid in debugging backends. The ``PrintRecords`` Backend; ----------------------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. The format of the output is guaranteed to be; constant over time, so that the output can be compared in tests. The output; looks like this:. .. code-block:: text. ------------- Classes -----------------; ...; class XEntry<string XEntry:str = ?, int XEntry:val1 = ?> { // XBase; string Str = XEntry:str;; bits<8> Val1 = { !cast<bits<8>>(XEntry:val1){7}, ... };; bit Val3 = 1;; }; ...; ------------- Defs -----------------; def ATable {	// GenericTable; string FilterClass = ""AEntry"";; string CppTypeName = ""AEntry"";; list<string> Fields = [""Str"", ""Val1"", ""Val2""];; list<string> PrimaryKey = [""Val1"", ""Val2""];; string Primar",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:20328,Integrability,message,message,20328," = Records.getAllDerivedDefinitions(""Attribute"");; for (Record *AttrRec : AttrRecords) {; ...; }. Getting Record Names and Fields; ===============================. As described above (see `Record`_), there are multiple functions that; return the name of a record. One particularly useful one is; ``getNameInitAsString()``, which returns the name as a ``std::string``. There are also multiple functions that return the fields of a record. To; obtain and iterate over all the fields:. .. code-block:: text. for (const RecordVal &Field : SomeRec->getValues()) {; ...; }. You will recall that ``RecordVal`` is the class whose instances contain; information about the fields in records. The ``getValue()`` function returns the ``RecordVal`` instance for a field; specified by name. There are multiple overloaded functions, some taking a; ``StringRef`` and others taking a ``const Init *``. Some functions return a; ``RecordVal *`` and others return a ``const RecordVal *``. If the field does; not exist, a fatal error message is printed. More often than not, you are interested in the value of the field, not all; the information in the ``RecordVal``. There is a large set of functions that; take a field name in some form and return its value. One function,; ``getValueInit``, returns the value as an ``Init *``. Another function,; ``isValueUnset``, returns a boolean specifying whether the value is unset; (uninitialized). Most of the functions return the value in some more useful form. For; example:. .. code-block:: text. std::vector<int64_t> RegCosts =; SomeRec->getValueAsListOfInts(""RegCosts"");. The field ``RegCosts`` is assumed to be a list of integers. That list is; returned as a ``std::vector`` of 64-bit integers. If the field is not a list; of integers, a fatal error message is printed. Here is a function that returns a field value as a ``Record``, but returns; null if the field does not exist. .. code-block:: text. if (Record *BaseRec = SomeRec->getValueAsOptionalDef(BaseFieldName)) {",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:21093,Integrability,message,message,21093,"pecified by name. There are multiple overloaded functions, some taking a; ``StringRef`` and others taking a ``const Init *``. Some functions return a; ``RecordVal *`` and others return a ``const RecordVal *``. If the field does; not exist, a fatal error message is printed. More often than not, you are interested in the value of the field, not all; the information in the ``RecordVal``. There is a large set of functions that; take a field name in some form and return its value. One function,; ``getValueInit``, returns the value as an ``Init *``. Another function,; ``isValueUnset``, returns a boolean specifying whether the value is unset; (uninitialized). Most of the functions return the value in some more useful form. For; example:. .. code-block:: text. std::vector<int64_t> RegCosts =; SomeRec->getValueAsListOfInts(""RegCosts"");. The field ``RegCosts`` is assumed to be a list of integers. That list is; returned as a ``std::vector`` of 64-bit integers. If the field is not a list; of integers, a fatal error message is printed. Here is a function that returns a field value as a ``Record``, but returns; null if the field does not exist. .. code-block:: text. if (Record *BaseRec = SomeRec->getValueAsOptionalDef(BaseFieldName)) {; ...; }. The field is assumed to have another record as its value. That record is returned; as a pointer to a ``Record``. If the field does not exist or is unset, the; functions returns null. Getting Record Superclasses; ===========================. The ``Record`` class provides a function to obtain the superclasses of a; record. It is named ``getSuperClasses`` and returns an ``ArrayRef`` of an; array of ``std::pair`` pairs. The superclasses are in post-order: the order; in which the superclasses were visited while copying their fields into the; record. Each pair consists of a pointer to the ``Record`` instance for a; superclass record and an instance of the ``SMRange`` class. The range; indicates the source file locations of the beginning and end o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:23750,Integrability,message,messages,23750,"n, this stream is saved in the emitter class member; named ``OS``, although some ``run`` functions are simple and just use the; stream without saving it. The output can be produced by writing values; directly to the output stream, or by using the ``std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:23929,Integrability,message,message,23929,"values; directly to the output stream, or by using the ``std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:23984,Integrability,message,message,23984,"`std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Usi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:24043,Integrability,message,message,24043,": text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:24364,Integrability,message,message,24364,"mment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error report; possible. Debugging Tools; ===============. TableGen provides some tools to aid in debugging backends. The ``PrintRecords`` Backend; ----------------------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:24476,Integrability,message,message,24476,"eGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error report; possible. Debugging Tools; ===============. TableGen provides some tools to aid in debugging backends. The ``PrintRecords`` Backend; ----------------------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. The format of the output is guaranteed to be; constant over time, so that the outp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:24717,Integrability,message,message,24717,"to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error report; possible. Debugging Tools; ===============. TableGen provides some tools to aid in debugging backends. The ``PrintRecords`` Backend; ----------------------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. The format of the output is guaranteed to be; constant over time, so that the output can be compared in tests. The output; looks like this:. .. code-block:: text. ------------- Classes -----------------; ...; class XEntry<string XEntry:str = ?, int XEntry:val1 = ?> { // XBase; string Str = XEntry:str;; bits<8> Val1 = { !cast",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:24881,Integrability,message,message,24881,"rloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error report; possible. Debugging Tools; ===============. TableGen provides some tools to aid in debugging backends. The ``PrintRecords`` Backend; ----------------------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. The format of the output is guaranteed to be; constant over time, so that the output can be compared in tests. The output; looks like this:. .. code-block:: text. ------------- Classes -----------------; ...; class XEntry<string XEntry:str = ?, int XEntry:val1 = ?> { // XBase; string Str = XEntry:str;; bits<8> Val1 = { !cast<bits<8>>(XEntry:val1){7}, ... };; bit Val3 = 1;; }; ...; ------------- Defs -----------------; def ATable {	// GenericTable; string FilterClass = ""AEntry"";; string C",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:2474,Modifiability,variab,variables,2474,"ructures that contain the classes; and records that are collected from the TableGen source files by the; TableGen parser. Note that the term *class* refers to an abstract record; class, while the term *record* refers to a concrete record. Unless otherwise noted, functions associated with classes are instance; functions. ``RecordKeeper``; ----------------. An instance of the ``RecordKeeper`` class acts as the container for all the; classes and records parsed and collected by TableGen. The ``RecordKeeper``; instance is passed to the backend when it is invoked by TableGen. This class; is usually abbreviated ``RK``. There are two maps in the recordkeeper, one for classes and one for records; (the latter often referred to as *defs*). Each map maps the class or record; name to an instance of the ``Record`` class (see `Record`_), which contains; all the information about that class or record. In addition to the two maps, the ``RecordKeeper`` instance contains:. * A map that maps the names of global variables to their values.; Global variables are defined in TableGen files with outer; ``defvar`` statements. * A counter for naming anonymous records. The ``RecordKeeper`` class provides a few useful functions. * Functions to get the complete class and record maps. * Functions to get a subset of the records based on their parent classes. * Functions to get individual classes, records, and globals, by name. A ``RecordKeeper`` instance can be printed to an output stream with the ``<<``; operator. ``Record``; ----------. Each class or record built by TableGen is represented by an instance of; the ``Record`` class. The ``RecordKeeper`` instance contains one map for the; classes and one for the records. The primary data members of a record are; the record name, the vector of field names and their values, and the vector of; superclasses of the record. The record name is stored as a pointer to an ``Init`` (see `Init`_), which; is a class whose instances hold TableGen values (sometimes ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:2509,Modifiability,variab,variables,2509," TableGen source files by the; TableGen parser. Note that the term *class* refers to an abstract record; class, while the term *record* refers to a concrete record. Unless otherwise noted, functions associated with classes are instance; functions. ``RecordKeeper``; ----------------. An instance of the ``RecordKeeper`` class acts as the container for all the; classes and records parsed and collected by TableGen. The ``RecordKeeper``; instance is passed to the backend when it is invoked by TableGen. This class; is usually abbreviated ``RK``. There are two maps in the recordkeeper, one for classes and one for records; (the latter often referred to as *defs*). Each map maps the class or record; name to an instance of the ``Record`` class (see `Record`_), which contains; all the information about that class or record. In addition to the two maps, the ``RecordKeeper`` instance contains:. * A map that maps the names of global variables to their values.; Global variables are defined in TableGen files with outer; ``defvar`` statements. * A counter for naming anonymous records. The ``RecordKeeper`` class provides a few useful functions. * Functions to get the complete class and record maps. * Functions to get a subset of the records based on their parent classes. * Functions to get individual classes, records, and globals, by name. A ``RecordKeeper`` instance can be printed to an output stream with the ``<<``; operator. ``Record``; ----------. Each class or record built by TableGen is represented by an instance of; the ``Record`` class. The ``RecordKeeper`` instance contains one map for the; classes and one for the records. The primary data members of a record are; the record name, the vector of field names and their values, and the vector of; superclasses of the record. The record name is stored as a pointer to an ``Init`` (see `Init`_), which; is a class whose instances hold TableGen values (sometimes referred to as; *initializers*). The field names and values are stored in ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:6763,Modifiability,inherit,inherit,6763,"l`` instance can be printed to an output stream with the ``<<``; operator. ``RecTy``; ---------. The ``RecTy`` class is used to represent the types of field values. It is; the base class for a series of subclasses, one for each of the; available field types. The ``RecTy`` class has one data member that is an; enumerated type specifying the specific type of field value. (A better; name for this class might be ``FieldTy``.). The ``RecTy`` class provides a few useful functions. * A virtual function to get the type name as a string. * A virtual function to check whether all the values of this type can; be converted to another given type. * A virtual function to check whether this type is a subtype of; another given type. * A function to get the corresponding ``list``; type for lists with elements of this type. For example, the function; returns the ``list<int>`` type when called with the ``int`` type. The subclasses that inherit from ``RecTy`` are; ``BitRecTy``,; ``BitsRecTy``,; ``CodeRecTy``,; ``DagRecTy``,; ``IntRecTy``,; ``ListRecTy``,; ``RecordRecTy``, and; ``StringRecTy``.; Some of these classes have additional members that; are described in the following subsections. *All* of the classes derived from ``RecTy`` provide the ``get()`` function.; It returns an instance of ``Recty`` corresponding to the derived class.; Some of the ``get()`` functions require an argument to; specify which particular variant of the type is desired. These arguments are; described in the following subsections. A ``RecTy`` instance can be printed to an output stream with the ``<<``; operator. .. warning::; It is not specified whether there is a single ``RecTy`` instance of a; particular type or multiple instances. ``BitsRecTy``; ~~~~~~~~~~~~~. This class includes a data member with the size of the ``bits`` value and a; function to get that size. The ``get()`` function takes the length of the sequence, *n*, and returns the; ``BitsRecTy`` type corresponding to ``bits<``\ *n*\ ``>``. ``ListRec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:8711,Modifiability,inherit,inheriting,8711,"he ``bits`` value and a; function to get that size. The ``get()`` function takes the length of the sequence, *n*, and returns the; ``BitsRecTy`` type corresponding to ``bits<``\ *n*\ ``>``. ``ListRecTy``; ~~~~~~~~~~~~~. This class includes a data member that specifies the type of the list's; elements and a function to get that type. The ``get()`` function takes the ``RecTy`` *type* of the list members and; returns the ``ListRecTy`` type corresponding to ``list<``\ *type*\ ``>``. ``RecordRecTy``; ~~~~~~~~~~~~~~~. This class includes data members that contain the list of parent classes of; this record. It also provides a function to obtain the array of classes and; two functions to get the iterator ``begin()`` and ``end()`` values. The; class defines a type for the return values of the latter two functions. .. code-block:: text. using const_record_iterator = Record * const *;. The ``get()`` function takes an ``ArrayRef`` of pointers to the ``Record``; instances of the *direct* superclasses of the record and returns the ``RecordRecTy``; corresponding to the record inheriting from those superclasses. ``Init``; --------. The ``Init`` class is used to represent TableGen values. The name derives; from *initialization value*. This class should not be confused with the; ``RecordVal`` class, which represents record fields, both their names and; values. The ``Init`` class is the base class for a series of subclasses, one; for each of the available value types. The primary data member of ``Init``; is an enumerated type that represents the specific type of the value. The ``Init`` class provides a few useful functions. * A function to get the type enumerator. * A boolean virtual function to determine whether a value is completely; specified; that is, has no uninitialized subvalues. * Virtual functions to get the value as a string. * Virtual functions to cast the value to other types, implement the bit; range feature of TableGen, and implement the list slice feature. * A virtual f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:9699,Modifiability,inherit,inherit,9699,"from those superclasses. ``Init``; --------. The ``Init`` class is used to represent TableGen values. The name derives; from *initialization value*. This class should not be confused with the; ``RecordVal`` class, which represents record fields, both their names and; values. The ``Init`` class is the base class for a series of subclasses, one; for each of the available value types. The primary data member of ``Init``; is an enumerated type that represents the specific type of the value. The ``Init`` class provides a few useful functions. * A function to get the type enumerator. * A boolean virtual function to determine whether a value is completely; specified; that is, has no uninitialized subvalues. * Virtual functions to get the value as a string. * Virtual functions to cast the value to other types, implement the bit; range feature of TableGen, and implement the list slice feature. * A virtual function to get a particular bit of the value. The subclasses that inherit directly from ``Init`` are; ``UnsetInit`` and ``TypedInit``. An ``Init`` instance can be printed to an output stream with the ``<<``; operator. .. warning::; It is not specified whether two separate initialization values with; the same underlying type and value (e.g., two strings with the value; ""Hello"") are represented by two ``Init``\ s or share the same ``Init``. ``UnsetInit``; ~~~~~~~~~~~~~. This class, a subclass of ``Init``, represents the unset (uninitialized); value. The static function ``get()`` can be used to obtain the singleton; ``Init`` of this type. ``TypedInit``; ~~~~~~~~~~~~~. This class, a subclass of ``Init``, acts as the parent class of the classes; that represent specific value types (except for the unset value). These; classes include ``BitInit``, ``BitsInit``, ``DagInit``, ``DefInit``,; ``IntInit``, ``ListInit``, and ``StringInit``. (There are additional derived; types used by the TableGen parser.). This class includes a data member that specifies the ``RecTy`` type of the; value",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:26656,Modifiability,variab,variables,26656,"try:str = ?, int XEntry:val1 = ?> { // XBase; string Str = XEntry:str;; bits<8> Val1 = { !cast<bits<8>>(XEntry:val1){7}, ... };; bit Val3 = 1;; }; ...; ------------- Defs -----------------; def ATable {	// GenericTable; string FilterClass = ""AEntry"";; string CppTypeName = ""AEntry"";; list<string> Fields = [""Str"", ""Val1"", ""Val2""];; list<string> PrimaryKey = [""Val1"", ""Val2""];; string PrimaryKeyName = ""lookupATableByValues"";; bit PrimaryKeyEarlyOut = 0;; }; ...; def anonymous_0 {	// AEntry; string Str = ""Bob"";; bits<8> Val1 = { 0, 0, 0, 0, 0, 1, 0, 1 };; bits<10> Val2 = { 0, 0, 0, 0, 0, 0, 0, 0, 1, 1 };; }. Classes are shown with their template arguments, parent classes (following; ``//``), and fields. Records are shown with their parent classes and; fields. Note that anonymous records are named ``anonymous_0``,; ``anonymous_1``, etc. The ``PrintDetailedRecords`` Backend; ------------------------------------. The TableGen command option ``--print-detailed-records`` invokes a backend; that prints all the global variables, classes, and records defined in the; source files. The format of the output is *not* guaranteed to be constant; over time. The output looks like this. .. code-block:: text. DETAILED RECORDS for file llvm-project\llvm\lib\target\arc\arc.td. -------------------- Global Variables (5) --------------------. AMDGPUBufferIntrinsics = [int_amdgcn_buffer_load_format, ...; AMDGPUImageDimAtomicIntrinsics = [int_amdgcn_image_atomic_swap_1d, ...; ...; -------------------- Classes (758) --------------------. AMDGPUBufferLoad |IntrinsicsAMDGPU.td:879|; Template args:; LLVMType AMDGPUBufferLoad:data_ty = llvm_any_ty |IntrinsicsAMDGPU.td:879|; Superclasses: (SDPatternOperator) Intrinsic AMDGPURsrcIntrinsic; Fields:; list<SDNodeProperty> Properties = [SDNPMemOperand] |Intrinsics.td:348|; string LLVMName = """" |Intrinsics.td:343|; ...; -------------------- Records (12303) --------------------. AMDGPUSample_lz_o |IntrinsicsAMDGPU.td:560|; Defm sequence: |IntrinsicsAMDGPU.td:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:27838,Modifiability,variab,variables,27838,"oject\llvm\lib\target\arc\arc.td. -------------------- Global Variables (5) --------------------. AMDGPUBufferIntrinsics = [int_amdgcn_buffer_load_format, ...; AMDGPUImageDimAtomicIntrinsics = [int_amdgcn_image_atomic_swap_1d, ...; ...; -------------------- Classes (758) --------------------. AMDGPUBufferLoad |IntrinsicsAMDGPU.td:879|; Template args:; LLVMType AMDGPUBufferLoad:data_ty = llvm_any_ty |IntrinsicsAMDGPU.td:879|; Superclasses: (SDPatternOperator) Intrinsic AMDGPURsrcIntrinsic; Fields:; list<SDNodeProperty> Properties = [SDNPMemOperand] |Intrinsics.td:348|; string LLVMName = """" |Intrinsics.td:343|; ...; -------------------- Records (12303) --------------------. AMDGPUSample_lz_o |IntrinsicsAMDGPU.td:560|; Defm sequence: |IntrinsicsAMDGPU.td:584| |IntrinsicsAMDGPU.td:566|; Superclasses: AMDGPUSampleVariant; Fields:; string UpperCaseMod = ""_LZ_O"" |IntrinsicsAMDGPU.td:542|; string LowerCaseMod = ""_lz_o"" |IntrinsicsAMDGPU.td:543|; ... * Global variables defined with outer ``defvar`` statements are shown with; their values. * The classes are shown with their source location, template arguments,; superclasses, and fields. * The records are shown with their source location, ``defm`` sequence,; superclasses, and fields. Superclasses are shown in the order processed, with indirect superclasses in; parentheses. Each field is shown with its value and the source location at; which it was set.; The ``defm`` sequence gives the locations of the ``defm`` statements that; were involved in generating the record, in the order they were invoked. Timing TableGen Phases; ----------------------. TableGen provides a phase timing feature that produces a report of the time; used by the various phases of parsing the source files and running the; selected backend. This feature is enabled with the ``--time-phases`` option; of the TableGen command. If the backend is *not* instrumented for timing, then a report such as the; following is produced. This is the timing for the; ``--print-d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:25506,Testability,test,tests,25506,"Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error report; possible. Debugging Tools; ===============. TableGen provides some tools to aid in debugging backends. The ``PrintRecords`` Backend; ----------------------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. The format of the output is guaranteed to be; constant over time, so that the output can be compared in tests. The output; looks like this:. .. code-block:: text. ------------- Classes -----------------; ...; class XEntry<string XEntry:str = ?, int XEntry:val1 = ?> { // XBase; string Str = XEntry:str;; bits<8> Val1 = { !cast<bits<8>>(XEntry:val1){7}, ... };; bit Val3 = 1;; }; ...; ------------- Defs -----------------; def ATable {	// GenericTable; string FilterClass = ""AEntry"";; string CppTypeName = ""AEntry"";; list<string> Fields = [""Str"", ""Val1"", ""Val2""];; list<string> PrimaryKey = [""Val1"", ""Val2""];; string PrimaryKeyName = ""lookupATableByValues"";; bit PrimaryKeyEarlyOut = 0;; }; ...; def anonymous_0 {	// AEntry; string Str = ""Bob"";; bits<8> Val1 = { 0, 0, 0, 0, 0, 1, 0, 1 };; bits<10> Val2 = { 0, 0, 0, 0, 0, 0, 0, 0, 1, 1 };; }. Classes are shown with their template arguments, parent classes (following; ``//``), and fields. Records are shown with their parent classes and; fields. Note that anonymous records are named ``anonymous_0``,; ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:801,Usability,guid,guide,801,"===================================; TableGen Backend Developer's Guide; ===================================. .. sectnum::. .. contents::; :local:. Introduction; ============. The purpose of TableGen is to generate complex output files based on; information from source files that are significantly easier to code than the; output files would be, and also easier to maintain and modify over time. The; information is coded in a declarative style involving classes and records,; which are then processed by TableGen. The internalized records are passed on; to various backends, which extract information from a subset of the records; and generate an output file. These output files are typically ``.inc`` files; for C++, but may be any type of file that the backend developer needs. This document is a guide to writing a backend for TableGen. It is not a; complete reference manual, but rather a guide to using the facilities; provided by TableGen for the backends. For a complete reference to the; various data structures and functions involved, see the primary TableGen; header file (``record.h``) and/or the Doxygen documentation. This document assumes that you have read the :doc:`TableGen Programmer's; Reference <./ProgRef>`, which provides a detailed reference for coding; TableGen source files. For a description of the existing backends, see; :doc:`TableGen BackEnds <./BackEnds>`. Data Structures; ===============. The following sections describe the data structures that contain the classes; and records that are collected from the TableGen source files by the; TableGen parser. Note that the term *class* refers to an abstract record; class, while the term *record* refers to a concrete record. Unless otherwise noted, functions associated with classes are instance; functions. ``RecordKeeper``; ----------------. An instance of the ``RecordKeeper`` class acts as the container for all the; classes and records parsed and collected by TableGen. The ``RecordKeeper``; instance is passed to t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:895,Usability,guid,guide,895,"===================================; TableGen Backend Developer's Guide; ===================================. .. sectnum::. .. contents::; :local:. Introduction; ============. The purpose of TableGen is to generate complex output files based on; information from source files that are significantly easier to code than the; output files would be, and also easier to maintain and modify over time. The; information is coded in a declarative style involving classes and records,; which are then processed by TableGen. The internalized records are passed on; to various backends, which extract information from a subset of the records; and generate an output file. These output files are typically ``.inc`` files; for C++, but may be any type of file that the backend developer needs. This document is a guide to writing a backend for TableGen. It is not a; complete reference manual, but rather a guide to using the facilities; provided by TableGen for the backends. For a complete reference to the; various data structures and functions involved, see the primary TableGen; header file (``record.h``) and/or the Doxygen documentation. This document assumes that you have read the :doc:`TableGen Programmer's; Reference <./ProgRef>`, which provides a detailed reference for coding; TableGen source files. For a description of the existing backends, see; :doc:`TableGen BackEnds <./BackEnds>`. Data Structures; ===============. The following sections describe the data structures that contain the classes; and records that are collected from the TableGen source files by the; TableGen parser. Note that the term *class* refers to an abstract record; class, while the term *record* refers to a concrete record. Unless otherwise noted, functions associated with classes are instance; functions. ``RecordKeeper``; ----------------. An instance of the ``RecordKeeper`` class acts as the container for all the; classes and records parsed and collected by TableGen. The ``RecordKeeper``; instance is passed to t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:22839,Usability,simpl,simple,22839,"uperclasses were visited while copying their fields into the; record. Each pair consists of a pointer to the ``Record`` instance for a; superclass record and an instance of the ``SMRange`` class. The range; indicates the source file locations of the beginning and end of the class; definition. This example obtains the superclasses of the ``Prototype`` record and then; iterates over the pairs in the returned array. .. code-block:: text. ArrayRef<std::pair<Record *, SMRange>>; Superclasses = Prototype->getSuperClasses();; for (const auto &SuperPair : Superclasses) {; ...; }. The ``Record`` class also provides a function, ``getDirectSuperClasses``, to; append the *direct* superclasses of a record to a given vector of type; ``SmallVectorImpl<Record *>``. Emitting Text to the Output Stream; ==================================. The ``run`` function is passed a ``raw_ostream`` to which it prints the; output file. By convention, this stream is saved in the emitter class member; named ``OS``, although some ``run`` functions are simple and just use the; stream without saving it. The output can be produced by writing values; directly to the output stream, or by using the ``std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:23738,Usability,clear,clear,23738,"n, this stream is saved in the emitter class member; named ``OS``, although some ``run`` functions are simple and just use the; stream without saving it. The output can be produced by writing values; directly to the output stream, or by using the ``std::format()`` or; ``llvm::formatv()`` functions. .. code-block:: text. OS << ""#ifndef "" << NodeName << ""\n"";. OS << format(""0x%0*x, "", Digits, Value);. Instances of the following classes can be printed using the ``<<`` operator:; ``RecordKeeper``,; ``Record``,; ``RecTy``,; ``RecordVal``, and; ``Init``. The helper function ``emitSourceFileHeader()`` prints the header comment; that should be included at the top of every output file. A call to it is; included in the skeleton backend file ``TableGenBackendSkeleton.cpp``. Printing Error Messages; =======================. TableGen records are often derived from multiple classes and also often; defined through a sequence of multiclasses. Because of this, it can be; difficult for backends to report clear error messages with accurate source; file locations. To make error reporting easier, five error reporting; functions are provided, each with four overloads. * ``PrintWarning`` prints a message tagged as a warning. * ``PrintError`` prints a message tagged as an error. * ``PrintFatalError`` prints a message tagged as an error and then terminates. * ``PrintNote`` prints a note. It is often used after one of the previous; functions to provide more information. * ``PrintFatalNote`` prints a note and then terminates. Each of these five functions is overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst:25280,Usability,simpl,simple,25280," overloaded four times. * ``PrintError(const Twine &Msg)``:; Prints the message with no source file location. * ``PrintError(ArrayRef<SMLoc> ErrorLoc, const Twine &Msg)``:; Prints the message followed by the specified source line,; along with a pointer to the item in error. The array of; source file locations is typically taken from a ``Record`` instance. * ``PrintError(const Record *Rec, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record (see `Record`_). * ``PrintError(const RecordVal *RecVal, const Twine &Msg)``:; Prints the message followed by the source line associated with the; specified record field (see `RecordVal`_). Using these functions, the goal is to produce the most specific error report; possible. Debugging Tools; ===============. TableGen provides some tools to aid in debugging backends. The ``PrintRecords`` Backend; ----------------------------. The TableGen command option ``--print-records`` invokes a simple backend; that prints all the classes and records defined in the source files. This is; the default backend option. The format of the output is guaranteed to be; constant over time, so that the output can be compared in tests. The output; looks like this:. .. code-block:: text. ------------- Classes -----------------; ...; class XEntry<string XEntry:str = ?, int XEntry:val1 = ?> { // XBase; string Str = XEntry:str;; bits<8> Val1 = { !cast<bits<8>>(XEntry:val1){7}, ... };; bit Val3 = 1;; }; ...; ------------- Defs -----------------; def ATable {	// GenericTable; string FilterClass = ""AEntry"";; string CppTypeName = ""AEntry"";; list<string> Fields = [""Str"", ""Val1"", ""Val2""];; list<string> PrimaryKey = [""Val1"", ""Val2""];; string PrimaryKeyName = ""lookupATableByValues"";; bit PrimaryKeyEarlyOut = 0;; }; ...; def anonymous_0 {	// AEntry; string Str = ""Bob"";; bits<8> Val1 = { 0, 0, 0, 0, 0, 1, 0, 1 };; bits<10> Val2 = { 0, 0, 0, 0, 0, 0, 0, 0, 1, 1 };; }. Classes are shown with their template arguments,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/BackGuide.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:529,Availability,error,error,529,"=================; TableGen Overview; =================. .. contents::; :local:. .. toctree::; :hidden:. BackEnds; BackGuide; ProgRef. Introduction; ============. TableGen's purpose is to help a human develop and maintain records of; domain-specific information. Because there may be a large number of these; records, it is specifically designed to allow writing flexible descriptions and; for common features of these records to be factored out. This reduces the; amount of duplication in the description, reduces the chance of error, and makes; it easier to structure domain specific information. The TableGen front end parses a file, instantiates the declarations, and; hands the result off to a domain-specific `backend`_ for processing. See; the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. See :doc:`tblgen - Description to C++ Code; <../CommandGuide/tblgen>` for details on the ``*-tblgen`` commands; that run the various flavors of TableGen. The current major users of TableGen are :doc:`The LLVM Target-Independent; Code Generator <../CodeGenerator>` and the `Clang diagnostics and attributes; <https://clang.llvm.org/docs/UsersManual.html#controlling-errors-and-warnings>`_. Note that if you work with TableGen frequently and use emacs or vim,; you can find an emacs ""TableGen mode"" and a vim language file in the; ``llvm/utils/emacs`` and ``llvm/utils/vim`` directories of your LLVM; distribution, respectively. .. _intro:. The TableGen program; ====================. TableGen files are interpreted by the TableGen program: `llvm-tblgen` available; on your build directory under `bin`. It is not installed in the system (or where; your sysroot is set to), since it has no use beyond LLVM's build process. Running TableGen; ----------------. TableGen runs just like any other LLVM tool. The first (optional) argument; specifies the file to read. If a filename is not specified, ``llvm-tblgen``; reads from standard input. To be useful, one of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:1209,Availability,error,errors-and-warnings,1209," maintain records of; domain-specific information. Because there may be a large number of these; records, it is specifically designed to allow writing flexible descriptions and; for common features of these records to be factored out. This reduces the; amount of duplication in the description, reduces the chance of error, and makes; it easier to structure domain specific information. The TableGen front end parses a file, instantiates the declarations, and; hands the result off to a domain-specific `backend`_ for processing. See; the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. See :doc:`tblgen - Description to C++ Code; <../CommandGuide/tblgen>` for details on the ``*-tblgen`` commands; that run the various flavors of TableGen. The current major users of TableGen are :doc:`The LLVM Target-Independent; Code Generator <../CodeGenerator>` and the `Clang diagnostics and attributes; <https://clang.llvm.org/docs/UsersManual.html#controlling-errors-and-warnings>`_. Note that if you work with TableGen frequently and use emacs or vim,; you can find an emacs ""TableGen mode"" and a vim language file in the; ``llvm/utils/emacs`` and ``llvm/utils/vim`` directories of your LLVM; distribution, respectively. .. _intro:. The TableGen program; ====================. TableGen files are interpreted by the TableGen program: `llvm-tblgen` available; on your build directory under `bin`. It is not installed in the system (or where; your sysroot is set to), since it has no use beyond LLVM's build process. Running TableGen; ----------------. TableGen runs just like any other LLVM tool. The first (optional) argument; specifies the file to read. If a filename is not specified, ``llvm-tblgen``; reads from standard input. To be useful, one of the `backends`_ must be used. These backends are; selectable on the command line (type '``llvm-tblgen -help``' for a list). For; example, to get a list of all of the definitions that subclass a particular type; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:1597,Availability,avail,available,1597," information. The TableGen front end parses a file, instantiates the declarations, and; hands the result off to a domain-specific `backend`_ for processing. See; the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. See :doc:`tblgen - Description to C++ Code; <../CommandGuide/tblgen>` for details on the ``*-tblgen`` commands; that run the various flavors of TableGen. The current major users of TableGen are :doc:`The LLVM Target-Independent; Code Generator <../CodeGenerator>` and the `Clang diagnostics and attributes; <https://clang.llvm.org/docs/UsersManual.html#controlling-errors-and-warnings>`_. Note that if you work with TableGen frequently and use emacs or vim,; you can find an emacs ""TableGen mode"" and a vim language file in the; ``llvm/utils/emacs`` and ``llvm/utils/vim`` directories of your LLVM; distribution, respectively. .. _intro:. The TableGen program; ====================. TableGen files are interpreted by the TableGen program: `llvm-tblgen` available; on your build directory under `bin`. It is not installed in the system (or where; your sysroot is set to), since it has no use beyond LLVM's build process. Running TableGen; ----------------. TableGen runs just like any other LLVM tool. The first (optional) argument; specifies the file to read. If a filename is not specified, ``llvm-tblgen``; reads from standard input. To be useful, one of the `backends`_ must be used. These backends are; selectable on the command line (type '``llvm-tblgen -help``' for a list). For; example, to get a list of all of the definitions that subclass a particular type; (which can be useful for building up an enum list of these records), use the; ``-print-enums`` option:. .. code-block:: bash. $ llvm-tblgen X86.td -print-enums -class=Register; AH, AL, AX, BH, BL, BP, BPL, BX, CH, CL, CX, DH, DI, DIL, DL, DX, EAX, EBP, EBX,; ECX, EDI, EDX, EFLAGS, EIP, ESI, ESP, FP0, FP1, FP2, FP3, FP4, FP5, FP6, IP,; MM0, MM1, MM2, MM3, MM4, MM5, MM6, M",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:9422,Availability,down,down,9422," the; keyword `class` either on the same file or some other included. Most target; TableGen files include the generic ones in ``include/llvm/Target``. **TableGen classes** are abstract records that are used to build and describe; other records. These classes allow the end-user to build abstractions for; either the domain they are targeting (such as ""Register"", ""RegisterClass"", and; ""Instruction"" in the LLVM code generator) or for the implementor to help factor; out common properties of records (such as ""FPInst"", which is used to represent; floating point instructions in the X86 backend). TableGen keeps track of all of; the classes that are used to build up a definition, so the backend can find all; definitions of a particular class, such as ""Instruction"". .. code-block:: text. class ProcNoItin<string Name, list<SubtargetFeature> Features>; : Processor<Name, NoItineraries, Features>;. Here, the class ProcNoItin, receiving parameters `Name` of type `string` and; a list of target features is specializing the class Processor by passing the; arguments down as well as hard-coding NoItineraries. **TableGen multiclasses** are groups of abstract records that are instantiated; all at once. Each instantiation can result in multiple TableGen definitions.; If a multiclass inherits from another multiclass, the definitions in the; sub-multiclass become part of the current multiclass, as if they were declared; in the current multiclass. .. code-block:: text. multiclass ro_signed_pats<string T, string Rm, dag Base, dag Offset, dag Extend,; dag address, ValueType sty> {; def : Pat<(i32 (!cast<SDNode>(""sextload"" # sty) address)),; (!cast<Instruction>(""LDRS"" # T # ""w_"" # Rm # ""_RegOffset""); Base, Offset, Extend)>;. def : Pat<(i64 (!cast<SDNode>(""sextload"" # sty) address)),; (!cast<Instruction>(""LDRS"" # T # ""x_"" # Rm # ""_RegOffset""); Base, Offset, Extend)>;; }. defm : ro_signed_pats<""B"", Rm, Base, Offset, Extend,; !foreach(decls.pattern, address,; !subst(SHIFT, imm_eq0, decls.pattern)),;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:11263,Availability,error,error,11263,"et, Extend)>;; }. defm : ro_signed_pats<""B"", Rm, Base, Offset, Extend,; !foreach(decls.pattern, address,; !subst(SHIFT, imm_eq0, decls.pattern)),; i8>;. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. .. _backend:; .. _backends:. TableGen backends; =================. TableGen files have no real meaning without a backend. The default operation; when running ``*-tblgen`` is to print the information in a textual format, but; that's only useful for debugging the TableGen files themselves. The power; in TableGen is, however, to interpret the source files into an internal; representation that can be generated into anything you want. Current usage of TableGen is to create huge include files with tables that you; can either include directly (if the output is in the language you're coding),; or be used in pre-processing via macros surrounding the include of the file. Direct output can be used if the backend already prints a table in C format; or if the output is just a list of strings (for error and warning messages).; Pre-processed output should be used if the same information needs to be used; in different contexts (like Instruction names), so your backend should print; a meta-information list that can be shaped into different compile-time formats. See :doc:`TableGen BackEnds <./BackEnds>` for a list of available; backends, and see the :doc:`TableGen Backend Developer's Guide <./BackGuide>`; for information on how to write and debug a new backend. Tools and Resources; ===================. In addition to this documentation, a list of tools and resources for TableGen; can be found in TableGen's; `README <https://github.com/llvm/llvm-project/blob/main/llvm/utils/TableGen/README.md>`_. TableGen Deficiencies; =====================. Despite being very generic, TableGen has some deficiencies that have been; pointed out numerous times. The common theme is that, while TableGen allows; you to build domain specific languages, the f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:11585,Availability,avail,available,11585,"n; when running ``*-tblgen`` is to print the information in a textual format, but; that's only useful for debugging the TableGen files themselves. The power; in TableGen is, however, to interpret the source files into an internal; representation that can be generated into anything you want. Current usage of TableGen is to create huge include files with tables that you; can either include directly (if the output is in the language you're coding),; or be used in pre-processing via macros surrounding the include of the file. Direct output can be used if the backend already prints a table in C format; or if the output is just a list of strings (for error and warning messages).; Pre-processed output should be used if the same information needs to be used; in different contexts (like Instruction names), so your backend should print; a meta-information list that can be shaped into different compile-time formats. See :doc:`TableGen BackEnds <./BackEnds>` for a list of available; backends, and see the :doc:`TableGen Backend Developer's Guide <./BackGuide>`; for information on how to write and debug a new backend. Tools and Resources; ===================. In addition to this documentation, a list of tools and resources for TableGen; can be found in TableGen's; `README <https://github.com/llvm/llvm-project/blob/main/llvm/utils/TableGen/README.md>`_. TableGen Deficiencies; =====================. Despite being very generic, TableGen has some deficiencies that have been; pointed out numerous times. The common theme is that, while TableGen allows; you to build domain specific languages, the final languages that you create; lack the power of other DSLs, which in turn increase considerably the size; and complexity of TableGen files. At the same time, TableGen allows you to create virtually any meaning of; the basic concepts via custom-made backends, which can pervert the original; design and make it very hard for newcomers to understand the evil TableGen; file. There are some in fav",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:1655,Deployability,install,installed,1655,"in-specific `backend`_ for processing. See; the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. See :doc:`tblgen - Description to C++ Code; <../CommandGuide/tblgen>` for details on the ``*-tblgen`` commands; that run the various flavors of TableGen. The current major users of TableGen are :doc:`The LLVM Target-Independent; Code Generator <../CodeGenerator>` and the `Clang diagnostics and attributes; <https://clang.llvm.org/docs/UsersManual.html#controlling-errors-and-warnings>`_. Note that if you work with TableGen frequently and use emacs or vim,; you can find an emacs ""TableGen mode"" and a vim language file in the; ``llvm/utils/emacs`` and ``llvm/utils/vim`` directories of your LLVM; distribution, respectively. .. _intro:. The TableGen program; ====================. TableGen files are interpreted by the TableGen program: `llvm-tblgen` available; on your build directory under `bin`. It is not installed in the system (or where; your sysroot is set to), since it has no use beyond LLVM's build process. Running TableGen; ----------------. TableGen runs just like any other LLVM tool. The first (optional) argument; specifies the file to read. If a filename is not specified, ``llvm-tblgen``; reads from standard input. To be useful, one of the `backends`_ must be used. These backends are; selectable on the command line (type '``llvm-tblgen -help``' for a list). For; example, to get a list of all of the definitions that subclass a particular type; (which can be useful for building up an enum list of these records), use the; ``-print-enums`` option:. .. code-block:: bash. $ llvm-tblgen X86.td -print-enums -class=Register; AH, AL, AX, BH, BL, BP, BPL, BX, CH, CL, CX, DH, DI, DIL, DL, DX, EAX, EBP, EBX,; ECX, EDI, EDX, EFLAGS, EIP, ESI, ESP, FP0, FP1, FP2, FP3, FP4, FP5, FP6, IP,; MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7, R10, R10B, R10D, R10W, R11, R11B, R11D,; R11W, R12, R12B, R12D, R12W, R13, R13B, R13D, R13W, R14, R14B, R14D, R14W,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:452,Energy Efficiency,reduce,reduces,452,"=================; TableGen Overview; =================. .. contents::; :local:. .. toctree::; :hidden:. BackEnds; BackGuide; ProgRef. Introduction; ============. TableGen's purpose is to help a human develop and maintain records of; domain-specific information. Because there may be a large number of these; records, it is specifically designed to allow writing flexible descriptions and; for common features of these records to be factored out. This reduces the; amount of duplication in the description, reduces the chance of error, and makes; it easier to structure domain specific information. The TableGen front end parses a file, instantiates the declarations, and; hands the result off to a domain-specific `backend`_ for processing. See; the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. See :doc:`tblgen - Description to C++ Code; <../CommandGuide/tblgen>` for details on the ``*-tblgen`` commands; that run the various flavors of TableGen. The current major users of TableGen are :doc:`The LLVM Target-Independent; Code Generator <../CodeGenerator>` and the `Clang diagnostics and attributes; <https://clang.llvm.org/docs/UsersManual.html#controlling-errors-and-warnings>`_. Note that if you work with TableGen frequently and use emacs or vim,; you can find an emacs ""TableGen mode"" and a vim language file in the; ``llvm/utils/emacs`` and ``llvm/utils/vim`` directories of your LLVM; distribution, respectively. .. _intro:. The TableGen program; ====================. TableGen files are interpreted by the TableGen program: `llvm-tblgen` available; on your build directory under `bin`. It is not installed in the system (or where; your sysroot is set to), since it has no use beyond LLVM's build process. Running TableGen; ----------------. TableGen runs just like any other LLVM tool. The first (optional) argument; specifies the file to read. If a filename is not specified, ``llvm-tblgen``; reads from standard input. To be useful, one of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:507,Energy Efficiency,reduce,reduces,507,"=================; TableGen Overview; =================. .. contents::; :local:. .. toctree::; :hidden:. BackEnds; BackGuide; ProgRef. Introduction; ============. TableGen's purpose is to help a human develop and maintain records of; domain-specific information. Because there may be a large number of these; records, it is specifically designed to allow writing flexible descriptions and; for common features of these records to be factored out. This reduces the; amount of duplication in the description, reduces the chance of error, and makes; it easier to structure domain specific information. The TableGen front end parses a file, instantiates the declarations, and; hands the result off to a domain-specific `backend`_ for processing. See; the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. See :doc:`tblgen - Description to C++ Code; <../CommandGuide/tblgen>` for details on the ``*-tblgen`` commands; that run the various flavors of TableGen. The current major users of TableGen are :doc:`The LLVM Target-Independent; Code Generator <../CodeGenerator>` and the `Clang diagnostics and attributes; <https://clang.llvm.org/docs/UsersManual.html#controlling-errors-and-warnings>`_. Note that if you work with TableGen frequently and use emacs or vim,; you can find an emacs ""TableGen mode"" and a vim language file in the; ``llvm/utils/emacs`` and ``llvm/utils/vim`` directories of your LLVM; distribution, respectively. .. _intro:. The TableGen program; ====================. TableGen files are interpreted by the TableGen program: `llvm-tblgen` available; on your build directory under `bin`. It is not installed in the system (or where; your sysroot is set to), since it has no use beyond LLVM's build process. Running TableGen; ----------------. TableGen runs just like any other LLVM tool. The first (optional) argument; specifies the file to read. If a filename is not specified, ``llvm-tblgen``; reads from standard input. To be useful, one of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:10761,Energy Efficiency,power,power,10761,"lticlass ro_signed_pats<string T, string Rm, dag Base, dag Offset, dag Extend,; dag address, ValueType sty> {; def : Pat<(i32 (!cast<SDNode>(""sextload"" # sty) address)),; (!cast<Instruction>(""LDRS"" # T # ""w_"" # Rm # ""_RegOffset""); Base, Offset, Extend)>;. def : Pat<(i64 (!cast<SDNode>(""sextload"" # sty) address)),; (!cast<Instruction>(""LDRS"" # T # ""x_"" # Rm # ""_RegOffset""); Base, Offset, Extend)>;; }. defm : ro_signed_pats<""B"", Rm, Base, Offset, Extend,; !foreach(decls.pattern, address,; !subst(SHIFT, imm_eq0, decls.pattern)),; i8>;. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. .. _backend:; .. _backends:. TableGen backends; =================. TableGen files have no real meaning without a backend. The default operation; when running ``*-tblgen`` is to print the information in a textual format, but; that's only useful for debugging the TableGen files themselves. The power; in TableGen is, however, to interpret the source files into an internal; representation that can be generated into anything you want. Current usage of TableGen is to create huge include files with tables that you; can either include directly (if the output is in the language you're coding),; or be used in pre-processing via macros surrounding the include of the file. Direct output can be used if the backend already prints a table in C format; or if the output is just a list of strings (for error and warning messages).; Pre-processed output should be used if the same information needs to be used; in different contexts (like Instruction names), so your backend should print; a meta-information list that can be shaped into different compile-time formats. See :doc:`TableGen BackEnds <./BackEnds>` for a list of available; backends, and see the :doc:`TableGen Backend Developer's Guide <./BackGuide>`; for information on how to write and debug a new backend. Tools and Resources; ===================. In addition to this documentation, a list of tools and ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:12255,Energy Efficiency,power,power,12255,"ternal; representation that can be generated into anything you want. Current usage of TableGen is to create huge include files with tables that you; can either include directly (if the output is in the language you're coding),; or be used in pre-processing via macros surrounding the include of the file. Direct output can be used if the backend already prints a table in C format; or if the output is just a list of strings (for error and warning messages).; Pre-processed output should be used if the same information needs to be used; in different contexts (like Instruction names), so your backend should print; a meta-information list that can be shaped into different compile-time formats. See :doc:`TableGen BackEnds <./BackEnds>` for a list of available; backends, and see the :doc:`TableGen Backend Developer's Guide <./BackGuide>`; for information on how to write and debug a new backend. Tools and Resources; ===================. In addition to this documentation, a list of tools and resources for TableGen; can be found in TableGen's; `README <https://github.com/llvm/llvm-project/blob/main/llvm/utils/TableGen/README.md>`_. TableGen Deficiencies; =====================. Despite being very generic, TableGen has some deficiencies that have been; pointed out numerous times. The common theme is that, while TableGen allows; you to build domain specific languages, the final languages that you create; lack the power of other DSLs, which in turn increase considerably the size; and complexity of TableGen files. At the same time, TableGen allows you to create virtually any meaning of; the basic concepts via custom-made backends, which can pervert the original; design and make it very hard for newcomers to understand the evil TableGen; file. There are some in favor of extending the semantics even more, but making sure; backends adhere to strict rules. Others are suggesting we should move to less,; more powerful DSLs designed with specific purposes, or even reusing existing; DSLs.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:12753,Energy Efficiency,power,powerful,12753,"ternal; representation that can be generated into anything you want. Current usage of TableGen is to create huge include files with tables that you; can either include directly (if the output is in the language you're coding),; or be used in pre-processing via macros surrounding the include of the file. Direct output can be used if the backend already prints a table in C format; or if the output is just a list of strings (for error and warning messages).; Pre-processed output should be used if the same information needs to be used; in different contexts (like Instruction names), so your backend should print; a meta-information list that can be shaped into different compile-time formats. See :doc:`TableGen BackEnds <./BackEnds>` for a list of available; backends, and see the :doc:`TableGen Backend Developer's Guide <./BackGuide>`; for information on how to write and debug a new backend. Tools and Resources; ===================. In addition to this documentation, a list of tools and resources for TableGen; can be found in TableGen's; `README <https://github.com/llvm/llvm-project/blob/main/llvm/utils/TableGen/README.md>`_. TableGen Deficiencies; =====================. Despite being very generic, TableGen has some deficiencies that have been; pointed out numerous times. The common theme is that, while TableGen allows; you to build domain specific languages, the final languages that you create; lack the power of other DSLs, which in turn increase considerably the size; and complexity of TableGen files. At the same time, TableGen allows you to create virtually any meaning of; the basic concepts via custom-made backends, which can pervert the original; design and make it very hard for newcomers to understand the evil TableGen; file. There are some in favor of extending the semantics even more, but making sure; backends adhere to strict rules. Others are suggesting we should move to less,; more powerful DSLs designed with specific purposes, or even reusing existing; DSLs.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:11281,Integrability,message,messages,11281,"et, Extend)>;; }. defm : ro_signed_pats<""B"", Rm, Base, Offset, Extend,; !foreach(decls.pattern, address,; !subst(SHIFT, imm_eq0, decls.pattern)),; i8>;. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. .. _backend:; .. _backends:. TableGen backends; =================. TableGen files have no real meaning without a backend. The default operation; when running ``*-tblgen`` is to print the information in a textual format, but; that's only useful for debugging the TableGen files themselves. The power; in TableGen is, however, to interpret the source files into an internal; representation that can be generated into anything you want. Current usage of TableGen is to create huge include files with tables that you; can either include directly (if the output is in the language you're coding),; or be used in pre-processing via macros surrounding the include of the file. Direct output can be used if the backend already prints a table in C format; or if the output is just a list of strings (for error and warning messages).; Pre-processed output should be used if the same information needs to be used; in different contexts (like Instruction names), so your backend should print; a meta-information list that can be shaped into different compile-time formats. See :doc:`TableGen BackEnds <./BackEnds>` for a list of available; backends, and see the :doc:`TableGen Backend Developer's Guide <./BackGuide>`; for information on how to write and debug a new backend. Tools and Resources; ===================. In addition to this documentation, a list of tools and resources for TableGen; can be found in TableGen's; `README <https://github.com/llvm/llvm-project/blob/main/llvm/utils/TableGen/README.md>`_. TableGen Deficiencies; =====================. Despite being very generic, TableGen has some deficiencies that have been; pointed out numerous times. The common theme is that, while TableGen allows; you to build domain specific languages, the f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:363,Modifiability,flexible,flexible,363,"=================; TableGen Overview; =================. .. contents::; :local:. .. toctree::; :hidden:. BackEnds; BackGuide; ProgRef. Introduction; ============. TableGen's purpose is to help a human develop and maintain records of; domain-specific information. Because there may be a large number of these; records, it is specifically designed to allow writing flexible descriptions and; for common features of these records to be factored out. This reduces the; amount of duplication in the description, reduces the chance of error, and makes; it easier to structure domain specific information. The TableGen front end parses a file, instantiates the declarations, and; hands the result off to a domain-specific `backend`_ for processing. See; the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. See :doc:`tblgen - Description to C++ Code; <../CommandGuide/tblgen>` for details on the ``*-tblgen`` commands; that run the various flavors of TableGen. The current major users of TableGen are :doc:`The LLVM Target-Independent; Code Generator <../CodeGenerator>` and the `Clang diagnostics and attributes; <https://clang.llvm.org/docs/UsersManual.html#controlling-errors-and-warnings>`_. Note that if you work with TableGen frequently and use emacs or vim,; you can find an emacs ""TableGen mode"" and a vim language file in the; ``llvm/utils/emacs`` and ``llvm/utils/vim`` directories of your LLVM; distribution, respectively. .. _intro:. The TableGen program; ====================. TableGen files are interpreted by the TableGen program: `llvm-tblgen` available; on your build directory under `bin`. It is not installed in the system (or where; your sysroot is set to), since it has no use beyond LLVM's build process. Running TableGen; ----------------. TableGen runs just like any other LLVM tool. The first (optional) argument; specifies the file to read. If a filename is not specified, ``llvm-tblgen``; reads from standard input. To be useful, one of ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:3753,Modifiability,extend,extending,3753,", RIP, RSI, RSP, SI, SIL, SP, SPL, ST0, ST1, ST2, ST3, ST4, ST5, ST6, ST7,; XMM0, XMM1, XMM10, XMM11, XMM12, XMM13, XMM14, XMM15, XMM2, XMM3, XMM4, XMM5,; XMM6, XMM7, XMM8, XMM9,. $ llvm-tblgen X86.td -print-enums -class=Instruction; ABS_F, ABS_Fp32, ABS_Fp64, ABS_Fp80, ADC32mi, ADC32mi8, ADC32mr, ADC32ri,; ADC32ri8, ADC32rm, ADC32rr, ADC64mi32, ADC64mi8, ADC64mr, ADC64ri32, ADC64ri8,; ADC64rm, ADC64rr, ADD16mi, ADD16mi8, ADD16mr, ADD16ri, ADD16ri8, ADD16rm,; ADD16rr, ADD32mi, ADD32mi8, ADD32mr, ADD32ri, ADD32ri8, ADD32rm, ADD32rr,; ADD64mi32, ADD64mi8, ADD64mr, ADD64ri32, ... The default backend prints out all of the records. There is also a general; backend which outputs all the records as a JSON data structure, enabled using; the `-dump-json` option. If you plan to use TableGen, you will most likely have to write a `backend`_; that extracts the information specific to what you need and formats it in the; appropriate way. You can do this by extending TableGen itself in C++, or by; writing a script in any language that can consume the JSON output. Example; -------. With no other arguments, `llvm-tblgen` parses the specified file and prints out all; of the classes, then all of the definitions. This is a good way to see what the; various definitions expand to fully. Running this on the ``X86.td`` file prints; this (at the time of this writing):. .. code-block:: text. ...; def ADD32rr { // Instruction X86Inst I; string Namespace = ""X86"";; dag OutOperandList = (outs GR32:$dst);; dag InOperandList = (ins GR32:$src1, GR32:$src2);; string AsmString = ""add{l}\t{$src2, $dst|$dst, $src2}"";; list<dag> Pattern = [(set GR32:$dst, (add GR32:$src1, GR32:$src2))];; list<Register> Uses = [];; list<Register> Defs = [EFLAGS];; list<Predicate> Predicates = [];; int CodeSize = 3;; int AddedComplexity = 0;; bit isReturn = 0;; bit isBranch = 0;; bit isIndirectBranch = 0;; bit isBarrier = 0;; bit isCall = 0;; bit canFoldAsLoad = 0;; bit mayLoad = 0;; bit mayStore = 0;; bit isImplicitDef = ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:6891,Modifiability,extend,extended,6891,"is selected by; the code generator, that it is a two-address instruction, has a particular; encoding, etc. The contents and semantics of the information in the record are; specific to the needs of the X86 backend, and are only shown as an example. As you can see, a lot of information is needed for every instruction supported; by the code generator, and specifying it all manually would be unmaintainable,; prone to bugs, and tiring to do in the first place. Because we are using; TableGen, all of the information was derived from the following definition:. .. code-block:: text. let Defs = [EFLAGS],; isCommutable = 1, // X = ADD Y,Z --> X = ADD Z,Y; isConvertibleToThreeAddress = 1 in // Can transform into LEA.; def ADD32rr : I<0x01, MRMDestReg, (outs GR32:$dst),; (ins GR32:$src1, GR32:$src2),; ""add{l}\t{$src2, $dst|$dst, $src2}"",; [(set GR32:$dst, (add GR32:$src1, GR32:$src2))]>;. This definition makes use of the custom class ``I`` (extended from the custom; class ``X86Inst``), which is defined in the X86-specific TableGen file, to; factor out the common features that instructions of its class share. A key; feature of TableGen is that it allows the end-user to define the abstractions; they prefer to use when describing their information. Syntax; ======. TableGen has a syntax that is loosely based on C++ templates, with built-in; types and specification. In addition, TableGen's syntax introduces some; automation concepts like multiclass, foreach, let, etc. Basic concepts; --------------. TableGen files consist of two key parts: 'classes' and 'definitions', both of; which are considered 'records'. **TableGen records** have a unique name, a list of values, and a list of; superclasses. The list of values is the main data that TableGen builds for each; record; it is this that holds the domain specific information for the; application. The interpretation of this data is left to a specific `backend`_,; but the structure and format rules are taken care of and are fixed by; Table",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:9639,Modifiability,inherit,inherits,9639,"egister"", ""RegisterClass"", and; ""Instruction"" in the LLVM code generator) or for the implementor to help factor; out common properties of records (such as ""FPInst"", which is used to represent; floating point instructions in the X86 backend). TableGen keeps track of all of; the classes that are used to build up a definition, so the backend can find all; definitions of a particular class, such as ""Instruction"". .. code-block:: text. class ProcNoItin<string Name, list<SubtargetFeature> Features>; : Processor<Name, NoItineraries, Features>;. Here, the class ProcNoItin, receiving parameters `Name` of type `string` and; a list of target features is specializing the class Processor by passing the; arguments down as well as hard-coding NoItineraries. **TableGen multiclasses** are groups of abstract records that are instantiated; all at once. Each instantiation can result in multiple TableGen definitions.; If a multiclass inherits from another multiclass, the definitions in the; sub-multiclass become part of the current multiclass, as if they were declared; in the current multiclass. .. code-block:: text. multiclass ro_signed_pats<string T, string Rm, dag Base, dag Offset, dag Extend,; dag address, ValueType sty> {; def : Pat<(i32 (!cast<SDNode>(""sextload"" # sty) address)),; (!cast<Instruction>(""LDRS"" # T # ""w_"" # Rm # ""_RegOffset""); Base, Offset, Extend)>;. def : Pat<(i64 (!cast<SDNode>(""sextload"" # sty) address)),; (!cast<Instruction>(""LDRS"" # T # ""x_"" # Rm # ""_RegOffset""); Base, Offset, Extend)>;; }. defm : ro_signed_pats<""B"", Rm, Base, Offset, Extend,; !foreach(decls.pattern, address,; !subst(SHIFT, imm_eq0, decls.pattern)),; i8>;. See the :doc:`TableGen Programmer's Reference <./ProgRef>` for an in-depth; description of TableGen. .. _backend:; .. _backends:. TableGen backends; =================. TableGen files have no real meaning without a backend. The default operation; when running ``*-tblgen`` is to print the information in a textual format, but; that's only useful ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst:12616,Modifiability,extend,extending,12616,"ternal; representation that can be generated into anything you want. Current usage of TableGen is to create huge include files with tables that you; can either include directly (if the output is in the language you're coding),; or be used in pre-processing via macros surrounding the include of the file. Direct output can be used if the backend already prints a table in C format; or if the output is just a list of strings (for error and warning messages).; Pre-processed output should be used if the same information needs to be used; in different contexts (like Instruction names), so your backend should print; a meta-information list that can be shaped into different compile-time formats. See :doc:`TableGen BackEnds <./BackEnds>` for a list of available; backends, and see the :doc:`TableGen Backend Developer's Guide <./BackGuide>`; for information on how to write and debug a new backend. Tools and Resources; ===================. In addition to this documentation, a list of tools and resources for TableGen; can be found in TableGen's; `README <https://github.com/llvm/llvm-project/blob/main/llvm/utils/TableGen/README.md>`_. TableGen Deficiencies; =====================. Despite being very generic, TableGen has some deficiencies that have been; pointed out numerous times. The common theme is that, while TableGen allows; you to build domain specific languages, the final languages that you create; lack the power of other DSLs, which in turn increase considerably the size; and complexity of TableGen files. At the same time, TableGen allows you to create virtually any meaning of; the basic concepts via custom-made backends, which can pervert the original; design and make it very hard for newcomers to understand the evil TableGen; file. There are some in favor of extending the semantics even more, but making sure; backends adhere to strict rules. Others are suggesting we should move to less,; more powerful DSLs designed with specific purposes, or even reusing existing; DSLs.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:10290,Availability,error,errors,10290," !ne; : !not !or !range !repr !setdagarg; : !setdagname !setdagop !shl !size !sra; : !srl !strconcat !sub !subst !substr; : !tail !tolower !toupper !xor. The ``!cond`` operator has a slightly different; syntax compared to other bang operators, so it is defined separately:. .. productionlist::; CondOperator: !cond. See `Appendix A: Bang Operators`_ for a description of each bang operator. Include files; -------------. TableGen has an include mechanism. The content of the included file; lexically replaces the ``include`` directive and is then parsed as if it was; originally in the main file. .. productionlist::; IncludeDirective: ""include"" `TokString`. Portions of the main file and included files can be conditionalized using; preprocessor directives. .. productionlist::; PreprocessorDirective: ""#define"" | ""#ifdef"" | ""#ifndef"". Types; =====. The TableGen language is statically typed, using a simple but complete type; system. Types are used to check for errors, to perform implicit conversions,; and to help interface designers constrain the allowed input. Every value is; required to have an associated type. TableGen supports a mixture of low-level types (e.g., ``bit``) and; high-level types (e.g., ``dag``). This flexibility allows you to describe a; wide range of records conveniently and compactly. .. productionlist::; Type: ""bit"" | ""int"" | ""string"" | ""dag""; :| ""bits"" ""<"" `TokInteger` "">""; :| ""list"" ""<"" `Type` "">""; :| `ClassID`; ClassID: `TokIdentifier`. ``bit``; A ``bit`` is a boolean value that can be 0 or 1. ``int``; The ``int`` type represents a simple 64-bit integer value, such as 5 or; -42. ``string``; The ``string`` type represents an ordered sequence of characters of arbitrary; length. ``bits<``\ *n*\ ``>``; The ``bits`` type is a fixed-sized integer of arbitrary length *n* that; is treated as separate bits. These bits can be accessed individually.; A field of this type is useful for representing an instruction operation; code, register number, or address mode/reg",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:18262,Availability,avail,available,18262,"nt of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined with the ``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Value` "":"" `Value`. The bang operators provide functions that are not available with the other; simple values. Except in the case of ``!cond``, a bang operator takes a list; of arguments enclosed in parentheses and performs some function on those; arguments, producing a value for that bang operator. The ``!cond`` operator; takes a list of pairs of arguments separated by colons. See `Appendix A:; Bang Operators`_ for a description of each bang operator. Suffixed values; ---------------. The :token:`SimpleValue` values described above can be specified with; certain suffixes. The purpose of a suffix is to obtain a subvalue of the; primary value. Here are the possible suffixes for some primary *value*. *value*\ ``{17}``; The final value is bit 17 of the integer *value* (note the braces). *value*\ ``{8...15}``; The final value is bits 8--15 of the integer *value*. The order of the; bits can be reversed by specifying ``{15...8}``. *value*\ ``[i]``; The final value is element `i` of the list *value* (note the brackets).; In other words, the brackets act as",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:20192,Availability,avail,available,20192," the list *value* (note the brackets).; In other words, the brackets act as a subscripting operator on the list.; This is the case only when a single element is specified. *value*\ ``[i,]``; The final value is a list that contains a single element `i` of the list.; In short, a list slice with a single element. *value*\ ``[4...7,17,2...3,4]``; The final value is a new list that is a slice of the list *value*.; The new list contains elements 4, 5, 6, 7, 17, 2, 3, and 4.; Elements may be included multiple times and in any order. This is the result; only when more than one element is specified. *value*\ ``[i,m...n,j,ls]``; Each element may be an expression (variables, bang operators).; The type of `m` and `n` should be `int`.; The type of `i`, `j`, and `ls` should be either `int` or `list<int>`. *value*\ ``.``\ *field*; The final value is the value of the specified *field* in the specified; record *value*. The paste operator; ------------------. The paste operator (``#``) is the only infix operator available in TableGen; expressions. It allows you to concatenate strings or lists, but has a few; unusual features. The paste operator can be used when specifying the record name in a; :token:`Def` or :token:`Defm` statement, in which case it must construct a; string. If an operand is an undefined name (:token:`TokIdentifier`) or the; name of a global :token:`Defvar` or :token:`Defset`, it is treated as a; verbatim string of characters. The value of a global name is not used. The paste operator can be used in all other value expressions, in which case; it can construct a string or a list. Rather oddly, but consistent with the; previous case, if the *right-hand-side* operand is an undefined name or a; global name, it is treated as a verbatim string of characters. The; left-hand-side operand is treated normally. Values can have a trailing paste operator, in which case the left-hand-side ; operand is concatenated to an empty string. `Appendix B: Paste Operator Examples`_ present",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:44338,Availability,error,error,44338,"sic_r <bits<4> opc> {; let Predicates = [HasSSE2] in {; def rr : Instruction<opc, ""rr"">;; def rm : Instruction<opc, ""rm"">;; }; let Predicates = [HasSSE3] in; def rx : Instruction<opc, ""rx"">;; }. multiclass basic_ss <bits<4> opc> {; let IsDouble = false in; defm SS : basic_r<opc>;. let IsDouble = true in; defm SD : basic_r<opc>;; }. defm ADD : basic_ss<0xf>;. ``defset`` --- create a definition set; --------------------------------------. The ``defset`` statement is used to collect a set of records into a global; list of records. .. productionlist::; Defset: ""defset"" `Type` `TokIdentifier` ""="" ""{"" `Statement`* ""}"". All records defined inside the braces via ``def`` and ``defm`` are defined; as usual, and they are also collected in a global list of the given name; (:token:`TokIdentifier`). The specified type must be ``list<``\ *class*\ ``>``, where *class* is some; record class. The ``defset`` statement establishes a scope for its; statements. It is an error to define a record in the scope of the; ``defset`` that is not of type *class*. The ``defset`` statement can be nested. The inner ``defset`` adds the; records to its own set, and all those records are also added to the outer; set. Anonymous records created inside initialization expressions using the; ``ClassID<...>`` syntax are not collected in the set. ``defvar`` --- define a variable; --------------------------------. A ``defvar`` statement defines a global variable. Its value can be used; throughout the statements that follow the definition. .. productionlist::; Defvar: ""defvar"" `TokIdentifier` ""="" `Value` "";"". The identifier on the left of the ``=`` is defined to be a global variable; whose value is given by the value expression on the right of the ``=``. The; type of the variable is automatically inferred. Once a variable has been defined, it cannot be set to another value. Variables defined in a top-level ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not avai",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:45372,Availability,avail,available,45372,"tements. It is an error to define a record in the scope of the; ``defset`` that is not of type *class*. The ``defset`` statement can be nested. The inner ``defset`` adds the; records to its own set, and all those records are also added to the outer; set. Anonymous records created inside initialization expressions using the; ``ClassID<...>`` syntax are not collected in the set. ``defvar`` --- define a variable; --------------------------------. A ``defvar`` statement defines a global variable. Its value can be used; throughout the statements that follow the definition. .. productionlist::; Defvar: ""defvar"" `TokIdentifier` ""="" `Value` "";"". The identifier on the left of the ``=`` is defined to be a global variable; whose value is given by the value expression on the right of the ``=``. The; type of the variable is automatically inferred. Once a variable has been defined, it cannot be set to another value. Variables defined in a top-level ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1);. Variables can also be defined with ``defvar`` in a record body. See; `Defvar in a Record Body`_ for more details. ``foreach`` --- iterate over a sequence of statements; -----------------------------------------------------. The ``foreach`` statement iterates over a series of statements, varying a; variable over a sequence of values. .. productionlist::; Foreach: ""foreach"" `ForeachIterator` ""in"" ""{"" `Statement`* ""}""; :| ""foreach"" `ForeachIterator` ""in"" `Statement`; ForeachIterator: `TokIdentifier` ""="" (""{"" `RangeList` ""}"" | `RangePiece` | `Value`). The body of the ``foreach`` is a series of statements in braces or a; single statement with no braces. The statements are re-evaluated once for; each value in the range list, range piece, or single value. On each; iteration, the :token:`TokIdentifier` variable is set to the value and can; be u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:47074,Availability,error,error,47074,"` is a series of statements in braces or a; single statement with no braces. The statements are re-evaluated once for; each value in the range list, range piece, or single value. On each; iteration, the :token:`TokIdentifier` variable is set to the value and can; be used in the statements. The statement list establishes an inner scope. Variables local to a; ``foreach`` go out of scope at the end of each loop iteration, so their; values do not carry over from one iteration to the next. Foreach loops may; be nested. .. Note that the productions involving RangeList and RangePiece have precedence; over the more generic value parsing based on the first token. .. code-block:: text. foreach i = [0, 1, 2, 3] in {; def R#i : Register<...>;; def F#i : Register<...>;; }. This loop defines records named ``R0``, ``R1``, ``R2``, and ``R3``, along; with ``F0``, ``F1``, ``F2``, and ``F3``. ``dump`` --- print messages to stderr; -------------------------------------. A ``dump`` statement prints the input string to standard error; output. It is intended for debugging purpose. * At top level, the message is printed immediately. * Within a record/class/multiclass, `dump` gets evaluated at each; instantiation point of the containing record. .. productionlist::; Dump: ""dump"" `string` "";"". For example, it can be used in combination with `!repr` to investigate; the values passed to a multiclass:. .. code-block:: text. multiclass MC<dag s> {; dump ""s = "" # !repr(s);; }. ``if`` --- select statements based on a test; --------------------------------------------. The ``if`` statement allows one of two statement groups to be selected based; on the value of an expression. .. productionlist::; If: ""if"" `Value` ""then"" `IfBody`; :| ""if"" `Value` ""then"" `IfBody` ""else"" `IfBody`; IfBody: ""{"" `Statement`* ""}"" | `Statement`. The value expression is evaluated. If it evaluates to true (in the same; sense used by the bang operators), then the statements following the; ``then`` reserved word are processed. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49030,Availability,error,error,49030,"ts following the; ``then`` reserved word are processed. Otherwise, if there is an ``else``; reserved word, the statements following the ``else`` are processed. If the; value is false and there is no ``else`` arm, no statements are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49238,Availability,error,error,49238,"ts are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""pers",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49335,Availability,error,error,49335," grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:53127,Availability,avail,available,53127,"--------------. In addition to defining global variables, the ``defvar`` statement can; be used inside the :token:`Body` of a class or record definition to define; local variables. Template arguments of ``class`` or ``multiclass`` can be; used in the value expression. The scope of the variable extends from the; ``defvar`` statement to the end of the body. It cannot be set to a different; value within its scope. The ``defvar`` statement can also be used in the statement; list of a ``foreach``, which establishes a scope. A variable named ``V`` in an inner scope shadows (hides) any variables ``V``; in outer scopes. In particular, there are several cases:. * ``V`` in a record body shadows a global ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through the same steps. 1. Build the record name (:token:`NameValue`) and create an empty record. 2. Parse the parent classes in the :token:`ParentClassList` from left to; right, visiting each parent class's ancestor classes from top to bottom. a. Add the fields from the parent class to the record.; b. Substitute the template arguments into those fields.; c. Add the parent class to the record's list of inherited classes. 3. Apply any top-level ``let`` bindings to the record. Recall that top-level; bindings only apply to inherited fields. 4. Parse the body of the record. * Add any fields to the record.; * Modify the values of fields according to local ``let`` statements.; * Define any ``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:60804,Availability,error,error,60804,"is allows; casting a record to a class. If a record is cast to ``string``, the; record's name is produced. If *a* is a string, then it is treated as a record name and looked up in; the list of all defined records. The resulting record is expected to be of; the specified *type*. For example, if ``!cast<``\ *type*\ ``>(``\ *name*\ ``)``; appears in a multiclass definition, or in a; class instantiated inside a multiclass definition, and the *name* does not; reference any template arguments of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (or later, if the defm occurs in another; multiclass and template arguments of the inner multiclass that are; referenced by *name* are substituted by values that themselves contain; references to template arguments of the outer multiclass). If the type of *a* does not match *type*, TableGen raises an error. ``!con(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator concatenates the DAG nodes *a*, *b*, etc. Their operations; must equal. ``!con((op a1:$name1, a2:$name2), (op b1:$name3))``. results in the DAG node ``(op a1:$name1, a2:$name2, b1:$name3)``. ``!cond(``\ *cond1* ``:`` *val1*\ ``,`` *cond2* ``:`` *val2*\ ``, ...,`` *condn* ``:`` *valn*\ ``)``; This operator tests *cond1* and returns *val1* if the result is true.; If false, the operator tests *cond2* and returns *val2* if the result is; true. And so forth. An error is reported if no conditions are true. This example produces the sign word for an integer::. !cond(!lt(x, 0) : ""negative"", !eq(x, 0) : ""zero"", true : ""positive""). ``!dag(``\ *op*\ ``,`` *arguments*\ ``,`` *names*\ ``)``; This operator creates a DAG node with the given operator and; arguments. The *arguments* and *names* arguments must be lists; of equal length or uninitialized (``?``). The *names* argument; must be of type ``list<string>``. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:61325,Availability,error,error,61325,"rlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (or later, if the defm occurs in another; multiclass and template arguments of the inner multiclass that are; referenced by *name* are substituted by values that themselves contain; references to template arguments of the outer multiclass). If the type of *a* does not match *type*, TableGen raises an error. ``!con(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator concatenates the DAG nodes *a*, *b*, etc. Their operations; must equal. ``!con((op a1:$name1, a2:$name2), (op b1:$name3))``. results in the DAG node ``(op a1:$name1, a2:$name2, b1:$name3)``. ``!cond(``\ *cond1* ``:`` *val1*\ ``,`` *cond2* ``:`` *val2*\ ``, ...,`` *condn* ``:`` *valn*\ ``)``; This operator tests *cond1* and returns *val1* if the result is true.; If false, the operator tests *cond2* and returns *val2* if the result is; true. And so forth. An error is reported if no conditions are true. This example produces the sign word for an integer::. !cond(!lt(x, 0) : ""negative"", !eq(x, 0) : ""zero"", true : ""positive""). ``!dag(``\ *op*\ ``,`` *arguments*\ ``,`` *names*\ ``)``; This operator creates a DAG node with the given operator and; arguments. The *arguments* and *names* arguments must be lists; of equal length or uninitialized (``?``). The *names* argument; must be of type ``list<string>``. Due to limitations of the type system, *arguments* must be a list of items; of a common type. In practice, this means that they should either have the; same type or be records with a common parent class. Mixing ``dag`` and; non-``dag`` items is not possible. However, ``?`` can be used. Example: ``!dag(op, [a1, a2, ?], [""name1"", ""name2"", ""name3""])`` results in; ``(op a1-value:$name1, a2-value:$name2, ?:$name3)``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of *a* by *b*, and produces the quotient.; Division by 0 produces an error. Divi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:62334,Availability,error,error,62334,"n error is reported if no conditions are true. This example produces the sign word for an integer::. !cond(!lt(x, 0) : ""negative"", !eq(x, 0) : ""zero"", true : ""positive""). ``!dag(``\ *op*\ ``,`` *arguments*\ ``,`` *names*\ ``)``; This operator creates a DAG node with the given operator and; arguments. The *arguments* and *names* arguments must be lists; of equal length or uninitialized (``?``). The *names* argument; must be of type ``list<string>``. Due to limitations of the type system, *arguments* must be a list of items; of a common type. In practice, this means that they should either have the; same type or be records with a common parent class. Mixing ``dag`` and; non-``dag`` items is not possible. However, ``?`` can be used. Example: ``!dag(op, [a1, a2, ?], [""name1"", ""name2"", ""name3""])`` results in; ``(op a1-value:$name1, a2-value:$name2, ?:$name3)``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of *a* by *b*, and produces the quotient.; Division by 0 produces an error. Division of INT64_MIN by -1 produces an error. ``!empty(``\ *a*\ ``)``; This operator produces 1 if the string, list, or DAG *a* is empty; 0 otherwise.; A dag is empty if it has no arguments; the operator does not count. ``!eq(`` *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``, or; record values. Use ``!cast<string>`` to compare other types of objects. ``!exists<``\ *type*\ ``>(``\ *name*\ ``)``; This operator produces 1 if a record of the given *type* whose name is *name*; exists; 0 otherwise. *name* should be of type *string*. ``!filter(``\ *var*\ ``,`` *list*\ ``,`` *predicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably; refers to *var*. The predicate must; produce a boolean value (``bit``, `",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:62381,Availability,error,error,62381," true. This example produces the sign word for an integer::. !cond(!lt(x, 0) : ""negative"", !eq(x, 0) : ""zero"", true : ""positive""). ``!dag(``\ *op*\ ``,`` *arguments*\ ``,`` *names*\ ``)``; This operator creates a DAG node with the given operator and; arguments. The *arguments* and *names* arguments must be lists; of equal length or uninitialized (``?``). The *names* argument; must be of type ``list<string>``. Due to limitations of the type system, *arguments* must be a list of items; of a common type. In practice, this means that they should either have the; same type or be records with a common parent class. Mixing ``dag`` and; non-``dag`` items is not possible. However, ``?`` can be used. Example: ``!dag(op, [a1, a2, ?], [""name1"", ""name2"", ""name3""])`` results in; ``(op a1-value:$name1, a2-value:$name2, ?:$name3)``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of *a* by *b*, and produces the quotient.; Division by 0 produces an error. Division of INT64_MIN by -1 produces an error. ``!empty(``\ *a*\ ``)``; This operator produces 1 if the string, list, or DAG *a* is empty; 0 otherwise.; A dag is empty if it has no arguments; the operator does not count. ``!eq(`` *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``, or; record values. Use ``!cast<string>`` to compare other types of objects. ``!exists<``\ *type*\ ``>(``\ *name*\ ``)``; This operator produces 1 if a record of the given *type* whose name is *name*; exists; 0 otherwise. *name* should be of type *string*. ``!filter(``\ *var*\ ``,`` *list*\ ``,`` *predicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably; refers to *var*. The predicate must; produce a boolean value (``bit``, ``bits``, or ``int``). The value is; inter",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:68680,Availability,error,error,68680,"`)``; This operator produces 1 if the type of *a* is a subtype of the given *type*; 0; otherwise. ``!le(``\ *a*\ ``,`` *b*\ ``)``; This operator produces 1 if *a* is less than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!listconcat(``\ *list1*\ ``,`` *list2*\ ``, ...)``; This operator concatenates the list arguments *list1*, *list2*, etc., and; produces the resulting list. The lists must have the same element type. ``!listremove(``\ *list1*\ ``,`` *list2*\ ``)``; This operator returns a copy of *list1* removing all elements that also occur in; *list2*. The lists must have the same element type. ``!listsplat(``\ *value*\ ``,`` *count*\ ``)``; This operator produces a list of length *count* whose elements are all; equal to the *value*. For example, ``!listsplat(42, 3)`` results in; ``[42, 42, 42]``. ``!logtwo(``\ *a*\ ``)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 or 1. ``!range([``\ *start*\ ``,]`` *end*\ ``[, ``\ *step*\ ``])``; This operato",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:3826,Energy Efficiency,reduce,reduce,3826,"irely up to the backends and the programs that incorporate; the output of those backends. .. note::. The term ""parent class"" can refer to a class that is a parent of another; class, and also to a class from which a concrete record inherits. This; nonstandard use of the term arises because TableGen treats classes and; concrete records similarly. A backend processes some subset of the concrete records built by the; TableGen parser and emits the output files. These files are usually C++; ``.inc`` files that are included by the programs that require the data in; those records. However, a backend can produce any type of output files. For; example, it could produce a data file containing messages tagged with; identifiers and substitution parameters. In a complex use case such as the; LLVM code generator, there can be many concrete records and some of them can; have an unexpectedly large number of fields, resulting in large output files. In order to reduce the complexity of TableGen files, classes are used to; abstract out groups of record fields. For example, a few classes may; abstract the concept of a machine register file, while other classes may; abstract the instruction formats, and still others may abstract the; individual instructions. TableGen allows an arbitrary hierarchy of classes,; so that the abstract classes for two concepts can share a third superclass that; abstracts common ""sub-concepts"" from the two original concepts. In order to make classes more useful, a concrete record (or another class); can request a class as a parent class and pass *template arguments* to it.; These template arguments can be used in the fields of the parent class to; initialize them in a custom manner. That is, record or class ``A`` can; request parent class ``S`` with one set of template arguments, while record or class; ``B`` can request ``S`` with a different set of arguments. Without template; arguments, many more classes would be required, one for each combination of; the tem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:54354,Energy Efficiency,power,power,54354,"eps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through the same steps. 1. Build the record name (:token:`NameValue`) and create an empty record. 2. Parse the parent classes in the :token:`ParentClassList` from left to; right, visiting each parent class's ancestor classes from top to bottom. a. Add the fields from the parent class to the record.; b. Substitute the template arguments into those fields.; c. Add the parent class to the record's list of inherited classes. 3. Apply any top-level ``let`` bindings to the record. Recall that top-level; bindings only apply to inherited fields. 4. Parse the body of the record. * Add any fields to the record.; * Modify the values of fields according to local ``let`` statements.; * Define any ``defvar`` variables. 5. Make a pass over all the fields to resolve any inter-field references. 6. Add the record to the final record list. Because references between fields are resolved (step 5) after ``let`` bindings are; applied (step 3), the ``let`` statement has unusual power. For example:. .. code-block:: text. class C <int x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }; def rec2 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }. ``Yplus1`` is 11 because the ``let Y`` is performed before the ``!add(Y,; 1)`` is resolved. Use this power wisely. Using Classes as Subroutines; ============================. As described in `Simple values`_, a class can be invoked in an expression; and passed template arguments. This causes TableGen to create a new anonymous; record inheriting from that class. As usual, the record receives all the; fields defined in the class. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:54956,Energy Efficiency,power,power,54956,"ord. * Add any fields to the record.; * Modify the values of fields according to local ``let`` statements.; * Define any ``defvar`` variables. 5. Make a pass over all the fields to resolve any inter-field references. 6. Add the record to the final record list. Because references between fields are resolved (step 5) after ``let`` bindings are; applied (step 3), the ``let`` statement has unusual power. For example:. .. code-block:: text. class C <int x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }; def rec2 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }. ``Yplus1`` is 11 because the ``let Y`` is performed before the ``!add(Y,; 1)`` is resolved. Use this power wisely. Using Classes as Subroutines; ============================. As described in `Simple values`_, a class can be invoked in an expression; and passed template arguments. This causes TableGen to create a new anonymous; record inheriting from that class. As usual, the record receives all the; fields defined in the class. This feature can be employed as a simple subroutine facility. The class can; use the template arguments to define various variables and fields, which end; up in the anonymous record. Those fields can then be retrieved in the; expression invoking the class as follows. Assume that the field ``ret``; contains the final value of the subroutine. .. code-block:: text. int Result = ... CalcValue<arg>.ret ...;. The ``CalcValue`` class is invoked with the template argument ``arg``. It; calculates a value for the ``ret`` field, which is then retrieved at the; ""point of call"" in the initialization for the Result field. The anonymous; record created in this example serves no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:3560,Integrability,message,messages,3560,"er or generated by TableGen. Associated with that name; is a list of *fields* with values and an optional list of *parent classes*; (sometimes called base or super classes). The fields are the primary data that; backends will process. Note that TableGen assigns no meanings to fields; the; meanings are entirely up to the backends and the programs that incorporate; the output of those backends. .. note::. The term ""parent class"" can refer to a class that is a parent of another; class, and also to a class from which a concrete record inherits. This; nonstandard use of the term arises because TableGen treats classes and; concrete records similarly. A backend processes some subset of the concrete records built by the; TableGen parser and emits the output files. These files are usually C++; ``.inc`` files that are included by the programs that require the data in; those records. However, a backend can produce any type of output files. For; example, it could produce a data file containing messages tagged with; identifiers and substitution parameters. In a complex use case such as the; LLVM code generator, there can be many concrete records and some of them can; have an unexpectedly large number of fields, resulting in large output files. In order to reduce the complexity of TableGen files, classes are used to; abstract out groups of record fields. For example, a few classes may; abstract the concept of a machine register file, while other classes may; abstract the instruction formats, and still others may abstract the; individual instructions. TableGen allows an arbitrary hierarchy of classes,; so that the abstract classes for two concepts can share a third superclass that; abstracts common ""sub-concepts"" from the two original concepts. In order to make classes more useful, a concrete record (or another class); can request a class as a parent class and pass *template arguments* to it.; These template arguments can be used in the fields of the parent class to; initialize the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:10344,Integrability,interface,interface,10344," !ne; : !not !or !range !repr !setdagarg; : !setdagname !setdagop !shl !size !sra; : !srl !strconcat !sub !subst !substr; : !tail !tolower !toupper !xor. The ``!cond`` operator has a slightly different; syntax compared to other bang operators, so it is defined separately:. .. productionlist::; CondOperator: !cond. See `Appendix A: Bang Operators`_ for a description of each bang operator. Include files; -------------. TableGen has an include mechanism. The content of the included file; lexically replaces the ``include`` directive and is then parsed as if it was; originally in the main file. .. productionlist::; IncludeDirective: ""include"" `TokString`. Portions of the main file and included files can be conditionalized using; preprocessor directives. .. productionlist::; PreprocessorDirective: ""#define"" | ""#ifdef"" | ""#ifndef"". Types; =====. The TableGen language is statically typed, using a simple but complete type; system. Types are used to check for errors, to perform implicit conversions,; and to help interface designers constrain the allowed input. Every value is; required to have an associated type. TableGen supports a mixture of low-level types (e.g., ``bit``) and; high-level types (e.g., ``dag``). This flexibility allows you to describe a; wide range of records conveniently and compactly. .. productionlist::; Type: ""bit"" | ""int"" | ""string"" | ""dag""; :| ""bits"" ""<"" `TokInteger` "">""; :| ""list"" ""<"" `Type` "">""; :| `ClassID`; ClassID: `TokIdentifier`. ``bit``; A ``bit`` is a boolean value that can be 0 or 1. ``int``; The ``int`` type represents a simple 64-bit integer value, such as 5 or; -42. ``string``; The ``string`` type represents an ordered sequence of characters of arbitrary; length. ``bits<``\ *n*\ ``>``; The ``bits`` type is a fixed-sized integer of arbitrary length *n* that; is treated as separate bits. These bits can be accessed individually.; A field of this type is useful for representing an instruction operation; code, register number, or address mode/reg",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:32060,Integrability,interface,interface,32060,"example of classes with template arguments. First, we; define a class similar to the ``FPFormat`` class above. It takes a template; argument and uses it to initialize a field named ``Value``. Then we define; four records that inherit the ``Value`` field with its four different; integer values. .. code-block:: text. class ModRefVal <bits<2> val> {; bits<2> Value = val;; }. def None : ModRefVal<0>;; def Mod : ModRefVal<1>;; def Ref : ModRefVal<2>;; def ModRef : ModRefVal<3>;. This is somewhat contrived, but let's say we would like to examine the two; bits of the ``Value`` field independently. We can define a class that; accepts a ``ModRefVal`` record as a template argument and splits up its; value into two fields, one bit each. Then we can define records that inherit from; ``ModRefBits`` and so acquire two fields from it, one for each bit in the; ``ModRefVal`` record passed as the template argument. .. code-block:: text. class ModRefBits <ModRefVal mrv> {; // Break the value up into its bits, which can provide a nice; // interface to the ModRefVal values.; bit isMod = mrv.Value{0};; bit isRef = mrv.Value{1};; }. // Example uses.; def foo : ModRefBits<Mod>;; def bar : ModRefBits<Ref>;; def snork : ModRefBits<ModRef>;. This illustrates how one class can be defined to reorganize the; fields in another class, thus hiding the internal representation of that; other class. Running ``llvm-tblgen`` on the example prints the following definitions:. .. code-block:: text. def bar { // Value; bit isMod = 0;; bit isRef = 1;; }; def foo { // Value; bit isMod = 1;; bit isRef = 0;; }; def snork { // Value; bit isMod = 1;; bit isRef = 1;; }. ``let`` --- override fields in classes or records; -------------------------------------------------. A ``let`` statement collects a set of field values (sometimes called; *bindings*) and applies them to all the classes and records defined by; statements within the scope of the ``let``. .. productionlist::; Let: ""let"" `LetList` ""in"" ""{"" `Statement`*",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:46958,Integrability,message,messages,46958," (""{"" `RangeList` ""}"" | `RangePiece` | `Value`). The body of the ``foreach`` is a series of statements in braces or a; single statement with no braces. The statements are re-evaluated once for; each value in the range list, range piece, or single value. On each; iteration, the :token:`TokIdentifier` variable is set to the value and can; be used in the statements. The statement list establishes an inner scope. Variables local to a; ``foreach`` go out of scope at the end of each loop iteration, so their; values do not carry over from one iteration to the next. Foreach loops may; be nested. .. Note that the productions involving RangeList and RangePiece have precedence; over the more generic value parsing based on the first token. .. code-block:: text. foreach i = [0, 1, 2, 3] in {; def R#i : Register<...>;; def F#i : Register<...>;; }. This loop defines records named ``R0``, ``R1``, ``R2``, and ``R3``, along; with ``F0``, ``F1``, ``F2``, and ``F3``. ``dump`` --- print messages to stderr; -------------------------------------. A ``dump`` statement prints the input string to standard error; output. It is intended for debugging purpose. * At top level, the message is printed immediately. * Within a record/class/multiclass, `dump` gets evaluated at each; instantiation point of the containing record. .. productionlist::; Dump: ""dump"" `string` "";"". For example, it can be used in combination with `!repr` to investigate; the values passed to a multiclass:. .. code-block:: text. multiclass MC<dag s> {; dump ""s = "" # !repr(s);; }. ``if`` --- select statements based on a test; --------------------------------------------. The ``if`` statement allows one of two statement groups to be selected based; on the value of an expression. .. productionlist::; If: ""if"" `Value` ""then"" `IfBody`; :| ""if"" `Value` ""then"" `IfBody` ""else"" `IfBody`; IfBody: ""{"" `Statement`* ""}"" | `Statement`. The value expression is evaluated. If it evaluates to true (in the same; sense used by the bang operators)",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:47147,Integrability,message,message,47147,"valuated once for; each value in the range list, range piece, or single value. On each; iteration, the :token:`TokIdentifier` variable is set to the value and can; be used in the statements. The statement list establishes an inner scope. Variables local to a; ``foreach`` go out of scope at the end of each loop iteration, so their; values do not carry over from one iteration to the next. Foreach loops may; be nested. .. Note that the productions involving RangeList and RangePiece have precedence; over the more generic value parsing based on the first token. .. code-block:: text. foreach i = [0, 1, 2, 3] in {; def R#i : Register<...>;; def F#i : Register<...>;; }. This loop defines records named ``R0``, ``R1``, ``R2``, and ``R3``, along; with ``F0``, ``F1``, ``F2``, and ``F3``. ``dump`` --- print messages to stderr; -------------------------------------. A ``dump`` statement prints the input string to standard error; output. It is intended for debugging purpose. * At top level, the message is printed immediately. * Within a record/class/multiclass, `dump` gets evaluated at each; instantiation point of the containing record. .. productionlist::; Dump: ""dump"" `string` "";"". For example, it can be used in combination with `!repr` to investigate; the values passed to a multiclass:. .. code-block:: text. multiclass MC<dag s> {; dump ""s = "" # !repr(s);; }. ``if`` --- select statements based on a test; --------------------------------------------. The ``if`` statement allows one of two statement groups to be selected based; on the value of an expression. .. productionlist::; If: ""if"" `Value` ""then"" `IfBody`; :| ""if"" `Value` ""then"" `IfBody` ""else"" `IfBody`; IfBody: ""{"" `Statement`* ""}"" | `Statement`. The value expression is evaluated. If it evaluates to true (in the same; sense used by the bang operators), then the statements following the; ``then`` reserved word are processed. Otherwise, if there is an ``else``; reserved word, the statements following the ``else`` are process",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49036,Integrability,message,message,49036,"ts following the; ``then`` reserved word are processed. Otherwise, if there is an ``else``; reserved word, the statements following the ``else`` are processed. If the; value is false and there is no ``else`` arm, no statements are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49113,Integrability,message,message,49113,"ved word, the statements following the ``else`` are processed. If the; value is false and there is no ``else`` arm, no statements are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49244,Integrability,message,message,49244,"ts are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""pers",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49259,Integrability,message,message,49259," grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49341,Integrability,message,message,49341," grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49408,Integrability,depend,depends,49408," the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:3100,Modifiability,inherit,inherits,3100,"-----. TableGen source files contain two primary items: *abstract records* and; *concrete records*. In this and other TableGen documents, abstract records; are called *classes.* (These classes are different from C++ classes and do; not map onto them.) In addition, concrete records are usually just called; records, although sometimes the term *record* refers to both classes and; concrete records. The distinction should be clear in context. Classes and concrete records have a unique *name*, either chosen by; the programmer or generated by TableGen. Associated with that name; is a list of *fields* with values and an optional list of *parent classes*; (sometimes called base or super classes). The fields are the primary data that; backends will process. Note that TableGen assigns no meanings to fields; the; meanings are entirely up to the backends and the programs that incorporate; the output of those backends. .. note::. The term ""parent class"" can refer to a class that is a parent of another; class, and also to a class from which a concrete record inherits. This; nonstandard use of the term arises because TableGen treats classes and; concrete records similarly. A backend processes some subset of the concrete records built by the; TableGen parser and emits the output files. These files are usually C++; ``.inc`` files that are included by the programs that require the data in; those records. However, a backend can produce any type of output files. For; example, it could produce a data file containing messages tagged with; identifiers and substitution parameters. In a complex use case such as the; LLVM code generator, there can be many concrete records and some of them can; have an unexpectedly large number of fields, resulting in large output files. In order to reduce the complexity of TableGen files, classes are used to; abstract out groups of record fields. For example, a few classes may; abstract the concept of a machine register file, while other classes may; abstrac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:5132,Modifiability,inherit,inherited,5132,"he; individual instructions. TableGen allows an arbitrary hierarchy of classes,; so that the abstract classes for two concepts can share a third superclass that; abstracts common ""sub-concepts"" from the two original concepts. In order to make classes more useful, a concrete record (or another class); can request a class as a parent class and pass *template arguments* to it.; These template arguments can be used in the fields of the parent class to; initialize them in a custom manner. That is, record or class ``A`` can; request parent class ``S`` with one set of template arguments, while record or class; ``B`` can request ``S`` with a different set of arguments. Without template; arguments, many more classes would be required, one for each combination of; the template arguments. Both classes and concrete records can include fields that are uninitialized.; The uninitialized ""value"" is represented by a question mark (``?``). Classes; often have uninitialized fields that are expected to be filled in when those; classes are inherited by concrete records. Even so, some fields of concrete; records may remain uninitialized. TableGen provides *multiclasses* to collect a group of record definitions in; one place. A multiclass is a sort of macro that can be ""invoked"" to define; multiple concrete records all at once. A multiclass can inherit from other; multiclasses, which means that the multiclass inherits all the definitions; from its parent multiclasses. `Appendix C: Sample Record`_ illustrates a complex record in the Intel X86; target and the simple way in which it is defined. Source Files; ============. TableGen source files are plain ASCII text files. The files can contain; statements, comments, and blank lines (see `Lexical Analysis`_). The standard file; extension for TableGen files is ``.td``. TableGen files can grow quite large, so there is an include mechanism that; allows one file to include the content of another file (see `Include; Files`_). This allows large file",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:5441,Modifiability,inherit,inherit,5441,"ments can be used in the fields of the parent class to; initialize them in a custom manner. That is, record or class ``A`` can; request parent class ``S`` with one set of template arguments, while record or class; ``B`` can request ``S`` with a different set of arguments. Without template; arguments, many more classes would be required, one for each combination of; the template arguments. Both classes and concrete records can include fields that are uninitialized.; The uninitialized ""value"" is represented by a question mark (``?``). Classes; often have uninitialized fields that are expected to be filled in when those; classes are inherited by concrete records. Even so, some fields of concrete; records may remain uninitialized. TableGen provides *multiclasses* to collect a group of record definitions in; one place. A multiclass is a sort of macro that can be ""invoked"" to define; multiple concrete records all at once. A multiclass can inherit from other; multiclasses, which means that the multiclass inherits all the definitions; from its parent multiclasses. `Appendix C: Sample Record`_ illustrates a complex record in the Intel X86; target and the simple way in which it is defined. Source Files; ============. TableGen source files are plain ASCII text files. The files can contain; statements, comments, and blank lines (see `Lexical Analysis`_). The standard file; extension for TableGen files is ``.td``. TableGen files can grow quite large, so there is an include mechanism that; allows one file to include the content of another file (see `Include; Files`_). This allows large files to be broken up into smaller ones, and; also provides a simple library mechanism where multiple source files can; include the same library file. TableGen supports a simple preprocessor that can be used to conditionalize; portions of ``.td`` files. See `Preprocessing Facilities`_ for more; information. Lexical Analysis; ================. The lexical and syntax notation used here is intended to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:5507,Modifiability,inherit,inherits,5507,"ments can be used in the fields of the parent class to; initialize them in a custom manner. That is, record or class ``A`` can; request parent class ``S`` with one set of template arguments, while record or class; ``B`` can request ``S`` with a different set of arguments. Without template; arguments, many more classes would be required, one for each combination of; the template arguments. Both classes and concrete records can include fields that are uninitialized.; The uninitialized ""value"" is represented by a question mark (``?``). Classes; often have uninitialized fields that are expected to be filled in when those; classes are inherited by concrete records. Even so, some fields of concrete; records may remain uninitialized. TableGen provides *multiclasses* to collect a group of record definitions in; one place. A multiclass is a sort of macro that can be ""invoked"" to define; multiple concrete records all at once. A multiclass can inherit from other; multiclasses, which means that the multiclass inherits all the definitions; from its parent multiclasses. `Appendix C: Sample Record`_ illustrates a complex record in the Intel X86; target and the simple way in which it is defined. Source Files; ============. TableGen source files are plain ASCII text files. The files can contain; statements, comments, and blank lines (see `Lexical Analysis`_). The standard file; extension for TableGen files is ``.td``. TableGen files can grow quite large, so there is an include mechanism that; allows one file to include the content of another file (see `Include; Files`_). This allows large files to be broken up into smaller ones, and; also provides a simple library mechanism where multiple source files can; include the same library file. TableGen supports a simple preprocessor that can be used to conditionalize; portions of ``.td`` files. See `Preprocessing Facilities`_ for more; information. Lexical Analysis; ================. The lexical and syntax notation used here is intended to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:17164,Modifiability,inherit,inherited,17164,". .. productionlist::; SimpleValue7: `TokIdentifier`. The resulting value is the value of the entity named by the identifier. The; possible identifiers are described here, but the descriptions will make more; sense after reading the remainder of this guide. .. The code for this is exceptionally abstruse. These examples are a; best-effort attempt. * A template argument of a ``class``, such as the use of ``Bar`` in::. class Foo <int Bar> {; int Baz = Bar;; }. * The implicit template argument ``NAME`` in a ``class`` or ``multiclass``; definition (see `NAME`_). * A field local to a ``class``, such as the use of ``Bar`` in::. class Foo {; int Bar = 5;; int Baz = Bar;; }. * The name of a record definition, such as the use of ``Bar`` in the; definition of ``Foo``::. def Bar : SomeClass {; int X = 5;; }. def Foo {; SomeClass Baz = Bar;; }. * A field local to a record definition, such as the use of ``Bar`` in::. def Foo {; int Bar = 5;; int Baz = Bar;; }. Fields inherited from the record's parent classes can be accessed the same way. * A template argument of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined with the ``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Valu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:17371,Modifiability,variab,variable,17371,"more; sense after reading the remainder of this guide. .. The code for this is exceptionally abstruse. These examples are a; best-effort attempt. * A template argument of a ``class``, such as the use of ``Bar`` in::. class Foo <int Bar> {; int Baz = Bar;; }. * The implicit template argument ``NAME`` in a ``class`` or ``multiclass``; definition (see `NAME`_). * A field local to a ``class``, such as the use of ``Bar`` in::. class Foo {; int Bar = 5;; int Baz = Bar;; }. * The name of a record definition, such as the use of ``Bar`` in the; definition of ``Foo``::. def Bar : SomeClass {; int X = 5;; }. def Foo {; SomeClass Baz = Bar;; }. * A field local to a record definition, such as the use of ``Bar`` in::. def Foo {; int Bar = 5;; int Baz = Bar;; }. Fields inherited from the record's parent classes can be accessed the same way. * A template argument of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined with the ``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Value` "":"" `Value`. The bang operators provide functions that are not available with the other; simple values. Except in the case of ``!cond``, a bang operator takes a list; of arguments enclosed in parenthes",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:17450,Modifiability,variab,variable,17450,"this is exceptionally abstruse. These examples are a; best-effort attempt. * A template argument of a ``class``, such as the use of ``Bar`` in::. class Foo <int Bar> {; int Baz = Bar;; }. * The implicit template argument ``NAME`` in a ``class`` or ``multiclass``; definition (see `NAME`_). * A field local to a ``class``, such as the use of ``Bar`` in::. class Foo {; int Bar = 5;; int Baz = Bar;; }. * The name of a record definition, such as the use of ``Bar`` in the; definition of ``Foo``::. def Bar : SomeClass {; int X = 5;; }. def Foo {; SomeClass Baz = Bar;; }. * A field local to a record definition, such as the use of ``Bar`` in::. def Foo {; int Bar = 5;; int Baz = Bar;; }. Fields inherited from the record's parent classes can be accessed the same way. * A template argument of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined with the ``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Value` "":"" `Value`. The bang operators provide functions that are not available with the other; simple values. Except in the case of ``!cond``, a bang operator takes a list; of arguments enclosed in parentheses and performs some function on those; arguments, producing a value f",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:17707,Modifiability,inherit,inheriting,17707,"` or ``multiclass``; definition (see `NAME`_). * A field local to a ``class``, such as the use of ``Bar`` in::. class Foo {; int Bar = 5;; int Baz = Bar;; }. * The name of a record definition, such as the use of ``Bar`` in the; definition of ``Foo``::. def Bar : SomeClass {; int X = 5;; }. def Foo {; SomeClass Baz = Bar;; }. * A field local to a record definition, such as the use of ``Bar`` in::. def Foo {; int Bar = 5;; int Baz = Bar;; }. Fields inherited from the record's parent classes can be accessed the same way. * A template argument of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined with the ``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Value` "":"" `Value`. The bang operators provide functions that are not available with the other; simple values. Except in the case of ``!cond``, a bang operator takes a list; of arguments enclosed in parentheses and performs some function on those; arguments, producing a value for that bang operator. The ``!cond`` operator; takes a list of pairs of arguments separated by colons. See `Appendix A:; Bang Operators`_ for a description of each bang operator. Suffixed values; ---------------. The :token:`SimpleValue` value",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:19844,Modifiability,variab,variables,19844,"y value. Here are the possible suffixes for some primary *value*. *value*\ ``{17}``; The final value is bit 17 of the integer *value* (note the braces). *value*\ ``{8...15}``; The final value is bits 8--15 of the integer *value*. The order of the; bits can be reversed by specifying ``{15...8}``. *value*\ ``[i]``; The final value is element `i` of the list *value* (note the brackets).; In other words, the brackets act as a subscripting operator on the list.; This is the case only when a single element is specified. *value*\ ``[i,]``; The final value is a list that contains a single element `i` of the list.; In short, a list slice with a single element. *value*\ ``[4...7,17,2...3,4]``; The final value is a new list that is a slice of the list *value*.; The new list contains elements 4, 5, 6, 7, 17, 2, 3, and 4.; Elements may be included multiple times and in any order. This is the result; only when more than one element is specified. *value*\ ``[i,m...n,j,ls]``; Each element may be an expression (variables, bang operators).; The type of `m` and `n` should be `int`.; The type of `i`, `j`, and `ls` should be either `int` or `list<int>`. *value*\ ``.``\ *field*; The final value is the value of the specified *field* in the specified; record *value*. The paste operator; ------------------. The paste operator (``#``) is the only infix operator available in TableGen; expressions. It allows you to concatenate strings or lists, but has a few; unusual features. The paste operator can be used when specifying the record name in a; :token:`Def` or :token:`Defm` statement, in which case it must construct a; string. If an operand is an undefined name (:token:`TokIdentifier`) or the; name of a global :token:`Defvar` or :token:`Defset`, it is treated as a; verbatim string of characters. The value of a global name is not used. The paste operator can be used in all other value expressions, in which case; it can construct a string or a list. Rather oddly, but consistent with the; previous",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:21819,Modifiability,inherit,inherit,21819,"ng or a list. Rather oddly, but consistent with the; previous case, if the *right-hand-side* operand is an undefined name or a; global name, it is treated as a verbatim string of characters. The; left-hand-side operand is treated normally. Values can have a trailing paste operator, in which case the left-hand-side ; operand is concatenated to an empty string. `Appendix B: Paste Operator Examples`_ presents examples of the behavior of; the paste operator. Statements; ==========. The following statements may appear at the top level of TableGen source; files. .. productionlist::; TableGenFile: (`Statement` | `IncludeDirective`; :| `PreprocessorDirective`)*; Statement: `Assert` | `Class` | `Def` | `Defm` | `Defset` | `Defvar`; :| `Dump` | `Foreach` | `If` | `Let` | `MultiClass`. The following sections describe each of these top-level statements. ``class`` --- define an abstract record class; ---------------------------------------------. A ``class`` statement defines an abstract record class from which other; classes and records can inherit. .. productionlist::; Class: ""class"" `ClassID` [`TemplateArgList`] `RecordBody`; TemplateArgList: ""<"" `TemplateArgDecl` ("","" `TemplateArgDecl`)* "">""; TemplateArgDecl: `Type` `TokIdentifier` [""="" `Value`]. A class can be parameterized by a list of ""template arguments,"" whose values; can be used in the class's record body. These template arguments are; specified each time the class is inherited by another class or record. If a template argument is not assigned a default value with ``=``, it is; uninitialized (has the ""value"" ``?``) and must be specified in the template; argument list when the class is inherited (required argument). If an; argument is assigned a default value, then it need not be specified in the; argument list (optional argument). In the declaration, all required template; arguments must precede any optional arguments. The template argument default; values are evaluated from left to right. The :token:`RecordBody` is de",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:22047,Modifiability,parameteriz,parameterized,22047,"; operand is concatenated to an empty string. `Appendix B: Paste Operator Examples`_ presents examples of the behavior of; the paste operator. Statements; ==========. The following statements may appear at the top level of TableGen source; files. .. productionlist::; TableGenFile: (`Statement` | `IncludeDirective`; :| `PreprocessorDirective`)*; Statement: `Assert` | `Class` | `Def` | `Defm` | `Defset` | `Defvar`; :| `Dump` | `Foreach` | `If` | `Let` | `MultiClass`. The following sections describe each of these top-level statements. ``class`` --- define an abstract record class; ---------------------------------------------. A ``class`` statement defines an abstract record class from which other; classes and records can inherit. .. productionlist::; Class: ""class"" `ClassID` [`TemplateArgList`] `RecordBody`; TemplateArgList: ""<"" `TemplateArgDecl` ("","" `TemplateArgDecl`)* "">""; TemplateArgDecl: `Type` `TokIdentifier` [""="" `Value`]. A class can be parameterized by a list of ""template arguments,"" whose values; can be used in the class's record body. These template arguments are; specified each time the class is inherited by another class or record. If a template argument is not assigned a default value with ``=``, it is; uninitialized (has the ""value"" ``?``) and must be specified in the template; argument list when the class is inherited (required argument). If an; argument is assigned a default value, then it need not be specified in the; argument list (optional argument). In the declaration, all required template; arguments must precede any optional arguments. The template argument default; values are evaluated from left to right. The :token:`RecordBody` is defined below. It can include a list of; parent classes from which the current class inherits, along with field; definitions and other statements. When a class ``C`` inherits from another; class ``D``, the fields of ``D`` are effectively merged into the fields of; ``C``. A given class can only be defined once. A ``cl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:22213,Modifiability,inherit,inherited,22213," behavior of; the paste operator. Statements; ==========. The following statements may appear at the top level of TableGen source; files. .. productionlist::; TableGenFile: (`Statement` | `IncludeDirective`; :| `PreprocessorDirective`)*; Statement: `Assert` | `Class` | `Def` | `Defm` | `Defset` | `Defvar`; :| `Dump` | `Foreach` | `If` | `Let` | `MultiClass`. The following sections describe each of these top-level statements. ``class`` --- define an abstract record class; ---------------------------------------------. A ``class`` statement defines an abstract record class from which other; classes and records can inherit. .. productionlist::; Class: ""class"" `ClassID` [`TemplateArgList`] `RecordBody`; TemplateArgList: ""<"" `TemplateArgDecl` ("","" `TemplateArgDecl`)* "">""; TemplateArgDecl: `Type` `TokIdentifier` [""="" `Value`]. A class can be parameterized by a list of ""template arguments,"" whose values; can be used in the class's record body. These template arguments are; specified each time the class is inherited by another class or record. If a template argument is not assigned a default value with ``=``, it is; uninitialized (has the ""value"" ``?``) and must be specified in the template; argument list when the class is inherited (required argument). If an; argument is assigned a default value, then it need not be specified in the; argument list (optional argument). In the declaration, all required template; arguments must precede any optional arguments. The template argument default; values are evaluated from left to right. The :token:`RecordBody` is defined below. It can include a list of; parent classes from which the current class inherits, along with field; definitions and other statements. When a class ``C`` inherits from another; class ``D``, the fields of ``D`` are effectively merged into the fields of; ``C``. A given class can only be defined once. A ``class`` statement is; considered to define the class if *any* of the following are true (the; :token:`RecordBody",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:22434,Modifiability,inherit,inherited,22434," TableGenFile: (`Statement` | `IncludeDirective`; :| `PreprocessorDirective`)*; Statement: `Assert` | `Class` | `Def` | `Defm` | `Defset` | `Defvar`; :| `Dump` | `Foreach` | `If` | `Let` | `MultiClass`. The following sections describe each of these top-level statements. ``class`` --- define an abstract record class; ---------------------------------------------. A ``class`` statement defines an abstract record class from which other; classes and records can inherit. .. productionlist::; Class: ""class"" `ClassID` [`TemplateArgList`] `RecordBody`; TemplateArgList: ""<"" `TemplateArgDecl` ("","" `TemplateArgDecl`)* "">""; TemplateArgDecl: `Type` `TokIdentifier` [""="" `Value`]. A class can be parameterized by a list of ""template arguments,"" whose values; can be used in the class's record body. These template arguments are; specified each time the class is inherited by another class or record. If a template argument is not assigned a default value with ``=``, it is; uninitialized (has the ""value"" ``?``) and must be specified in the template; argument list when the class is inherited (required argument). If an; argument is assigned a default value, then it need not be specified in the; argument list (optional argument). In the declaration, all required template; arguments must precede any optional arguments. The template argument default; values are evaluated from left to right. The :token:`RecordBody` is defined below. It can include a list of; parent classes from which the current class inherits, along with field; definitions and other statements. When a class ``C`` inherits from another; class ``D``, the fields of ``D`` are effectively merged into the fields of; ``C``. A given class can only be defined once. A ``class`` statement is; considered to define the class if *any* of the following are true (the; :token:`RecordBody` elements are described below). * The :token:`TemplateArgList` is present, or; * The :token:`ParentClassList` in the :token:`RecordBody` is present, or; * T",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:22857,Modifiability,inherit,inherits,22857,"ss: ""class"" `ClassID` [`TemplateArgList`] `RecordBody`; TemplateArgList: ""<"" `TemplateArgDecl` ("","" `TemplateArgDecl`)* "">""; TemplateArgDecl: `Type` `TokIdentifier` [""="" `Value`]. A class can be parameterized by a list of ""template arguments,"" whose values; can be used in the class's record body. These template arguments are; specified each time the class is inherited by another class or record. If a template argument is not assigned a default value with ``=``, it is; uninitialized (has the ""value"" ``?``) and must be specified in the template; argument list when the class is inherited (required argument). If an; argument is assigned a default value, then it need not be specified in the; argument list (optional argument). In the declaration, all required template; arguments must precede any optional arguments. The template argument default; values are evaluated from left to right. The :token:`RecordBody` is defined below. It can include a list of; parent classes from which the current class inherits, along with field; definitions and other statements. When a class ``C`` inherits from another; class ``D``, the fields of ``D`` are effectively merged into the fields of; ``C``. A given class can only be defined once. A ``class`` statement is; considered to define the class if *any* of the following are true (the; :token:`RecordBody` elements are described below). * The :token:`TemplateArgList` is present, or; * The :token:`ParentClassList` in the :token:`RecordBody` is present, or; * The :token:`Body` in the :token:`RecordBody` is present and not empty. You can declare an empty class by specifying an empty :token:`TemplateArgList`; and an empty :token:`RecordBody`. This can serve as a restricted form of; forward declaration. Note that records derived from a forward-declared; class will inherit no fields from it, because those records are built when; their declarations are parsed, and thus before the class is finally defined. .. _NAME:. Every class has an implicit templat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:22938,Modifiability,inherit,inherits,22938,"plateArgDecl: `Type` `TokIdentifier` [""="" `Value`]. A class can be parameterized by a list of ""template arguments,"" whose values; can be used in the class's record body. These template arguments are; specified each time the class is inherited by another class or record. If a template argument is not assigned a default value with ``=``, it is; uninitialized (has the ""value"" ``?``) and must be specified in the template; argument list when the class is inherited (required argument). If an; argument is assigned a default value, then it need not be specified in the; argument list (optional argument). In the declaration, all required template; arguments must precede any optional arguments. The template argument default; values are evaluated from left to right. The :token:`RecordBody` is defined below. It can include a list of; parent classes from which the current class inherits, along with field; definitions and other statements. When a class ``C`` inherits from another; class ``D``, the fields of ``D`` are effectively merged into the fields of; ``C``. A given class can only be defined once. A ``class`` statement is; considered to define the class if *any* of the following are true (the; :token:`RecordBody` elements are described below). * The :token:`TemplateArgList` is present, or; * The :token:`ParentClassList` in the :token:`RecordBody` is present, or; * The :token:`Body` in the :token:`RecordBody` is present and not empty. You can declare an empty class by specifying an empty :token:`TemplateArgList`; and an empty :token:`RecordBody`. This can serve as a restricted form of; forward declaration. Note that records derived from a forward-declared; class will inherit no fields from it, because those records are built when; their declarations are parsed, and thus before the class is finally defined. .. _NAME:. Every class has an implicit template argument named ``NAME`` (uppercase),; which is bound to the name of the :token:`Def` or :token:`Defm` inheriting; from the clas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:23664,Modifiability,inherit,inherit,23664," values are evaluated from left to right. The :token:`RecordBody` is defined below. It can include a list of; parent classes from which the current class inherits, along with field; definitions and other statements. When a class ``C`` inherits from another; class ``D``, the fields of ``D`` are effectively merged into the fields of; ``C``. A given class can only be defined once. A ``class`` statement is; considered to define the class if *any* of the following are true (the; :token:`RecordBody` elements are described below). * The :token:`TemplateArgList` is present, or; * The :token:`ParentClassList` in the :token:`RecordBody` is present, or; * The :token:`Body` in the :token:`RecordBody` is present and not empty. You can declare an empty class by specifying an empty :token:`TemplateArgList`; and an empty :token:`RecordBody`. This can serve as a restricted form of; forward declaration. Note that records derived from a forward-declared; class will inherit no fields from it, because those records are built when; their declarations are parsed, and thus before the class is finally defined. .. _NAME:. Every class has an implicit template argument named ``NAME`` (uppercase),; which is bound to the name of the :token:`Def` or :token:`Defm` inheriting; from the class. If the class is inherited by an anonymous record, the name; is unspecified but globally unique. See `Examples: classes and records`_ for examples. Record Bodies; `````````````. Record bodies appear in both class and record definitions. A record body can; include a parent class list, which specifies the classes from which the; current class or record inherits fields. Such classes are called the; parent classes of the class or record. The record body also; includes the main body of the definition, which contains the specification; of the fields of the class or record. .. productionlist::; RecordBody: `ParentClassList` `Body`; ParentClassList: ["":"" `ParentClassListNE`]; ParentClassListNE: `ClassRef` ("","" `ClassRe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:23956,Modifiability,inherit,inheriting,23956,"d other statements. When a class ``C`` inherits from another; class ``D``, the fields of ``D`` are effectively merged into the fields of; ``C``. A given class can only be defined once. A ``class`` statement is; considered to define the class if *any* of the following are true (the; :token:`RecordBody` elements are described below). * The :token:`TemplateArgList` is present, or; * The :token:`ParentClassList` in the :token:`RecordBody` is present, or; * The :token:`Body` in the :token:`RecordBody` is present and not empty. You can declare an empty class by specifying an empty :token:`TemplateArgList`; and an empty :token:`RecordBody`. This can serve as a restricted form of; forward declaration. Note that records derived from a forward-declared; class will inherit no fields from it, because those records are built when; their declarations are parsed, and thus before the class is finally defined. .. _NAME:. Every class has an implicit template argument named ``NAME`` (uppercase),; which is bound to the name of the :token:`Def` or :token:`Defm` inheriting; from the class. If the class is inherited by an anonymous record, the name; is unspecified but globally unique. See `Examples: classes and records`_ for examples. Record Bodies; `````````````. Record bodies appear in both class and record definitions. A record body can; include a parent class list, which specifies the classes from which the; current class or record inherits fields. Such classes are called the; parent classes of the class or record. The record body also; includes the main body of the definition, which contains the specification; of the fields of the class or record. .. productionlist::; RecordBody: `ParentClassList` `Body`; ParentClassList: ["":"" `ParentClassListNE`]; ParentClassListNE: `ClassRef` ("","" `ClassRef`)*; ClassRef: (`ClassID` | `MultiClassID`) [""<"" [`ArgValueList`] "">""]; ArgValueList: `PostionalArgValueList` ["",""] `NamedArgValueList`; PostionalArgValueList: [`Value` {"","" `Value`}*]; NamedArgVa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:24000,Modifiability,inherit,inherited,24000,"s of; ``C``. A given class can only be defined once. A ``class`` statement is; considered to define the class if *any* of the following are true (the; :token:`RecordBody` elements are described below). * The :token:`TemplateArgList` is present, or; * The :token:`ParentClassList` in the :token:`RecordBody` is present, or; * The :token:`Body` in the :token:`RecordBody` is present and not empty. You can declare an empty class by specifying an empty :token:`TemplateArgList`; and an empty :token:`RecordBody`. This can serve as a restricted form of; forward declaration. Note that records derived from a forward-declared; class will inherit no fields from it, because those records are built when; their declarations are parsed, and thus before the class is finally defined. .. _NAME:. Every class has an implicit template argument named ``NAME`` (uppercase),; which is bound to the name of the :token:`Def` or :token:`Defm` inheriting; from the class. If the class is inherited by an anonymous record, the name; is unspecified but globally unique. See `Examples: classes and records`_ for examples. Record Bodies; `````````````. Record bodies appear in both class and record definitions. A record body can; include a parent class list, which specifies the classes from which the; current class or record inherits fields. Such classes are called the; parent classes of the class or record. The record body also; includes the main body of the definition, which contains the specification; of the fields of the class or record. .. productionlist::; RecordBody: `ParentClassList` `Body`; ParentClassList: ["":"" `ParentClassListNE`]; ParentClassListNE: `ClassRef` ("","" `ClassRef`)*; ClassRef: (`ClassID` | `MultiClassID`) [""<"" [`ArgValueList`] "">""]; ArgValueList: `PostionalArgValueList` ["",""] `NamedArgValueList`; PostionalArgValueList: [`Value` {"","" `Value`}*]; NamedArgValueList: [`NameValue` ""="" `Value` {"","" `NameValue` ""="" `Value`}*]. A :token:`ParentClassList` containing a :token:`MultiClassID` is",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:24336,Modifiability,inherit,inherits,24336," :token:`ParentClassList` in the :token:`RecordBody` is present, or; * The :token:`Body` in the :token:`RecordBody` is present and not empty. You can declare an empty class by specifying an empty :token:`TemplateArgList`; and an empty :token:`RecordBody`. This can serve as a restricted form of; forward declaration. Note that records derived from a forward-declared; class will inherit no fields from it, because those records are built when; their declarations are parsed, and thus before the class is finally defined. .. _NAME:. Every class has an implicit template argument named ``NAME`` (uppercase),; which is bound to the name of the :token:`Def` or :token:`Defm` inheriting; from the class. If the class is inherited by an anonymous record, the name; is unspecified but globally unique. See `Examples: classes and records`_ for examples. Record Bodies; `````````````. Record bodies appear in both class and record definitions. A record body can; include a parent class list, which specifies the classes from which the; current class or record inherits fields. Such classes are called the; parent classes of the class or record. The record body also; includes the main body of the definition, which contains the specification; of the fields of the class or record. .. productionlist::; RecordBody: `ParentClassList` `Body`; ParentClassList: ["":"" `ParentClassListNE`]; ParentClassListNE: `ClassRef` ("","" `ClassRef`)*; ClassRef: (`ClassID` | `MultiClassID`) [""<"" [`ArgValueList`] "">""]; ArgValueList: `PostionalArgValueList` ["",""] `NamedArgValueList`; PostionalArgValueList: [`Value` {"","" `Value`}*]; NamedArgValueList: [`NameValue` ""="" `Value` {"","" `NameValue` ""="" `Value`}*]. A :token:`ParentClassList` containing a :token:`MultiClassID` is valid only; in the class list of a ``defm`` statement. In that case, the ID must be the; name of a multiclass. The argument values can be specified in two forms:. * Positional argument (``value``). The value is assigned to the argument in the; correspond",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:26583,Modifiability,inherit,inherited,26583,"the; argument with name ``a`` and ``a1`` will be assigned to the argument with; name ``b``. Required arguments can also be specified as named argument. Note that the argument can only be specified once regardless of the way (named; or positional) to specify and positional arguments should be put before named; arguments. .. productionlist::; Body: "";"" | ""{"" `BodyItem`* ""}""; BodyItem: (`Type` | ""code"") `TokIdentifier` [""="" `Value`] "";""; :| ""let"" `TokIdentifier` [""{"" `RangeList` ""}""] ""="" `Value` "";""; :| ""defvar"" `TokIdentifier` ""="" `Value` "";""; :| `Assert`. A field definition in the body specifies a field to be included in the class; or record. If no initial value is specified, then the field's value is; uninitialized. The type must be specified; TableGen will not infer it from; the value. The keyword ``code`` may be used to emphasize that the field; has a string value that is code. The ``let`` form is used to reset a field to a new value. This can be done; for fields defined directly in the body or fields inherited from parent; classes. A :token:`RangeList` can be specified to reset certain bits in a; ``bit<n>`` field. The ``defvar`` form defines a variable whose value can be used in other; value expressions within the body. The variable is not a field: it does not; become a field of the class or record being defined. Variables are provided; to hold temporary values while processing the body. See `Defvar in a Record; Body`_ for more details. When class ``C2`` inherits from class ``C1``, it acquires all the field; definitions of ``C1``. As those definitions are merged into class ``C2``, any; template arguments passed to ``C1`` by ``C2`` are substituted into the; definitions. In other words, the abstract record fields defined by ``C1`` are; expanded with the template arguments before being merged into ``C2``. .. _def:. ``def`` --- define a concrete record; ------------------------------------. A ``def`` statement defines a new concrete record. .. productionlist::; Def: ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:26729,Modifiability,variab,variable,26729,"pecified once regardless of the way (named; or positional) to specify and positional arguments should be put before named; arguments. .. productionlist::; Body: "";"" | ""{"" `BodyItem`* ""}""; BodyItem: (`Type` | ""code"") `TokIdentifier` [""="" `Value`] "";""; :| ""let"" `TokIdentifier` [""{"" `RangeList` ""}""] ""="" `Value` "";""; :| ""defvar"" `TokIdentifier` ""="" `Value` "";""; :| `Assert`. A field definition in the body specifies a field to be included in the class; or record. If no initial value is specified, then the field's value is; uninitialized. The type must be specified; TableGen will not infer it from; the value. The keyword ``code`` may be used to emphasize that the field; has a string value that is code. The ``let`` form is used to reset a field to a new value. This can be done; for fields defined directly in the body or fields inherited from parent; classes. A :token:`RangeList` can be specified to reset certain bits in a; ``bit<n>`` field. The ``defvar`` form defines a variable whose value can be used in other; value expressions within the body. The variable is not a field: it does not; become a field of the class or record being defined. Variables are provided; to hold temporary values while processing the body. See `Defvar in a Record; Body`_ for more details. When class ``C2`` inherits from class ``C1``, it acquires all the field; definitions of ``C1``. As those definitions are merged into class ``C2``, any; template arguments passed to ``C1`` by ``C2`` are substituted into the; definitions. In other words, the abstract record fields defined by ``C1`` are; expanded with the template arguments before being merged into ``C2``. .. _def:. ``def`` --- define a concrete record; ------------------------------------. A ``def`` statement defines a new concrete record. .. productionlist::; Def: ""def"" [`NameValue`] `RecordBody`; NameValue: `Value` (parsed in a special mode). The name value is optional. If specified, it is parsed in a special mode; where undefined (unrecognized) id",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:26811,Modifiability,variab,variable,26811," be put before named; arguments. .. productionlist::; Body: "";"" | ""{"" `BodyItem`* ""}""; BodyItem: (`Type` | ""code"") `TokIdentifier` [""="" `Value`] "";""; :| ""let"" `TokIdentifier` [""{"" `RangeList` ""}""] ""="" `Value` "";""; :| ""defvar"" `TokIdentifier` ""="" `Value` "";""; :| `Assert`. A field definition in the body specifies a field to be included in the class; or record. If no initial value is specified, then the field's value is; uninitialized. The type must be specified; TableGen will not infer it from; the value. The keyword ``code`` may be used to emphasize that the field; has a string value that is code. The ``let`` form is used to reset a field to a new value. This can be done; for fields defined directly in the body or fields inherited from parent; classes. A :token:`RangeList` can be specified to reset certain bits in a; ``bit<n>`` field. The ``defvar`` form defines a variable whose value can be used in other; value expressions within the body. The variable is not a field: it does not; become a field of the class or record being defined. Variables are provided; to hold temporary values while processing the body. See `Defvar in a Record; Body`_ for more details. When class ``C2`` inherits from class ``C1``, it acquires all the field; definitions of ``C1``. As those definitions are merged into class ``C2``, any; template arguments passed to ``C1`` by ``C2`` are substituted into the; definitions. In other words, the abstract record fields defined by ``C1`` are; expanded with the template arguments before being merged into ``C2``. .. _def:. ``def`` --- define a concrete record; ------------------------------------. A ``def`` statement defines a new concrete record. .. productionlist::; Def: ""def"" [`NameValue`] `RecordBody`; NameValue: `Value` (parsed in a special mode). The name value is optional. If specified, it is parsed in a special mode; where undefined (unrecognized) identifiers are interpreted as literal; strings. In particular, global identifiers are considered unreco",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:27046,Modifiability,inherit,inherits,27046,"ar"" `TokIdentifier` ""="" `Value` "";""; :| `Assert`. A field definition in the body specifies a field to be included in the class; or record. If no initial value is specified, then the field's value is; uninitialized. The type must be specified; TableGen will not infer it from; the value. The keyword ``code`` may be used to emphasize that the field; has a string value that is code. The ``let`` form is used to reset a field to a new value. This can be done; for fields defined directly in the body or fields inherited from parent; classes. A :token:`RangeList` can be specified to reset certain bits in a; ``bit<n>`` field. The ``defvar`` form defines a variable whose value can be used in other; value expressions within the body. The variable is not a field: it does not; become a field of the class or record being defined. Variables are provided; to hold temporary values while processing the body. See `Defvar in a Record; Body`_ for more details. When class ``C2`` inherits from class ``C1``, it acquires all the field; definitions of ``C1``. As those definitions are merged into class ``C2``, any; template arguments passed to ``C1`` by ``C2`` are substituted into the; definitions. In other words, the abstract record fields defined by ``C1`` are; expanded with the template arguments before being merged into ``C2``. .. _def:. ``def`` --- define a concrete record; ------------------------------------. A ``def`` statement defines a new concrete record. .. productionlist::; Def: ""def"" [`NameValue`] `RecordBody`; NameValue: `Value` (parsed in a special mode). The name value is optional. If specified, it is parsed in a special mode; where undefined (unrecognized) identifiers are interpreted as literal; strings. In particular, global identifiers are considered unrecognized.; These include global variables defined by ``defvar`` and ``defset``. A; record name can be the null string. If no name value is given, the record is *anonymous*. The final name of an; anonymous record is unspecif",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:27884,Modifiability,variab,variables,27884,"ined. Variables are provided; to hold temporary values while processing the body. See `Defvar in a Record; Body`_ for more details. When class ``C2`` inherits from class ``C1``, it acquires all the field; definitions of ``C1``. As those definitions are merged into class ``C2``, any; template arguments passed to ``C1`` by ``C2`` are substituted into the; definitions. In other words, the abstract record fields defined by ``C1`` are; expanded with the template arguments before being merged into ``C2``. .. _def:. ``def`` --- define a concrete record; ------------------------------------. A ``def`` statement defines a new concrete record. .. productionlist::; Def: ""def"" [`NameValue`] `RecordBody`; NameValue: `Value` (parsed in a special mode). The name value is optional. If specified, it is parsed in a special mode; where undefined (unrecognized) identifiers are interpreted as literal; strings. In particular, global identifiers are considered unrecognized.; These include global variables defined by ``defvar`` and ``defset``. A; record name can be the null string. If no name value is given, the record is *anonymous*. The final name of an; anonymous record is unspecified but globally unique. Special handling occurs if a ``def`` appears inside a ``multiclass``; statement. See the ``multiclass`` section below for details. A record can inherit from one or more classes by specifying the; :token:`ParentClassList` clause at the beginning of its record body. All of; the fields in the parent classes are added to the record. If two or more; parent classes provide the same field, the record ends up with the field value; of the last parent class. As a special case, the name of a record can be passed as a template argument; to that record's parent classes. For example:. .. code-block:: text. class A <dag d> {; dag the_dag = d;; }. def rec1 : A<(ops rec1)>;. The DAG ``(ops rec1)`` is passed as a template argument to class ``A``. Notice; that the DAG includes ``rec1``, the record being ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:28244,Modifiability,inherit,inherit,28244,"ord fields defined by ``C1`` are; expanded with the template arguments before being merged into ``C2``. .. _def:. ``def`` --- define a concrete record; ------------------------------------. A ``def`` statement defines a new concrete record. .. productionlist::; Def: ""def"" [`NameValue`] `RecordBody`; NameValue: `Value` (parsed in a special mode). The name value is optional. If specified, it is parsed in a special mode; where undefined (unrecognized) identifiers are interpreted as literal; strings. In particular, global identifiers are considered unrecognized.; These include global variables defined by ``defvar`` and ``defset``. A; record name can be the null string. If no name value is given, the record is *anonymous*. The final name of an; anonymous record is unspecified but globally unique. Special handling occurs if a ``def`` appears inside a ``multiclass``; statement. See the ``multiclass`` section below for details. A record can inherit from one or more classes by specifying the; :token:`ParentClassList` clause at the beginning of its record body. All of; the fields in the parent classes are added to the record. If two or more; parent classes provide the same field, the record ends up with the field value; of the last parent class. As a special case, the name of a record can be passed as a template argument; to that record's parent classes. For example:. .. code-block:: text. class A <dag d> {; dag the_dag = d;; }. def rec1 : A<(ops rec1)>;. The DAG ``(ops rec1)`` is passed as a template argument to class ``A``. Notice; that the DAG includes ``rec1``, the record being defined. The steps taken to create a new record are somewhat complex. See `How; records are built`_. See `Examples: classes and records`_ for examples. Examples: classes and records; -----------------------------. Here is a simple TableGen file with one class and two record definitions. .. code-block:: text. class C {; bit V = true;; }. def X : C;; def Y : C {; let V = false;; string Greeting = ""He",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:29533,Modifiability,inherit,inherit,29533,"t parent class. As a special case, the name of a record can be passed as a template argument; to that record's parent classes. For example:. .. code-block:: text. class A <dag d> {; dag the_dag = d;; }. def rec1 : A<(ops rec1)>;. The DAG ``(ops rec1)`` is passed as a template argument to class ``A``. Notice; that the DAG includes ``rec1``, the record being defined. The steps taken to create a new record are somewhat complex. See `How; records are built`_. See `Examples: classes and records`_ for examples. Examples: classes and records; -----------------------------. Here is a simple TableGen file with one class and two record definitions. .. code-block:: text. class C {; bit V = true;; }. def X : C;; def Y : C {; let V = false;; string Greeting = ""Hello!"";; }. First, the abstract class ``C`` is defined. It has one field named ``V``; that is a bit initialized to true. Next, two records are defined, derived from class ``C``; that is, with ``C``; as their parent class. Thus they both inherit the ``V`` field. Record ``Y``; also defines another string field, ``Greeting``, which is initialized to; ``""Hello!""``. In addition, ``Y`` overrides the inherited ``V`` field,; setting it to false. A class is useful for isolating the common features of multiple records in; one place. A class can initialize common fields to default values, but; records inheriting from that class can override the defaults. TableGen supports the definition of parameterized classes as well as; nonparameterized ones. Parameterized classes specify a list of variable; declarations, which may optionally have defaults, that are bound when the; class is specified as a parent class of another class or record. .. code-block:: text. class FPFormat <bits<3> val> {; bits<3> Value = val;; }. def NotFP : FPFormat<0>;; def ZeroArgFP : FPFormat<1>;; def OneArgFP : FPFormat<2>;; def OneArgFPRW : FPFormat<3>;; def TwoArgFP : FPFormat<4>;; def CompareFP : FPFormat<5>;; def CondMovFP : FPFormat<6>;; def SpecialFP : FPForm",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:29693,Modifiability,inherit,inherited,29693,". class A <dag d> {; dag the_dag = d;; }. def rec1 : A<(ops rec1)>;. The DAG ``(ops rec1)`` is passed as a template argument to class ``A``. Notice; that the DAG includes ``rec1``, the record being defined. The steps taken to create a new record are somewhat complex. See `How; records are built`_. See `Examples: classes and records`_ for examples. Examples: classes and records; -----------------------------. Here is a simple TableGen file with one class and two record definitions. .. code-block:: text. class C {; bit V = true;; }. def X : C;; def Y : C {; let V = false;; string Greeting = ""Hello!"";; }. First, the abstract class ``C`` is defined. It has one field named ``V``; that is a bit initialized to true. Next, two records are defined, derived from class ``C``; that is, with ``C``; as their parent class. Thus they both inherit the ``V`` field. Record ``Y``; also defines another string field, ``Greeting``, which is initialized to; ``""Hello!""``. In addition, ``Y`` overrides the inherited ``V`` field,; setting it to false. A class is useful for isolating the common features of multiple records in; one place. A class can initialize common fields to default values, but; records inheriting from that class can override the defaults. TableGen supports the definition of parameterized classes as well as; nonparameterized ones. Parameterized classes specify a list of variable; declarations, which may optionally have defaults, that are bound when the; class is specified as a parent class of another class or record. .. code-block:: text. class FPFormat <bits<3> val> {; bits<3> Value = val;; }. def NotFP : FPFormat<0>;; def ZeroArgFP : FPFormat<1>;; def OneArgFP : FPFormat<2>;; def OneArgFPRW : FPFormat<3>;; def TwoArgFP : FPFormat<4>;; def CompareFP : FPFormat<5>;; def CondMovFP : FPFormat<6>;; def SpecialFP : FPFormat<7>;. The purpose of the ``FPFormat`` class is to act as a sort of enumerated; type. It provides a single field, ``Value``, which holds a 3-bit number. Its; te",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:29894,Modifiability,inherit,inheriting,29894,"cord being defined. The steps taken to create a new record are somewhat complex. See `How; records are built`_. See `Examples: classes and records`_ for examples. Examples: classes and records; -----------------------------. Here is a simple TableGen file with one class and two record definitions. .. code-block:: text. class C {; bit V = true;; }. def X : C;; def Y : C {; let V = false;; string Greeting = ""Hello!"";; }. First, the abstract class ``C`` is defined. It has one field named ``V``; that is a bit initialized to true. Next, two records are defined, derived from class ``C``; that is, with ``C``; as their parent class. Thus they both inherit the ``V`` field. Record ``Y``; also defines another string field, ``Greeting``, which is initialized to; ``""Hello!""``. In addition, ``Y`` overrides the inherited ``V`` field,; setting it to false. A class is useful for isolating the common features of multiple records in; one place. A class can initialize common fields to default values, but; records inheriting from that class can override the defaults. TableGen supports the definition of parameterized classes as well as; nonparameterized ones. Parameterized classes specify a list of variable; declarations, which may optionally have defaults, that are bound when the; class is specified as a parent class of another class or record. .. code-block:: text. class FPFormat <bits<3> val> {; bits<3> Value = val;; }. def NotFP : FPFormat<0>;; def ZeroArgFP : FPFormat<1>;; def OneArgFP : FPFormat<2>;; def OneArgFPRW : FPFormat<3>;; def TwoArgFP : FPFormat<4>;; def CompareFP : FPFormat<5>;; def CondMovFP : FPFormat<6>;; def SpecialFP : FPFormat<7>;. The purpose of the ``FPFormat`` class is to act as a sort of enumerated; type. It provides a single field, ``Value``, which holds a 3-bit number. Its; template argument, ``val``, is used to set the ``Value`` field. Each of the; eight records is defined with ``FPFormat`` as its parent class. The; enumeration value is passed in angle bracket",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:29984,Modifiability,parameteriz,parameterized,29984,"`_. See `Examples: classes and records`_ for examples. Examples: classes and records; -----------------------------. Here is a simple TableGen file with one class and two record definitions. .. code-block:: text. class C {; bit V = true;; }. def X : C;; def Y : C {; let V = false;; string Greeting = ""Hello!"";; }. First, the abstract class ``C`` is defined. It has one field named ``V``; that is a bit initialized to true. Next, two records are defined, derived from class ``C``; that is, with ``C``; as their parent class. Thus they both inherit the ``V`` field. Record ``Y``; also defines another string field, ``Greeting``, which is initialized to; ``""Hello!""``. In addition, ``Y`` overrides the inherited ``V`` field,; setting it to false. A class is useful for isolating the common features of multiple records in; one place. A class can initialize common fields to default values, but; records inheriting from that class can override the defaults. TableGen supports the definition of parameterized classes as well as; nonparameterized ones. Parameterized classes specify a list of variable; declarations, which may optionally have defaults, that are bound when the; class is specified as a parent class of another class or record. .. code-block:: text. class FPFormat <bits<3> val> {; bits<3> Value = val;; }. def NotFP : FPFormat<0>;; def ZeroArgFP : FPFormat<1>;; def OneArgFP : FPFormat<2>;; def OneArgFPRW : FPFormat<3>;; def TwoArgFP : FPFormat<4>;; def CompareFP : FPFormat<5>;; def CondMovFP : FPFormat<6>;; def SpecialFP : FPFormat<7>;. The purpose of the ``FPFormat`` class is to act as a sort of enumerated; type. It provides a single field, ``Value``, which holds a 3-bit number. Its; template argument, ``val``, is used to set the ``Value`` field. Each of the; eight records is defined with ``FPFormat`` as its parent class. The; enumeration value is passed in angle brackets as the template argument. Each; record will inherent the ``Value`` field with the appropriate enumeration;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:30081,Modifiability,variab,variable,30081," file with one class and two record definitions. .. code-block:: text. class C {; bit V = true;; }. def X : C;; def Y : C {; let V = false;; string Greeting = ""Hello!"";; }. First, the abstract class ``C`` is defined. It has one field named ``V``; that is a bit initialized to true. Next, two records are defined, derived from class ``C``; that is, with ``C``; as their parent class. Thus they both inherit the ``V`` field. Record ``Y``; also defines another string field, ``Greeting``, which is initialized to; ``""Hello!""``. In addition, ``Y`` overrides the inherited ``V`` field,; setting it to false. A class is useful for isolating the common features of multiple records in; one place. A class can initialize common fields to default values, but; records inheriting from that class can override the defaults. TableGen supports the definition of parameterized classes as well as; nonparameterized ones. Parameterized classes specify a list of variable; declarations, which may optionally have defaults, that are bound when the; class is specified as a parent class of another class or record. .. code-block:: text. class FPFormat <bits<3> val> {; bits<3> Value = val;; }. def NotFP : FPFormat<0>;; def ZeroArgFP : FPFormat<1>;; def OneArgFP : FPFormat<2>;; def OneArgFPRW : FPFormat<3>;; def TwoArgFP : FPFormat<4>;; def CompareFP : FPFormat<5>;; def CondMovFP : FPFormat<6>;; def SpecialFP : FPFormat<7>;. The purpose of the ``FPFormat`` class is to act as a sort of enumerated; type. It provides a single field, ``Value``, which holds a 3-bit number. Its; template argument, ``val``, is used to set the ``Value`` field. Each of the; eight records is defined with ``FPFormat`` as its parent class. The; enumeration value is passed in angle brackets as the template argument. Each; record will inherent the ``Value`` field with the appropriate enumeration; value. Here is a more complex example of classes with template arguments. First, we; define a class similar to the ``FPFormat`` class above.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:31251,Modifiability,inherit,inherit,31251," <bits<3> val> {; bits<3> Value = val;; }. def NotFP : FPFormat<0>;; def ZeroArgFP : FPFormat<1>;; def OneArgFP : FPFormat<2>;; def OneArgFPRW : FPFormat<3>;; def TwoArgFP : FPFormat<4>;; def CompareFP : FPFormat<5>;; def CondMovFP : FPFormat<6>;; def SpecialFP : FPFormat<7>;. The purpose of the ``FPFormat`` class is to act as a sort of enumerated; type. It provides a single field, ``Value``, which holds a 3-bit number. Its; template argument, ``val``, is used to set the ``Value`` field. Each of the; eight records is defined with ``FPFormat`` as its parent class. The; enumeration value is passed in angle brackets as the template argument. Each; record will inherent the ``Value`` field with the appropriate enumeration; value. Here is a more complex example of classes with template arguments. First, we; define a class similar to the ``FPFormat`` class above. It takes a template; argument and uses it to initialize a field named ``Value``. Then we define; four records that inherit the ``Value`` field with its four different; integer values. .. code-block:: text. class ModRefVal <bits<2> val> {; bits<2> Value = val;; }. def None : ModRefVal<0>;; def Mod : ModRefVal<1>;; def Ref : ModRefVal<2>;; def ModRef : ModRefVal<3>;. This is somewhat contrived, but let's say we would like to examine the two; bits of the ``Value`` field independently. We can define a class that; accepts a ``ModRefVal`` record as a template argument and splits up its; value into two fields, one bit each. Then we can define records that inherit from; ``ModRefBits`` and so acquire two fields from it, one for each bit in the; ``ModRefVal`` record passed as the template argument. .. code-block:: text. class ModRefBits <ModRefVal mrv> {; // Break the value up into its bits, which can provide a nice; // interface to the ModRefVal values.; bit isMod = mrv.Value{0};; bit isRef = mrv.Value{1};; }. // Example uses.; def foo : ModRefBits<Mod>;; def bar : ModRefBits<Ref>;; def snork : ModRefBits<ModRef>;. This ill",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:31793,Modifiability,inherit,inherit,31793,"ration value is passed in angle brackets as the template argument. Each; record will inherent the ``Value`` field with the appropriate enumeration; value. Here is a more complex example of classes with template arguments. First, we; define a class similar to the ``FPFormat`` class above. It takes a template; argument and uses it to initialize a field named ``Value``. Then we define; four records that inherit the ``Value`` field with its four different; integer values. .. code-block:: text. class ModRefVal <bits<2> val> {; bits<2> Value = val;; }. def None : ModRefVal<0>;; def Mod : ModRefVal<1>;; def Ref : ModRefVal<2>;; def ModRef : ModRefVal<3>;. This is somewhat contrived, but let's say we would like to examine the two; bits of the ``Value`` field independently. We can define a class that; accepts a ``ModRefVal`` record as a template argument and splits up its; value into two fields, one bit each. Then we can define records that inherit from; ``ModRefBits`` and so acquire two fields from it, one for each bit in the; ``ModRefVal`` record passed as the template argument. .. code-block:: text. class ModRefBits <ModRefVal mrv> {; // Break the value up into its bits, which can provide a nice; // interface to the ModRefVal values.; bit isMod = mrv.Value{0};; bit isRef = mrv.Value{1};; }. // Example uses.; def foo : ModRefBits<Mod>;; def bar : ModRefBits<Ref>;; def snork : ModRefBits<ModRef>;. This illustrates how one class can be defined to reorganize the; fields in another class, thus hiding the internal representation of that; other class. Running ``llvm-tblgen`` on the example prints the following definitions:. .. code-block:: text. def bar { // Value; bit isMod = 0;; bit isRef = 1;; }; def foo { // Value; bit isMod = 1;; bit isRef = 0;; }; def snork { // Value; bit isMod = 1;; bit isRef = 1;; }. ``let`` --- override fields in classes or records; -------------------------------------------------. A ``let`` statement collects a set of field values (sometimes called; *",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:33437,Modifiability,inherit,inherited,33437,"` on the example prints the following definitions:. .. code-block:: text. def bar { // Value; bit isMod = 0;; bit isRef = 1;; }; def foo { // Value; bit isMod = 1;; bit isRef = 0;; }; def snork { // Value; bit isMod = 1;; bit isRef = 1;; }. ``let`` --- override fields in classes or records; -------------------------------------------------. A ``let`` statement collects a set of field values (sometimes called; *bindings*) and applies them to all the classes and records defined by; statements within the scope of the ``let``. .. productionlist::; Let: ""let"" `LetList` ""in"" ""{"" `Statement`* ""}""; :| ""let"" `LetList` ""in"" `Statement`; LetList: `LetItem` ("","" `LetItem`)*; LetItem: `TokIdentifier` [""<"" `RangeList` "">""] ""="" `Value`. The ``let`` statement establishes a scope, which is a sequence of statements; in braces or a single statement with no braces. The bindings in the; :token:`LetList` apply to the statements in that scope. The field names in the :token:`LetList` must name fields in classes inherited by; the classes and records defined in the statements. The field values are; applied to the classes and records *after* the records inherit all the fields from; their parent classes. So the ``let`` acts to override inherited field; values. A ``let`` cannot override the value of a template argument. Top-level ``let`` statements are often useful when a few fields need to be; overridden in several records. Here are two examples. Note that ``let``; statements can be nested. .. code-block:: text. let isTerminator = true, isReturn = true, isBarrier = true, hasCtrlDep = true in; def RET : I<0xC3, RawFrm, (outs), (ins), ""ret"", [(X86retflag 0)]>;. let isCall = true in; // All calls clobber the non-callee saved registers...; let Defs = [EAX, ECX, EDX, FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0,; MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7, XMM0, XMM1, XMM2,; XMM3, XMM4, XMM5, XMM6, XMM7, EFLAGS] in {; def CALLpcrel32 : Ii32<0xE8, RawFrm, (outs), (ins i32imm:$dst, variable_ops),; ""call\t${dst:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:33579,Modifiability,inherit,inherit,33579,"f foo { // Value; bit isMod = 1;; bit isRef = 0;; }; def snork { // Value; bit isMod = 1;; bit isRef = 1;; }. ``let`` --- override fields in classes or records; -------------------------------------------------. A ``let`` statement collects a set of field values (sometimes called; *bindings*) and applies them to all the classes and records defined by; statements within the scope of the ``let``. .. productionlist::; Let: ""let"" `LetList` ""in"" ""{"" `Statement`* ""}""; :| ""let"" `LetList` ""in"" `Statement`; LetList: `LetItem` ("","" `LetItem`)*; LetItem: `TokIdentifier` [""<"" `RangeList` "">""] ""="" `Value`. The ``let`` statement establishes a scope, which is a sequence of statements; in braces or a single statement with no braces. The bindings in the; :token:`LetList` apply to the statements in that scope. The field names in the :token:`LetList` must name fields in classes inherited by; the classes and records defined in the statements. The field values are; applied to the classes and records *after* the records inherit all the fields from; their parent classes. So the ``let`` acts to override inherited field; values. A ``let`` cannot override the value of a template argument. Top-level ``let`` statements are often useful when a few fields need to be; overridden in several records. Here are two examples. Note that ``let``; statements can be nested. .. code-block:: text. let isTerminator = true, isReturn = true, isBarrier = true, hasCtrlDep = true in; def RET : I<0xC3, RawFrm, (outs), (ins), ""ret"", [(X86retflag 0)]>;. let isCall = true in; // All calls clobber the non-callee saved registers...; let Defs = [EAX, ECX, EDX, FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0,; MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7, XMM0, XMM1, XMM2,; XMM3, XMM4, XMM5, XMM6, XMM7, EFLAGS] in {; def CALLpcrel32 : Ii32<0xE8, RawFrm, (outs), (ins i32imm:$dst, variable_ops),; ""call\t${dst:call}"", []>;; def CALL32r : I<0xFF, MRM2r, (outs), (ins GR32:$dst, variable_ops),; ""call\t{*}$dst"", [(X86call GR32:$dst)]>;; def C",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:33662,Modifiability,inherit,inherited,33662,"it isRef = 1;; }. ``let`` --- override fields in classes or records; -------------------------------------------------. A ``let`` statement collects a set of field values (sometimes called; *bindings*) and applies them to all the classes and records defined by; statements within the scope of the ``let``. .. productionlist::; Let: ""let"" `LetList` ""in"" ""{"" `Statement`* ""}""; :| ""let"" `LetList` ""in"" `Statement`; LetList: `LetItem` ("","" `LetItem`)*; LetItem: `TokIdentifier` [""<"" `RangeList` "">""] ""="" `Value`. The ``let`` statement establishes a scope, which is a sequence of statements; in braces or a single statement with no braces. The bindings in the; :token:`LetList` apply to the statements in that scope. The field names in the :token:`LetList` must name fields in classes inherited by; the classes and records defined in the statements. The field values are; applied to the classes and records *after* the records inherit all the fields from; their parent classes. So the ``let`` acts to override inherited field; values. A ``let`` cannot override the value of a template argument. Top-level ``let`` statements are often useful when a few fields need to be; overridden in several records. Here are two examples. Note that ``let``; statements can be nested. .. code-block:: text. let isTerminator = true, isReturn = true, isBarrier = true, hasCtrlDep = true in; def RET : I<0xC3, RawFrm, (outs), (ins), ""ret"", [(X86retflag 0)]>;. let isCall = true in; // All calls clobber the non-callee saved registers...; let Defs = [EAX, ECX, EDX, FP0, FP1, FP2, FP3, FP4, FP5, FP6, ST0,; MM0, MM1, MM2, MM3, MM4, MM5, MM6, MM7, XMM0, XMM1, XMM2,; XMM3, XMM4, XMM5, XMM6, XMM7, EFLAGS] in {; def CALLpcrel32 : Ii32<0xE8, RawFrm, (outs), (ins i32imm:$dst, variable_ops),; ""call\t${dst:call}"", []>;; def CALL32r : I<0xFF, MRM2r, (outs), (ins GR32:$dst, variable_ops),; ""call\t{*}$dst"", [(X86call GR32:$dst)]>;; def CALL32m : I<0xFF, MRM2m, (outs), (ins i32mem:$dst, variable_ops),; ""call\t{*}$dst"", []>;; }. N",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:35844,Modifiability,inherit,inherit,35844," commonality; between multiple records, multiclasses allow a convenient method for; defining many records at once. For example, consider a 3-address; instruction architecture whose instructions come in two formats: ``reg = reg; op reg`` and ``reg = reg op imm`` (e.g., SPARC). We would like to specify in; one place that these two common formats exist, then in a separate place; specify what all the operations are. The ``multiclass`` and ``defm``; statements accomplish this goal. You can think of a multiclass as a macro or; template that expands into multiple records. .. productionlist::; MultiClass: ""multiclass"" `TokIdentifier` [`TemplateArgList`]; : `ParentClassList`; : ""{"" `MultiClassStatement`+ ""}""; MultiClassID: `TokIdentifier`; MultiClassStatement: `Assert` | `Def` | `Defm` | `Defvar` | `Foreach` | `If` | `Let`. As with regular classes, the multiclass has a name and can accept template; arguments. A multiclass can inherit from other multiclasses, which causes; the other multiclasses to be expanded and contribute to the record; definitions in the inheriting multiclass. The body of the multiclass; contains a series of statements that define records, using :token:`Def` and; :token:`Defm`. In addition, :token:`Defvar`, :token:`Foreach`, and; :token:`Let` statements can be used to factor out even more common elements.; The :token:`If` and :token:`Assert` statements can also be used. Also as with regular classes, the multiclass has the implicit template; argument ``NAME`` (see NAME_). When a named (non-anonymous) record is; defined in a multiclass and the record's name does not include a use of the; template argument ``NAME``, such a use is automatically *prepended*; to the name. That is, the following are equivalent inside a multiclass::. def Foo ...; def NAME # Foo ... The records defined in a multiclass are created when the multiclass is; ""instantiated"" or ""invoked"" by a ``defm`` statement outside the multiclass; definition. Each ``def`` statement in the multiclass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:35978,Modifiability,inherit,inheriting,35978," commonality; between multiple records, multiclasses allow a convenient method for; defining many records at once. For example, consider a 3-address; instruction architecture whose instructions come in two formats: ``reg = reg; op reg`` and ``reg = reg op imm`` (e.g., SPARC). We would like to specify in; one place that these two common formats exist, then in a separate place; specify what all the operations are. The ``multiclass`` and ``defm``; statements accomplish this goal. You can think of a multiclass as a macro or; template that expands into multiple records. .. productionlist::; MultiClass: ""multiclass"" `TokIdentifier` [`TemplateArgList`]; : `ParentClassList`; : ""{"" `MultiClassStatement`+ ""}""; MultiClassID: `TokIdentifier`; MultiClassStatement: `Assert` | `Def` | `Defm` | `Defvar` | `Foreach` | `If` | `Let`. As with regular classes, the multiclass has a name and can accept template; arguments. A multiclass can inherit from other multiclasses, which causes; the other multiclasses to be expanded and contribute to the record; definitions in the inheriting multiclass. The body of the multiclass; contains a series of statements that define records, using :token:`Def` and; :token:`Defm`. In addition, :token:`Defvar`, :token:`Foreach`, and; :token:`Let` statements can be used to factor out even more common elements.; The :token:`If` and :token:`Assert` statements can also be used. Also as with regular classes, the multiclass has the implicit template; argument ``NAME`` (see NAME_). When a named (non-anonymous) record is; defined in a multiclass and the record's name does not include a use of the; template argument ``NAME``, such a use is automatically *prepended*; to the name. That is, the following are equivalent inside a multiclass::. def Foo ...; def NAME # Foo ... The records defined in a multiclass are created when the multiclass is; ""instantiated"" or ""invoked"" by a ``defm`` statement outside the multiclass; definition. Each ``def`` statement in the multiclass ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:36993,Modifiability,inherit,inherit,36993,"eriting multiclass. The body of the multiclass; contains a series of statements that define records, using :token:`Def` and; :token:`Defm`. In addition, :token:`Defvar`, :token:`Foreach`, and; :token:`Let` statements can be used to factor out even more common elements.; The :token:`If` and :token:`Assert` statements can also be used. Also as with regular classes, the multiclass has the implicit template; argument ``NAME`` (see NAME_). When a named (non-anonymous) record is; defined in a multiclass and the record's name does not include a use of the; template argument ``NAME``, such a use is automatically *prepended*; to the name. That is, the following are equivalent inside a multiclass::. def Foo ...; def NAME # Foo ... The records defined in a multiclass are created when the multiclass is; ""instantiated"" or ""invoked"" by a ``defm`` statement outside the multiclass; definition. Each ``def`` statement in the multiclass produces a record. As; with top-level ``def`` statements, these definitions can inherit from; multiple parent classes. See `Examples: multiclasses and defms`_ for examples. ``defm`` --- invoke multiclasses to define multiple records; -----------------------------------------------------------. Once multiclasses have been defined, you use the ``defm`` statement to; ""invoke"" them and process the multiple record definitions in those; multiclasses. Those record definitions are specified by ``def``; statements in the multiclasses, and indirectly by ``defm`` statements. .. productionlist::; Defm: ""defm"" [`NameValue`] `ParentClassList` "";"". The optional :token:`NameValue` is formed in the same way as the name of a; ``def``. The :token:`ParentClassList` is a colon followed by a list of at; least one multiclass and any number of regular classes. The multiclasses; must precede the regular classes. Note that the ``defm`` does not have a; body. This statement instantiates all the records defined in all the specified; multiclasses, either directly by ``def`` statem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:42179,Modifiability,inherit,inherit,42179,"multiclass. In the following example, the ``basic_s``; and ``basic_p`` multiclasses contain ``defm`` statements that refer to the; ``basic_r`` multiclass. The ``basic_r`` multiclass contains only ``def``; statements. .. code-block:: text. class Instruction <bits<4> opc, string Name> {; bits<4> opcode = opc;; string name = Name;; }. multiclass basic_r <bits<4> opc> {; def rr : Instruction<opc, ""rr"">;; def rm : Instruction<opc, ""rm"">;; }. multiclass basic_s <bits<4> opc> {; defm SS : basic_r<opc>;; defm SD : basic_r<opc>;; def X : Instruction<opc, ""x"">;; }. multiclass basic_p <bits<4> opc> {; defm PS : basic_r<opc>;; defm PD : basic_r<opc>;; def Y : Instruction<opc, ""y"">;; }. defm ADD : basic_s<0xf>, basic_p<0xf>;. The final ``defm`` creates the following records, five from the ``basic_s``; multiclass and five from the ``basic_p`` multiclass::. ADDSSrr, ADDSSrm; ADDSDrr, ADDSDrm; ADDX; ADDPSrr, ADDPSrm; ADDPDrr, ADDPDrm; ADDY. A ``defm`` statement, both at top level and in a multiclass, can inherit; from regular classes in addition to multiclasses. The rule is that the; regular classes must be listed after the multiclasses, and there must be at least; one multiclass. .. code-block:: text. class XD {; bits<4> Prefix = 11;; }; class XS {; bits<4> Prefix = 12;; }; class I <bits<4> op> {; bits<4> opcode = op;; }. multiclass R {; def rr : I<4>;; def rm : I<2>;; }. multiclass Y {; defm SS : R, XD; // First multiclass R, then regular class XD.; defm SD : R, XS;; }. defm Instr : Y;. This example will create four records, shown here in alphabetical order with; their fields. .. code-block:: text. def InstrSDrm {; bits<4> opcode = { 0, 0, 1, 0 };; bits<4> Prefix = { 1, 1, 0, 0 };; }. def InstrSDrr {; bits<4> opcode = { 0, 1, 0, 0 };; bits<4> Prefix = { 1, 1, 0, 0 };; }. def InstrSSrm {; bits<4> opcode = { 0, 0, 1, 0 };; bits<4> Prefix = { 1, 0, 1, 1 };; }. def InstrSSrr {; bits<4> opcode = { 0, 1, 0, 0 };; bits<4> Prefix = { 1, 0, 1, 1 };; }. It's also possible to use ``let`` st",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:44724,Modifiability,variab,variable,44724,";. ``defset`` --- create a definition set; --------------------------------------. The ``defset`` statement is used to collect a set of records into a global; list of records. .. productionlist::; Defset: ""defset"" `Type` `TokIdentifier` ""="" ""{"" `Statement`* ""}"". All records defined inside the braces via ``def`` and ``defm`` are defined; as usual, and they are also collected in a global list of the given name; (:token:`TokIdentifier`). The specified type must be ``list<``\ *class*\ ``>``, where *class* is some; record class. The ``defset`` statement establishes a scope for its; statements. It is an error to define a record in the scope of the; ``defset`` that is not of type *class*. The ``defset`` statement can be nested. The inner ``defset`` adds the; records to its own set, and all those records are also added to the outer; set. Anonymous records created inside initialization expressions using the; ``ClassID<...>`` syntax are not collected in the set. ``defvar`` --- define a variable; --------------------------------. A ``defvar`` statement defines a global variable. Its value can be used; throughout the statements that follow the definition. .. productionlist::; Defvar: ""defvar"" `TokIdentifier` ""="" `Value` "";"". The identifier on the left of the ``=`` is defined to be a global variable; whose value is given by the value expression on the right of the ``=``. The; type of the variable is automatically inferred. Once a variable has been defined, it cannot be set to another value. Variables defined in a top-level ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1);. Variables can also be defined with ``defvar`` in a record body. See; `Defvar in a Record Body`_ for more details. ``foreach`` --- iterate over a sequence of statements; -----------------------------------------------------. The ``foreach`` statement iterates ov",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:44808,Modifiability,variab,variable,44808,"----------------------. The ``defset`` statement is used to collect a set of records into a global; list of records. .. productionlist::; Defset: ""defset"" `Type` `TokIdentifier` ""="" ""{"" `Statement`* ""}"". All records defined inside the braces via ``def`` and ``defm`` are defined; as usual, and they are also collected in a global list of the given name; (:token:`TokIdentifier`). The specified type must be ``list<``\ *class*\ ``>``, where *class* is some; record class. The ``defset`` statement establishes a scope for its; statements. It is an error to define a record in the scope of the; ``defset`` that is not of type *class*. The ``defset`` statement can be nested. The inner ``defset`` adds the; records to its own set, and all those records are also added to the outer; set. Anonymous records created inside initialization expressions using the; ``ClassID<...>`` syntax are not collected in the set. ``defvar`` --- define a variable; --------------------------------. A ``defvar`` statement defines a global variable. Its value can be used; throughout the statements that follow the definition. .. productionlist::; Defvar: ""defvar"" `TokIdentifier` ""="" `Value` "";"". The identifier on the left of the ``=`` is defined to be a global variable; whose value is given by the value expression on the right of the ``=``. The; type of the variable is automatically inferred. Once a variable has been defined, it cannot be set to another value. Variables defined in a top-level ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1);. Variables can also be defined with ``defvar`` in a record body. See; `Defvar in a Record Body`_ for more details. ``foreach`` --- iterate over a sequence of statements; -----------------------------------------------------. The ``foreach`` statement iterates over a series of statements, varying a; variable over a seque",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:45032,Modifiability,variab,variable,45032,"`def`` and ``defm`` are defined; as usual, and they are also collected in a global list of the given name; (:token:`TokIdentifier`). The specified type must be ``list<``\ *class*\ ``>``, where *class* is some; record class. The ``defset`` statement establishes a scope for its; statements. It is an error to define a record in the scope of the; ``defset`` that is not of type *class*. The ``defset`` statement can be nested. The inner ``defset`` adds the; records to its own set, and all those records are also added to the outer; set. Anonymous records created inside initialization expressions using the; ``ClassID<...>`` syntax are not collected in the set. ``defvar`` --- define a variable; --------------------------------. A ``defvar`` statement defines a global variable. Its value can be used; throughout the statements that follow the definition. .. productionlist::; Defvar: ""defvar"" `TokIdentifier` ""="" `Value` "";"". The identifier on the left of the ``=`` is defined to be a global variable; whose value is given by the value expression on the right of the ``=``. The; type of the variable is automatically inferred. Once a variable has been defined, it cannot be set to another value. Variables defined in a top-level ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1);. Variables can also be defined with ``defvar`` in a record body. See; `Defvar in a Record Body`_ for more details. ``foreach`` --- iterate over a sequence of statements; -----------------------------------------------------. The ``foreach`` statement iterates over a series of statements, varying a; variable over a sequence of values. .. productionlist::; Foreach: ""foreach"" `ForeachIterator` ""in"" ""{"" `Statement`* ""}""; :| ""foreach"" `ForeachIterator` ""in"" `Statement`; ForeachIterator: `TokIdentifier` ""="" (""{"" `RangeList` ""}"" | `RangePiece` | `Value`). The body of t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:45131,Modifiability,variab,variable,45131," name; (:token:`TokIdentifier`). The specified type must be ``list<``\ *class*\ ``>``, where *class* is some; record class. The ``defset`` statement establishes a scope for its; statements. It is an error to define a record in the scope of the; ``defset`` that is not of type *class*. The ``defset`` statement can be nested. The inner ``defset`` adds the; records to its own set, and all those records are also added to the outer; set. Anonymous records created inside initialization expressions using the; ``ClassID<...>`` syntax are not collected in the set. ``defvar`` --- define a variable; --------------------------------. A ``defvar`` statement defines a global variable. Its value can be used; throughout the statements that follow the definition. .. productionlist::; Defvar: ""defvar"" `TokIdentifier` ""="" `Value` "";"". The identifier on the left of the ``=`` is defined to be a global variable; whose value is given by the value expression on the right of the ``=``. The; type of the variable is automatically inferred. Once a variable has been defined, it cannot be set to another value. Variables defined in a top-level ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1);. Variables can also be defined with ``defvar`` in a record body. See; `Defvar in a Record Body`_ for more details. ``foreach`` --- iterate over a sequence of statements; -----------------------------------------------------. The ``foreach`` statement iterates over a series of statements, varying a; variable over a sequence of values. .. productionlist::; Foreach: ""foreach"" `ForeachIterator` ""in"" ""{"" `Statement`* ""}""; :| ""foreach"" `ForeachIterator` ""in"" `Statement`; ForeachIterator: `TokIdentifier` ""="" (""{"" `RangeList` ""}"" | `RangePiece` | `Value`). The body of the ``foreach`` is a series of statements in braces or a; single statement with no braces. The stateme",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:45174,Modifiability,variab,variable,45174,"`list<``\ *class*\ ``>``, where *class* is some; record class. The ``defset`` statement establishes a scope for its; statements. It is an error to define a record in the scope of the; ``defset`` that is not of type *class*. The ``defset`` statement can be nested. The inner ``defset`` adds the; records to its own set, and all those records are also added to the outer; set. Anonymous records created inside initialization expressions using the; ``ClassID<...>`` syntax are not collected in the set. ``defvar`` --- define a variable; --------------------------------. A ``defvar`` statement defines a global variable. Its value can be used; throughout the statements that follow the definition. .. productionlist::; Defvar: ""defvar"" `TokIdentifier` ""="" `Value` "";"". The identifier on the left of the ``=`` is defined to be a global variable; whose value is given by the value expression on the right of the ``=``. The; type of the variable is automatically inferred. Once a variable has been defined, it cannot be set to another value. Variables defined in a top-level ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1);. Variables can also be defined with ``defvar`` in a record body. See; `Defvar in a Record Body`_ for more details. ``foreach`` --- iterate over a sequence of statements; -----------------------------------------------------. The ``foreach`` statement iterates over a series of statements, varying a; variable over a sequence of values. .. productionlist::; Foreach: ""foreach"" `ForeachIterator` ""in"" ""{"" `Statement`* ""}""; :| ""foreach"" `ForeachIterator` ""in"" `Statement`; ForeachIterator: `TokIdentifier` ""="" (""{"" `RangeList` ""}"" | `RangePiece` | `Value`). The body of the ``foreach`` is a series of statements in braces or a; single statement with no braces. The statements are re-evaluated once for; each value in the range list, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:45771,Modifiability,variab,variable,45771,"----------------. A ``defvar`` statement defines a global variable. Its value can be used; throughout the statements that follow the definition. .. productionlist::; Defvar: ""defvar"" `TokIdentifier` ""="" `Value` "";"". The identifier on the left of the ``=`` is defined to be a global variable; whose value is given by the value expression on the right of the ``=``. The; type of the variable is automatically inferred. Once a variable has been defined, it cannot be set to another value. Variables defined in a top-level ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1);. Variables can also be defined with ``defvar`` in a record body. See; `Defvar in a Record Body`_ for more details. ``foreach`` --- iterate over a sequence of statements; -----------------------------------------------------. The ``foreach`` statement iterates over a series of statements, varying a; variable over a sequence of values. .. productionlist::; Foreach: ""foreach"" `ForeachIterator` ""in"" ""{"" `Statement`* ""}""; :| ""foreach"" `ForeachIterator` ""in"" `Statement`; ForeachIterator: `TokIdentifier` ""="" (""{"" `RangeList` ""}"" | `RangePiece` | `Value`). The body of the ``foreach`` is a series of statements in braces or a; single statement with no braces. The statements are re-evaluated once for; each value in the range list, range piece, or single value. On each; iteration, the :token:`TokIdentifier` variable is set to the value and can; be used in the statements. The statement list establishes an inner scope. Variables local to a; ``foreach`` go out of scope at the end of each loop iteration, so their; values do not carry over from one iteration to the next. Foreach loops may; be nested. .. Note that the productions involving RangeList and RangePiece have precedence; over the more generic value parsing based on the first token. .. code-block:: text. foreach i = [0",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:46278,Modifiability,variab,variable,46278,"t of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1);. Variables can also be defined with ``defvar`` in a record body. See; `Defvar in a Record Body`_ for more details. ``foreach`` --- iterate over a sequence of statements; -----------------------------------------------------. The ``foreach`` statement iterates over a series of statements, varying a; variable over a sequence of values. .. productionlist::; Foreach: ""foreach"" `ForeachIterator` ""in"" ""{"" `Statement`* ""}""; :| ""foreach"" `ForeachIterator` ""in"" `Statement`; ForeachIterator: `TokIdentifier` ""="" (""{"" `RangeList` ""}"" | `RangePiece` | `Value`). The body of the ``foreach`` is a series of statements in braces or a; single statement with no braces. The statements are re-evaluated once for; each value in the range list, range piece, or single value. On each; iteration, the :token:`TokIdentifier` variable is set to the value and can; be used in the statements. The statement list establishes an inner scope. Variables local to a; ``foreach`` go out of scope at the end of each loop iteration, so their; values do not carry over from one iteration to the next. Foreach loops may; be nested. .. Note that the productions involving RangeList and RangePiece have precedence; over the more generic value parsing based on the first token. .. code-block:: text. foreach i = [0, 1, 2, 3] in {; def R#i : Register<...>;; def F#i : Register<...>;; }. This loop defines records named ``R0``, ``R1``, ``R2``, and ``R3``, along; with ``F0``, ``F1``, ``F2``, and ``F3``. ``dump`` --- print messages to stderr; -------------------------------------. A ``dump`` statement prints the input string to standard error; output. It is intended for debugging purpose. * At top level, the message is printed immediately. * Within a record/class/multiclass, `dump` gets evaluated at each; instantiation point of the containing re",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:48648,Modifiability,variab,variables,48648," of an expression. .. productionlist::; If: ""if"" `Value` ""then"" `IfBody`; :| ""if"" `Value` ""then"" `IfBody` ""else"" `IfBody`; IfBody: ""{"" `Statement`* ""}"" | `Statement`. The value expression is evaluated. If it evaluates to true (in the same; sense used by the bang operators), then the statements following the; ``then`` reserved word are processed. Otherwise, if there is an ``else``; reserved word, the statements following the ``else`` are processed. If the; value is false and there is no ``else`` arm, no statements are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and recor",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49662,Modifiability,inherit,inherited,49662,"e bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` dataty",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49712,Modifiability,inherit,inherit,49712,"e bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` dataty",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:52127,Modifiability,variab,variables,52127,"==========================================; Format Meaning; ====================== =============================================; *value* argument value; *value*\ ``:``\ *name* argument value and associated name; *name* argument name with unset (uninitialized) value; ====================== =============================================. The *value* can be any TableGen value. The *name*, if present, must be a; :token:`TokVarName`, which starts with a dollar sign (``$``). The purpose of; a name is to tag an operator or argument in a DAG with a particular meaning,; or to associate an argument in one DAG with a like-named argument in another; DAG. The following bang operators are useful for working with DAGs:; ``!con``, ``!dag``, ``!empty``, ``!foreach``, ``!getdagarg``, ``!getdagname``,; ``!getdagop``, ``!setdagarg``, ``!setdagname``, ``!setdagop``, ``!size``. Defvar in a record body; -----------------------. In addition to defining global variables, the ``defvar`` statement can; be used inside the :token:`Body` of a class or record definition to define; local variables. Template arguments of ``class`` or ``multiclass`` can be; used in the value expression. The scope of the variable extends from the; ``defvar`` statement to the end of the body. It cannot be set to a different; value within its scope. The ``defvar`` statement can also be used in the statement; list of a ``foreach``, which establishes a scope. A variable named ``V`` in an inner scope shadows (hides) any variables ``V``; in outer scopes. In particular, there are several cases:. * ``V`` in a record body shadows a global ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:52250,Modifiability,variab,variables,52250,"==========================================; Format Meaning; ====================== =============================================; *value* argument value; *value*\ ``:``\ *name* argument value and associated name; *name* argument name with unset (uninitialized) value; ====================== =============================================. The *value* can be any TableGen value. The *name*, if present, must be a; :token:`TokVarName`, which starts with a dollar sign (``$``). The purpose of; a name is to tag an operator or argument in a DAG with a particular meaning,; or to associate an argument in one DAG with a like-named argument in another; DAG. The following bang operators are useful for working with DAGs:; ``!con``, ``!dag``, ``!empty``, ``!foreach``, ``!getdagarg``, ``!getdagname``,; ``!getdagop``, ``!setdagarg``, ``!setdagname``, ``!setdagop``, ``!size``. Defvar in a record body; -----------------------. In addition to defining global variables, the ``defvar`` statement can; be used inside the :token:`Body` of a class or record definition to define; local variables. Template arguments of ``class`` or ``multiclass`` can be; used in the value expression. The scope of the variable extends from the; ``defvar`` statement to the end of the body. It cannot be set to a different; value within its scope. The ``defvar`` statement can also be used in the statement; list of a ``foreach``, which establishes a scope. A variable named ``V`` in an inner scope shadows (hides) any variables ``V``; in outer scopes. In particular, there are several cases:. * ``V`` in a record body shadows a global ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:52366,Modifiability,variab,variable,52366,"ame* argument name with unset (uninitialized) value; ====================== =============================================. The *value* can be any TableGen value. The *name*, if present, must be a; :token:`TokVarName`, which starts with a dollar sign (``$``). The purpose of; a name is to tag an operator or argument in a DAG with a particular meaning,; or to associate an argument in one DAG with a like-named argument in another; DAG. The following bang operators are useful for working with DAGs:; ``!con``, ``!dag``, ``!empty``, ``!foreach``, ``!getdagarg``, ``!getdagname``,; ``!getdagop``, ``!setdagarg``, ``!setdagname``, ``!setdagop``, ``!size``. Defvar in a record body; -----------------------. In addition to defining global variables, the ``defvar`` statement can; be used inside the :token:`Body` of a class or record definition to define; local variables. Template arguments of ``class`` or ``multiclass`` can be; used in the value expression. The scope of the variable extends from the; ``defvar`` statement to the end of the body. It cannot be set to a different; value within its scope. The ``defvar`` statement can also be used in the statement; list of a ``foreach``, which establishes a scope. A variable named ``V`` in an inner scope shadows (hides) any variables ``V``; in outer scopes. In particular, there are several cases:. * ``V`` in a record body shadows a global ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:52375,Modifiability,extend,extends,52375,"ame* argument name with unset (uninitialized) value; ====================== =============================================. The *value* can be any TableGen value. The *name*, if present, must be a; :token:`TokVarName`, which starts with a dollar sign (``$``). The purpose of; a name is to tag an operator or argument in a DAG with a particular meaning,; or to associate an argument in one DAG with a like-named argument in another; DAG. The following bang operators are useful for working with DAGs:; ``!con``, ``!dag``, ``!empty``, ``!foreach``, ``!getdagarg``, ``!getdagname``,; ``!getdagop``, ``!setdagarg``, ``!setdagname``, ``!setdagop``, ``!size``. Defvar in a record body; -----------------------. In addition to defining global variables, the ``defvar`` statement can; be used inside the :token:`Body` of a class or record definition to define; local variables. Template arguments of ``class`` or ``multiclass`` can be; used in the value expression. The scope of the variable extends from the; ``defvar`` statement to the end of the body. It cannot be set to a different; value within its scope. The ``defvar`` statement can also be used in the statement; list of a ``foreach``, which establishes a scope. A variable named ``V`` in an inner scope shadows (hides) any variables ``V``; in outer scopes. In particular, there are several cases:. * ``V`` in a record body shadows a global ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:52607,Modifiability,variab,variable,52607,"The purpose of; a name is to tag an operator or argument in a DAG with a particular meaning,; or to associate an argument in one DAG with a like-named argument in another; DAG. The following bang operators are useful for working with DAGs:; ``!con``, ``!dag``, ``!empty``, ``!foreach``, ``!getdagarg``, ``!getdagname``,; ``!getdagop``, ``!setdagarg``, ``!setdagname``, ``!setdagop``, ``!size``. Defvar in a record body; -----------------------. In addition to defining global variables, the ``defvar`` statement can; be used inside the :token:`Body` of a class or record definition to define; local variables. Template arguments of ``class`` or ``multiclass`` can be; used in the value expression. The scope of the variable extends from the; ``defvar`` statement to the end of the body. It cannot be set to a different; value within its scope. The ``defvar`` statement can also be used in the statement; list of a ``foreach``, which establishes a scope. A variable named ``V`` in an inner scope shadows (hides) any variables ``V``; in outer scopes. In particular, there are several cases:. * ``V`` in a record body shadows a global ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through the same steps. 1. Build the record name (:token:`NameValue`) and create an empty record. 2. Parse the parent classes in the :token:`ParentClassList` from left to; right, visiting each parent class's ancestor classes from top to bottom. a. Add the fields from",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:52666,Modifiability,variab,variables,52666,"The purpose of; a name is to tag an operator or argument in a DAG with a particular meaning,; or to associate an argument in one DAG with a like-named argument in another; DAG. The following bang operators are useful for working with DAGs:; ``!con``, ``!dag``, ``!empty``, ``!foreach``, ``!getdagarg``, ``!getdagname``,; ``!getdagop``, ``!setdagarg``, ``!setdagname``, ``!setdagop``, ``!size``. Defvar in a record body; -----------------------. In addition to defining global variables, the ``defvar`` statement can; be used inside the :token:`Body` of a class or record definition to define; local variables. Template arguments of ``class`` or ``multiclass`` can be; used in the value expression. The scope of the variable extends from the; ``defvar`` statement to the end of the body. It cannot be set to a different; value within its scope. The ``defvar`` statement can also be used in the statement; list of a ``foreach``, which establishes a scope. A variable named ``V`` in an inner scope shadows (hides) any variables ``V``; in outer scopes. In particular, there are several cases:. * ``V`` in a record body shadows a global ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through the same steps. 1. Build the record name (:token:`NameValue`) and create an empty record. 2. Parse the parent classes in the :token:`ParentClassList` from left to; right, visiting each parent class's ancestor classes from top to bottom. a. Add the fields from",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:53791,Modifiability,inherit,inherited,53791,"lobal ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through the same steps. 1. Build the record name (:token:`NameValue`) and create an empty record. 2. Parse the parent classes in the :token:`ParentClassList` from left to; right, visiting each parent class's ancestor classes from top to bottom. a. Add the fields from the parent class to the record.; b. Substitute the template arguments into those fields.; c. Add the parent class to the record's list of inherited classes. 3. Apply any top-level ``let`` bindings to the record. Recall that top-level; bindings only apply to inherited fields. 4. Parse the body of the record. * Add any fields to the record.; * Modify the values of fields according to local ``let`` statements.; * Define any ``defvar`` variables. 5. Make a pass over all the fields to resolve any inter-field references. 6. Add the record to the final record list. Because references between fields are resolved (step 5) after ``let`` bindings are; applied (step 3), the ``let`` statement has unusual power. For example:. .. code-block:: text. class C <int x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:53911,Modifiability,inherit,inherited,53911,"`V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through the same steps. 1. Build the record name (:token:`NameValue`) and create an empty record. 2. Parse the parent classes in the :token:`ParentClassList` from left to; right, visiting each parent class's ancestor classes from top to bottom. a. Add the fields from the parent class to the record.; b. Substitute the template arguments into those fields.; c. Add the parent class to the record's list of inherited classes. 3. Apply any top-level ``let`` bindings to the record. Recall that top-level; bindings only apply to inherited fields. 4. Parse the body of the record. * Add any fields to the record.; * Modify the values of fields according to local ``let`` statements.; * Define any ``defvar`` variables. 5. Make a pass over all the fields to resolve any inter-field references. 6. Add the record to the final record list. Because references between fields are resolved (step 5) after ``let`` bindings are; applied (step 3), the ``let`` statement has unusual power. For example:. .. code-block:: text. class C <int x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }; def rec2 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }. ``Yplus1`` is 11 because the ``let Y`` is",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:54089,Modifiability,variab,variables,54089,"ation, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through the same steps. 1. Build the record name (:token:`NameValue`) and create an empty record. 2. Parse the parent classes in the :token:`ParentClassList` from left to; right, visiting each parent class's ancestor classes from top to bottom. a. Add the fields from the parent class to the record.; b. Substitute the template arguments into those fields.; c. Add the parent class to the record's list of inherited classes. 3. Apply any top-level ``let`` bindings to the record. Recall that top-level; bindings only apply to inherited fields. 4. Parse the body of the record. * Add any fields to the record.; * Modify the values of fields according to local ``let`` statements.; * Define any ``defvar`` variables. 5. Make a pass over all the fields to resolve any inter-field references. 6. Add the record to the final record list. Because references between fields are resolved (step 5) after ``let`` bindings are; applied (step 3), the ``let`` statement has unusual power. For example:. .. code-block:: text. class C <int x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }; def rec2 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }. ``Yplus1`` is 11 because the ``let Y`` is performed before the ``!add(Y,; 1)`` is resolved. Use this power wisely. Using Classes as Subroutines; ============================. As described in `Simple values`_, a class can be in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:55191,Modifiability,inherit,inheriting,55191," Add the record to the final record list. Because references between fields are resolved (step 5) after ``let`` bindings are; applied (step 3), the ``let`` statement has unusual power. For example:. .. code-block:: text. class C <int x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }; def rec2 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }. ``Yplus1`` is 11 because the ``let Y`` is performed before the ``!add(Y,; 1)`` is resolved. Use this power wisely. Using Classes as Subroutines; ============================. As described in `Simple values`_, a class can be invoked in an expression; and passed template arguments. This causes TableGen to create a new anonymous; record inheriting from that class. As usual, the record receives all the; fields defined in the class. This feature can be employed as a simple subroutine facility. The class can; use the template arguments to define various variables and fields, which end; up in the anonymous record. Those fields can then be retrieved in the; expression invoking the class as follows. Assume that the field ``ret``; contains the final value of the subroutine. .. code-block:: text. int Result = ... CalcValue<arg>.ret ...;. The ``CalcValue`` class is invoked with the template argument ``arg``. It; calculates a value for the ``ret`` field, which is then retrieved at the; ""point of call"" in the initialization for the Result field. The anonymous; record created in this example serves no other purpose than to carry the; result value. Here is a practical example. The class ``isValidSize`` determines whether a; specified number of bytes represents a valid data size. The bit ``ret`` is; set appropriately.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:55409,Modifiability,variab,variables,55409,"t x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }; def rec2 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }. ``Yplus1`` is 11 because the ``let Y`` is performed before the ``!add(Y,; 1)`` is resolved. Use this power wisely. Using Classes as Subroutines; ============================. As described in `Simple values`_, a class can be invoked in an expression; and passed template arguments. This causes TableGen to create a new anonymous; record inheriting from that class. As usual, the record receives all the; fields defined in the class. This feature can be employed as a simple subroutine facility. The class can; use the template arguments to define various variables and fields, which end; up in the anonymous record. Those fields can then be retrieved in the; expression invoking the class as follows. Assume that the field ``ret``; contains the final value of the subroutine. .. code-block:: text. int Result = ... CalcValue<arg>.ret ...;. The ``CalcValue`` class is invoked with the template argument ``arg``. It; calculates a value for the ``ret`` field, which is then retrieved at the; ""point of call"" in the initialization for the Result field. The anonymous; record created in this example serves no other purpose than to carry the; result value. Here is a practical example. The class ``isValidSize`` determines whether a; specified number of bytes represents a valid data size. The bit ``ret`` is; set appropriately. The field ``ValidSize`` obtains its initial value by; invoking ``isValidSize`` with the data size and retrieving the ``ret`` field; from the resulting anonymous record. .. code-block:: text. class isValidSize<int size> {; bit ret =",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:63154,Modifiability,variab,variable,63154,")``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of *a* by *b*, and produces the quotient.; Division by 0 produces an error. Division of INT64_MIN by -1 produces an error. ``!empty(``\ *a*\ ``)``; This operator produces 1 if the string, list, or DAG *a* is empty; 0 otherwise.; A dag is empty if it has no arguments; the operator does not count. ``!eq(`` *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``, or; record values. Use ``!cast<string>`` to compare other types of objects. ``!exists<``\ *type*\ ``>(``\ *name*\ ``)``; This operator produces 1 if a record of the given *type* whose name is *name*; exists; 0 otherwise. *name* should be of type *string*. ``!filter(``\ *var*\ ``,`` *list*\ ``,`` *predicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably; refers to *var*. The predicate must; produce a boolean value (``bit``, ``bits``, or ``int``). The value is; interpreted as with ``!if``:; if the value is 0, the element is not included in the new list. If the value; is anything else, the element is included. ``!find(``\ *string1*\ ``,`` *string2*\ [``,`` *start*]\ ``)``; This operator searches for *string2* in *string1* and produces its; position. The starting position of the search may be specified by *start*,; which can range between 0 and the length of *string1*; the default is 0.; If the string is not found, the result is -1. ``!foldl(``\ *init*\ ``,`` *list*\ ``,`` *acc*\ ``,`` *var*\ ``,`` *expr*\ ``)``; This operator performs a left-fold over the items in *list*. The; variable *acc* acts as the accumulator and is initialized to *init*.; The variable *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:63986,Modifiability,variab,variable,63986,"edicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably; refers to *var*. The predicate must; produce a boolean value (``bit``, ``bits``, or ``int``). The value is; interpreted as with ``!if``:; if the value is 0, the element is not included in the new list. If the value; is anything else, the element is included. ``!find(``\ *string1*\ ``,`` *string2*\ [``,`` *start*]\ ``)``; This operator searches for *string2* in *string1* and produces its; position. The starting position of the search may be specified by *start*,; which can range between 0 and the length of *string1*; the default is 0.; If the string is not found, the result is -1. ``!foldl(``\ *init*\ ``,`` *list*\ ``,`` *acc*\ ``,`` *var*\ ``,`` *expr*\ ``)``; This operator performs a left-fold over the items in *list*. The; variable *acc* acts as the accumulator and is initialized to *init*.; The variable *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* and; *var* to calculate the accumulated value, which ``!foldl`` stores back in; *acc*. The type of *acc* is the same as *init*; the type of *var* is the; same as the elements of *list*; *expr* must have the same type as *init*. The following example computes the total of the ``Number`` field in the; list of records in ``RecList``::. int x = !foldl(0, RecList, total, rec, !add(total, rec.Number));. If your goal is to filter the list and produce a new list that includes only; some of the elements, see ``!filter``. ``!foreach(``\ *var*\ ``,`` *sequence*\ ``,`` *expr*\ ``)``; This operator creates a new ``list``/``dag`` in which each element is a; function of the corresponding element in the *sequence* ``list``/``dag``.; To perform the function, TableGen binds the variable *var* to an element; and then evaluates the expres",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:64060,Modifiability,variab,variable,64060,"the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably; refers to *var*. The predicate must; produce a boolean value (``bit``, ``bits``, or ``int``). The value is; interpreted as with ``!if``:; if the value is 0, the element is not included in the new list. If the value; is anything else, the element is included. ``!find(``\ *string1*\ ``,`` *string2*\ [``,`` *start*]\ ``)``; This operator searches for *string2* in *string1* and produces its; position. The starting position of the search may be specified by *start*,; which can range between 0 and the length of *string1*; the default is 0.; If the string is not found, the result is -1. ``!foldl(``\ *init*\ ``,`` *list*\ ``,`` *acc*\ ``,`` *var*\ ``,`` *expr*\ ``)``; This operator performs a left-fold over the items in *list*. The; variable *acc* acts as the accumulator and is initialized to *init*.; The variable *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* and; *var* to calculate the accumulated value, which ``!foldl`` stores back in; *acc*. The type of *acc* is the same as *init*; the type of *var* is the; same as the elements of *list*; *expr* must have the same type as *init*. The following example computes the total of the ``Number`` field in the; list of records in ``RecList``::. int x = !foldl(0, RecList, total, rec, !add(total, rec.Number));. If your goal is to filter the list and produce a new list that includes only; some of the elements, see ``!filter``. ``!foreach(``\ *var*\ ``,`` *sequence*\ ``,`` *expr*\ ``)``; This operator creates a new ``list``/``dag`` in which each element is a; function of the corresponding element in the *sequence* ``list``/``dag``.; To perform the function, TableGen binds the variable *var* to an element; and then evaluates the expression. The expression presumably refers; to the variable *var* and c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:64958,Modifiability,variab,variable,64958,"ms in *list*. The; variable *acc* acts as the accumulator and is initialized to *init*.; The variable *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* and; *var* to calculate the accumulated value, which ``!foldl`` stores back in; *acc*. The type of *acc* is the same as *init*; the type of *var* is the; same as the elements of *list*; *expr* must have the same type as *init*. The following example computes the total of the ``Number`` field in the; list of records in ``RecList``::. int x = !foldl(0, RecList, total, rec, !add(total, rec.Number));. If your goal is to filter the list and produce a new list that includes only; some of the elements, see ``!filter``. ``!foreach(``\ *var*\ ``,`` *sequence*\ ``,`` *expr*\ ``)``; This operator creates a new ``list``/``dag`` in which each element is a; function of the corresponding element in the *sequence* ``list``/``dag``.; To perform the function, TableGen binds the variable *var* to an element; and then evaluates the expression. The expression presumably refers; to the variable *var* and calculates the result value. If you simply want to create a list of a certain length containing; the same value repeated multiple times, see ``!listsplat``. ``!ge(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is greater than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!getdagarg<``\ *type*\ ``>(``\ *dag*\ ``,``\ *key*\ ``)``; This operator retrieves the argument from the given *dag* node by the; specified *key*, which is either an integer index or a string name. If that; argument is not convertible to the specified *type*, ``?`` is returned. ``!getdagname(``\ *dag*\ ``,``\ *index*\ ``)``; This operator retrieves the argument name from the given *dag* node by the; specified *index*. If that argument has no name associated, ``?`` is; returned. ``!getdagop(``\ *dag*\ ``)`` --or-- ``!getdagop<``\ *type*\ ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:65064,Modifiability,variab,variable,65064,"le *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* and; *var* to calculate the accumulated value, which ``!foldl`` stores back in; *acc*. The type of *acc* is the same as *init*; the type of *var* is the; same as the elements of *list*; *expr* must have the same type as *init*. The following example computes the total of the ``Number`` field in the; list of records in ``RecList``::. int x = !foldl(0, RecList, total, rec, !add(total, rec.Number));. If your goal is to filter the list and produce a new list that includes only; some of the elements, see ``!filter``. ``!foreach(``\ *var*\ ``,`` *sequence*\ ``,`` *expr*\ ``)``; This operator creates a new ``list``/``dag`` in which each element is a; function of the corresponding element in the *sequence* ``list``/``dag``.; To perform the function, TableGen binds the variable *var* to an element; and then evaluates the expression. The expression presumably refers; to the variable *var* and calculates the result value. If you simply want to create a list of a certain length containing; the same value repeated multiple times, see ``!listsplat``. ``!ge(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is greater than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!getdagarg<``\ *type*\ ``>(``\ *dag*\ ``,``\ *key*\ ``)``; This operator retrieves the argument from the given *dag* node by the; specified *key*, which is either an integer index or a string name. If that; argument is not convertible to the specified *type*, ``?`` is returned. ``!getdagname(``\ *dag*\ ``,``\ *index*\ ``)``; This operator retrieves the argument name from the given *dag* node by the; specified *index*. If that argument has no name associated, ``?`` is; returned. ``!getdagop(``\ *dag*\ ``)`` --or-- ``!getdagop<``\ *type*\ ``>(``\ *dag*\ ``)``; This operator produces the operator of the given *dag* node.; Example: ``!getd",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:74179,Modifiability,variab,variable,74179,"string; is specified by *length*; if not specified, the rest of the string is; extracted. The *start* and *length* arguments must be integers. ``!tail(``\ *a*\ ``)``; This operator produces a new list with all the elements; of the list *a* except for the zeroth one. (See also ``!head``.). ``!tolower(``\ *a*\ ``)``; This operator converts a string input *a* to lower case. ``!toupper(``\ *a*\ ``)``; This operator converts a string input *a* to upper case. ``!xor(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise EXCLUSIVE OR on *a*, *b*, etc., and produces; the result. A logical XOR can be performed if all the arguments are either; 0 or 1. Appendix B: Paste Operator Examples; ===================================. Here is an example illustrating the use of the paste operator in record names. .. code-block:: text. defvar suffix = ""_suffstring"";; defvar some_ints = [0, 1, 2, 3];. def name # suffix {; }. foreach i = [1, 2] in {; def rec # i {; }; }. The first ``def`` does not use the value of the ``suffix`` variable. The; second def does use the value of the ``i`` iterator variable, because it is not a; global name. The following records are produced. .. code-block:: text. def namesuffix {; }; def rec1 {; }; def rec2 {; }. Here is a second example illustrating the paste operator in field value expressions. .. code-block:: text. def test {; string strings = suffix # suffix;; list<int> integers = some_ints # [4, 5, 6];; }. The ``strings`` field expression uses ``suffix`` on both sides of the paste; operator. It is evaluated normally on the left hand side, but taken verbatim; on the right hand side. The ``integers`` field expression uses the value of; the ``some_ints`` variable and a literal list. The following record is; produced. .. code-block:: text. def test {; string strings = ""_suffstringsuffix"";; list<int> ints = [0, 1, 2, 3, 4, 5, 6];; }. Appendix C: Sample Record; =========================. One target machine supported by LLVM is the Intel x86. The following",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:74246,Modifiability,variab,variable,74246,"ted. The *start* and *length* arguments must be integers. ``!tail(``\ *a*\ ``)``; This operator produces a new list with all the elements; of the list *a* except for the zeroth one. (See also ``!head``.). ``!tolower(``\ *a*\ ``)``; This operator converts a string input *a* to lower case. ``!toupper(``\ *a*\ ``)``; This operator converts a string input *a* to upper case. ``!xor(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise EXCLUSIVE OR on *a*, *b*, etc., and produces; the result. A logical XOR can be performed if all the arguments are either; 0 or 1. Appendix B: Paste Operator Examples; ===================================. Here is an example illustrating the use of the paste operator in record names. .. code-block:: text. defvar suffix = ""_suffstring"";; defvar some_ints = [0, 1, 2, 3];. def name # suffix {; }. foreach i = [1, 2] in {; def rec # i {; }; }. The first ``def`` does not use the value of the ``suffix`` variable. The; second def does use the value of the ``i`` iterator variable, because it is not a; global name. The following records are produced. .. code-block:: text. def namesuffix {; }; def rec1 {; }; def rec2 {; }. Here is a second example illustrating the paste operator in field value expressions. .. code-block:: text. def test {; string strings = suffix # suffix;; list<int> integers = some_ints # [4, 5, 6];; }. The ``strings`` field expression uses ``suffix`` on both sides of the paste; operator. It is evaluated normally on the left hand side, but taken verbatim; on the right hand side. The ``integers`` field expression uses the value of; the ``some_ints`` variable and a literal list. The following record is; produced. .. code-block:: text. def test {; string strings = ""_suffstringsuffix"";; list<int> ints = [0, 1, 2, 3, 4, 5, 6];; }. Appendix C: Sample Record; =========================. One target machine supported by LLVM is the Intel x86. The following output; from TableGen shows the record that is created to represent the 32-bit; regi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:74851,Modifiability,variab,variable,74851,"erator Examples; ===================================. Here is an example illustrating the use of the paste operator in record names. .. code-block:: text. defvar suffix = ""_suffstring"";; defvar some_ints = [0, 1, 2, 3];. def name # suffix {; }. foreach i = [1, 2] in {; def rec # i {; }; }. The first ``def`` does not use the value of the ``suffix`` variable. The; second def does use the value of the ``i`` iterator variable, because it is not a; global name. The following records are produced. .. code-block:: text. def namesuffix {; }; def rec1 {; }; def rec2 {; }. Here is a second example illustrating the paste operator in field value expressions. .. code-block:: text. def test {; string strings = suffix # suffix;; list<int> integers = some_ints # [4, 5, 6];; }. The ``strings`` field expression uses ``suffix`` on both sides of the paste; operator. It is evaluated normally on the left hand side, but taken verbatim; on the right hand side. The ``integers`` field expression uses the value of; the ``some_ints`` variable and a literal list. The following record is; produced. .. code-block:: text. def test {; string strings = ""_suffstringsuffix"";; list<int> ints = [0, 1, 2, 3, 4, 5, 6];; }. Appendix C: Sample Record; =========================. One target machine supported by LLVM is the Intel x86. The following output; from TableGen shows the record that is created to represent the 32-bit; register-to-register ADD instruction. .. code-block:: text. def ADD32rr {	// InstructionEncoding Instruction X86Inst I ITy Sched BinOpRR BinOpRR_RF; int Size = 0;; string DecoderNamespace = """";; list<Predicate> Predicates = [];; string DecoderMethod = """";; bit hasCompleteDecoder = 1;; string Namespace = ""X86"";; dag OutOperandList = (outs GR32:$dst);; dag InOperandList = (ins GR32:$src1, GR32:$src2);; string AsmString = ""add{l}	{$src2, $src1|$src1, $src2}"";; EncodingByHwMode EncodingInfos = ?;; list<dag> Pattern = [(set GR32:$dst, EFLAGS, (X86add_flag GR32:$src1, GR32:$src2))];; list<Regi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:78724,Modifiability,inherit,inherited,78724,"ts<8> Opcode = { 0, 0, 0, 0, 0, 0, 0, 1 };; Format Form = MRMDestReg;; bits<7> FormBits = { 0, 1, 0, 1, 0, 0, 0 };; ImmType ImmT = NoImm;; bit ForceDisassemble = 0;; OperandSize OpSize = OpSize32;; bits<2> OpSizeBits = { 1, 0 };; AddressSize AdSize = AdSizeX;; bits<2> AdSizeBits = { 0, 0 };; Prefix OpPrefix = NoPrfx;; bits<3> OpPrefixBits = { 0, 0, 0 };; Map OpMap = OB;; bits<3> OpMapBits = { 0, 0, 0 };; bit hasREX_WPrefix = 0;; FPFormat FPForm = NotFP;; bit hasLockPrefix = 0;; Domain ExeDomain = GenericDomain;; bit hasREPPrefix = 0;; Encoding OpEnc = EncNormal;; bits<2> OpEncBits = { 0, 0 };; bit HasVEX_W = 0;; bit IgnoresVEX_W = 0;; bit EVEX_W1_VEX_W0 = 0;; bit hasVEX_4V = 0;; bit hasVEX_L = 0;; bit ignoresVEX_L = 0;; bit hasEVEX_K = 0;; bit hasEVEX_Z = 0;; bit hasEVEX_L2 = 0;; bit hasEVEX_B = 0;; bits<3> CD8_Form = { 0, 0, 0 };; int CD8_EltSize = 0;; bit hasEVEX_RC = 0;; bit hasNoTrackPrefix = 0;; bits<7> VectSize = { 0, 0, 1, 0, 0, 0, 0 };; bits<7> CD8_Scale = { 0, 0, 0, 0, 0, 0, 0 };; string FoldGenRegForm = ?;; string EVEX2VEXOverride = ?;; bit isMemoryFoldable = 1;; bit notEVEX2VEXConvertible = 0;; }. On the first line of the record, you can see that the ``ADD32rr`` record; inherited from eight classes. Although the inheritance hierarchy is complex,; using parent classes is much simpler than specifying the 109 individual; fields for each instruction. Here is the code fragment used to define ``ADD32rr`` and multiple other; ``ADD`` instructions:. .. code-block:: text. defm ADD : ArithBinOp_RF<0x00, 0x02, 0x04, ""add"", MRM0r, MRM0m,; X86add_flag, add, 1, 1, 1>;. The ``defm`` statement tells TableGen that ``ArithBinOp_RF`` is a; multiclass, which contains multiple concrete record definitions that inherit; from ``BinOpRR_RF``. That class, in turn, inherits from ``BinOpRR``, which; inherits from ``ITy`` and ``Sched``, and so forth. The fields are inherited; from all the parent classes; for example, ``IsIndirectBranch`` is inherited; from the ``Instruction`` class.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:78767,Modifiability,inherit,inheritance,78767,"ts<8> Opcode = { 0, 0, 0, 0, 0, 0, 0, 1 };; Format Form = MRMDestReg;; bits<7> FormBits = { 0, 1, 0, 1, 0, 0, 0 };; ImmType ImmT = NoImm;; bit ForceDisassemble = 0;; OperandSize OpSize = OpSize32;; bits<2> OpSizeBits = { 1, 0 };; AddressSize AdSize = AdSizeX;; bits<2> AdSizeBits = { 0, 0 };; Prefix OpPrefix = NoPrfx;; bits<3> OpPrefixBits = { 0, 0, 0 };; Map OpMap = OB;; bits<3> OpMapBits = { 0, 0, 0 };; bit hasREX_WPrefix = 0;; FPFormat FPForm = NotFP;; bit hasLockPrefix = 0;; Domain ExeDomain = GenericDomain;; bit hasREPPrefix = 0;; Encoding OpEnc = EncNormal;; bits<2> OpEncBits = { 0, 0 };; bit HasVEX_W = 0;; bit IgnoresVEX_W = 0;; bit EVEX_W1_VEX_W0 = 0;; bit hasVEX_4V = 0;; bit hasVEX_L = 0;; bit ignoresVEX_L = 0;; bit hasEVEX_K = 0;; bit hasEVEX_Z = 0;; bit hasEVEX_L2 = 0;; bit hasEVEX_B = 0;; bits<3> CD8_Form = { 0, 0, 0 };; int CD8_EltSize = 0;; bit hasEVEX_RC = 0;; bit hasNoTrackPrefix = 0;; bits<7> VectSize = { 0, 0, 1, 0, 0, 0, 0 };; bits<7> CD8_Scale = { 0, 0, 0, 0, 0, 0, 0 };; string FoldGenRegForm = ?;; string EVEX2VEXOverride = ?;; bit isMemoryFoldable = 1;; bit notEVEX2VEXConvertible = 0;; }. On the first line of the record, you can see that the ``ADD32rr`` record; inherited from eight classes. Although the inheritance hierarchy is complex,; using parent classes is much simpler than specifying the 109 individual; fields for each instruction. Here is the code fragment used to define ``ADD32rr`` and multiple other; ``ADD`` instructions:. .. code-block:: text. defm ADD : ArithBinOp_RF<0x00, 0x02, 0x04, ""add"", MRM0r, MRM0m,; X86add_flag, add, 1, 1, 1>;. The ``defm`` statement tells TableGen that ``ArithBinOp_RF`` is a; multiclass, which contains multiple concrete record definitions that inherit; from ``BinOpRR_RF``. That class, in turn, inherits from ``BinOpRR``, which; inherits from ``ITy`` and ``Sched``, and so forth. The fields are inherited; from all the parent classes; for example, ``IsIndirectBranch`` is inherited; from the ``Instruction`` class.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:79252,Modifiability,inherit,inherit,79252,"ts<8> Opcode = { 0, 0, 0, 0, 0, 0, 0, 1 };; Format Form = MRMDestReg;; bits<7> FormBits = { 0, 1, 0, 1, 0, 0, 0 };; ImmType ImmT = NoImm;; bit ForceDisassemble = 0;; OperandSize OpSize = OpSize32;; bits<2> OpSizeBits = { 1, 0 };; AddressSize AdSize = AdSizeX;; bits<2> AdSizeBits = { 0, 0 };; Prefix OpPrefix = NoPrfx;; bits<3> OpPrefixBits = { 0, 0, 0 };; Map OpMap = OB;; bits<3> OpMapBits = { 0, 0, 0 };; bit hasREX_WPrefix = 0;; FPFormat FPForm = NotFP;; bit hasLockPrefix = 0;; Domain ExeDomain = GenericDomain;; bit hasREPPrefix = 0;; Encoding OpEnc = EncNormal;; bits<2> OpEncBits = { 0, 0 };; bit HasVEX_W = 0;; bit IgnoresVEX_W = 0;; bit EVEX_W1_VEX_W0 = 0;; bit hasVEX_4V = 0;; bit hasVEX_L = 0;; bit ignoresVEX_L = 0;; bit hasEVEX_K = 0;; bit hasEVEX_Z = 0;; bit hasEVEX_L2 = 0;; bit hasEVEX_B = 0;; bits<3> CD8_Form = { 0, 0, 0 };; int CD8_EltSize = 0;; bit hasEVEX_RC = 0;; bit hasNoTrackPrefix = 0;; bits<7> VectSize = { 0, 0, 1, 0, 0, 0, 0 };; bits<7> CD8_Scale = { 0, 0, 0, 0, 0, 0, 0 };; string FoldGenRegForm = ?;; string EVEX2VEXOverride = ?;; bit isMemoryFoldable = 1;; bit notEVEX2VEXConvertible = 0;; }. On the first line of the record, you can see that the ``ADD32rr`` record; inherited from eight classes. Although the inheritance hierarchy is complex,; using parent classes is much simpler than specifying the 109 individual; fields for each instruction. Here is the code fragment used to define ``ADD32rr`` and multiple other; ``ADD`` instructions:. .. code-block:: text. defm ADD : ArithBinOp_RF<0x00, 0x02, 0x04, ""add"", MRM0r, MRM0m,; X86add_flag, add, 1, 1, 1>;. The ``defm`` statement tells TableGen that ``ArithBinOp_RF`` is a; multiclass, which contains multiple concrete record definitions that inherit; from ``BinOpRR_RF``. That class, in turn, inherits from ``BinOpRR``, which; inherits from ``ITy`` and ``Sched``, and so forth. The fields are inherited; from all the parent classes; for example, ``IsIndirectBranch`` is inherited; from the ``Instruction`` class.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:79303,Modifiability,inherit,inherits,79303,"ts<8> Opcode = { 0, 0, 0, 0, 0, 0, 0, 1 };; Format Form = MRMDestReg;; bits<7> FormBits = { 0, 1, 0, 1, 0, 0, 0 };; ImmType ImmT = NoImm;; bit ForceDisassemble = 0;; OperandSize OpSize = OpSize32;; bits<2> OpSizeBits = { 1, 0 };; AddressSize AdSize = AdSizeX;; bits<2> AdSizeBits = { 0, 0 };; Prefix OpPrefix = NoPrfx;; bits<3> OpPrefixBits = { 0, 0, 0 };; Map OpMap = OB;; bits<3> OpMapBits = { 0, 0, 0 };; bit hasREX_WPrefix = 0;; FPFormat FPForm = NotFP;; bit hasLockPrefix = 0;; Domain ExeDomain = GenericDomain;; bit hasREPPrefix = 0;; Encoding OpEnc = EncNormal;; bits<2> OpEncBits = { 0, 0 };; bit HasVEX_W = 0;; bit IgnoresVEX_W = 0;; bit EVEX_W1_VEX_W0 = 0;; bit hasVEX_4V = 0;; bit hasVEX_L = 0;; bit ignoresVEX_L = 0;; bit hasEVEX_K = 0;; bit hasEVEX_Z = 0;; bit hasEVEX_L2 = 0;; bit hasEVEX_B = 0;; bits<3> CD8_Form = { 0, 0, 0 };; int CD8_EltSize = 0;; bit hasEVEX_RC = 0;; bit hasNoTrackPrefix = 0;; bits<7> VectSize = { 0, 0, 1, 0, 0, 0, 0 };; bits<7> CD8_Scale = { 0, 0, 0, 0, 0, 0, 0 };; string FoldGenRegForm = ?;; string EVEX2VEXOverride = ?;; bit isMemoryFoldable = 1;; bit notEVEX2VEXConvertible = 0;; }. On the first line of the record, you can see that the ``ADD32rr`` record; inherited from eight classes. Although the inheritance hierarchy is complex,; using parent classes is much simpler than specifying the 109 individual; fields for each instruction. Here is the code fragment used to define ``ADD32rr`` and multiple other; ``ADD`` instructions:. .. code-block:: text. defm ADD : ArithBinOp_RF<0x00, 0x02, 0x04, ""add"", MRM0r, MRM0m,; X86add_flag, add, 1, 1, 1>;. The ``defm`` statement tells TableGen that ``ArithBinOp_RF`` is a; multiclass, which contains multiple concrete record definitions that inherit; from ``BinOpRR_RF``. That class, in turn, inherits from ``BinOpRR``, which; inherits from ``ITy`` and ``Sched``, and so forth. The fields are inherited; from all the parent classes; for example, ``IsIndirectBranch`` is inherited; from the ``Instruction`` class.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:79337,Modifiability,inherit,inherits,79337,"ts<8> Opcode = { 0, 0, 0, 0, 0, 0, 0, 1 };; Format Form = MRMDestReg;; bits<7> FormBits = { 0, 1, 0, 1, 0, 0, 0 };; ImmType ImmT = NoImm;; bit ForceDisassemble = 0;; OperandSize OpSize = OpSize32;; bits<2> OpSizeBits = { 1, 0 };; AddressSize AdSize = AdSizeX;; bits<2> AdSizeBits = { 0, 0 };; Prefix OpPrefix = NoPrfx;; bits<3> OpPrefixBits = { 0, 0, 0 };; Map OpMap = OB;; bits<3> OpMapBits = { 0, 0, 0 };; bit hasREX_WPrefix = 0;; FPFormat FPForm = NotFP;; bit hasLockPrefix = 0;; Domain ExeDomain = GenericDomain;; bit hasREPPrefix = 0;; Encoding OpEnc = EncNormal;; bits<2> OpEncBits = { 0, 0 };; bit HasVEX_W = 0;; bit IgnoresVEX_W = 0;; bit EVEX_W1_VEX_W0 = 0;; bit hasVEX_4V = 0;; bit hasVEX_L = 0;; bit ignoresVEX_L = 0;; bit hasEVEX_K = 0;; bit hasEVEX_Z = 0;; bit hasEVEX_L2 = 0;; bit hasEVEX_B = 0;; bits<3> CD8_Form = { 0, 0, 0 };; int CD8_EltSize = 0;; bit hasEVEX_RC = 0;; bit hasNoTrackPrefix = 0;; bits<7> VectSize = { 0, 0, 1, 0, 0, 0, 0 };; bits<7> CD8_Scale = { 0, 0, 0, 0, 0, 0, 0 };; string FoldGenRegForm = ?;; string EVEX2VEXOverride = ?;; bit isMemoryFoldable = 1;; bit notEVEX2VEXConvertible = 0;; }. On the first line of the record, you can see that the ``ADD32rr`` record; inherited from eight classes. Although the inheritance hierarchy is complex,; using parent classes is much simpler than specifying the 109 individual; fields for each instruction. Here is the code fragment used to define ``ADD32rr`` and multiple other; ``ADD`` instructions:. .. code-block:: text. defm ADD : ArithBinOp_RF<0x00, 0x02, 0x04, ""add"", MRM0r, MRM0m,; X86add_flag, add, 1, 1, 1>;. The ``defm`` statement tells TableGen that ``ArithBinOp_RF`` is a; multiclass, which contains multiple concrete record definitions that inherit; from ``BinOpRR_RF``. That class, in turn, inherits from ``BinOpRR``, which; inherits from ``ITy`` and ``Sched``, and so forth. The fields are inherited; from all the parent classes; for example, ``IsIndirectBranch`` is inherited; from the ``Instruction`` class.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:79403,Modifiability,inherit,inherited,79403,"ts<8> Opcode = { 0, 0, 0, 0, 0, 0, 0, 1 };; Format Form = MRMDestReg;; bits<7> FormBits = { 0, 1, 0, 1, 0, 0, 0 };; ImmType ImmT = NoImm;; bit ForceDisassemble = 0;; OperandSize OpSize = OpSize32;; bits<2> OpSizeBits = { 1, 0 };; AddressSize AdSize = AdSizeX;; bits<2> AdSizeBits = { 0, 0 };; Prefix OpPrefix = NoPrfx;; bits<3> OpPrefixBits = { 0, 0, 0 };; Map OpMap = OB;; bits<3> OpMapBits = { 0, 0, 0 };; bit hasREX_WPrefix = 0;; FPFormat FPForm = NotFP;; bit hasLockPrefix = 0;; Domain ExeDomain = GenericDomain;; bit hasREPPrefix = 0;; Encoding OpEnc = EncNormal;; bits<2> OpEncBits = { 0, 0 };; bit HasVEX_W = 0;; bit IgnoresVEX_W = 0;; bit EVEX_W1_VEX_W0 = 0;; bit hasVEX_4V = 0;; bit hasVEX_L = 0;; bit ignoresVEX_L = 0;; bit hasEVEX_K = 0;; bit hasEVEX_Z = 0;; bit hasEVEX_L2 = 0;; bit hasEVEX_B = 0;; bits<3> CD8_Form = { 0, 0, 0 };; int CD8_EltSize = 0;; bit hasEVEX_RC = 0;; bit hasNoTrackPrefix = 0;; bits<7> VectSize = { 0, 0, 1, 0, 0, 0, 0 };; bits<7> CD8_Scale = { 0, 0, 0, 0, 0, 0, 0 };; string FoldGenRegForm = ?;; string EVEX2VEXOverride = ?;; bit isMemoryFoldable = 1;; bit notEVEX2VEXConvertible = 0;; }. On the first line of the record, you can see that the ``ADD32rr`` record; inherited from eight classes. Although the inheritance hierarchy is complex,; using parent classes is much simpler than specifying the 109 individual; fields for each instruction. Here is the code fragment used to define ``ADD32rr`` and multiple other; ``ADD`` instructions:. .. code-block:: text. defm ADD : ArithBinOp_RF<0x00, 0x02, 0x04, ""add"", MRM0r, MRM0m,; X86add_flag, add, 1, 1, 1>;. The ``defm`` statement tells TableGen that ``ArithBinOp_RF`` is a; multiclass, which contains multiple concrete record definitions that inherit; from ``BinOpRR_RF``. That class, in turn, inherits from ``BinOpRR``, which; inherits from ``ITy`` and ``Sched``, and so forth. The fields are inherited; from all the parent classes; for example, ``IsIndirectBranch`` is inherited; from the ``Instruction`` class.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:79480,Modifiability,inherit,inherited,79480,"ts<8> Opcode = { 0, 0, 0, 0, 0, 0, 0, 1 };; Format Form = MRMDestReg;; bits<7> FormBits = { 0, 1, 0, 1, 0, 0, 0 };; ImmType ImmT = NoImm;; bit ForceDisassemble = 0;; OperandSize OpSize = OpSize32;; bits<2> OpSizeBits = { 1, 0 };; AddressSize AdSize = AdSizeX;; bits<2> AdSizeBits = { 0, 0 };; Prefix OpPrefix = NoPrfx;; bits<3> OpPrefixBits = { 0, 0, 0 };; Map OpMap = OB;; bits<3> OpMapBits = { 0, 0, 0 };; bit hasREX_WPrefix = 0;; FPFormat FPForm = NotFP;; bit hasLockPrefix = 0;; Domain ExeDomain = GenericDomain;; bit hasREPPrefix = 0;; Encoding OpEnc = EncNormal;; bits<2> OpEncBits = { 0, 0 };; bit HasVEX_W = 0;; bit IgnoresVEX_W = 0;; bit EVEX_W1_VEX_W0 = 0;; bit hasVEX_4V = 0;; bit hasVEX_L = 0;; bit ignoresVEX_L = 0;; bit hasEVEX_K = 0;; bit hasEVEX_Z = 0;; bit hasEVEX_L2 = 0;; bit hasEVEX_B = 0;; bits<3> CD8_Form = { 0, 0, 0 };; int CD8_EltSize = 0;; bit hasEVEX_RC = 0;; bit hasNoTrackPrefix = 0;; bits<7> VectSize = { 0, 0, 1, 0, 0, 0, 0 };; bits<7> CD8_Scale = { 0, 0, 0, 0, 0, 0, 0 };; string FoldGenRegForm = ?;; string EVEX2VEXOverride = ?;; bit isMemoryFoldable = 1;; bit notEVEX2VEXConvertible = 0;; }. On the first line of the record, you can see that the ``ADD32rr`` record; inherited from eight classes. Although the inheritance hierarchy is complex,; using parent classes is much simpler than specifying the 109 individual; fields for each instruction. Here is the code fragment used to define ``ADD32rr`` and multiple other; ``ADD`` instructions:. .. code-block:: text. defm ADD : ArithBinOp_RF<0x00, 0x02, 0x04, ""add"", MRM0r, MRM0m,; X86add_flag, add, 1, 1, 1>;. The ``defm`` statement tells TableGen that ``ArithBinOp_RF`` is a; multiclass, which contains multiple concrete record definitions that inherit; from ``BinOpRR_RF``. That class, in turn, inherits from ``BinOpRR``, which; inherits from ``ITy`` and ``Sched``, and so forth. The fields are inherited; from all the parent classes; for example, ``IsIndirectBranch`` is inherited; from the ``Instruction`` class.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:10301,Performance,perform,perform,10301," !ne; : !not !or !range !repr !setdagarg; : !setdagname !setdagop !shl !size !sra; : !srl !strconcat !sub !subst !substr; : !tail !tolower !toupper !xor. The ``!cond`` operator has a slightly different; syntax compared to other bang operators, so it is defined separately:. .. productionlist::; CondOperator: !cond. See `Appendix A: Bang Operators`_ for a description of each bang operator. Include files; -------------. TableGen has an include mechanism. The content of the included file; lexically replaces the ``include`` directive and is then parsed as if it was; originally in the main file. .. productionlist::; IncludeDirective: ""include"" `TokString`. Portions of the main file and included files can be conditionalized using; preprocessor directives. .. productionlist::; PreprocessorDirective: ""#define"" | ""#ifdef"" | ""#ifndef"". Types; =====. The TableGen language is statically typed, using a simple but complete type; system. Types are used to check for errors, to perform implicit conversions,; and to help interface designers constrain the allowed input. Every value is; required to have an associated type. TableGen supports a mixture of low-level types (e.g., ``bit``) and; high-level types (e.g., ``dag``). This flexibility allows you to describe a; wide range of records conveniently and compactly. .. productionlist::; Type: ""bit"" | ""int"" | ""string"" | ""dag""; :| ""bits"" ""<"" `TokInteger` "">""; :| ""list"" ""<"" `Type` "">""; :| `ClassID`; ClassID: `TokIdentifier`. ``bit``; A ``bit`` is a boolean value that can be 0 or 1. ``int``; The ``int`` type represents a simple 64-bit integer value, such as 5 or; -42. ``string``; The ``string`` type represents an ordered sequence of characters of arbitrary; length. ``bits<``\ *n*\ ``>``; The ``bits`` type is a fixed-sized integer of arbitrary length *n* that; is treated as separate bits. These bits can be accessed individually.; A field of this type is useful for representing an instruction operation; code, register number, or address mode/reg",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:18407,Performance,perform,performs,18407,"``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Value` "":"" `Value`. The bang operators provide functions that are not available with the other; simple values. Except in the case of ``!cond``, a bang operator takes a list; of arguments enclosed in parentheses and performs some function on those; arguments, producing a value for that bang operator. The ``!cond`` operator; takes a list of pairs of arguments separated by colons. See `Appendix A:; Bang Operators`_ for a description of each bang operator. Suffixed values; ---------------. The :token:`SimpleValue` values described above can be specified with; certain suffixes. The purpose of a suffix is to obtain a subvalue of the; primary value. Here are the possible suffixes for some primary *value*. *value*\ ``{17}``; The final value is bit 17 of the integer *value* (note the braces). *value*\ ``{8...15}``; The final value is bits 8--15 of the integer *value*. The order of the; bits can be reversed by specifying ``{15...8}``. *value*\ ``[i]``; The final value is element `i` of the list *value* (note the brackets).; In other words, the brackets act as a subscripting operator on the list.; This is the case only when a single element is specified. *value*\ ``[i,]``; The final value is a lis",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:54897,Performance,perform,performed,54897,"y apply to inherited fields. 4. Parse the body of the record. * Add any fields to the record.; * Modify the values of fields according to local ``let`` statements.; * Define any ``defvar`` variables. 5. Make a pass over all the fields to resolve any inter-field references. 6. Add the record to the final record list. Because references between fields are resolved (step 5) after ``let`` bindings are; applied (step 3), the ``let`` statement has unusual power. For example:. .. code-block:: text. class C <int x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }; def rec2 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }. ``Yplus1`` is 11 because the ``let Y`` is performed before the ``!add(Y,; 1)`` is resolved. Use this power wisely. Using Classes as Subroutines; ============================. As described in `Simple values`_, a class can be invoked in an expression; and passed template arguments. This causes TableGen to create a new anonymous; record inheriting from that class. As usual, the record receives all the; fields defined in the class. This feature can be employed as a simple subroutine facility. The class can; use the template arguments to define various variables and fields, which end; up in the anonymous record. Those fields can then be retrieved in the; expression invoking the class as follows. Assume that the field ``ret``; contains the final value of the subroutine. .. code-block:: text. int Result = ... CalcValue<arg>.ret ...;. The ``CalcValue`` class is invoked with the template argument ``arg``. It; calculates a value for the ``ret`` field, which is then retrieved at the; ""point of call"" in the initialization for the Result fiel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:59490,Performance,perform,performed,59490,"tarted in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise AND on *a*, *b*, etc., and produces the; result. A logical AND can be performed if all the arguments are either; 0 or 1. ``!cast<``\ *type*\ ``>(``\ *a*\ ``)``; This operator performs a cast on *a* and produces the result.; If *a* is not a string, then a straightforward cast is performed, say; between an ``int`` and a ``bit``, or between record types. This allows; casting a record to a class. If a record is cast to ``string``, the; record's name is produced. If *a* is a string, then it is treated as a record name and looked up in; the list of all defined records. The resulting record is expected to be of; the specified *type*. For example, if ``!cast<``\ *type*\ ``>(``\ *name*\ ``)``; appears in a multiclass definition, or in a; class instantiated inside a multiclass definition, and the *name* does not; reference any template arguments of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:59595,Performance,perform,performs,59595,"file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise AND on *a*, *b*, etc., and produces the; result. A logical AND can be performed if all the arguments are either; 0 or 1. ``!cast<``\ *type*\ ``>(``\ *a*\ ``)``; This operator performs a cast on *a* and produces the result.; If *a* is not a string, then a straightforward cast is performed, say; between an ``int`` and a ``bit``, or between record types. This allows; casting a record to a class. If a record is cast to ``string``, the; record's name is produced. If *a* is a string, then it is treated as a record name and looked up in; the list of all defined records. The resulting record is expected to be of; the specified *type*. For example, if ``!cast<``\ *type*\ ``>(``\ *name*\ ``)``; appears in a multiclass definition, or in a; class instantiated inside a multiclass definition, and the *name* does not; reference any template arguments of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (or later, if the defm occurs in another; multiclass and template arguments of the inner ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:59699,Performance,perform,performed,59699,"vm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise AND on *a*, *b*, etc., and produces the; result. A logical AND can be performed if all the arguments are either; 0 or 1. ``!cast<``\ *type*\ ``>(``\ *a*\ ``)``; This operator performs a cast on *a* and produces the result.; If *a* is not a string, then a straightforward cast is performed, say; between an ``int`` and a ``bit``, or between record types. This allows; casting a record to a class. If a record is cast to ``string``, the; record's name is produced. If *a* is a string, then it is treated as a record name and looked up in; the list of all defined records. The resulting record is expected to be of; the specified *type*. For example, if ``!cast<``\ *type*\ ``>(``\ *name*\ ``)``; appears in a multiclass definition, or in a; class instantiated inside a multiclass definition, and the *name* does not; reference any template arguments of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (or later, if the defm occurs in another; multiclass and template arguments of the inner multiclass that are; referenced by *name* are substituted by values that themselves contain; references to template a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:62240,Performance,perform,performs,62240,"or tests *cond2* and returns *val2* if the result is; true. And so forth. An error is reported if no conditions are true. This example produces the sign word for an integer::. !cond(!lt(x, 0) : ""negative"", !eq(x, 0) : ""zero"", true : ""positive""). ``!dag(``\ *op*\ ``,`` *arguments*\ ``,`` *names*\ ``)``; This operator creates a DAG node with the given operator and; arguments. The *arguments* and *names* arguments must be lists; of equal length or uninitialized (``?``). The *names* argument; must be of type ``list<string>``. Due to limitations of the type system, *arguments* must be a list of items; of a common type. In practice, this means that they should either have the; same type or be records with a common parent class. Mixing ``dag`` and; non-``dag`` items is not possible. However, ``?`` can be used. Example: ``!dag(op, [a1, a2, ?], [""name1"", ""name2"", ""name3""])`` results in; ``(op a1-value:$name1, a2-value:$name2, ?:$name3)``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of *a* by *b*, and produces the quotient.; Division by 0 produces an error. Division of INT64_MIN by -1 produces an error. ``!empty(``\ *a*\ ``)``; This operator produces 1 if the string, list, or DAG *a* is empty; 0 otherwise.; A dag is empty if it has no arguments; the operator does not count. ``!eq(`` *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``, or; record values. Use ``!cast<string>`` to compare other types of objects. ``!exists<``\ *type*\ ``>(``\ *name*\ ``)``; This operator produces 1 if a record of the given *type* whose name is *name*; exists; 0 otherwise. *name* should be of type *string*. ``!filter(``\ *var*\ ``,`` *list*\ ``,`` *predicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:63112,Performance,perform,perform,63112,")``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of *a* by *b*, and produces the quotient.; Division by 0 produces an error. Division of INT64_MIN by -1 produces an error. ``!empty(``\ *a*\ ``)``; This operator produces 1 if the string, list, or DAG *a* is empty; 0 otherwise.; A dag is empty if it has no arguments; the operator does not count. ``!eq(`` *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``, or; record values. Use ``!cast<string>`` to compare other types of objects. ``!exists<``\ *type*\ ``>(``\ *name*\ ``)``; This operator produces 1 if a record of the given *type* whose name is *name*; exists; 0 otherwise. *name* should be of type *string*. ``!filter(``\ *var*\ ``,`` *list*\ ``,`` *predicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably; refers to *var*. The predicate must; produce a boolean value (``bit``, ``bits``, or ``int``). The value is; interpreted as with ``!if``:; if the value is 0, the element is not included in the new list. If the value; is anything else, the element is included. ``!find(``\ *string1*\ ``,`` *string2*\ [``,`` *start*]\ ``)``; This operator searches for *string2* in *string1* and produces its; position. The starting position of the search may be specified by *start*,; which can range between 0 and the length of *string1*; the default is 0.; If the string is not found, the result is -1. ``!foldl(``\ *init*\ ``,`` *list*\ ``,`` *acc*\ ``,`` *var*\ ``,`` *expr*\ ``)``; This operator performs a left-fold over the items in *list*. The; variable *acc* acts as the accumulator and is initialized to *init*.; The variable *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:63934,Performance,perform,performs,63934," *name*; exists; 0 otherwise. *name* should be of type *string*. ``!filter(``\ *var*\ ``,`` *list*\ ``,`` *predicate*\ ``)``. This operator creates a new ``list`` by filtering the elements in; *list*. To perform the filtering, TableGen binds the variable *var* to each; element and then evaluates the *predicate* expression, which presumably; refers to *var*. The predicate must; produce a boolean value (``bit``, ``bits``, or ``int``). The value is; interpreted as with ``!if``:; if the value is 0, the element is not included in the new list. If the value; is anything else, the element is included. ``!find(``\ *string1*\ ``,`` *string2*\ [``,`` *start*]\ ``)``; This operator searches for *string2* in *string1* and produces its; position. The starting position of the search may be specified by *start*,; which can range between 0 and the length of *string1*; the default is 0.; If the string is not found, the result is -1. ``!foldl(``\ *init*\ ``,`` *list*\ ``,`` *acc*\ ``,`` *var*\ ``,`` *expr*\ ``)``; This operator performs a left-fold over the items in *list*. The; variable *acc* acts as the accumulator and is initialized to *init*.; The variable *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* and; *var* to calculate the accumulated value, which ``!foldl`` stores back in; *acc*. The type of *acc* is the same as *init*; the type of *var* is the; same as the elements of *list*; *expr* must have the same type as *init*. The following example computes the total of the ``Number`` field in the; list of records in ``RecList``::. int x = !foldl(0, RecList, total, rec, !add(total, rec.Number));. If your goal is to filter the list and produce a new list that includes only; some of the elements, see ``!filter``. ``!foreach(``\ *var*\ ``,`` *sequence*\ ``,`` *expr*\ ``)``; This operator creates a new ``list``/``dag`` in which each element is a; function of the corresponding element in the *sequence* ``list``/``dag",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:64917,Performance,perform,perform,64917,"ms in *list*. The; variable *acc* acts as the accumulator and is initialized to *init*.; The variable *var* is bound to each element in the *list*. The; expression is evaluated for each element and presumably uses *acc* and; *var* to calculate the accumulated value, which ``!foldl`` stores back in; *acc*. The type of *acc* is the same as *init*; the type of *var* is the; same as the elements of *list*; *expr* must have the same type as *init*. The following example computes the total of the ``Number`` field in the; list of records in ``RecList``::. int x = !foldl(0, RecList, total, rec, !add(total, rec.Number));. If your goal is to filter the list and produce a new list that includes only; some of the elements, see ``!filter``. ``!foreach(``\ *var*\ ``,`` *sequence*\ ``,`` *expr*\ ``)``; This operator creates a new ``list``/``dag`` in which each element is a; function of the corresponding element in the *sequence* ``list``/``dag``.; To perform the function, TableGen binds the variable *var* to an element; and then evaluates the expression. The expression presumably refers; to the variable *var* and calculates the result value. If you simply want to create a list of a certain length containing; the same value repeated multiple times, see ``!listsplat``. ``!ge(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is greater than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!getdagarg<``\ *type*\ ``>(``\ *dag*\ ``,``\ *key*\ ``)``; This operator retrieves the argument from the given *dag* node by the; specified *key*, which is either an integer index or a string name. If that; argument is not convertible to the specified *type*, ``?`` is returned. ``!getdagname(``\ *dag*\ ``,``\ *index*\ ``)``; This operator retrieves the argument name from the given *dag* node by the; specified *index*. If that argument has no name associated, ``?`` is; returned. ``!getdagop(``\ *dag*\ ``)`` --or-- ``!getdagop<``\ *type*\ ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:69264,Performance,perform,performs,69264,"st2*. The lists must have the same element type. ``!listsplat(``\ *value*\ ``,`` *count*\ ``)``; This operator produces a list of length *count* whose elements are all; equal to the *value*. For example, ``!listsplat(42, 3)`` results in; ``[42, 42, 42]``. ``!logtwo(``\ *a*\ ``)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 or 1. ``!range([``\ *start*\ ``,]`` *end*\ ``[, ``\ *step*\ ``])``; This operator produces half-open range sequence ``[start : end : step)`` as; ``list<int>``. *start* is ``0`` and *step* is ``1`` by default. *step* can; be negative and cannot be 0. If *start* ``<`` *end* and *step* is negative,; or *start* ``>`` *end* and *step* is positive, the result is an empty list; ``[]<list<int>>``. For example:. * ``!range(4)`` is equivalent to ``!range(0, 4, 1)`` and the result is; `[0, 1, 2, 3]`.; * ``!range(1, 4)`` is equivalent to ``!range(1, 4, 1)`` and the result is; `[1, 2, 3]`.; * The result of ``!range(0, 4, 2)`` is `[0, 2]`.; * The results of ``!range(0, 4, -1)`` and ``!range(4, 0, 1)",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:69535,Performance,perform,performed,69535,"`)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 or 1. ``!range([``\ *start*\ ``,]`` *end*\ ``[, ``\ *step*\ ``])``; This operator produces half-open range sequence ``[start : end : step)`` as; ``list<int>``. *start* is ``0`` and *step* is ``1`` by default. *step* can; be negative and cannot be 0. If *start* ``<`` *end* and *step* is negative,; or *start* ``>`` *end* and *step* is positive, the result is an empty list; ``[]<list<int>>``. For example:. * ``!range(4)`` is equivalent to ``!range(0, 4, 1)`` and the result is; `[0, 1, 2, 3]`.; * ``!range(1, 4)`` is equivalent to ``!range(1, 4, 1)`` and the result is; `[1, 2, 3]`.; * The result of ``!range(0, 4, 2)`` is `[0, 2]`.; * The results of ``!range(0, 4, -1)`` and ``!range(4, 0, 1)`` are empty. ``!range(``\ *list*\ ``)``; Equivalent to ``!range(0, !size(list))``. ``!repr(``\ *value*\ ``)``; Represents *value* as a string. String format for the value is not; guaranteed to be stable. Intended for debugging purposes only. ``!setdagarg(``\ *dag*\ ``,``\ *k",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:71443,Performance,perform,performed,71443,"le. Intended for debugging purposes only. ``!setdagarg(``\ *dag*\ ``,``\ *key*\ ``,``\ *arg*\ ``)``; This operator produces a DAG node with the same operator and arguments as; *dag*, but replacing the value of the argument specified by the *key* with; *arg*. That *key* could be either an integer index or a string name. ``!setdagname(``\ *dag*\ ``,``\ *key*\ ``,``\ *name*\ ``)``; This operator produces a DAG node with the same operator and arguments as; *dag*, but replacing the name of the argument specified by the *key* with; *name*. That *key* could be either an integer index or a string name. ``!setdagop(``\ *dag*\ ``,`` *op*\ ``)``; This operator produces a DAG node with the same arguments as *dag*, but with its; operator replaced with *op*. Example: ``!setdagop((foo 1, 2), bar)`` results in ``(bar 1, 2)``. ``!shl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* left logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!size(``\ *a*\ ``)``; This operator produces the size of the string, list, or dag *a*.; The size of a DAG is the number of arguments; the operator does not count. ``!sra(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right arithmetically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!srl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!strconcat(``\ *str1*\ ``,`` *str2*\ ``, ...)``; This operator concatenates the string arguments *str1*, *str2*, etc., and; produces the resulting string. ``!sub(``\ *a*\ ``,`` *b*\ ``)``; This operator subtracts *b* from *a* and produces the arithmetic difference. ``!subst(``\ *target*\ ``,`` *repl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:71849,Performance,perform,performed,71849," DAG node with the same operator and arguments as; *dag*, but replacing the name of the argument specified by the *key* with; *name*. That *key* could be either an integer index or a string name. ``!setdagop(``\ *dag*\ ``,`` *op*\ ``)``; This operator produces a DAG node with the same arguments as *dag*, but with its; operator replaced with *op*. Example: ``!setdagop((foo 1, 2), bar)`` results in ``(bar 1, 2)``. ``!shl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* left logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!size(``\ *a*\ ``)``; This operator produces the size of the string, list, or dag *a*.; The size of a DAG is the number of arguments; the operator does not count. ``!sra(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right arithmetically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!srl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!strconcat(``\ *str1*\ ``,`` *str2*\ ``, ...)``; This operator concatenates the string arguments *str1*, *str2*, etc., and; produces the resulting string. ``!sub(``\ *a*\ ``,`` *b*\ ``)``; This operator subtracts *b* from *a* and produces the arithmetic difference. ``!subst(``\ *target*\ ``,`` *repl*\ ``,`` *value*\ ``)``; This operator replaces all occurrences of the *target* in the *value* with; the *repl* and produces the resulting value. The *value* can; be a string, in which case substring substitution is performed. The *value* can be a record name, in which case the operator produces the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *va",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:72085,Performance,perform,performed,72085,"; This operator produces a DAG node with the same arguments as *dag*, but with its; operator replaced with *op*. Example: ``!setdagop((foo 1, 2), bar)`` results in ``(bar 1, 2)``. ``!shl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* left logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!size(``\ *a*\ ``)``; This operator produces the size of the string, list, or dag *a*.; The size of a DAG is the number of arguments; the operator does not count. ``!sra(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right arithmetically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!srl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!strconcat(``\ *str1*\ ``,`` *str2*\ ``, ...)``; This operator concatenates the string arguments *str1*, *str2*, etc., and; produces the resulting string. ``!sub(``\ *a*\ ``,`` *b*\ ``)``; This operator subtracts *b* from *a* and produces the arithmetic difference. ``!subst(``\ *target*\ ``,`` *repl*\ ``,`` *value*\ ``)``; This operator replaces all occurrences of the *target* in the *value* with; the *repl* and produces the resulting value. The *value* can; be a string, in which case substring substitution is performed. The *value* can be a record name, in which case the operator produces the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *value*. ``!substr(``\ *string*\ ``,`` *start*\ [``,`` *length*]\ ``)``; This operator extracts a substring of the given *string*. The starting; position of the substring is specified by *start*, which can range; between 0 and the length o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:72692,Performance,perform,performed,72692,"ments; the operator does not count. ``!sra(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right arithmetically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!srl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!strconcat(``\ *str1*\ ``,`` *str2*\ ``, ...)``; This operator concatenates the string arguments *str1*, *str2*, etc., and; produces the resulting string. ``!sub(``\ *a*\ ``,`` *b*\ ``)``; This operator subtracts *b* from *a* and produces the arithmetic difference. ``!subst(``\ *target*\ ``,`` *repl*\ ``,`` *value*\ ``)``; This operator replaces all occurrences of the *target* in the *value* with; the *repl* and produces the resulting value. The *value* can; be a string, in which case substring substitution is performed. The *value* can be a record name, in which case the operator produces the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *value*. ``!substr(``\ *string*\ ``,`` *start*\ [``,`` *length*]\ ``)``; This operator extracts a substring of the given *string*. The starting; position of the substring is specified by *start*, which can range; between 0 and the length of the string. The length of the substring; is specified by *length*; if not specified, the rest of the string is; extracted. The *start* and *length* arguments must be integers. ``!tail(``\ *a*\ ``)``; This operator produces a new list with all the elements; of the list *a* except for the zeroth one. (See also ``!head``.). ``!tolower(``\ *a*\ ``)``; This operator converts a string input *a* to lower case. ``!toupper(``\ *a*\ ``)``; This operator converts a string input *a* to upper case. ``!xor(``\ *a*\ ``,`` *b*\ ``, ...)``; This operato",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:73758,Performance,perform,performed,73758," the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *value*. ``!substr(``\ *string*\ ``,`` *start*\ [``,`` *length*]\ ``)``; This operator extracts a substring of the given *string*. The starting; position of the substring is specified by *start*, which can range; between 0 and the length of the string. The length of the substring; is specified by *length*; if not specified, the rest of the string is; extracted. The *start* and *length* arguments must be integers. ``!tail(``\ *a*\ ``)``; This operator produces a new list with all the elements; of the list *a* except for the zeroth one. (See also ``!head``.). ``!tolower(``\ *a*\ ``)``; This operator converts a string input *a* to lower case. ``!toupper(``\ *a*\ ``)``; This operator converts a string input *a* to upper case. ``!xor(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise EXCLUSIVE OR on *a*, *b*, etc., and produces; the result. A logical XOR can be performed if all the arguments are either; 0 or 1. Appendix B: Paste Operator Examples; ===================================. Here is an example illustrating the use of the paste operator in record names. .. code-block:: text. defvar suffix = ""_suffstring"";; defvar some_ints = [0, 1, 2, 3];. def name # suffix {; }. foreach i = [1, 2] in {; def rec # i {; }; }. The first ``def`` does not use the value of the ``suffix`` variable. The; second def does use the value of the ``i`` iterator variable, because it is not a; global name. The following records are produced. .. code-block:: text. def namesuffix {; }; def rec1 {; }; def rec2 {; }. Here is a second example illustrating the paste operator in field value expressions. .. code-block:: text. def test {; string strings = suffix # suffix;; list<int> integers = some_ints # [4, 5, 6];; }. The ``strings`` field expression uses ``suffix`` on both sides of the paste; operator. It is evaluated normally on the left hand side, but taken verbatim; on the right han",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:11187,Security,access,accessed,11187,"n language is statically typed, using a simple but complete type; system. Types are used to check for errors, to perform implicit conversions,; and to help interface designers constrain the allowed input. Every value is; required to have an associated type. TableGen supports a mixture of low-level types (e.g., ``bit``) and; high-level types (e.g., ``dag``). This flexibility allows you to describe a; wide range of records conveniently and compactly. .. productionlist::; Type: ""bit"" | ""int"" | ""string"" | ""dag""; :| ""bits"" ""<"" `TokInteger` "">""; :| ""list"" ""<"" `Type` "">""; :| `ClassID`; ClassID: `TokIdentifier`. ``bit``; A ``bit`` is a boolean value that can be 0 or 1. ``int``; The ``int`` type represents a simple 64-bit integer value, such as 5 or; -42. ``string``; The ``string`` type represents an ordered sequence of characters of arbitrary; length. ``bits<``\ *n*\ ``>``; The ``bits`` type is a fixed-sized integer of arbitrary length *n* that; is treated as separate bits. These bits can be accessed individually.; A field of this type is useful for representing an instruction operation; code, register number, or address mode/register/displacement. The bits of; the field can be set individually or as subfields. For example, in an; instruction address, the addressing mode, base register number, and; displacement can be set separately. ``list<``\ *type*\ ``>``; This type represents a list whose elements are of the *type* specified in; angle brackets. The element type is arbitrary; it can even be another; list type. List elements are indexed from 0. ``dag``; This type represents a nestable directed acyclic graph (DAG) of nodes.; Each node has an *operator* and zero or more *arguments* (or *operands*).; An argument can be; another ``dag`` object, allowing an arbitrary tree of nodes and edges.; As an example, DAGs are used to represent code patterns for use by; the code generator instruction selection algorithms. See `Directed; acyclic graphs (DAGs)`_ for more details;. :token:`C",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:17214,Security,access,accessed,17214,". .. productionlist::; SimpleValue7: `TokIdentifier`. The resulting value is the value of the entity named by the identifier. The; possible identifiers are described here, but the descriptions will make more; sense after reading the remainder of this guide. .. The code for this is exceptionally abstruse. These examples are a; best-effort attempt. * A template argument of a ``class``, such as the use of ``Bar`` in::. class Foo <int Bar> {; int Baz = Bar;; }. * The implicit template argument ``NAME`` in a ``class`` or ``multiclass``; definition (see `NAME`_). * A field local to a ``class``, such as the use of ``Bar`` in::. class Foo {; int Bar = 5;; int Baz = Bar;; }. * The name of a record definition, such as the use of ``Bar`` in the; definition of ``Foo``::. def Bar : SomeClass {; int X = 5;; }. def Foo {; SomeClass Baz = Bar;; }. * A field local to a record definition, such as the use of ``Bar`` in::. def Foo {; int Bar = 5;; int Baz = Bar;; }. Fields inherited from the record's parent classes can be accessed the same way. * A template argument of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined with the ``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Valu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:8635,Testability,assert,assert,8635,"onlist::; TokString: '""' (non-'""' characters and escapes) '""'; TokCode: ""[{"" (shortest text not containing ""}]"") ""}]"". A :token:`TokCode` is nothing more than a multi-line string literal; delimited by ``[{`` and ``}]``. It can break across lines and the; line breaks are retained in the string. The current implementation accepts the following escape sequences::. \\ \' \"" \t \n. Identifiers; -----------. TableGen has name- and identifier-like tokens, which are case-sensitive. .. productionlist::; ualpha: ""a""...""z"" | ""A""...""Z"" | ""_""; TokIdentifier: (""0""...""9"")* `ualpha` (`ualpha` | ""0""...""9"")*; TokVarName: ""$"" `ualpha` (`ualpha` | ""0""...""9"")*. Note that, unlike most languages, TableGen allows :token:`TokIdentifier` to; begin with an integer. In case of ambiguity, a token is interpreted as a; numeric literal rather than an identifier. TableGen has the following reserved keywords, which cannot be used as; identifiers::. assert bit bits class code; dag def dump else false; foreach defm defset defvar field; if in include int let; list multiclass string then true. .. warning::; The ``field`` reserved word is deprecated, except when used with the; CodeEmitterGen backend where it's used to distinguish normal record; fields from encoding fields. Bang operators; --------------. TableGen provides ""bang operators"" that have a wide variety of uses:. .. productionlist::; BangOperator: one of; : !add !and !cast !con !dag; : !div !empty !eq !exists !filter; : !find !foldl !foreach !ge !getdagarg; : !getdagname !getdagop !gt !head !if; : !interleave !isa !le !listconcat !listremove; : !listsplat !logtwo !lt !mul !ne; : !not !or !range !repr !setdagarg; : !setdagname !setdagop !shl !size !sra; : !srl !strconcat !sub !subst !substr; : !tail !tolower !toupper !xor. The ``!cond`` operator has a slightly different; syntax compared to other bang operators, so it is defined separately:. .. productionlist::; CondOperator: !cond. See `Appendix A: Bang Operators`_ for a description of each bang",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:9311,Testability,log,logtwo,9311,"ualpha` (`ualpha` | ""0""...""9"")*; TokVarName: ""$"" `ualpha` (`ualpha` | ""0""...""9"")*. Note that, unlike most languages, TableGen allows :token:`TokIdentifier` to; begin with an integer. In case of ambiguity, a token is interpreted as a; numeric literal rather than an identifier. TableGen has the following reserved keywords, which cannot be used as; identifiers::. assert bit bits class code; dag def dump else false; foreach defm defset defvar field; if in include int let; list multiclass string then true. .. warning::; The ``field`` reserved word is deprecated, except when used with the; CodeEmitterGen backend where it's used to distinguish normal record; fields from encoding fields. Bang operators; --------------. TableGen provides ""bang operators"" that have a wide variety of uses:. .. productionlist::; BangOperator: one of; : !add !and !cast !con !dag; : !div !empty !eq !exists !filter; : !find !foldl !foreach !ge !getdagarg; : !getdagname !getdagop !gt !head !if; : !interleave !isa !le !listconcat !listremove; : !listsplat !logtwo !lt !mul !ne; : !not !or !range !repr !setdagarg; : !setdagname !setdagop !shl !size !sra; : !srl !strconcat !sub !subst !substr; : !tail !tolower !toupper !xor. The ``!cond`` operator has a slightly different; syntax compared to other bang operators, so it is defined separately:. .. productionlist::; CondOperator: !cond. See `Appendix A: Bang Operators`_ for a description of each bang operator. Include files; -------------. TableGen has an include mechanism. The content of the included file; lexically replaces the ``include`` directive and is then parsed as if it was; originally in the main file. .. productionlist::; IncludeDirective: ""include"" `TokString`. Portions of the main file and included files can be conditionalized using; preprocessor directives. .. productionlist::; PreprocessorDirective: ""#define"" | ""#ifdef"" | ""#ifndef"". Types; =====. The TableGen language is statically typed, using a simple but complete type; system. Types are ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:47562,Testability,test,test,47562,"ted. .. Note that the productions involving RangeList and RangePiece have precedence; over the more generic value parsing based on the first token. .. code-block:: text. foreach i = [0, 1, 2, 3] in {; def R#i : Register<...>;; def F#i : Register<...>;; }. This loop defines records named ``R0``, ``R1``, ``R2``, and ``R3``, along; with ``F0``, ``F1``, ``F2``, and ``F3``. ``dump`` --- print messages to stderr; -------------------------------------. A ``dump`` statement prints the input string to standard error; output. It is intended for debugging purpose. * At top level, the message is printed immediately. * Within a record/class/multiclass, `dump` gets evaluated at each; instantiation point of the containing record. .. productionlist::; Dump: ""dump"" `string` "";"". For example, it can be used in combination with `!repr` to investigate; the values passed to a multiclass:. .. code-block:: text. multiclass MC<dag s> {; dump ""s = "" # !repr(s);; }. ``if`` --- select statements based on a test; --------------------------------------------. The ``if`` statement allows one of two statement groups to be selected based; on the value of an expression. .. productionlist::; If: ""if"" `Value` ""then"" `IfBody`; :| ""if"" `Value` ""then"" `IfBody` ""else"" `IfBody`; IfBody: ""{"" `Statement`* ""}"" | `Statement`. The value expression is evaluated. If it evaluates to true (in the same; sense used by the bang operators), then the statements following the; ``then`` reserved word are processed. Otherwise, if there is an ``else``; reserved word, the statements following the ``else`` are processed. If the; value is false and there is no ``else`` arm, no statements are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:48844,Testability,assert,assert,48844,"ion is evaluated. If it evaluates to true (in the same; sense used by the bang operators), then the statements following the; ``then`` reserved word are processed. Otherwise, if there is an ``else``; reserved word, the statements following the ``else`` are processed. If the; value is false and there is no ``else`` arm, no statements are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; component",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:48942,Testability,assert,assert,48942,"ts following the; ``then`` reserved word are processed. Otherwise, if there is an ``else``; reserved word, the statements following the ``else`` are processed. If the; value is false and there is no ``else`` arm, no statements are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49088,Testability,assert,assert,49088,"ved word, the statements following the ``else`` are processed. If the; value is false and there is no ``else`` arm, no statements are processed. Because the braces around the ``then`` statements are optional, this grammar rule; has the usual ambiguity with ""dangling else"" clauses, and it is resolved in; the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49389,Testability,assert,assert,49389," the usual way: in a case like ``if v1 then if v2 then {...} else {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49455,Testability,assert,assertion,49455,"e {...}``, the; ``else`` associates with the inner ``if`` rather than the outer one. The :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"",",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49546,Testability,assert,assertions,49546,"he :token:`IfBody` of the then and else arms of the ``if`` establish an; inner scope. Any ``defvar`` variables defined in the bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs);",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49637,Testability,assert,assertions,49637,"e bodies go out of scope; when the bodies are finished (see `Defvar in a Record Body`_ for more details). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` dataty",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49740,Testability,assert,assertions,49740,"ils). The ``if`` statement can also be used in a record :token:`Body`. ``assert`` --- check that a condition is true; ---------------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` datatype. A DAG node consists of an operator and zero or more; arguments (or operands). Each argument can ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49842,Testability,assert,assertions,49842,"----------------------------------------. The ``assert`` statement checks a boolean condition to be sure that it is true; and prints an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` datatype. A DAG node consists of an operator and zero or more; arguments (or operands). Each argument can be of any desired type. By using; another DAG node as an argument, an arbitrary graph of DAG nodes can be; built. The synta",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:49988,Testability,assert,assertions,49988,"ts an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` datatype. A DAG node consists of an operator and zero or more; arguments (or operands). Each argument can be of any desired type. By using; another DAG node as an argument, an arbitrary graph of DAG nodes can be; built. The syntax of a ``dag`` instance is:. ``(`` *operator* *argument1*\ ``,`` *argument2*\ ``,`` ... ``)``. The operator must be present and mus",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:50096,Testability,assert,assert,50096,"ndition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` datatype. A DAG node consists of an operator and zero or more; arguments (or operands). Each argument can be of any desired type. By using; another DAG node as an argument, an arbitrary graph of DAG nodes can be; built. The syntax of a ``dag`` instance is:. ``(`` *operator* *argument1*\ ``,`` *argument2*\ ``,`` ... ``)``. The operator must be present and must be a record. There can be zero or more; arguments, separated by commas. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:50186,Testability,assert,assert,50186,"se, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` datatype. A DAG node consists of an operator and zero or more; arguments (or operands). Each argument can be of any desired type. By using; another DAG node as an argument, an arbitrary graph of DAG nodes can be; built. The syntax of a ``dag`` instance is:. ``(`` *operator* *argument1*\ ``,`` *argument2*\ ``,`` ... ``)``. The operator must be present and must be a record. There can be zero or more; arguments, separated by commas. The operator and arguments can have three; formats. ====================== ========================================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:50333,Testability,assert,assert,50333,"s a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` datatype. A DAG node consists of an operator and zero or more; arguments (or operands). Each argument can be of any desired type. By using; another DAG node as an argument, an arbitrary graph of DAG nodes can be; built. The syntax of a ``dag`` instance is:. ``(`` *operator* *argument1*\ ``,`` *argument2*\ ``,`` ... ``)``. The operator must be present and must be a record. There can be zero or more; arguments, separated by commas. The operator and arguments can have three; formats. ====================== =============================================; Format Meaning; ====================== =============================================; *value* argument value; *value*\ ``:``\ *nam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:57915,Testability,test,tested,57915," space | tab; CComment: ""/*"" ... ""*/""; BCPLComment: ""//"" ... `LineEnd`; WhiteSpaceOrCComment: `WhiteSpace` | `CComment`; WhiteSpaceOrAnyComment: `WhiteSpace` | `CComment` | `BCPLComment`; MacroName: `ualpha` (`ualpha` | ""0""...""9"")*; PreDefine: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#define"" (`WhiteSpace`)+ `MacroName`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreIfdef: `LineBegin` (`WhiteSpaceOrCComment`)*; : (""#ifdef"" | ""#ifndef"") (`WhiteSpace`)+ `MacroName`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreElse: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#else"" (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreEndif: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#endif"" (`WhiteSpaceOrAnyComment`)* `LineEnd`. ..; PreRegContentException: `PreIfdef` | `PreElse` | `PreEndif` | EOF; PreRegion: .* - `PreRegContentException`; :| `PreIfdef`; : (`PreRegion`)*; : [`PreElse`]; : (`PreRegion`)*; : `PreEndif`. A :token:`MacroName` can be defined anywhere in a TableGen file. The name has; no value; it can only be tested to see whether it is defined. A macro test region begins with an ``#ifdef`` or ``#ifndef`` directive. If; the macro name is defined (``#ifdef``) or undefined (``#ifndef``), then the; source code between the directive and the corresponding ``#else`` or; ``#endif`` is processed. If the test fails but there is an ``#else``; clause, the source code between the ``#else`` and the ``#endif`` is; processed. If the test fails and there is no ``#else`` clause, then no; source code in the test region is processed. Test regions may be nested, but they must be properly nested. A region; started in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, ope",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:57960,Testability,test,test,57960,"iteSpaceOrCComment: `WhiteSpace` | `CComment`; WhiteSpaceOrAnyComment: `WhiteSpace` | `CComment` | `BCPLComment`; MacroName: `ualpha` (`ualpha` | ""0""...""9"")*; PreDefine: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#define"" (`WhiteSpace`)+ `MacroName`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreIfdef: `LineBegin` (`WhiteSpaceOrCComment`)*; : (""#ifdef"" | ""#ifndef"") (`WhiteSpace`)+ `MacroName`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreElse: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#else"" (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreEndif: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#endif"" (`WhiteSpaceOrAnyComment`)* `LineEnd`. ..; PreRegContentException: `PreIfdef` | `PreElse` | `PreEndif` | EOF; PreRegion: .* - `PreRegContentException`; :| `PreIfdef`; : (`PreRegion`)*; : [`PreElse`]; : (`PreRegion`)*; : `PreEndif`. A :token:`MacroName` can be defined anywhere in a TableGen file. The name has; no value; it can only be tested to see whether it is defined. A macro test region begins with an ``#ifdef`` or ``#ifndef`` directive. If; the macro name is defined (``#ifdef``) or undefined (``#ifndef``), then the; source code between the directive and the corresponding ``#else`` or; ``#endif`` is processed. If the test fails but there is an ``#else``; clause, the source code between the ``#else`` and the ``#endif`` is; processed. If the test fails and there is no ``#else`` clause, then no; source code in the test region is processed. Test regions may be nested, but they must be properly nested. A region; started in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:58207,Testability,test,test,58207,"nt`)* `LineEnd`; PreIfdef: `LineBegin` (`WhiteSpaceOrCComment`)*; : (""#ifdef"" | ""#ifndef"") (`WhiteSpace`)+ `MacroName`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreElse: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#else"" (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreEndif: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#endif"" (`WhiteSpaceOrAnyComment`)* `LineEnd`. ..; PreRegContentException: `PreIfdef` | `PreElse` | `PreEndif` | EOF; PreRegion: .* - `PreRegContentException`; :| `PreIfdef`; : (`PreRegion`)*; : [`PreElse`]; : (`PreRegion`)*; : `PreEndif`. A :token:`MacroName` can be defined anywhere in a TableGen file. The name has; no value; it can only be tested to see whether it is defined. A macro test region begins with an ``#ifdef`` or ``#ifndef`` directive. If; the macro name is defined (``#ifdef``) or undefined (``#ifndef``), then the; source code between the directive and the corresponding ``#else`` or; ``#endif`` is processed. If the test fails but there is an ``#else``; clause, the source code between the ``#else`` and the ``#endif`` is; processed. If the test fails and there is no ``#else`` clause, then no; source code in the test region is processed. Test regions may be nested, but they must be properly nested. A region; started in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:58332,Testability,test,test,58332,"e`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreElse: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#else"" (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreEndif: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#endif"" (`WhiteSpaceOrAnyComment`)* `LineEnd`. ..; PreRegContentException: `PreIfdef` | `PreElse` | `PreEndif` | EOF; PreRegion: .* - `PreRegContentException`; :| `PreIfdef`; : (`PreRegion`)*; : [`PreElse`]; : (`PreRegion`)*; : `PreEndif`. A :token:`MacroName` can be defined anywhere in a TableGen file. The name has; no value; it can only be tested to see whether it is defined. A macro test region begins with an ``#ifdef`` or ``#ifndef`` directive. If; the macro name is defined (``#ifdef``) or undefined (``#ifndef``), then the; source code between the directive and the corresponding ``#else`` or; ``#endif`` is processed. If the test fails but there is an ``#else``; clause, the source code between the ``#else`` and the ``#endif`` is; processed. If the test fails and there is no ``#else`` clause, then no; source code in the test region is processed. Test regions may be nested, but they must be properly nested. A region; started in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:58405,Testability,test,test,58405,"e`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreElse: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#else"" (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreEndif: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#endif"" (`WhiteSpaceOrAnyComment`)* `LineEnd`. ..; PreRegContentException: `PreIfdef` | `PreElse` | `PreEndif` | EOF; PreRegion: .* - `PreRegContentException`; :| `PreIfdef`; : (`PreRegion`)*; : [`PreElse`]; : (`PreRegion`)*; : `PreEndif`. A :token:`MacroName` can be defined anywhere in a TableGen file. The name has; no value; it can only be tested to see whether it is defined. A macro test region begins with an ``#ifdef`` or ``#ifndef`` directive. If; the macro name is defined (``#ifdef``) or undefined (``#ifndef``), then the; source code between the directive and the corresponding ``#else`` or; ``#endif`` is processed. If the test fails but there is an ``#else``; clause, the source code between the ``#else`` and the ``#endif`` is; processed. If the test fails and there is no ``#else`` clause, then no; source code in the test region is processed. Test regions may be nested, but they must be properly nested. A region; started in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:59065,Testability,test,tests,59065,"`), then the; source code between the directive and the corresponding ``#else`` or; ``#endif`` is processed. If the test fails but there is an ``#else``; clause, the source code between the ``#else`` and the ``#endif`` is; processed. If the test fails and there is no ``#else`` clause, then no; source code in the test region is processed. Test regions may be nested, but they must be properly nested. A region; started in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise AND on *a*, *b*, etc., and produces the; result. A logical AND can be performed if all the arguments are either; 0 or 1. ``!cast<``\ *type*\ ``>(``\ *a*\ ``)``; This operator performs a cast on *a* and produces the result.; If *a* is not a string, then a straightforward cast is performed, say; between an ``int`` and a ``bit``, or between record types. This allows; casting a record to a class. If a record is cast to ``string``, the; record's name is produced. If *a* is a string, then it is treated as a record name and looked up in; the list of all defined records. The resulting record is expected to be of; the specified *type*. For example, if ``!cast<``\ *type*\ ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:59471,Testability,log,logical,59471,"tarted in a file must end in that file; that is, must have its; ``#endif`` in the same file. A :token:`MacroName` may be defined externally using the ``-D`` option on the; ``*-tblgen`` command line::. llvm-tblgen self-reference.td -Dmacro1 -Dmacro3. Appendix A: Bang Operators; ==========================. Bang operators act as functions in value expressions. A bang operator takes; one or more arguments, operates on them, and produces a result. If the; operator produces a boolean result, the result value will be 1 for true or 0; for false. When an operator tests a boolean argument, it interprets 0 as false; and non-0 as true. .. warning::; The ``!getop`` and ``!setop`` bang operators are deprecated in favor of; ``!getdagop`` and ``!setdagop``. ``!add(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator adds *a*, *b*, etc., and produces the sum. ``!and(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise AND on *a*, *b*, etc., and produces the; result. A logical AND can be performed if all the arguments are either; 0 or 1. ``!cast<``\ *type*\ ``>(``\ *a*\ ``)``; This operator performs a cast on *a* and produces the result.; If *a* is not a string, then a straightforward cast is performed, say; between an ``int`` and a ``bit``, or between record types. This allows; casting a record to a class. If a record is cast to ``string``, the; record's name is produced. If *a* is a string, then it is treated as a record name and looked up in; the list of all defined records. The resulting record is expected to be of; the specified *type*. For example, if ``!cast<``\ *type*\ ``>(``\ *name*\ ``)``; appears in a multiclass definition, or in a; class instantiated inside a multiclass definition, and the *name* does not; reference any template arguments of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:61171,Testability,test,tests,61171,"ed inside a multiclass definition, and the *name* does not; reference any template arguments of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (or later, if the defm occurs in another; multiclass and template arguments of the inner multiclass that are; referenced by *name* are substituted by values that themselves contain; references to template arguments of the outer multiclass). If the type of *a* does not match *type*, TableGen raises an error. ``!con(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator concatenates the DAG nodes *a*, *b*, etc. Their operations; must equal. ``!con((op a1:$name1, a2:$name2), (op b1:$name3))``. results in the DAG node ``(op a1:$name1, a2:$name2, b1:$name3)``. ``!cond(``\ *cond1* ``:`` *val1*\ ``,`` *cond2* ``:`` *val2*\ ``, ...,`` *condn* ``:`` *valn*\ ``)``; This operator tests *cond1* and returns *val1* if the result is true.; If false, the operator tests *cond2* and returns *val2* if the result is; true. And so forth. An error is reported if no conditions are true. This example produces the sign word for an integer::. !cond(!lt(x, 0) : ""negative"", !eq(x, 0) : ""zero"", true : ""positive""). ``!dag(``\ *op*\ ``,`` *arguments*\ ``,`` *names*\ ``)``; This operator creates a DAG node with the given operator and; arguments. The *arguments* and *names* arguments must be lists; of equal length or uninitialized (``?``). The *names* argument; must be of type ``list<string>``. Due to limitations of the type system, *arguments* must be a list of items; of a common type. In practice, this means that they should either have the; same type or be records with a common parent class. Mixing ``dag`` and; non-``dag`` items is not possible. However, ``?`` can be used. Example: ``!dag(op, [a1, a2, ?], [""name1"", ""name2"", ""name3""])`` results in; ``(op a1-value:$name1, a2-value:$nam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:61251,Testability,test,tests,61251,"s of the multiclass, then a record by; that name must have been instantiated earlier; in the source file. If *name* does reference; a template argument, then the lookup is delayed until ``defm`` statements; instantiating the multiclass (or later, if the defm occurs in another; multiclass and template arguments of the inner multiclass that are; referenced by *name* are substituted by values that themselves contain; references to template arguments of the outer multiclass). If the type of *a* does not match *type*, TableGen raises an error. ``!con(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator concatenates the DAG nodes *a*, *b*, etc. Their operations; must equal. ``!con((op a1:$name1, a2:$name2), (op b1:$name3))``. results in the DAG node ``(op a1:$name1, a2:$name2, b1:$name3)``. ``!cond(``\ *cond1* ``:`` *val1*\ ``,`` *cond2* ``:`` *val2*\ ``, ...,`` *condn* ``:`` *valn*\ ``)``; This operator tests *cond1* and returns *val1* if the result is true.; If false, the operator tests *cond2* and returns *val2* if the result is; true. And so forth. An error is reported if no conditions are true. This example produces the sign word for an integer::. !cond(!lt(x, 0) : ""negative"", !eq(x, 0) : ""zero"", true : ""positive""). ``!dag(``\ *op*\ ``,`` *arguments*\ ``,`` *names*\ ``)``; This operator creates a DAG node with the given operator and; arguments. The *arguments* and *names* arguments must be lists; of equal length or uninitialized (``?``). The *names* argument; must be of type ``list<string>``. Due to limitations of the type system, *arguments* must be a list of items; of a common type. In practice, this means that they should either have the; same type or be records with a common parent class. Mixing ``dag`` and; non-``dag`` items is not possible. However, ``?`` can be used. Example: ``!dag(op, [a1, a2, ?], [""name1"", ""name2"", ""name3""])`` results in; ``(op a1-value:$name1, a2-value:$name2, ?:$name3)``. ``!div(``\ *a*\ ``,`` *b*\ ``)``; This operator performs signed division of",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:67078,Testability,test,test,67078,"s are always records. The result of ``!getdagop`` can be used directly in a context where; any record class at all is acceptable (typically placing it into; another dag value). But in other contexts, it must be explicitly; cast to a particular class. The ``<``\ *type*\ ``>`` syntax is; provided to make this easy. For example, to assign the result to a value of type ``BaseClass``, you; could write either of these::. BaseClass b = !getdagop<BaseClass>(someDag);; BaseClass b = !cast<BaseClass>(!getdagop(someDag));. But to create a new DAG node that reuses the operator from another, no; cast is necessary::. dag d = !dag(!getdagop(someDag), args, names);. ``!gt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is greater than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!head(``\ *a*\ ``)``; This operator produces the zeroth element of the list *a*.; (See also ``!tail``.). ``!if(``\ *test*\ ``,`` *then*\ ``,`` *else*\ ``)``; This operator evaluates the *test*, which must produce a ``bit`` or; ``int``. If the result is not 0, the *then* expression is produced; otherwise; the *else* expression is produced. ``!interleave(``\ *list*\ ``,`` *delim*\ ``)``; This operator concatenates the items in the *list*, interleaving the; *delim* string between each pair, and produces the resulting string.; The list can be a list of string, int, bits, or bit. An empty list; results in an empty string. The delimiter can be the empty string. ``!isa<``\ *type*\ ``>(``\ *a*\ ``)``; This operator produces 1 if the type of *a* is a subtype of the given *type*; 0; otherwise. ``!le(``\ *a*\ ``,`` *b*\ ``)``; This operator produces 1 if *a* is less than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!listconcat(``\ *list1*\ ``,`` *list2*\ ``, ...)``; This operator concatenates the list arguments *list1*, *list2*, etc., and; produces the resulting list. The lists must have the same element",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:67149,Testability,test,test,67149,"s are always records. The result of ``!getdagop`` can be used directly in a context where; any record class at all is acceptable (typically placing it into; another dag value). But in other contexts, it must be explicitly; cast to a particular class. The ``<``\ *type*\ ``>`` syntax is; provided to make this easy. For example, to assign the result to a value of type ``BaseClass``, you; could write either of these::. BaseClass b = !getdagop<BaseClass>(someDag);; BaseClass b = !cast<BaseClass>(!getdagop(someDag));. But to create a new DAG node that reuses the operator from another, no; cast is necessary::. dag d = !dag(!getdagop(someDag), args, names);. ``!gt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is greater than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!head(``\ *a*\ ``)``; This operator produces the zeroth element of the list *a*.; (See also ``!tail``.). ``!if(``\ *test*\ ``,`` *then*\ ``,`` *else*\ ``)``; This operator evaluates the *test*, which must produce a ``bit`` or; ``int``. If the result is not 0, the *then* expression is produced; otherwise; the *else* expression is produced. ``!interleave(``\ *list*\ ``,`` *delim*\ ``)``; This operator concatenates the items in the *list*, interleaving the; *delim* string between each pair, and produces the resulting string.; The list can be a list of string, int, bits, or bit. An empty list; results in an empty string. The delimiter can be the empty string. ``!isa<``\ *type*\ ``>(``\ *a*\ ``)``; This operator produces 1 if the type of *a* is a subtype of the given *type*; 0; otherwise. ``!le(``\ *a*\ ``,`` *b*\ ``)``; This operator produces 1 if *a* is less than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!listconcat(``\ *list1*\ ``,`` *list2*\ ``, ...)``; This operator concatenates the list arguments *list1*, *list2*, etc., and; produces the resulting list. The lists must have the same element",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:68532,Testability,log,logtwo,68532,"tring. The delimiter can be the empty string. ``!isa<``\ *type*\ ``>(``\ *a*\ ``)``; This operator produces 1 if the type of *a* is a subtype of the given *type*; 0; otherwise. ``!le(``\ *a*\ ``,`` *b*\ ``)``; This operator produces 1 if *a* is less than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!listconcat(``\ *list1*\ ``,`` *list2*\ ``, ...)``; This operator concatenates the list arguments *list1*, *list2*, etc., and; produces the resulting list. The lists must have the same element type. ``!listremove(``\ *list1*\ ``,`` *list2*\ ``)``; This operator returns a copy of *list1* removing all elements that also occur in; *list2*. The lists must have the same element type. ``!listsplat(``\ *value*\ ``,`` *count*\ ``)``; This operator produces a list of length *count* whose elements are all; equal to the *value*. For example, ``!listsplat(42, 3)`` results in; ``[42, 42, 42]``. ``!logtwo(``\ *a*\ ``)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:68589,Testability,log,log,68589,"tring. The delimiter can be the empty string. ``!isa<``\ *type*\ ``>(``\ *a*\ ``)``; This operator produces 1 if the type of *a* is a subtype of the given *type*; 0; otherwise. ``!le(``\ *a*\ ``,`` *b*\ ``)``; This operator produces 1 if *a* is less than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!listconcat(``\ *list1*\ ``,`` *list2*\ ``, ...)``; This operator concatenates the list arguments *list1*, *list2*, etc., and; produces the resulting list. The lists must have the same element type. ``!listremove(``\ *list1*\ ``,`` *list2*\ ``)``; This operator returns a copy of *list1* removing all elements that also occur in; *list2*. The lists must have the same element type. ``!listsplat(``\ *value*\ ``,`` *count*\ ``)``; This operator produces a list of length *count* whose elements are all; equal to the *value*. For example, ``!listsplat(42, 3)`` results in; ``[42, 42, 42]``. ``!logtwo(``\ *a*\ ``)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:68638,Testability,log,log,68638,"`)``; This operator produces 1 if the type of *a* is a subtype of the given *type*; 0; otherwise. ``!le(``\ *a*\ ``,`` *b*\ ``)``; This operator produces 1 if *a* is less than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!listconcat(``\ *list1*\ ``,`` *list2*\ ``, ...)``; This operator concatenates the list arguments *list1*, *list2*, etc., and; produces the resulting list. The lists must have the same element type. ``!listremove(``\ *list1*\ ``,`` *list2*\ ``)``; This operator returns a copy of *list1* removing all elements that also occur in; *list2*. The lists must have the same element type. ``!listsplat(``\ *value*\ ``,`` *count*\ ``)``; This operator produces a list of length *count* whose elements are all; equal to the *value*. For example, ``!listsplat(42, 3)`` results in; ``[42, 42, 42]``. ``!logtwo(``\ *a*\ ``)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 or 1. ``!range([``\ *start*\ ``,]`` *end*\ ``[, ``\ *step*\ ``])``; This operato",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:69275,Testability,log,logical,69275,"st2*. The lists must have the same element type. ``!listsplat(``\ *value*\ ``,`` *count*\ ``)``; This operator produces a list of length *count* whose elements are all; equal to the *value*. For example, ``!listsplat(42, 3)`` results in; ``[42, 42, 42]``. ``!logtwo(``\ *a*\ ``)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 or 1. ``!range([``\ *start*\ ``,]`` *end*\ ``[, ``\ *step*\ ``])``; This operator produces half-open range sequence ``[start : end : step)`` as; ``list<int>``. *start* is ``0`` and *step* is ``1`` by default. *step* can; be negative and cannot be 0. If *start* ``<`` *end* and *step* is negative,; or *start* ``>`` *end* and *step* is positive, the result is an empty list; ``[]<list<int>>``. For example:. * ``!range(4)`` is equivalent to ``!range(0, 4, 1)`` and the result is; `[0, 1, 2, 3]`.; * ``!range(1, 4)`` is equivalent to ``!range(1, 4, 1)`` and the result is; `[1, 2, 3]`.; * The result of ``!range(0, 4, 2)`` is `[0, 2]`.; * The results of ``!range(0, 4, -1)`` and ``!range(4, 0, 1)",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:69517,Testability,log,logical,69517,"`)``; This operator produces the base 2 log of *a* and produces the integer; result. The log of 0 or a negative number produces an error. This; is a flooring operation. ``!lt(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is less than *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!mul(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator multiplies *a*, *b*, etc., and produces the product. ``!ne(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is not equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, ``string``,; or record values. Use ``!cast<string>`` to compare other types of objects. ``!not(``\ *a*\ ``)``; This operator performs a logical NOT on *a*, which must be; an integer. The argument 0 results in 1 (true); any other; argument results in 0 (false). ``!or(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise OR on *a*, *b*, etc., and produces the; result. A logical OR can be performed if all the arguments are either; 0 or 1. ``!range([``\ *start*\ ``,]`` *end*\ ``[, ``\ *step*\ ``])``; This operator produces half-open range sequence ``[start : end : step)`` as; ``list<int>``. *start* is ``0`` and *step* is ``1`` by default. *step* can; be negative and cannot be 0. If *start* ``<`` *end* and *step* is negative,; or *start* ``>`` *end* and *step* is positive, the result is an empty list; ``[]<list<int>>``. For example:. * ``!range(4)`` is equivalent to ``!range(0, 4, 1)`` and the result is; `[0, 1, 2, 3]`.; * ``!range(1, 4)`` is equivalent to ``!range(1, 4, 1)`` and the result is; `[1, 2, 3]`.; * The result of ``!range(0, 4, 2)`` is `[0, 2]`.; * The results of ``!range(0, 4, -1)`` and ``!range(4, 0, 1)`` are empty. ``!range(``\ *list*\ ``)``; Equivalent to ``!range(0, !size(list))``. ``!repr(``\ *value*\ ``)``; Represents *value* as a string. String format for the value is not; guaranteed to be stable. Intended for debugging purposes only. ``!setdagarg(``\ *dag*\ ``,``\ *k",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:71365,Testability,log,logically,71365,"!repr(``\ *value*\ ``)``; Represents *value* as a string. String format for the value is not; guaranteed to be stable. Intended for debugging purposes only. ``!setdagarg(``\ *dag*\ ``,``\ *key*\ ``,``\ *arg*\ ``)``; This operator produces a DAG node with the same operator and arguments as; *dag*, but replacing the value of the argument specified by the *key* with; *arg*. That *key* could be either an integer index or a string name. ``!setdagname(``\ *dag*\ ``,``\ *key*\ ``,``\ *name*\ ``)``; This operator produces a DAG node with the same operator and arguments as; *dag*, but replacing the name of the argument specified by the *key* with; *name*. That *key* could be either an integer index or a string name. ``!setdagop(``\ *dag*\ ``,`` *op*\ ``)``; This operator produces a DAG node with the same arguments as *dag*, but with its; operator replaced with *op*. Example: ``!setdagop((foo 1, 2), bar)`` results in ``(bar 1, 2)``. ``!shl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* left logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!size(``\ *a*\ ``)``; This operator produces the size of the string, list, or dag *a*.; The size of a DAG is the number of arguments; the operator does not count. ``!sra(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right arithmetically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!srl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!strconcat(``\ *str1*\ ``,`` *str2*\ ``, ...)``; This operator concatenates the string arguments *str1*, *str2*, etc., and; produces the resulting string. ``!sub(``\ *a*\ ``,`` *b*\ ``)",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:72007,Testability,log,logically,72007,"ith; *name*. That *key* could be either an integer index or a string name. ``!setdagop(``\ *dag*\ ``,`` *op*\ ``)``; This operator produces a DAG node with the same arguments as *dag*, but with its; operator replaced with *op*. Example: ``!setdagop((foo 1, 2), bar)`` results in ``(bar 1, 2)``. ``!shl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* left logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!size(``\ *a*\ ``)``; This operator produces the size of the string, list, or dag *a*.; The size of a DAG is the number of arguments; the operator does not count. ``!sra(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right arithmetically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!srl(``\ *a*\ ``,`` *count*\ ``)``; This operator shifts *a* right logically by *count* bits and produces the resulting; value. The operation is performed on a 64-bit integer; the result; is undefined for shift counts outside 0...63. ``!strconcat(``\ *str1*\ ``,`` *str2*\ ``, ...)``; This operator concatenates the string arguments *str1*, *str2*, etc., and; produces the resulting string. ``!sub(``\ *a*\ ``,`` *b*\ ``)``; This operator subtracts *b* from *a* and produces the arithmetic difference. ``!subst(``\ *target*\ ``,`` *repl*\ ``,`` *value*\ ``)``; This operator replaces all occurrences of the *target* in the *value* with; the *repl* and produces the resulting value. The *value* can; be a string, in which case substring substitution is performed. The *value* can be a record name, in which case the operator produces the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *value*. ``!substr(``\ *string*\ ``,`` *start*\ [``,`` *length*]\ ``)``; This operator extracts a substring of the given *s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:73739,Testability,log,logical,73739," the *repl*; record if the *target* record name equals the *value* record name; otherwise it; produces the *value*. ``!substr(``\ *string*\ ``,`` *start*\ [``,`` *length*]\ ``)``; This operator extracts a substring of the given *string*. The starting; position of the substring is specified by *start*, which can range; between 0 and the length of the string. The length of the substring; is specified by *length*; if not specified, the rest of the string is; extracted. The *start* and *length* arguments must be integers. ``!tail(``\ *a*\ ``)``; This operator produces a new list with all the elements; of the list *a* except for the zeroth one. (See also ``!head``.). ``!tolower(``\ *a*\ ``)``; This operator converts a string input *a* to lower case. ``!toupper(``\ *a*\ ``)``; This operator converts a string input *a* to upper case. ``!xor(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise EXCLUSIVE OR on *a*, *b*, etc., and produces; the result. A logical XOR can be performed if all the arguments are either; 0 or 1. Appendix B: Paste Operator Examples; ===================================. Here is an example illustrating the use of the paste operator in record names. .. code-block:: text. defvar suffix = ""_suffstring"";; defvar some_ints = [0, 1, 2, 3];. def name # suffix {; }. foreach i = [1, 2] in {; def rec # i {; }; }. The first ``def`` does not use the value of the ``suffix`` variable. The; second def does use the value of the ``i`` iterator variable, because it is not a; global name. The following records are produced. .. code-block:: text. def namesuffix {; }; def rec1 {; }; def rec2 {; }. Here is a second example illustrating the paste operator in field value expressions. .. code-block:: text. def test {; string strings = suffix # suffix;; list<int> integers = some_ints # [4, 5, 6];; }. The ``strings`` field expression uses ``suffix`` on both sides of the paste; operator. It is evaluated normally on the left hand side, but taken verbatim; on the right han",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:74510,Testability,test,test,74510,"; This operator converts a string input *a* to upper case. ``!xor(``\ *a*\ ``,`` *b*\ ``, ...)``; This operator does a bitwise EXCLUSIVE OR on *a*, *b*, etc., and produces; the result. A logical XOR can be performed if all the arguments are either; 0 or 1. Appendix B: Paste Operator Examples; ===================================. Here is an example illustrating the use of the paste operator in record names. .. code-block:: text. defvar suffix = ""_suffstring"";; defvar some_ints = [0, 1, 2, 3];. def name # suffix {; }. foreach i = [1, 2] in {; def rec # i {; }; }. The first ``def`` does not use the value of the ``suffix`` variable. The; second def does use the value of the ``i`` iterator variable, because it is not a; global name. The following records are produced. .. code-block:: text. def namesuffix {; }; def rec1 {; }; def rec2 {; }. Here is a second example illustrating the paste operator in field value expressions. .. code-block:: text. def test {; string strings = suffix # suffix;; list<int> integers = some_ints # [4, 5, 6];; }. The ``strings`` field expression uses ``suffix`` on both sides of the paste; operator. It is evaluated normally on the left hand side, but taken verbatim; on the right hand side. The ``integers`` field expression uses the value of; the ``some_ints`` variable and a literal list. The following record is; produced. .. code-block:: text. def test {; string strings = ""_suffstringsuffix"";; list<int> ints = [0, 1, 2, 3, 4, 5, 6];; }. Appendix C: Sample Record; =========================. One target machine supported by LLVM is the Intel x86. The following output; from TableGen shows the record that is created to represent the 32-bit; register-to-register ADD instruction. .. code-block:: text. def ADD32rr {	// InstructionEncoding Instruction X86Inst I ITy Sched BinOpRR BinOpRR_RF; int Size = 0;; string DecoderNamespace = """";; list<Predicate> Predicates = [];; string DecoderMethod = """";; bit hasCompleteDecoder = 1;; string Namespace = ""X86"";; dag O",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:74941,Testability,test,test,74941," defvar suffix = ""_suffstring"";; defvar some_ints = [0, 1, 2, 3];. def name # suffix {; }. foreach i = [1, 2] in {; def rec # i {; }; }. The first ``def`` does not use the value of the ``suffix`` variable. The; second def does use the value of the ``i`` iterator variable, because it is not a; global name. The following records are produced. .. code-block:: text. def namesuffix {; }; def rec1 {; }; def rec2 {; }. Here is a second example illustrating the paste operator in field value expressions. .. code-block:: text. def test {; string strings = suffix # suffix;; list<int> integers = some_ints # [4, 5, 6];; }. The ``strings`` field expression uses ``suffix`` on both sides of the paste; operator. It is evaluated normally on the left hand side, but taken verbatim; on the right hand side. The ``integers`` field expression uses the value of; the ``some_ints`` variable and a literal list. The following record is; produced. .. code-block:: text. def test {; string strings = ""_suffstringsuffix"";; list<int> ints = [0, 1, 2, 3, 4, 5, 6];; }. Appendix C: Sample Record; =========================. One target machine supported by LLVM is the Intel x86. The following output; from TableGen shows the record that is created to represent the 32-bit; register-to-register ADD instruction. .. code-block:: text. def ADD32rr {	// InstructionEncoding Instruction X86Inst I ITy Sched BinOpRR BinOpRR_RF; int Size = 0;; string DecoderNamespace = """";; list<Predicate> Predicates = [];; string DecoderMethod = """";; bit hasCompleteDecoder = 1;; string Namespace = ""X86"";; dag OutOperandList = (outs GR32:$dst);; dag InOperandList = (ins GR32:$src1, GR32:$src2);; string AsmString = ""add{l}	{$src2, $src1|$src1, $src2}"";; EncodingByHwMode EncodingInfos = ?;; list<dag> Pattern = [(set GR32:$dst, EFLAGS, (X86add_flag GR32:$src1, GR32:$src2))];; list<Register> Uses = [];; list<Register> Defs = [EFLAGS];; int CodeSize = 3;; int AddedComplexity = 0;; bit isPreISelOpcode = 0;; bit isReturn = 0;; bit isBranch =",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:960,Usability,simpl,simple,960,"===============================; TableGen Programmer's Reference; ===============================. .. sectnum::. .. contents::; :local:. Introduction; ============. The purpose of TableGen is to generate complex output files based on; information from source files that are significantly easier to code than the; output files would be, and also easier to maintain and modify over time. The; information is coded in a declarative style involving classes and records,; which are then processed by TableGen. The internalized records are passed on; to various *backends*, which extract information from a subset of the records; and generate one or more output files. These output files are typically; ``.inc`` files for C++, but may be any type of file that the backend; developer needs. This document describes the LLVM TableGen facility in detail. It is intended; for the programmer who is using TableGen to produce code for a project. If; you are looking for a simple overview, check out the :doc:`TableGen Overview; <./index>`. The various ``*-tblgen`` commands used to invoke TableGen are; described in :doc:`tblgen Family - Description to C++; Code<../CommandGuide/tblgen>`. An example of a backend is ``RegisterInfo``, which generates the register; file information for a particular target machine, for use by the LLVM; target-independent code generator. See :doc:`TableGen Backends <./BackEnds>`; for a description of the LLVM TableGen backends, and :doc:`TableGen; Backend Developer's Guide <./BackGuide>` for a guide to writing a new; backend. Here are a few of the things backends can do. * Generate the register file information for a particular target machine. * Generate the instruction definitions for a target. * Generate the patterns that the code generator uses to match instructions; to intermediate representation (IR) nodes. * Generate semantic attribute identifiers for Clang. * Generate abstract syntax tree (AST) declaration node definitions for Clang. * Generate AST statement nod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:1517,Usability,guid,guide,1517,"ecords are passed on; to various *backends*, which extract information from a subset of the records; and generate one or more output files. These output files are typically; ``.inc`` files for C++, but may be any type of file that the backend; developer needs. This document describes the LLVM TableGen facility in detail. It is intended; for the programmer who is using TableGen to produce code for a project. If; you are looking for a simple overview, check out the :doc:`TableGen Overview; <./index>`. The various ``*-tblgen`` commands used to invoke TableGen are; described in :doc:`tblgen Family - Description to C++; Code<../CommandGuide/tblgen>`. An example of a backend is ``RegisterInfo``, which generates the register; file information for a particular target machine, for use by the LLVM; target-independent code generator. See :doc:`TableGen Backends <./BackEnds>`; for a description of the LLVM TableGen backends, and :doc:`TableGen; Backend Developer's Guide <./BackGuide>` for a guide to writing a new; backend. Here are a few of the things backends can do. * Generate the register file information for a particular target machine. * Generate the instruction definitions for a target. * Generate the patterns that the code generator uses to match instructions; to intermediate representation (IR) nodes. * Generate semantic attribute identifiers for Clang. * Generate abstract syntax tree (AST) declaration node definitions for Clang. * Generate AST statement node definitions for Clang. Concepts; --------. TableGen source files contain two primary items: *abstract records* and; *concrete records*. In this and other TableGen documents, abstract records; are called *classes.* (These classes are different from C++ classes and do; not map onto them.) In addition, concrete records are usually just called; records, although sometimes the term *record* refers to both classes and; concrete records. The distinction should be clear in context. Classes and concrete records have a unique",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:2464,Usability,clear,clear,2464,"`TableGen; Backend Developer's Guide <./BackGuide>` for a guide to writing a new; backend. Here are a few of the things backends can do. * Generate the register file information for a particular target machine. * Generate the instruction definitions for a target. * Generate the patterns that the code generator uses to match instructions; to intermediate representation (IR) nodes. * Generate semantic attribute identifiers for Clang. * Generate abstract syntax tree (AST) declaration node definitions for Clang. * Generate AST statement node definitions for Clang. Concepts; --------. TableGen source files contain two primary items: *abstract records* and; *concrete records*. In this and other TableGen documents, abstract records; are called *classes.* (These classes are different from C++ classes and do; not map onto them.) In addition, concrete records are usually just called; records, although sometimes the term *record* refers to both classes and; concrete records. The distinction should be clear in context. Classes and concrete records have a unique *name*, either chosen by; the programmer or generated by TableGen. Associated with that name; is a list of *fields* with values and an optional list of *parent classes*; (sometimes called base or super classes). The fields are the primary data that; backends will process. Note that TableGen assigns no meanings to fields; the; meanings are entirely up to the backends and the programs that incorporate; the output of those backends. .. note::. The term ""parent class"" can refer to a class that is a parent of another; class, and also to a class from which a concrete record inherits. This; nonstandard use of the term arises because TableGen treats classes and; concrete records similarly. A backend processes some subset of the concrete records built by the; TableGen parser and emits the output files. These files are usually C++; ``.inc`` files that are included by the programs that require the data in; those records. However, a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:5658,Usability,simpl,simple,5658," parent class ``S`` with one set of template arguments, while record or class; ``B`` can request ``S`` with a different set of arguments. Without template; arguments, many more classes would be required, one for each combination of; the template arguments. Both classes and concrete records can include fields that are uninitialized.; The uninitialized ""value"" is represented by a question mark (``?``). Classes; often have uninitialized fields that are expected to be filled in when those; classes are inherited by concrete records. Even so, some fields of concrete; records may remain uninitialized. TableGen provides *multiclasses* to collect a group of record definitions in; one place. A multiclass is a sort of macro that can be ""invoked"" to define; multiple concrete records all at once. A multiclass can inherit from other; multiclasses, which means that the multiclass inherits all the definitions; from its parent multiclasses. `Appendix C: Sample Record`_ illustrates a complex record in the Intel X86; target and the simple way in which it is defined. Source Files; ============. TableGen source files are plain ASCII text files. The files can contain; statements, comments, and blank lines (see `Lexical Analysis`_). The standard file; extension for TableGen files is ``.td``. TableGen files can grow quite large, so there is an include mechanism that; allows one file to include the content of another file (see `Include; Files`_). This allows large files to be broken up into smaller ones, and; also provides a simple library mechanism where multiple source files can; include the same library file. TableGen supports a simple preprocessor that can be used to conditionalize; portions of ``.td`` files. See `Preprocessing Facilities`_ for more; information. Lexical Analysis; ================. The lexical and syntax notation used here is intended to imitate; `Python's`_ notation. In particular, for lexical definitions, the productions; operate at the character level and there is no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:6155,Usability,simpl,simple,6155,"rds. Even so, some fields of concrete; records may remain uninitialized. TableGen provides *multiclasses* to collect a group of record definitions in; one place. A multiclass is a sort of macro that can be ""invoked"" to define; multiple concrete records all at once. A multiclass can inherit from other; multiclasses, which means that the multiclass inherits all the definitions; from its parent multiclasses. `Appendix C: Sample Record`_ illustrates a complex record in the Intel X86; target and the simple way in which it is defined. Source Files; ============. TableGen source files are plain ASCII text files. The files can contain; statements, comments, and blank lines (see `Lexical Analysis`_). The standard file; extension for TableGen files is ``.td``. TableGen files can grow quite large, so there is an include mechanism that; allows one file to include the content of another file (see `Include; Files`_). This allows large files to be broken up into smaller ones, and; also provides a simple library mechanism where multiple source files can; include the same library file. TableGen supports a simple preprocessor that can be used to conditionalize; portions of ``.td`` files. See `Preprocessing Facilities`_ for more; information. Lexical Analysis; ================. The lexical and syntax notation used here is intended to imitate; `Python's`_ notation. In particular, for lexical definitions, the productions; operate at the character level and there is no implied whitespace between; elements. The syntax definitions operate at the token level, so there is; implied whitespace between tokens. .. _`Python's`: http://docs.python.org/py3k/reference/introduction.html#notation. TableGen supports BCPL-style comments (``// ...``) and nestable C-style; comments (``/* ... */``).; TableGen also provides simple `Preprocessing Facilities`_. Formfeed characters may be used freely in files to produce page breaks when; the file is printed for review. The following are the basic punctuation to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:6264,Usability,simpl,simple,6264,"ord definitions in; one place. A multiclass is a sort of macro that can be ""invoked"" to define; multiple concrete records all at once. A multiclass can inherit from other; multiclasses, which means that the multiclass inherits all the definitions; from its parent multiclasses. `Appendix C: Sample Record`_ illustrates a complex record in the Intel X86; target and the simple way in which it is defined. Source Files; ============. TableGen source files are plain ASCII text files. The files can contain; statements, comments, and blank lines (see `Lexical Analysis`_). The standard file; extension for TableGen files is ``.td``. TableGen files can grow quite large, so there is an include mechanism that; allows one file to include the content of another file (see `Include; Files`_). This allows large files to be broken up into smaller ones, and; also provides a simple library mechanism where multiple source files can; include the same library file. TableGen supports a simple preprocessor that can be used to conditionalize; portions of ``.td`` files. See `Preprocessing Facilities`_ for more; information. Lexical Analysis; ================. The lexical and syntax notation used here is intended to imitate; `Python's`_ notation. In particular, for lexical definitions, the productions; operate at the character level and there is no implied whitespace between; elements. The syntax definitions operate at the token level, so there is; implied whitespace between tokens. .. _`Python's`: http://docs.python.org/py3k/reference/introduction.html#notation. TableGen supports BCPL-style comments (``// ...``) and nestable C-style; comments (``/* ... */``).; TableGen also provides simple `Preprocessing Facilities`_. Formfeed characters may be used freely in files to produce page breaks when; the file is printed for review. The following are the basic punctuation tokens::. - + [ ] { } ( ) < > : ; . ... = ? #. Literals; --------. Numeric literals take one of the following forms:. .. productionl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:6972,Usability,simpl,simple,6972,"e mechanism that; allows one file to include the content of another file (see `Include; Files`_). This allows large files to be broken up into smaller ones, and; also provides a simple library mechanism where multiple source files can; include the same library file. TableGen supports a simple preprocessor that can be used to conditionalize; portions of ``.td`` files. See `Preprocessing Facilities`_ for more; information. Lexical Analysis; ================. The lexical and syntax notation used here is intended to imitate; `Python's`_ notation. In particular, for lexical definitions, the productions; operate at the character level and there is no implied whitespace between; elements. The syntax definitions operate at the token level, so there is; implied whitespace between tokens. .. _`Python's`: http://docs.python.org/py3k/reference/introduction.html#notation. TableGen supports BCPL-style comments (``// ...``) and nestable C-style; comments (``/* ... */``).; TableGen also provides simple `Preprocessing Facilities`_. Formfeed characters may be used freely in files to produce page breaks when; the file is printed for review. The following are the basic punctuation tokens::. - + [ ] { } ( ) < > : ; . ... = ? #. Literals; --------. Numeric literals take one of the following forms:. .. productionlist::; TokInteger: `DecimalInteger` | `HexInteger` | `BinInteger`; DecimalInteger: [""+"" | ""-""] (""0""...""9"")+; HexInteger: ""0x"" (""0""...""9"" | ""a""...""f"" | ""A""...""F"")+; BinInteger: ""0b"" (""0"" | ""1"")+. Observe that the :token:`DecimalInteger` token includes the optional ``+``; or ``-`` sign, unlike most languages where the sign would be treated as a; unary operator. TableGen has two kinds of string literals:. .. productionlist::; TokString: '""' (non-'""' characters and escapes) '""'; TokCode: ""[{"" (shortest text not containing ""}]"") ""}]"". A :token:`TokCode` is nothing more than a multi-line string literal; delimited by ``[{`` and ``}]``. It can break across lines and the; line breaks are ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:10228,Usability,simpl,simple,10228,"gname !getdagop !gt !head !if; : !interleave !isa !le !listconcat !listremove; : !listsplat !logtwo !lt !mul !ne; : !not !or !range !repr !setdagarg; : !setdagname !setdagop !shl !size !sra; : !srl !strconcat !sub !subst !substr; : !tail !tolower !toupper !xor. The ``!cond`` operator has a slightly different; syntax compared to other bang operators, so it is defined separately:. .. productionlist::; CondOperator: !cond. See `Appendix A: Bang Operators`_ for a description of each bang operator. Include files; -------------. TableGen has an include mechanism. The content of the included file; lexically replaces the ``include`` directive and is then parsed as if it was; originally in the main file. .. productionlist::; IncludeDirective: ""include"" `TokString`. Portions of the main file and included files can be conditionalized using; preprocessor directives. .. productionlist::; PreprocessorDirective: ""#define"" | ""#ifdef"" | ""#ifndef"". Types; =====. The TableGen language is statically typed, using a simple but complete type; system. Types are used to check for errors, to perform implicit conversions,; and to help interface designers constrain the allowed input. Every value is; required to have an associated type. TableGen supports a mixture of low-level types (e.g., ``bit``) and; high-level types (e.g., ``dag``). This flexibility allows you to describe a; wide range of records conveniently and compactly. .. productionlist::; Type: ""bit"" | ""int"" | ""string"" | ""dag""; :| ""bits"" ""<"" `TokInteger` "">""; :| ""list"" ""<"" `Type` "">""; :| `ClassID`; ClassID: `TokIdentifier`. ``bit``; A ``bit`` is a boolean value that can be 0 or 1. ``int``; The ``int`` type represents a simple 64-bit integer value, such as 5 or; -42. ``string``; The ``string`` type represents an ordered sequence of characters of arbitrary; length. ``bits<``\ *n*\ ``>``; The ``bits`` type is a fixed-sized integer of arbitrary length *n* that; is treated as separate bits. These bits can be accessed individually.; A field ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:10897,Usability,simpl,simple,10897,"ally in the main file. .. productionlist::; IncludeDirective: ""include"" `TokString`. Portions of the main file and included files can be conditionalized using; preprocessor directives. .. productionlist::; PreprocessorDirective: ""#define"" | ""#ifdef"" | ""#ifndef"". Types; =====. The TableGen language is statically typed, using a simple but complete type; system. Types are used to check for errors, to perform implicit conversions,; and to help interface designers constrain the allowed input. Every value is; required to have an associated type. TableGen supports a mixture of low-level types (e.g., ``bit``) and; high-level types (e.g., ``dag``). This flexibility allows you to describe a; wide range of records conveniently and compactly. .. productionlist::; Type: ""bit"" | ""int"" | ""string"" | ""dag""; :| ""bits"" ""<"" `TokInteger` "">""; :| ""list"" ""<"" `Type` "">""; :| `ClassID`; ClassID: `TokIdentifier`. ``bit``; A ``bit`` is a boolean value that can be 0 or 1. ``int``; The ``int`` type represents a simple 64-bit integer value, such as 5 or; -42. ``string``; The ``string`` type represents an ordered sequence of characters of arbitrary; length. ``bits<``\ *n*\ ``>``; The ``bits`` type is a fixed-sized integer of arbitrary length *n* that; is treated as separate bits. These bits can be accessed individually.; A field of this type is useful for representing an instruction operation; code, register number, or address mode/register/displacement. The bits of; the field can be set individually or as subfields. For example, in an; instruction address, the addressing mode, base register number, and; displacement can be set separately. ``list<``\ *type*\ ``>``; This type represents a list whose elements are of the *type* specified in; angle brackets. The element type is arbitrary; it can even be another; list type. List elements are indexed from 0. ``dag``; This type represents a nestable directed acyclic graph (DAG) of nodes.; Each node has an *operator* and zero or more *arguments* (or *opera",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:14390,Usability,simpl,simple,14390," :| ""["" `SliceElements` ""]""; :| ""."" `TokIdentifier`; RangeList: `RangePiece` ("","" `RangePiece`)*; RangePiece: `TokInteger`; :| `TokInteger` ""..."" `TokInteger`; :| `TokInteger` ""-"" `TokInteger`; :| `TokInteger` `TokInteger`; SliceElements: (`SliceElement` "","")* `SliceElement` "",""?; SliceElement: `Value`; :| `Value` ""..."" `Value`; :| `Value` ""-"" `Value`; :| `Value` `TokInteger`. .. warning::; The peculiar last form of :token:`RangePiece` and :token:`SliceElement` is; due to the fact that the ""``-``"" is included in the :token:`TokInteger`,; hence ``1-5`` gets lexed as two consecutive tokens, with values ``1`` and; ``-5``, instead of ""1"", ""-"", and ""5"".; The use of hyphen as the range punctuation is deprecated. Simple values; -------------. The :token:`SimpleValue` has a number of forms. .. productionlist::; SimpleValue: `TokInteger` | `TokString`+ | `TokCode`. A value can be an integer literal, a string literal, or a code literal.; Multiple adjacent string literals are concatenated as in C/C++; the simple; value is the concatenation of the strings. Code literals become strings and; are then indistinguishable from them. .. productionlist::; SimpleValue2: ""true"" | ""false"". The ``true`` and ``false`` literals are essentially syntactic sugar for the; integer values 1 and 0. They improve the readability of TableGen files when; boolean values are used in field initializations, bit sequences, ``if``; statements, etc. When parsed, these literals are converted to integers. .. note::. Although ``true`` and ``false`` are literal names for 1 and 0, we; recommend as a stylistic rule that you use them for boolean; values only. .. productionlist::; SimpleValue3: ""?"". A question mark represents an uninitialized value. .. productionlist::; SimpleValue4: ""{"" [`ValueList`] ""}""; ValueList: `ValueListNE`; ValueListNE: `Value` ("","" `Value`)*. This value represents a sequence of bits, which can be used to initialize a; ``bits<``\ *n*\ ``>`` field (note the braces). When doing so, the values; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:16447,Usability,guid,guide,16447,"present a total of *n* bits. .. productionlist::; SimpleValue5: ""["" `ValueList` ""]"" [""<"" `Type` "">""]. This value is a list initializer (note the brackets). The values in brackets; are the elements of the list. The optional :token:`Type` can be used to; indicate a specific element type; otherwise the element type is inferred; from the given values. TableGen can usually infer the type, although; sometimes not when the value is the empty list (``[]``). .. productionlist::; SimpleValue6: ""("" `DagArg` [`DagArgList`] "")""; DagArgList: `DagArg` ("","" `DagArg`)*; DagArg: `Value` ["":"" `TokVarName`] | `TokVarName`. This represents a DAG initializer (note the parentheses). The first; :token:`DagArg` is called the ""operator"" of the DAG and must be a record.; See `Directed acyclic graphs (DAGs)`_ for more details. .. productionlist::; SimpleValue7: `TokIdentifier`. The resulting value is the value of the entity named by the identifier. The; possible identifiers are described here, but the descriptions will make more; sense after reading the remainder of this guide. .. The code for this is exceptionally abstruse. These examples are a; best-effort attempt. * A template argument of a ``class``, such as the use of ``Bar`` in::. class Foo <int Bar> {; int Baz = Bar;; }. * The implicit template argument ``NAME`` in a ``class`` or ``multiclass``; definition (see `NAME`_). * A field local to a ``class``, such as the use of ``Bar`` in::. class Foo {; int Bar = 5;; int Baz = Bar;; }. * The name of a record definition, such as the use of ``Bar`` in the; definition of ``Foo``::. def Bar : SomeClass {; int X = 5;; }. def Foo {; SomeClass Baz = Bar;; }. * A field local to a record definition, such as the use of ``Bar`` in::. def Foo {; int Bar = 5;; int Baz = Bar;; }. Fields inherited from the record's parent classes can be accessed the same way. * A template argument of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:17942,Usability,simpl,simple,17942," in the; definition of ``Foo``::. def Bar : SomeClass {; int X = 5;; }. def Foo {; SomeClass Baz = Bar;; }. * A field local to a record definition, such as the use of ``Bar`` in::. def Foo {; int Bar = 5;; int Baz = Bar;; }. Fields inherited from the record's parent classes can be accessed the same way. * A template argument of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined with the ``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Value` "":"" `Value`. The bang operators provide functions that are not available with the other; simple values. Except in the case of ``!cond``, a bang operator takes a list; of arguments enclosed in parentheses and performs some function on those; arguments, producing a value for that bang operator. The ``!cond`` operator; takes a list of pairs of arguments separated by colons. See `Appendix A:; Bang Operators`_ for a description of each bang operator. Suffixed values; ---------------. The :token:`SimpleValue` values described above can be specified with; certain suffixes. The purpose of a suffix is to obtain a subvalue of the; primary value. Here are the possible suffixes for some primary *value*. *value*\ ``{17}``; The final val",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:18288,Usability,simpl,simple,18288,"nt of a ``multiclass``, such as the use of ``Bar`` in::. multiclass Foo <int Bar> {; def : SomeClass<Bar>;; }. * A variable defined with the ``defvar`` or ``defset`` statements. * The iteration variable of a ``foreach``, such as the use of ``i`` in::. foreach i = 0...5 in; def Foo#i;. .. productionlist::; SimpleValue8: `ClassID` ""<"" `ArgValueList` "">"". This form creates a new anonymous record definition (as would be created by an; unnamed ``def`` inheriting from the given class with the given template; arguments; see `def`_) and the value is that record. A field of the record can be; obtained using a suffix; see `Suffixed Values`_. Invoking a class in this manner can provide a simple subroutine facility.; See `Using Classes as Subroutines`_ for more information. .. productionlist::; SimpleValue9: `BangOperator` [""<"" `Type` "">""] ""("" `ValueListNE` "")""; :| `CondOperator` ""("" `CondClause` ("","" `CondClause`)* "")""; CondClause: `Value` "":"" `Value`. The bang operators provide functions that are not available with the other; simple values. Except in the case of ``!cond``, a bang operator takes a list; of arguments enclosed in parentheses and performs some function on those; arguments, producing a value for that bang operator. The ``!cond`` operator; takes a list of pairs of arguments separated by colons. See `Appendix A:; Bang Operators`_ for a description of each bang operator. Suffixed values; ---------------. The :token:`SimpleValue` values described above can be specified with; certain suffixes. The purpose of a suffix is to obtain a subvalue of the; primary value. Here are the possible suffixes for some primary *value*. *value*\ ``{17}``; The final value is bit 17 of the integer *value* (note the braces). *value*\ ``{8...15}``; The final value is bits 8--15 of the integer *value*. The order of the; bits can be reversed by specifying ``{15...8}``. *value*\ ``[i]``; The final value is element `i` of the list *value* (note the brackets).; In other words, the brackets act as",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:29120,Usability,simpl,simple,29120,"nside a ``multiclass``; statement. See the ``multiclass`` section below for details. A record can inherit from one or more classes by specifying the; :token:`ParentClassList` clause at the beginning of its record body. All of; the fields in the parent classes are added to the record. If two or more; parent classes provide the same field, the record ends up with the field value; of the last parent class. As a special case, the name of a record can be passed as a template argument; to that record's parent classes. For example:. .. code-block:: text. class A <dag d> {; dag the_dag = d;; }. def rec1 : A<(ops rec1)>;. The DAG ``(ops rec1)`` is passed as a template argument to class ``A``. Notice; that the DAG includes ``rec1``, the record being defined. The steps taken to create a new record are somewhat complex. See `How; records are built`_. See `Examples: classes and records`_ for examples. Examples: classes and records; -----------------------------. Here is a simple TableGen file with one class and two record definitions. .. code-block:: text. class C {; bit V = true;; }. def X : C;; def Y : C {; let V = false;; string Greeting = ""Hello!"";; }. First, the abstract class ``C`` is defined. It has one field named ``V``; that is a bit initialized to true. Next, two records are defined, derived from class ``C``; that is, with ``C``; as their parent class. Thus they both inherit the ``V`` field. Record ``Y``; also defines another string field, ``Greeting``, which is initialized to; ``""Hello!""``. In addition, ``Y`` overrides the inherited ``V`` field,; setting it to false. A class is useful for isolating the common features of multiple records in; one place. A class can initialize common fields to default values, but; records inheriting from that class can override the defaults. TableGen supports the definition of parameterized classes as well as; nonparameterized ones. Parameterized classes specify a list of variable; declarations, which may optionally have defaults, that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:39135,Usability,simpl,simple,39135,"g; a common set of fields to all the records created by the ``defm``. The name is parsed in the same special mode used by ``def``. If the name is; not included, an unspecified but globally unique name is provided. That is,; the following examples end up with different names::. defm : SomeMultiClass<...>; // A globally unique name.; defm """" : SomeMultiClass<...>; // An empty name. The ``defm`` statement can be used in a multiclass body. When this occurs,; the second variant is equivalent to::. defm NAME : SomeMultiClass<...>;. More generally, when ``defm`` occurs in a multiclass and its name does not; include a use of the implicit template argument ``NAME``, then ``NAME`` will; be prepended automatically. That is, the following are equivalent inside a; multiclass::. defm Foo : SomeMultiClass<...>;; defm NAME # Foo : SomeMultiClass<...>;. See `Examples: multiclasses and defms`_ for examples. Examples: multiclasses and defms; --------------------------------. Here is a simple example using ``multiclass`` and ``defm``. Consider a; 3-address instruction architecture whose instructions come in two formats:; ``reg = reg op reg`` and ``reg = reg op imm`` (immediate). The SPARC is an; example of such an architecture. .. code-block:: text. def ops;; def GPR;; def Imm;; class inst <int opc, string asmstr, dag operandlist>;. multiclass ri_inst <int opc, string asmstr> {; def _rr : inst<opc, !strconcat(asmstr, "" $dst, $src1, $src2""),; (ops GPR:$dst, GPR:$src1, GPR:$src2)>;; def _ri : inst<opc, !strconcat(asmstr, "" $dst, $src1, $src2""),; (ops GPR:$dst, GPR:$src1, Imm:$src2)>;; }. // Define records for each instruction in the RR and RI formats.; defm ADD : ri_inst<0b111, ""add"">;; defm SUB : ri_inst<0b101, ""sub"">;; defm MUL : ri_inst<0b100, ""mul"">;. Each use of the ``ri_inst`` multiclass defines two records, one with the; ``_rr`` suffix and one with ``_ri``. Recall that the name of the ``defm``; that uses a multiclass is prepended to the names of the records defined in; that multic",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:50021,Usability,simpl,simplify,50021,"ts an error message if it is not. .. productionlist::; Assert: ""assert"" `condition` "","" `message` "";"". If the boolean condition is true, the statement does nothing. If the; condition is false, it prints a nonfatal error message. The **message**, which; can be an arbitrary string expression, is included in the error message as a; note. The exact behavior of the ``assert`` statement depends on its; placement. * At top level, the assertion is checked immediately. * In a record definition, the statement is saved and all assertions are; checked after the record is completely built. * In a class definition, the assertions are saved and inherited by all; the subclasses and records that inherit from the class. The assertions are; then checked when the records are completely built. * In a multiclass definition, the assertions are saved with the other; components of the multiclass and then checked each time the multiclass; is instantiated with ``defm``. Using assertions in TableGen files can simplify record checking in TableGen; backends. Here is an example of an ``assert`` in two class definitions. .. code-block:: text. class PersonName<string name> {; assert !le(!size(name), 32), ""person name is too long: "" # name;; string Name = name;; }. class Person<string name, int age> : PersonName<name> {; assert !and(!ge(age, 1), !le(age, 120)), ""person age is invalid: "" # age;; int Age = age;; }. def Rec20 : Person<""Donald Knuth"", 60> {; ...; }. Additional Details; ==================. Directed acyclic graphs (DAGs); ------------------------------. A directed acyclic graph can be represented directly in TableGen using the; ``dag`` datatype. A DAG node consists of an operator and zero or more; arguments (or operands). Each argument can be of any desired type. By using; another DAG node as an argument, an arbitrary graph of DAG nodes can be; built. The syntax of a ``dag`` instance is:. ``(`` *operator* *argument1*\ ``,`` *argument2*\ ``,`` ... ``)``. The operator must be present and mus",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:53350,Usability,simpl,simply,53350,"le extends from the; ``defvar`` statement to the end of the body. It cannot be set to a different; value within its scope. The ``defvar`` statement can also be used in the statement; list of a ``foreach``, which establishes a scope. A variable named ``V`` in an inner scope shadows (hides) any variables ``V``; in outer scopes. In particular, there are several cases:. * ``V`` in a record body shadows a global ``V``. * ``V`` in a record body shadows template argument ``V``. * ``V`` in template arguments shadows a global ``V``. * ``V`` in a ``foreach`` statement list shadows any ``V`` in surrounding record or; global scopes. Variables defined in a ``foreach`` go out of scope at the end of; each loop iteration, so their value in one iteration is not available in; the next iteration. The following ``defvar`` will not work::. defvar i = !add(i, 1). How records are built; ---------------------. The following steps are taken by TableGen when a record is built. Classes are simply; abstract records and so go through the same steps. 1. Build the record name (:token:`NameValue`) and create an empty record. 2. Parse the parent classes in the :token:`ParentClassList` from left to; right, visiting each parent class's ancestor classes from top to bottom. a. Add the fields from the parent class to the record.; b. Substitute the template arguments into those fields.; c. Add the parent class to the record's list of inherited classes. 3. Apply any top-level ``let`` bindings to the record. Recall that top-level; bindings only apply to inherited fields. 4. Parse the body of the record. * Add any fields to the record.; * Modify the values of fields according to local ``let`` statements.; * Define any ``defvar`` variables. 5. Make a pass over all the fields to resolve any inter-field references. 6. Add the record to the final record list. Because references between fields are resolved (step 5) after ``let`` bindings are; applied (step 3), the ``let`` statement has unusual power. For example:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:55321,Usability,simpl,simple,55321,"), the ``let`` statement has unusual power. For example:. .. code-block:: text. class C <int x> {; int Y = x;; int Yplus1 = !add(Y, 1);; int xplus1 = !add(x, 1);; }. let Y = 10 in {; def rec1 : C<5> {; }; }. def rec2 : C<5> {; let Y = 10;; }. In both cases, one where a top-level ``let`` is used to bind ``Y`` and one; where a local ``let`` does the same thing, the results are:. .. code-block:: text. def rec1 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }; def rec2 { // C; int Y = 10;; int Yplus1 = 11;; int xplus1 = 6;; }. ``Yplus1`` is 11 because the ``let Y`` is performed before the ``!add(Y,; 1)`` is resolved. Use this power wisely. Using Classes as Subroutines; ============================. As described in `Simple values`_, a class can be invoked in an expression; and passed template arguments. This causes TableGen to create a new anonymous; record inheriting from that class. As usual, the record receives all the; fields defined in the class. This feature can be employed as a simple subroutine facility. The class can; use the template arguments to define various variables and fields, which end; up in the anonymous record. Those fields can then be retrieved in the; expression invoking the class as follows. Assume that the field ``ret``; contains the final value of the subroutine. .. code-block:: text. int Result = ... CalcValue<arg>.ret ...;. The ``CalcValue`` class is invoked with the template argument ``arg``. It; calculates a value for the ``ret`` field, which is then retrieved at the; ""point of call"" in the initialization for the Result field. The anonymous; record created in this example serves no other purpose than to carry the; result value. Here is a practical example. The class ``isValidSize`` determines whether a; specified number of bytes represents a valid data size. The bit ``ret`` is; set appropriately. The field ``ValidSize`` obtains its initial value by; invoking ``isValidSize`` with the data size and retrieving the ``ret`` field; from th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:56705,Usability,simpl,simple,56705,";. The ``CalcValue`` class is invoked with the template argument ``arg``. It; calculates a value for the ``ret`` field, which is then retrieved at the; ""point of call"" in the initialization for the Result field. The anonymous; record created in this example serves no other purpose than to carry the; result value. Here is a practical example. The class ``isValidSize`` determines whether a; specified number of bytes represents a valid data size. The bit ``ret`` is; set appropriately. The field ``ValidSize`` obtains its initial value by; invoking ``isValidSize`` with the data size and retrieving the ``ret`` field; from the resulting anonymous record. .. code-block:: text. class isValidSize<int size> {; bit ret = !cond(!eq(size, 1): 1,; !eq(size, 2): 1,; !eq(size, 4): 1,; !eq(size, 8): 1,; !eq(size, 16): 1,; true: 0);; }. def Data1 {; int Size = ...;; bit ValidSize = isValidSize<Size>.ret;; }. Preprocessing Facilities; ========================. The preprocessor embedded in TableGen is intended only for simple; conditional compilation. It supports the following directives, which are; specified somewhat informally. .. productionlist::; LineBegin: beginning of line; LineEnd: newline | return | EOF; WhiteSpace: space | tab; CComment: ""/*"" ... ""*/""; BCPLComment: ""//"" ... `LineEnd`; WhiteSpaceOrCComment: `WhiteSpace` | `CComment`; WhiteSpaceOrAnyComment: `WhiteSpace` | `CComment` | `BCPLComment`; MacroName: `ualpha` (`ualpha` | ""0""...""9"")*; PreDefine: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#define"" (`WhiteSpace`)+ `MacroName`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreIfdef: `LineBegin` (`WhiteSpaceOrCComment`)*; : (""#ifdef"" | ""#ifndef"") (`WhiteSpace`)+ `MacroName`; : (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreElse: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#else"" (`WhiteSpaceOrAnyComment`)* `LineEnd`; PreEndif: `LineBegin` (`WhiteSpaceOrCComment`)*; : ""#endif"" (`WhiteSpaceOrAnyComment`)* `LineEnd`. ..; PreRegContentException: `PreIfdef` | `PreElse` | `PreEndif` | EOF; Pr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:65119,Usability,simpl,simply,65119," uses *acc* and; *var* to calculate the accumulated value, which ``!foldl`` stores back in; *acc*. The type of *acc* is the same as *init*; the type of *var* is the; same as the elements of *list*; *expr* must have the same type as *init*. The following example computes the total of the ``Number`` field in the; list of records in ``RecList``::. int x = !foldl(0, RecList, total, rec, !add(total, rec.Number));. If your goal is to filter the list and produce a new list that includes only; some of the elements, see ``!filter``. ``!foreach(``\ *var*\ ``,`` *sequence*\ ``,`` *expr*\ ``)``; This operator creates a new ``list``/``dag`` in which each element is a; function of the corresponding element in the *sequence* ``list``/``dag``.; To perform the function, TableGen binds the variable *var* to an element; and then evaluates the expression. The expression presumably refers; to the variable *var* and calculates the result value. If you simply want to create a list of a certain length containing; the same value repeated multiple times, see ``!listsplat``. ``!ge(``\ *a*\ `,` *b*\ ``)``; This operator produces 1 if *a* is greater than or equal to *b*; 0 otherwise.; The arguments must be ``bit``, ``bits``, ``int``, or ``string`` values. ``!getdagarg<``\ *type*\ ``>(``\ *dag*\ ``,``\ *key*\ ``)``; This operator retrieves the argument from the given *dag* node by the; specified *key*, which is either an integer index or a string name. If that; argument is not convertible to the specified *type*, ``?`` is returned. ``!getdagname(``\ *dag*\ ``,``\ *index*\ ``)``; This operator retrieves the argument name from the given *dag* node by the; specified *index*. If that argument has no name associated, ``?`` is; returned. ``!getdagop(``\ *dag*\ ``)`` --or-- ``!getdagop<``\ *type*\ ``>(``\ *dag*\ ``)``; This operator produces the operator of the given *dag* node.; Example: ``!getdagop((foo 1, 2))`` results in ``foo``. Recall that; DAG operators are always records. The result of ``!getda",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst:78831,Usability,simpl,simpler,78831,"ts<8> Opcode = { 0, 0, 0, 0, 0, 0, 0, 1 };; Format Form = MRMDestReg;; bits<7> FormBits = { 0, 1, 0, 1, 0, 0, 0 };; ImmType ImmT = NoImm;; bit ForceDisassemble = 0;; OperandSize OpSize = OpSize32;; bits<2> OpSizeBits = { 1, 0 };; AddressSize AdSize = AdSizeX;; bits<2> AdSizeBits = { 0, 0 };; Prefix OpPrefix = NoPrfx;; bits<3> OpPrefixBits = { 0, 0, 0 };; Map OpMap = OB;; bits<3> OpMapBits = { 0, 0, 0 };; bit hasREX_WPrefix = 0;; FPFormat FPForm = NotFP;; bit hasLockPrefix = 0;; Domain ExeDomain = GenericDomain;; bit hasREPPrefix = 0;; Encoding OpEnc = EncNormal;; bits<2> OpEncBits = { 0, 0 };; bit HasVEX_W = 0;; bit IgnoresVEX_W = 0;; bit EVEX_W1_VEX_W0 = 0;; bit hasVEX_4V = 0;; bit hasVEX_L = 0;; bit ignoresVEX_L = 0;; bit hasEVEX_K = 0;; bit hasEVEX_Z = 0;; bit hasEVEX_L2 = 0;; bit hasEVEX_B = 0;; bits<3> CD8_Form = { 0, 0, 0 };; int CD8_EltSize = 0;; bit hasEVEX_RC = 0;; bit hasNoTrackPrefix = 0;; bits<7> VectSize = { 0, 0, 1, 0, 0, 0, 0 };; bits<7> CD8_Scale = { 0, 0, 0, 0, 0, 0, 0 };; string FoldGenRegForm = ?;; string EVEX2VEXOverride = ?;; bit isMemoryFoldable = 1;; bit notEVEX2VEXConvertible = 0;; }. On the first line of the record, you can see that the ``ADD32rr`` record; inherited from eight classes. Although the inheritance hierarchy is complex,; using parent classes is much simpler than specifying the 109 individual; fields for each instruction. Here is the code fragment used to define ``ADD32rr`` and multiple other; ``ADD`` instructions:. .. code-block:: text. defm ADD : ArithBinOp_RF<0x00, 0x02, 0x04, ""add"", MRM0r, MRM0m,; X86add_flag, add, 1, 1, 1>;. The ``defm`` statement tells TableGen that ``ArithBinOp_RF`` is a; multiclass, which contains multiple concrete record definitions that inherit; from ``BinOpRR_RF``. That class, in turn, inherits from ``BinOpRR``, which; inherits from ``ITy`` and ``Sched``, and so forth. The fields are inherited; from all the parent classes; for example, ``IsIndirectBranch`` is inherited; from the ``Instruction`` class.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/TableGen/ProgRef.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:2936,Availability,avail,available,2936," with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will use a lightly modified version of the; Kaleidoscope REPL from `Chapter 7 <LangImpl07.html>`_ of the ""Implementing a; language in LLVM tutorial"". Finally, a word on API generations: ORC is the 3rd generation of LLVM JIT API.; It was preceded by MCJIT, and before that by the (now deleted) legacy JIT.; These tutorials don't assume any experience with these earlier APIs, but; readers acquainted with them will see many familiar elements. Where appropriate; we will make this connection with the earlier APIs explicit to help people who; are transitioning from them to ORC. JIT API Basics; ==============. The purpose of a JIT compiler is to compile code ""on-the-fly"" as it is needed,; rather than compiling whole programs to disk ahead of time as a traditional; compiler does. To support that aim our initial, bare-bones JIT API will have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module> M)``: Make the given IR module; available for execution.; 2. ``Expected<ExecutorSymbolDef> lookup()``: Search for pointers to; symbols (functions or variables) that have been added to the JIT. A basic use-case for this API, executing the 'main' function from a module,; will look like:. .. code-block:: c++. JIT J;; J.addModule(buildModule());; auto *Main = J.lookup(""main"").getAddress().toPtr<int(*)(int, char *[])>();; int Result = Main();. The APIs that we build in these tutorials will all be variations on this simple; theme. Behind this API we will refine the implementation of the JIT to add; support for concurrent compilation, optimization and lazy compilation.; Eventually we will extend the API itself to allow higher-level program; representations (e.g. ASTs) to be added to the JIT. KaleidoscopeJIT; ===============. In the previous section we described our API, now we examine a simple; implementation of it: The KaleidoscopeJIT class [1]_ that was used in the; `Implementing a language with L",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:6212,Availability,error,error,6212,"mory>. namespace llvm {; namespace orc {. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:; KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our class begins with six member variables: An ExecutionSession member, ``ES``,; which provides context for our running JIT'd code (including the string pool,; global mutex, and error reporting facilities); An RTDyldObjectLinkingLayer,; ``ObjectLayer``, that can be used to add object files to our JIT (though we will; not use it directly); An IRCompileLayer, ``CompileLayer``, that can be used to; add LLVM Modules to our JIT (and which builds on the ObjectLayer), A DataLayout; and MangleAndInterner, ``DL`` and ``Mangle``, that will be used for symbol mangling; (more on that later); and finally an LLVMContext that clients will use when; building IR files for the JIT. Next up we have our class constructor, which takes a `JITTargetMachineBuilder``; that will be used by our IRCompiler, and a ``DataLayout`` that we will use to; initialize our DL member. The constructor begins by initializing our; ObjectLayer. The ObjectLayer requires a reference to the ExecutionSession, and; a function object that will build a JIT memory manager for each module that is; added (a JIT memory manager manages memory allocations, memory permissions, and; registration of exception handlers for JIT'd code). For this we use a lambda; that returns a SectionMemoryManager, an off-the-shelf utility that provides all; the basic memory management funct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:9510,Availability,error,error,9510,"do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``DynamicLibrarySearchGenerator::GetForCurrentProcess`` method. .. code-block:: c++. static Expected<std::unique_ptr<KaleidoscopeJIT>> Create() {; auto JTMB = JITTargetMachineBuilder::detectHost();. if (!JTMB); return JTMB.takeError();. auto DL = JTMB->getDefaultDataLayoutForTarget();; if (!DL); return DL.takeError();. return std::make_unique<KaleidoscopeJIT>(std::move(*JTMB), std::move(*DL));; }. const DataLayout &getDataLayout() const { return DL; }. LLVMContext &getContext() { return *Ctx.getContext(); }. Next we have a named constructor, ``Create``, which will build a KaleidoscopeJIT; instance that is configured to generate code for our host process. It does this; by first generating a JITTargetMachineBuilder instance using that classes'; detectHost method and then using that instance to generate a datalayout for; the target process. Each of these operations can fail, so each returns its; result wrapped in an Expected value [3]_ that we must check for error before; continuing. If both operations succeed we can unwrap their results (using the; dereference operator) and pass them into KaleidoscopeJIT's constructor on the; last line of the function. Following the named constructor we have the ``getDataLayout()`` and; ``getContext()`` methods. These are used to make data structures created and; managed by the JIT (especially the LLVMContext) available to the REPL code that; will build our IR modules. .. code-block:: c++. void addModule(std::unique_ptr<Module> M) {; cantFail(CompileLayer.add(ES.getMainJITDylib(),; ThreadSafeModule(std::move(M), Ctx)));; }. Expected<ExecutorSymbolDef> lookup(StringRef Name) {; return ES.lookup({&ES.getMainJITDylib()}, Mangle(Name.str()));; }. Now we come to the first of our JIT API methods: addModule. This method is; responsible for adding IR to the JIT and making it available for execution. In; this initial implementation of our JIT we will make our modules",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:9904,Availability,avail,available,9904,"e<KaleidoscopeJIT>(std::move(*JTMB), std::move(*DL));; }. const DataLayout &getDataLayout() const { return DL; }. LLVMContext &getContext() { return *Ctx.getContext(); }. Next we have a named constructor, ``Create``, which will build a KaleidoscopeJIT; instance that is configured to generate code for our host process. It does this; by first generating a JITTargetMachineBuilder instance using that classes'; detectHost method and then using that instance to generate a datalayout for; the target process. Each of these operations can fail, so each returns its; result wrapped in an Expected value [3]_ that we must check for error before; continuing. If both operations succeed we can unwrap their results (using the; dereference operator) and pass them into KaleidoscopeJIT's constructor on the; last line of the function. Following the named constructor we have the ``getDataLayout()`` and; ``getContext()`` methods. These are used to make data structures created and; managed by the JIT (especially the LLVMContext) available to the REPL code that; will build our IR modules. .. code-block:: c++. void addModule(std::unique_ptr<Module> M) {; cantFail(CompileLayer.add(ES.getMainJITDylib(),; ThreadSafeModule(std::move(M), Ctx)));; }. Expected<ExecutorSymbolDef> lookup(StringRef Name) {; return ES.lookup({&ES.getMainJITDylib()}, Mangle(Name.str()));; }. Now we come to the first of our JIT API methods: addModule. This method is; responsible for adding IR to the JIT and making it available for execution. In; this initial implementation of our JIT we will make our modules ""available for; execution"" by adding them to the CompileLayer, which will it turn store the; Module in the main JITDylib. This process will create new symbol table entries; in the JITDylib for each definition in the module, and will defer compilation of; the module until any of its definitions is looked up. Note that this is not lazy; compilation: just referencing a definition, even if it is never used, will be; enou",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:10370,Availability,avail,available,10370,"ate a datalayout for; the target process. Each of these operations can fail, so each returns its; result wrapped in an Expected value [3]_ that we must check for error before; continuing. If both operations succeed we can unwrap their results (using the; dereference operator) and pass them into KaleidoscopeJIT's constructor on the; last line of the function. Following the named constructor we have the ``getDataLayout()`` and; ``getContext()`` methods. These are used to make data structures created and; managed by the JIT (especially the LLVMContext) available to the REPL code that; will build our IR modules. .. code-block:: c++. void addModule(std::unique_ptr<Module> M) {; cantFail(CompileLayer.add(ES.getMainJITDylib(),; ThreadSafeModule(std::move(M), Ctx)));; }. Expected<ExecutorSymbolDef> lookup(StringRef Name) {; return ES.lookup({&ES.getMainJITDylib()}, Mangle(Name.str()));; }. Now we come to the first of our JIT API methods: addModule. This method is; responsible for adding IR to the JIT and making it available for execution. In; this initial implementation of our JIT we will make our modules ""available for; execution"" by adding them to the CompileLayer, which will it turn store the; Module in the main JITDylib. This process will create new symbol table entries; in the JITDylib for each definition in the module, and will defer compilation of; the module until any of its definitions is looked up. Note that this is not lazy; compilation: just referencing a definition, even if it is never used, will be; enough to trigger compilation. In later chapters we will teach our JIT to defer; compilation of functions until they're actually called. To add our Module we; must first wrap it in a ThreadSafeModule instance, which manages the lifetime of; the Module's LLVMContext (our Ctx member) in a thread-friendly way. In our; example, all modules will share the Ctx member, which will exist for the; duration of the JIT. Once we switch to concurrent compilation in later chapter",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:10464,Availability,avail,available,10464,"at we must check for error before; continuing. If both operations succeed we can unwrap their results (using the; dereference operator) and pass them into KaleidoscopeJIT's constructor on the; last line of the function. Following the named constructor we have the ``getDataLayout()`` and; ``getContext()`` methods. These are used to make data structures created and; managed by the JIT (especially the LLVMContext) available to the REPL code that; will build our IR modules. .. code-block:: c++. void addModule(std::unique_ptr<Module> M) {; cantFail(CompileLayer.add(ES.getMainJITDylib(),; ThreadSafeModule(std::move(M), Ctx)));; }. Expected<ExecutorSymbolDef> lookup(StringRef Name) {; return ES.lookup({&ES.getMainJITDylib()}, Mangle(Name.str()));; }. Now we come to the first of our JIT API methods: addModule. This method is; responsible for adding IR to the JIT and making it available for execution. In; this initial implementation of our JIT we will make our modules ""available for; execution"" by adding them to the CompileLayer, which will it turn store the; Module in the main JITDylib. This process will create new symbol table entries; in the JITDylib for each definition in the module, and will defer compilation of; the module until any of its definitions is looked up. Note that this is not lazy; compilation: just referencing a definition, even if it is never used, will be; enough to trigger compilation. In later chapters we will teach our JIT to defer; compilation of functions until they're actually called. To add our Module we; must first wrap it in a ThreadSafeModule instance, which manages the lifetime of; the Module's LLVMContext (our Ctx member) in a thread-friendly way. In our; example, all modules will share the Ctx member, which will exist for the; duration of the JIT. Once we switch to concurrent compilation in later chapters; we will use a new context per module. Our last method is ``lookup``, which allows us to look up addresses for; function and variable defin",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:13287,Availability,down,down,13287,"un-mangled name, we just re-produce this mangling ourselves using our; ``Mangle`` member function object. This brings us to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter1/KaleidoscopeJIT.h; :language: c++. .. [1] Actually we use a cut-down version of KaleidoscopeJIT that makes a; simplifying assumption: symbols cannot be re-defined. This will make it; impossible to re-define symbols in the REPL, but will make our symbol; lookup logic simpler. Re-introducing support for symbol redefinition is; left as an exercise for the reader. (The KaleidoscopeJIT.h used in the; original tutorials will be a helpful reference). .. [2] +-----------------------------+-----------------------------------------------+; | File | Reason for inclusion |; +=============================+===============================================+; | CompileUtils.h | Provides the SimpleCompiler class. |; +-----------------------------+-----------------------------------------------+; | Core.h | Core utilities such as ExecutionSession and |; | | JITDylib. |; +-----------------------------+-----------------------------------------------+; | ExecutionUtils.h | Provides the DynamicLibrarySearchGenerator |; | | class. |; +-----------------------------+--------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:15321,Availability,error,error-handling,15321,"ifying assumption: symbols cannot be re-defined. This will make it; impossible to re-define symbols in the REPL, but will make our symbol; lookup logic simpler. Re-introducing support for symbol redefinition is; left as an exercise for the reader. (The KaleidoscopeJIT.h used in the; original tutorials will be a helpful reference). .. [2] +-----------------------------+-----------------------------------------------+; | File | Reason for inclusion |; +=============================+===============================================+; | CompileUtils.h | Provides the SimpleCompiler class. |; +-----------------------------+-----------------------------------------------+; | Core.h | Core utilities such as ExecutionSession and |; | | JITDylib. |; +-----------------------------+-----------------------------------------------+; | ExecutionUtils.h | Provides the DynamicLibrarySearchGenerator |; | | class. |; +-----------------------------+-----------------------------------------------+; | IRCompileLayer.h | Provides the IRCompileLayer class. |; +-----------------------------+-----------------------------------------------+; | JITTargetMachineBuilder.h | Provides the JITTargetMachineBuilder class. |; +-----------------------------+-----------------------------------------------+; | RTDyldObjectLinkingLayer.h | Provides the RTDyldObjectLinkingLayer class. |; +-----------------------------+-----------------------------------------------+; | SectionMemoryManager.h | Provides the SectionMemoryManager class. |; +-----------------------------+-----------------------------------------------+; | DataLayout.h | Provides the DataLayout class. |; +-----------------------------+-----------------------------------------------+; | LLVMContext.h | Provides the LLVMContext class. |; +-----------------------------+-----------------------------------------------+. .. [3] See the ErrorHandling section in the LLVM Programmer's Manual; (https://llvm.org/docs/ProgrammersManual.html#error-handling); ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:281,Deployability,update,updated,281,"=======================================================; Building a JIT: Starting out with KaleidoscopeJIT; =======================================================. .. contents::; :local:. Chapter 1 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:439,Deployability,update,updated,439,"=======================================================; Building a JIT: Starting out with KaleidoscopeJIT; =======================================================. .. contents::; :local:. Chapter 1 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:8260,Deployability,configurat,configuration,8260,"se a lambda; that returns a SectionMemoryManager, an off-the-shelf utility that provides all; the basic memory management functionality required for this chapter. Next we; initialize our CompileLayer. The CompileLayer needs three things: (1) A; reference to the ExecutionSession, (2) A reference to our object layer, and (3); a compiler instance to use to perform the actual compilation from IR to object; files. We use the off-the-shelf ConcurrentIRCompiler utility as our compiler,; which we construct using this constructor's JITTargetMachineBuilder argument.; The ConcurrentIRCompiler utility will use the JITTargetMachineBuilder to build; llvm TargetMachines (which are not thread safe) as needed for compiles. After; this, we initialize our supporting members: ``DL``, ``Mangler`` and ``Ctx`` with; the input DataLayout, the ExecutionSession and DL member, and a new default; constructed LLVMContext respectively. Now that our members have been initialized,; so the one thing that remains to do is to tweak the configuration of the; *JITDylib* that we will store our code in. We want to modify this dylib to; contain not only the symbols that we add to it, but also the symbols from our; REPL process as well. We do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``DynamicLibrarySearchGenerator::GetForCurrentProcess`` method. .. code-block:: c++. static Expected<std::unique_ptr<KaleidoscopeJIT>> Create() {; auto JTMB = JITTargetMachineBuilder::detectHost();. if (!JTMB); return JTMB.takeError();. auto DL = JTMB->getDefaultDataLayoutForTarget();; if (!DL); return DL.takeError();. return std::make_unique<KaleidoscopeJIT>(std::move(*JTMB), std::move(*DL));; }. const DataLayout &getDataLayout() const { return DL; }. LLVMContext &getContext() { return *Ctx.getContext(); }. Next we have a named constructor, ``Create``, which will build a KaleidoscopeJIT; instance that is configured to generate code for our host process. It does this; by first generating a JITT",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:1916,Energy Efficiency,reduce,reduced,1916,"e goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will use a lightly modified version of the; Kaleidoscope REPL from `Chapter 7 <LangImpl07.html>`_ of the ""Implementing a; language in LLVM tutorial"". Finally, a word on API generations: ORC is the 3rd generation of LLVM JIT API.; It was preceded by MCJIT, and before that by the (now deleted) legacy JIT.; These tutorials don't assume any experience with these earlier APIs, but; readers acquainted with them will see many familiar elements. Where appropriate; we will make this connection with the earlier APIs explicit to help people who; are transitioning from them to ORC. JIT API Basics; ==============. The purpose of a JIT compiler is to compile code ""on-the-fly"" as it is needed,; rather than compiling whole programs to disk ahead of time as a traditional; compiler does. To support that aim our initial, bare-bones JIT API will have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:9453,Integrability,wrap,wrapped,9453,"do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``DynamicLibrarySearchGenerator::GetForCurrentProcess`` method. .. code-block:: c++. static Expected<std::unique_ptr<KaleidoscopeJIT>> Create() {; auto JTMB = JITTargetMachineBuilder::detectHost();. if (!JTMB); return JTMB.takeError();. auto DL = JTMB->getDefaultDataLayoutForTarget();; if (!DL); return DL.takeError();. return std::make_unique<KaleidoscopeJIT>(std::move(*JTMB), std::move(*DL));; }. const DataLayout &getDataLayout() const { return DL; }. LLVMContext &getContext() { return *Ctx.getContext(); }. Next we have a named constructor, ``Create``, which will build a KaleidoscopeJIT; instance that is configured to generate code for our host process. It does this; by first generating a JITTargetMachineBuilder instance using that classes'; detectHost method and then using that instance to generate a datalayout for; the target process. Each of these operations can fail, so each returns its; result wrapped in an Expected value [3]_ that we must check for error before; continuing. If both operations succeed we can unwrap their results (using the; dereference operator) and pass them into KaleidoscopeJIT's constructor on the; last line of the function. Following the named constructor we have the ``getDataLayout()`` and; ``getContext()`` methods. These are used to make data structures created and; managed by the JIT (especially the LLVMContext) available to the REPL code that; will build our IR modules. .. code-block:: c++. void addModule(std::unique_ptr<Module> M) {; cantFail(CompileLayer.add(ES.getMainJITDylib(),; ThreadSafeModule(std::move(M), Ctx)));; }. Expected<ExecutorSymbolDef> lookup(StringRef Name) {; return ES.lookup({&ES.getMainJITDylib()}, Mangle(Name.str()));; }. Now we come to the first of our JIT API methods: addModule. This method is; responsible for adding IR to the JIT and making it available for execution. In; this initial implementation of our JIT we will make our modules",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:11049,Integrability,wrap,wrap,11049,"move(M), Ctx)));; }. Expected<ExecutorSymbolDef> lookup(StringRef Name) {; return ES.lookup({&ES.getMainJITDylib()}, Mangle(Name.str()));; }. Now we come to the first of our JIT API methods: addModule. This method is; responsible for adding IR to the JIT and making it available for execution. In; this initial implementation of our JIT we will make our modules ""available for; execution"" by adding them to the CompileLayer, which will it turn store the; Module in the main JITDylib. This process will create new symbol table entries; in the JITDylib for each definition in the module, and will defer compilation of; the module until any of its definitions is looked up. Note that this is not lazy; compilation: just referencing a definition, even if it is never used, will be; enough to trigger compilation. In later chapters we will teach our JIT to defer; compilation of functions until they're actually called. To add our Module we; must first wrap it in a ThreadSafeModule instance, which manages the lifetime of; the Module's LLVMContext (our Ctx member) in a thread-friendly way. In our; example, all modules will share the Ctx member, which will exist for the; duration of the JIT. Once we switch to concurrent compilation in later chapters; we will use a new context per module. Our last method is ``lookup``, which allows us to look up addresses for; function and variable definitions added to the JIT based on their symbol names.; As noted above, lookup will implicitly trigger compilation for any symbol; that has not already been compiled. Our lookup method calls through to; `ExecutionSession::lookup`, passing in a list of dylibs to search (in our case; just the main dylib), and the symbol name to search for, with a twist: We have; to *mangle* the name of the symbol we're searching for first. The ORC JIT; components use mangled symbols internally the same way a static compiler and; linker would, rather than using plain IR symbol names. This allows JIT'd code; to interoperate eas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:12194,Integrability,depend,depend,12194," will share the Ctx member, which will exist for the; duration of the JIT. Once we switch to concurrent compilation in later chapters; we will use a new context per module. Our last method is ``lookup``, which allows us to look up addresses for; function and variable definitions added to the JIT based on their symbol names.; As noted above, lookup will implicitly trigger compilation for any symbol; that has not already been compiled. Our lookup method calls through to; `ExecutionSession::lookup`, passing in a list of dylibs to search (in our case; just the main dylib), and the symbol name to search for, with a twist: We have; to *mangle* the name of the symbol we're searching for first. The ORC JIT; components use mangled symbols internally the same way a static compiler and; linker would, rather than using plain IR symbol names. This allows JIT'd code; to interoperate easily with precompiled code in the application or shared; libraries. The kind of mangling will depend on the DataLayout, which in turn; depends on the target platform. To allow us to remain portable and search based; on the un-mangled name, we just re-produce this mangling ourselves using our; ``Mangle`` member function object. This brings us to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Cha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:12235,Integrability,depend,depends,12235," will share the Ctx member, which will exist for the; duration of the JIT. Once we switch to concurrent compilation in later chapters; we will use a new context per module. Our last method is ``lookup``, which allows us to look up addresses for; function and variable definitions added to the JIT based on their symbol names.; As noted above, lookup will implicitly trigger compilation for any symbol; that has not already been compiled. Our lookup method calls through to; `ExecutionSession::lookup`, passing in a list of dylibs to search (in our case; just the main dylib), and the symbol name to search for, with a twist: We have; to *mangle* the name of the symbol we're searching for first. The ORC JIT; components use mangled symbols internally the same way a static compiler and; linker would, rather than using plain IR symbol names. This allows JIT'd code; to interoperate easily with precompiled code in the application or shared; libraries. The kind of mangling will depend on the DataLayout, which in turn; depends on the target platform. To allow us to remain portable and search based; on the un-mangled name, we just re-produce this mangling ourselves using our; ``Mangle`` member function object. This brings us to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Cha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:1505,Modifiability,extend,extend,1505,"orial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will use a lightly modified version of the; Kaleidoscope REPL from `Chapter 7 <LangImpl07.html>`_ of the ""Implementing a; language in LLVM tutorial"". Finally, a word on API generations: ORC is the 3rd generation of LLVM JIT API.; It was preceded by MCJIT, and before that by the (now deleted) legacy JIT.; These tutorials don't assume any experience with these earlier APIs, but; readers acquainted with them will see many familiar elements. Where appropriate; we will make this connection with the earlier APIs explicit to help people who",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:3053,Modifiability,variab,variables,3053,"of the; Kaleidoscope REPL from `Chapter 7 <LangImpl07.html>`_ of the ""Implementing a; language in LLVM tutorial"". Finally, a word on API generations: ORC is the 3rd generation of LLVM JIT API.; It was preceded by MCJIT, and before that by the (now deleted) legacy JIT.; These tutorials don't assume any experience with these earlier APIs, but; readers acquainted with them will see many familiar elements. Where appropriate; we will make this connection with the earlier APIs explicit to help people who; are transitioning from them to ORC. JIT API Basics; ==============. The purpose of a JIT compiler is to compile code ""on-the-fly"" as it is needed,; rather than compiling whole programs to disk ahead of time as a traditional; compiler does. To support that aim our initial, bare-bones JIT API will have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module> M)``: Make the given IR module; available for execution.; 2. ``Expected<ExecutorSymbolDef> lookup()``: Search for pointers to; symbols (functions or variables) that have been added to the JIT. A basic use-case for this API, executing the 'main' function from a module,; will look like:. .. code-block:: c++. JIT J;; J.addModule(buildModule());; auto *Main = J.lookup(""main"").getAddress().toPtr<int(*)(int, char *[])>();; int Result = Main();. The APIs that we build in these tutorials will all be variations on this simple; theme. Behind this API we will refine the implementation of the JIT to add; support for concurrent compilation, optimization and lazy compilation.; Eventually we will extend the API itself to allow higher-level program; representations (e.g. ASTs) to be added to the JIT. KaleidoscopeJIT; ===============. In the previous section we described our API, now we examine a simple; implementation of it: The KaleidoscopeJIT class [1]_ that was used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials. We will use; the REPL code from `Chapter 7 <LangImpl07.html>`_ of that tutorial to s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:3595,Modifiability,extend,extend,3595,"JIT compiler is to compile code ""on-the-fly"" as it is needed,; rather than compiling whole programs to disk ahead of time as a traditional; compiler does. To support that aim our initial, bare-bones JIT API will have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module> M)``: Make the given IR module; available for execution.; 2. ``Expected<ExecutorSymbolDef> lookup()``: Search for pointers to; symbols (functions or variables) that have been added to the JIT. A basic use-case for this API, executing the 'main' function from a module,; will look like:. .. code-block:: c++. JIT J;; J.addModule(buildModule());; auto *Main = J.lookup(""main"").getAddress().toPtr<int(*)(int, char *[])>();; int Result = Main();. The APIs that we build in these tutorials will all be variations on this simple; theme. Behind this API we will refine the implementation of the JIT to add; support for concurrent compilation, optimization and lazy compilation.; Eventually we will extend the API itself to allow higher-level program; representations (e.g. ASTs) to be added to the JIT. KaleidoscopeJIT; ===============. In the previous section we described our API, now we examine a simple; implementation of it: The KaleidoscopeJIT class [1]_ that was used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials. We will use; the REPL code from `Chapter 7 <LangImpl07.html>`_ of that tutorial to supply the; input for our JIT: Each time the user enters an expression the REPL will add a; new IR module containing the code for that expression to the JIT. If the; expression is a top-level expression like '1+1' or 'sin(x)', the REPL will also; use the lookup method of our JIT class find and execute the code for the; expression. In later chapters of this tutorial we will modify the REPL to enable; new interactions with our JIT class, but for now we will take this setup for; granted and focus our attention on the implementation of our JIT itself. Our KaleidoscopeJIT class is defined i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:6067,Modifiability,variab,variables,6067,"mory>. namespace llvm {; namespace orc {. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:; KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our class begins with six member variables: An ExecutionSession member, ``ES``,; which provides context for our running JIT'd code (including the string pool,; global mutex, and error reporting facilities); An RTDyldObjectLinkingLayer,; ``ObjectLayer``, that can be used to add object files to our JIT (though we will; not use it directly); An IRCompileLayer, ``CompileLayer``, that can be used to; add LLVM Modules to our JIT (and which builds on the ObjectLayer), A DataLayout; and MangleAndInterner, ``DL`` and ``Mangle``, that will be used for symbol mangling; (more on that later); and finally an LLVMContext that clients will use when; building IR files for the JIT. Next up we have our class constructor, which takes a `JITTargetMachineBuilder``; that will be used by our IRCompiler, and a ``DataLayout`` that we will use to; initialize our DL member. The constructor begins by initializing our; ObjectLayer. The ObjectLayer requires a reference to the ExecutionSession, and; a function object that will build a JIT memory manager for each module that is; added (a JIT memory manager manages memory allocations, memory permissions, and; registration of exception handlers for JIT'd code). For this we use a lambda; that returns a SectionMemoryManager, an off-the-shelf utility that provides all; the basic memory management funct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:8260,Modifiability,config,configuration,8260,"se a lambda; that returns a SectionMemoryManager, an off-the-shelf utility that provides all; the basic memory management functionality required for this chapter. Next we; initialize our CompileLayer. The CompileLayer needs three things: (1) A; reference to the ExecutionSession, (2) A reference to our object layer, and (3); a compiler instance to use to perform the actual compilation from IR to object; files. We use the off-the-shelf ConcurrentIRCompiler utility as our compiler,; which we construct using this constructor's JITTargetMachineBuilder argument.; The ConcurrentIRCompiler utility will use the JITTargetMachineBuilder to build; llvm TargetMachines (which are not thread safe) as needed for compiles. After; this, we initialize our supporting members: ``DL``, ``Mangler`` and ``Ctx`` with; the input DataLayout, the ExecutionSession and DL member, and a new default; constructed LLVMContext respectively. Now that our members have been initialized,; so the one thing that remains to do is to tweak the configuration of the; *JITDylib* that we will store our code in. We want to modify this dylib to; contain not only the symbols that we add to it, but also the symbols from our; REPL process as well. We do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``DynamicLibrarySearchGenerator::GetForCurrentProcess`` method. .. code-block:: c++. static Expected<std::unique_ptr<KaleidoscopeJIT>> Create() {; auto JTMB = JITTargetMachineBuilder::detectHost();. if (!JTMB); return JTMB.takeError();. auto DL = JTMB->getDefaultDataLayoutForTarget();; if (!DL); return DL.takeError();. return std::make_unique<KaleidoscopeJIT>(std::move(*JTMB), std::move(*DL));; }. const DataLayout &getDataLayout() const { return DL; }. LLVMContext &getContext() { return *Ctx.getContext(); }. Next we have a named constructor, ``Create``, which will build a KaleidoscopeJIT; instance that is configured to generate code for our host process. It does this; by first generating a JITT",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:9153,Modifiability,config,configured,9153,"nstructed LLVMContext respectively. Now that our members have been initialized,; so the one thing that remains to do is to tweak the configuration of the; *JITDylib* that we will store our code in. We want to modify this dylib to; contain not only the symbols that we add to it, but also the symbols from our; REPL process as well. We do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``DynamicLibrarySearchGenerator::GetForCurrentProcess`` method. .. code-block:: c++. static Expected<std::unique_ptr<KaleidoscopeJIT>> Create() {; auto JTMB = JITTargetMachineBuilder::detectHost();. if (!JTMB); return JTMB.takeError();. auto DL = JTMB->getDefaultDataLayoutForTarget();; if (!DL); return DL.takeError();. return std::make_unique<KaleidoscopeJIT>(std::move(*JTMB), std::move(*DL));; }. const DataLayout &getDataLayout() const { return DL; }. LLVMContext &getContext() { return *Ctx.getContext(); }. Next we have a named constructor, ``Create``, which will build a KaleidoscopeJIT; instance that is configured to generate code for our host process. It does this; by first generating a JITTargetMachineBuilder instance using that classes'; detectHost method and then using that instance to generate a datalayout for; the target process. Each of these operations can fail, so each returns its; result wrapped in an Expected value [3]_ that we must check for error before; continuing. If both operations succeed we can unwrap their results (using the; dereference operator) and pass them into KaleidoscopeJIT's constructor on the; last line of the function. Following the named constructor we have the ``getDataLayout()`` and; ``getContext()`` methods. These are used to make data structures created and; managed by the JIT (especially the LLVMContext) available to the REPL code that; will build our IR modules. .. code-block:: c++. void addModule(std::unique_ptr<Module> M) {; cantFail(CompileLayer.add(ES.getMainJITDylib(),; ThreadSafeModule(std::move(M), Ctx)));; }. Expect",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:11475,Modifiability,variab,variable,11475,"available for; execution"" by adding them to the CompileLayer, which will it turn store the; Module in the main JITDylib. This process will create new symbol table entries; in the JITDylib for each definition in the module, and will defer compilation of; the module until any of its definitions is looked up. Note that this is not lazy; compilation: just referencing a definition, even if it is never used, will be; enough to trigger compilation. In later chapters we will teach our JIT to defer; compilation of functions until they're actually called. To add our Module we; must first wrap it in a ThreadSafeModule instance, which manages the lifetime of; the Module's LLVMContext (our Ctx member) in a thread-friendly way. In our; example, all modules will share the Ctx member, which will exist for the; duration of the JIT. Once we switch to concurrent compilation in later chapters; we will use a new context per module. Our last method is ``lookup``, which allows us to look up addresses for; function and variable definitions added to the JIT based on their symbol names.; As noted above, lookup will implicitly trigger compilation for any symbol; that has not already been compiled. Our lookup method calls through to; `ExecutionSession::lookup`, passing in a list of dylibs to search (in our case; just the main dylib), and the symbol name to search for, with a twist: We have; to *mangle* the name of the symbol we're searching for first. The ORC JIT; components use mangled symbols internally the same way a static compiler and; linker would, rather than using plain IR symbol names. This allows JIT'd code; to interoperate easily with precompiled code in the application or shared; libraries. The kind of mangling will depend on the DataLayout, which in turn; depends on the target platform. To allow us to remain portable and search based; on the un-mangled name, we just re-produce this mangling ourselves using our; ``Mangle`` member function object. This brings us to the end of Chapter",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:12289,Modifiability,portab,portable,12289,"rs; we will use a new context per module. Our last method is ``lookup``, which allows us to look up addresses for; function and variable definitions added to the JIT based on their symbol names.; As noted above, lookup will implicitly trigger compilation for any symbol; that has not already been compiled. Our lookup method calls through to; `ExecutionSession::lookup`, passing in a list of dylibs to search (in our case; just the main dylib), and the symbol name to search for, with a twist: We have; to *mangle* the name of the symbol we're searching for first. The ORC JIT; components use mangled symbols internally the same way a static compiler and; linker would, rather than using plain IR symbol names. This allows JIT'd code; to interoperate easily with precompiled code in the application or shared; libraries. The kind of mangling will depend on the DataLayout, which in turn; depends on the target platform. To allow us to remain portable and search based; on the un-mangled name, we just re-produce this mangling ourselves using our; ``Mangle`` member function object. This brings us to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter1/KaleidoscopeJIT.h; :language: c++. .. [1] Actually we use a cut-down version of KaleidoscopeJIT that makes a; simplifying as",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:12680,Modifiability,extend,extend,12680,"p`, passing in a list of dylibs to search (in our case; just the main dylib), and the symbol name to search for, with a twist: We have; to *mangle* the name of the symbol we're searching for first. The ORC JIT; components use mangled symbols internally the same way a static compiler and; linker would, rather than using plain IR symbol names. This allows JIT'd code; to interoperate easily with precompiled code in the application or shared; libraries. The kind of mangling will depend on the DataLayout, which in turn; depends on the target platform. To allow us to remain portable and search based; on the un-mangled name, we just re-produce this mangling ourselves using our; ``Mangle`` member function object. This brings us to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter1/KaleidoscopeJIT.h; :language: c++. .. [1] Actually we use a cut-down version of KaleidoscopeJIT that makes a; simplifying assumption: symbols cannot be re-defined. This will make it; impossible to re-define symbols in the REPL, but will make our symbol; lookup logic simpler. Re-introducing support for symbol redefinition is; left as an exercise for the reader. (The KaleidoscopeJIT.h used in the; original tutorials will be a helpful reference). .. [2] +-----------------------------+------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:13039,Modifiability,config,config,13039,"de; to interoperate easily with precompiled code in the application or shared; libraries. The kind of mangling will depend on the DataLayout, which in turn; depends on the target platform. To allow us to remain portable and search based; on the un-mangled name, we just re-produce this mangling ourselves using our; ``Mangle`` member function object. This brings us to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter1/KaleidoscopeJIT.h; :language: c++. .. [1] Actually we use a cut-down version of KaleidoscopeJIT that makes a; simplifying assumption: symbols cannot be re-defined. This will make it; impossible to re-define symbols in the REPL, but will make our symbol; lookup logic simpler. Re-introducing support for symbol redefinition is; left as an exercise for the reader. (The KaleidoscopeJIT.h used in the; original tutorials will be a helpful reference). .. [2] +-----------------------------+-----------------------------------------------+; | File | Reason for inclusion |; +=============================+===============================================+; | CompileUtils.h | Provides the SimpleCompiler class. |; +-----------------------------+-----------------------------------------------+; | Core.h | Core utilities such as ExecutionSession and |; | | JITDyl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:821,Performance,concurren,concurrent,821,"=======================================================; Building a JIT: Starting out with KaleidoscopeJIT; =======================================================. .. contents::; :local:. Chapter 1 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:845,Performance,optimiz,optimization,845,"=======================================================; Building a JIT: Starting out with KaleidoscopeJIT; =======================================================. .. contents::; :local:. Chapter 1 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:1427,Performance,optimiz,optimize,1427,"ompile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will use a lightly modified version of the; Kaleidoscope REPL from `Chapter 7 <LangImpl07.html>`_ of the ""Implementing a; language in LLVM tutorial"". Finally, a word on API generations: ORC is the 3rd generation of LLVM JIT API.; It was preceded by MCJIT, and before that by the (now deleted) legacy JIT.; These tutorials don't assume any experience with these earlier APIs, but; readers acquainted with them wi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:3516,Performance,concurren,concurrent,3516,"s explicit to help people who; are transitioning from them to ORC. JIT API Basics; ==============. The purpose of a JIT compiler is to compile code ""on-the-fly"" as it is needed,; rather than compiling whole programs to disk ahead of time as a traditional; compiler does. To support that aim our initial, bare-bones JIT API will have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module> M)``: Make the given IR module; available for execution.; 2. ``Expected<ExecutorSymbolDef> lookup()``: Search for pointers to; symbols (functions or variables) that have been added to the JIT. A basic use-case for this API, executing the 'main' function from a module,; will look like:. .. code-block:: c++. JIT J;; J.addModule(buildModule());; auto *Main = J.lookup(""main"").getAddress().toPtr<int(*)(int, char *[])>();; int Result = Main();. The APIs that we build in these tutorials will all be variations on this simple; theme. Behind this API we will refine the implementation of the JIT to add; support for concurrent compilation, optimization and lazy compilation.; Eventually we will extend the API itself to allow higher-level program; representations (e.g. ASTs) to be added to the JIT. KaleidoscopeJIT; ===============. In the previous section we described our API, now we examine a simple; implementation of it: The KaleidoscopeJIT class [1]_ that was used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials. We will use; the REPL code from `Chapter 7 <LangImpl07.html>`_ of that tutorial to supply the; input for our JIT: Each time the user enters an expression the REPL will add a; new IR module containing the code for that expression to the JIT. If the; expression is a top-level expression like '1+1' or 'sin(x)', the REPL will also; use the lookup method of our JIT class find and execute the code for the; expression. In later chapters of this tutorial we will modify the REPL to enable; new interactions with our JIT class, but for now we will take this setup",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:3540,Performance,optimiz,optimization,3540,"s explicit to help people who; are transitioning from them to ORC. JIT API Basics; ==============. The purpose of a JIT compiler is to compile code ""on-the-fly"" as it is needed,; rather than compiling whole programs to disk ahead of time as a traditional; compiler does. To support that aim our initial, bare-bones JIT API will have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module> M)``: Make the given IR module; available for execution.; 2. ``Expected<ExecutorSymbolDef> lookup()``: Search for pointers to; symbols (functions or variables) that have been added to the JIT. A basic use-case for this API, executing the 'main' function from a module,; will look like:. .. code-block:: c++. JIT J;; J.addModule(buildModule());; auto *Main = J.lookup(""main"").getAddress().toPtr<int(*)(int, char *[])>();; int Result = Main();. The APIs that we build in these tutorials will all be variations on this simple; theme. Behind this API we will refine the implementation of the JIT to add; support for concurrent compilation, optimization and lazy compilation.; Eventually we will extend the API itself to allow higher-level program; representations (e.g. ASTs) to be added to the JIT. KaleidoscopeJIT; ===============. In the previous section we described our API, now we examine a simple; implementation of it: The KaleidoscopeJIT class [1]_ that was used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials. We will use; the REPL code from `Chapter 7 <LangImpl07.html>`_ of that tutorial to supply the; input for our JIT: Each time the user enters an expression the REPL will add a; new IR module containing the code for that expression to the JIT. If the; expression is a top-level expression like '1+1' or 'sin(x)', the REPL will also; use the lookup method of our JIT class find and execute the code for the; expression. In later chapters of this tutorial we will modify the REPL to enable; new interactions with our JIT class, but for now we will take this setup",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:7599,Performance,perform,perform,7599,"`Mangle``, that will be used for symbol mangling; (more on that later); and finally an LLVMContext that clients will use when; building IR files for the JIT. Next up we have our class constructor, which takes a `JITTargetMachineBuilder``; that will be used by our IRCompiler, and a ``DataLayout`` that we will use to; initialize our DL member. The constructor begins by initializing our; ObjectLayer. The ObjectLayer requires a reference to the ExecutionSession, and; a function object that will build a JIT memory manager for each module that is; added (a JIT memory manager manages memory allocations, memory permissions, and; registration of exception handlers for JIT'd code). For this we use a lambda; that returns a SectionMemoryManager, an off-the-shelf utility that provides all; the basic memory management functionality required for this chapter. Next we; initialize our CompileLayer. The CompileLayer needs three things: (1) A; reference to the ExecutionSession, (2) A reference to our object layer, and (3); a compiler instance to use to perform the actual compilation from IR to object; files. We use the off-the-shelf ConcurrentIRCompiler utility as our compiler,; which we construct using this constructor's JITTargetMachineBuilder argument.; The ConcurrentIRCompiler utility will use the JITTargetMachineBuilder to build; llvm TargetMachines (which are not thread safe) as needed for compiles. After; this, we initialize our supporting members: ``DL``, ``Mangler`` and ``Ctx`` with; the input DataLayout, the ExecutionSession and DL member, and a new default; constructed LLVMContext respectively. Now that our members have been initialized,; so the one thing that remains to do is to tweak the configuration of the; *JITDylib* that we will store our code in. We want to modify this dylib to; contain not only the symbols that we add to it, but also the symbols from our; REPL process as well. We do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``Dynamic",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:11309,Performance,concurren,concurrent,11309,"ng IR to the JIT and making it available for execution. In; this initial implementation of our JIT we will make our modules ""available for; execution"" by adding them to the CompileLayer, which will it turn store the; Module in the main JITDylib. This process will create new symbol table entries; in the JITDylib for each definition in the module, and will defer compilation of; the module until any of its definitions is looked up. Note that this is not lazy; compilation: just referencing a definition, even if it is never used, will be; enough to trigger compilation. In later chapters we will teach our JIT to defer; compilation of functions until they're actually called. To add our Module we; must first wrap it in a ThreadSafeModule instance, which manages the lifetime of; the Module's LLVMContext (our Ctx member) in a thread-friendly way. In our; example, all modules will share the Ctx member, which will exist for the; duration of the JIT. Once we switch to concurrent compilation in later chapters; we will use a new context per module. Our last method is ``lookup``, which allows us to look up addresses for; function and variable definitions added to the JIT based on their symbol names.; As noted above, lookup will implicitly trigger compilation for any symbol; that has not already been compiled. Our lookup method calls through to; `ExecutionSession::lookup`, passing in a list of dylibs to search (in our case; just the main dylib), and the symbol name to search for, with a twist: We have; to *mangle* the name of the symbol we're searching for first. The ORC JIT; components use mangled symbols internally the same way a static compiler and; linker would, rather than using plain IR symbol names. This allows JIT'd code; to interoperate easily with precompiled code in the application or shared; libraries. The kind of mangling will depend on the DataLayout, which in turn; depends on the target platform. To allow us to remain portable and search based; on the un-mangled name,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:7929,Safety,safe,safe,7929," DL member. The constructor begins by initializing our; ObjectLayer. The ObjectLayer requires a reference to the ExecutionSession, and; a function object that will build a JIT memory manager for each module that is; added (a JIT memory manager manages memory allocations, memory permissions, and; registration of exception handlers for JIT'd code). For this we use a lambda; that returns a SectionMemoryManager, an off-the-shelf utility that provides all; the basic memory management functionality required for this chapter. Next we; initialize our CompileLayer. The CompileLayer needs three things: (1) A; reference to the ExecutionSession, (2) A reference to our object layer, and (3); a compiler instance to use to perform the actual compilation from IR to object; files. We use the off-the-shelf ConcurrentIRCompiler utility as our compiler,; which we construct using this constructor's JITTargetMachineBuilder argument.; The ConcurrentIRCompiler utility will use the JITTargetMachineBuilder to build; llvm TargetMachines (which are not thread safe) as needed for compiles. After; this, we initialize our supporting members: ``DL``, ``Mangler`` and ``Ctx`` with; the input DataLayout, the ExecutionSession and DL member, and a new default; constructed LLVMContext respectively. Now that our members have been initialized,; so the one thing that remains to do is to tweak the configuration of the; *JITDylib* that we will store our code in. We want to modify this dylib to; contain not only the symbols that we add to it, but also the symbols from our; REPL process as well. We do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``DynamicLibrarySearchGenerator::GetForCurrentProcess`` method. .. code-block:: c++. static Expected<std::unique_ptr<KaleidoscopeJIT>> Create() {; auto JTMB = JITTargetMachineBuilder::detectHost();. if (!JTMB); return JTMB.takeError();. auto DL = JTMB->getDefaultDataLayoutForTarget();; if (!DL); return DL.takeError();. return std::make_uniq",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:8724,Safety,detect,detectHost,8724,"ConcurrentIRCompiler utility as our compiler,; which we construct using this constructor's JITTargetMachineBuilder argument.; The ConcurrentIRCompiler utility will use the JITTargetMachineBuilder to build; llvm TargetMachines (which are not thread safe) as needed for compiles. After; this, we initialize our supporting members: ``DL``, ``Mangler`` and ``Ctx`` with; the input DataLayout, the ExecutionSession and DL member, and a new default; constructed LLVMContext respectively. Now that our members have been initialized,; so the one thing that remains to do is to tweak the configuration of the; *JITDylib* that we will store our code in. We want to modify this dylib to; contain not only the symbols that we add to it, but also the symbols from our; REPL process as well. We do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``DynamicLibrarySearchGenerator::GetForCurrentProcess`` method. .. code-block:: c++. static Expected<std::unique_ptr<KaleidoscopeJIT>> Create() {; auto JTMB = JITTargetMachineBuilder::detectHost();. if (!JTMB); return JTMB.takeError();. auto DL = JTMB->getDefaultDataLayoutForTarget();; if (!DL); return DL.takeError();. return std::make_unique<KaleidoscopeJIT>(std::move(*JTMB), std::move(*DL));; }. const DataLayout &getDataLayout() const { return DL; }. LLVMContext &getContext() { return *Ctx.getContext(); }. Next we have a named constructor, ``Create``, which will build a KaleidoscopeJIT; instance that is configured to generate code for our host process. It does this; by first generating a JITTargetMachineBuilder instance using that classes'; detectHost method and then using that instance to generate a datalayout for; the target process. Each of these operations can fail, so each returns its; result wrapped in an Expected value [3]_ that we must check for error before; continuing. If both operations succeed we can unwrap their results (using the; dereference operator) and pass them into KaleidoscopeJIT's constructor on the;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:9293,Safety,detect,detectHost,9293,"at we will store our code in. We want to modify this dylib to; contain not only the symbols that we add to it, but also the symbols from our; REPL process as well. We do this by attaching a; ``DynamicLibrarySearchGenerator`` instance using the; ``DynamicLibrarySearchGenerator::GetForCurrentProcess`` method. .. code-block:: c++. static Expected<std::unique_ptr<KaleidoscopeJIT>> Create() {; auto JTMB = JITTargetMachineBuilder::detectHost();. if (!JTMB); return JTMB.takeError();. auto DL = JTMB->getDefaultDataLayoutForTarget();; if (!DL); return DL.takeError();. return std::make_unique<KaleidoscopeJIT>(std::move(*JTMB), std::move(*DL));; }. const DataLayout &getDataLayout() const { return DL; }. LLVMContext &getContext() { return *Ctx.getContext(); }. Next we have a named constructor, ``Create``, which will build a KaleidoscopeJIT; instance that is configured to generate code for our host process. It does this; by first generating a JITTargetMachineBuilder instance using that classes'; detectHost method and then using that instance to generate a datalayout for; the target process. Each of these operations can fail, so each returns its; result wrapped in an Expected value [3]_ that we must check for error before; continuing. If both operations succeed we can unwrap their results (using the; dereference operator) and pass them into KaleidoscopeJIT's constructor on the; last line of the function. Following the named constructor we have the ``getDataLayout()`` and; ``getContext()`` methods. These are used to make data structures created and; managed by the JIT (especially the LLVMContext) available to the REPL code that; will build our IR modules. .. code-block:: c++. void addModule(std::unique_ptr<Module> M) {; cantFail(CompileLayer.add(ES.getMainJITDylib(),; ThreadSafeModule(std::move(M), Ctx)));; }. Expected<ExecutorSymbolDef> lookup(StringRef Name) {; return ES.lookup({&ES.getMainJITDylib()}, Mangle(Name.str()));; }. Now we come to the first of our JIT API methods: addM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:13484,Testability,log,logic,13484,"s to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter1/KaleidoscopeJIT.h; :language: c++. .. [1] Actually we use a cut-down version of KaleidoscopeJIT that makes a; simplifying assumption: symbols cannot be re-defined. This will make it; impossible to re-define symbols in the REPL, but will make our symbol; lookup logic simpler. Re-introducing support for symbol redefinition is; left as an exercise for the reader. (The KaleidoscopeJIT.h used in the; original tutorials will be a helpful reference). .. [2] +-----------------------------+-----------------------------------------------+; | File | Reason for inclusion |; +=============================+===============================================+; | CompileUtils.h | Provides the SimpleCompiler class. |; +-----------------------------+-----------------------------------------------+; | Core.h | Core utilities such as ExecutionSession and |; | | JITDylib. |; +-----------------------------+-----------------------------------------------+; | ExecutionUtils.h | Provides the DynamicLibrarySearchGenerator |; | | class. |; +-----------------------------+-----------------------------------------------+; | IRCompileLayer.h | Provides the IRCompileLayer class. |; +-----------------------------+-----------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:655,Usability,simpl,simplified,655,"=======================================================; Building a JIT: Starting out with KaleidoscopeJIT; =======================================================. .. contents::; :local:. Chapter 1 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will us",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:1185,Usability,simpl,simple,1185,"al:. Chapter 1 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 1 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; tutorial runs through the implementation of a JIT compiler using LLVM's; On-Request-Compilation (ORC) APIs. It begins with a simplified version of the; KaleidoscopeJIT class used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials and then; introduces new features like concurrent compilation, optimization, lazy; compilation and remote execution. The goal of this tutorial is to introduce you to LLVM's ORC JIT APIs, show how; these APIs interact with other parts of LLVM, and to teach you how to recombine; them to build a custom JIT that is suited to your use-case. The structure of the tutorial is:. - Chapter #1: Investigate the simple KaleidoscopeJIT class. This will; introduce some of the basic concepts of the ORC JIT APIs, including the; idea of an ORC *Layer*. - `Chapter #2 <BuildingAJIT2.html>`_: Extend the basic KaleidoscopeJIT by adding; a new layer that will optimize IR and generated code. - `Chapter #3 <BuildingAJIT3.html>`_: Further extend the JIT by adding a; Compile-On-Demand layer to lazily compile IR. - `Chapter #4 <BuildingAJIT4.html>`_: Improve the laziness of our JIT by; replacing the Compile-On-Demand layer with a custom layer that uses the ORC; Compile Callbacks API directly to defer IR-generation until functions are; called. - `Chapter #5 <BuildingAJIT5.html>`_: Add process isolation by JITing code into; a remote process with reduced privileges using the JIT Remote APIs. To provide input for our JIT we will use a lightly modified version of the; Kaleidoscope REPL from `Chapter 7 <LangImpl07.html>`_ of the ""Implementing a; language in LLVM tutorial"". Finally, a word on API generations: ORC ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:3420,Usability,simpl,simple,3420,"ed with them will see many familiar elements. Where appropriate; we will make this connection with the earlier APIs explicit to help people who; are transitioning from them to ORC. JIT API Basics; ==============. The purpose of a JIT compiler is to compile code ""on-the-fly"" as it is needed,; rather than compiling whole programs to disk ahead of time as a traditional; compiler does. To support that aim our initial, bare-bones JIT API will have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module> M)``: Make the given IR module; available for execution.; 2. ``Expected<ExecutorSymbolDef> lookup()``: Search for pointers to; symbols (functions or variables) that have been added to the JIT. A basic use-case for this API, executing the 'main' function from a module,; will look like:. .. code-block:: c++. JIT J;; J.addModule(buildModule());; auto *Main = J.lookup(""main"").getAddress().toPtr<int(*)(int, char *[])>();; int Result = Main();. The APIs that we build in these tutorials will all be variations on this simple; theme. Behind this API we will refine the implementation of the JIT to add; support for concurrent compilation, optimization and lazy compilation.; Eventually we will extend the API itself to allow higher-level program; representations (e.g. ASTs) to be added to the JIT. KaleidoscopeJIT; ===============. In the previous section we described our API, now we examine a simple; implementation of it: The KaleidoscopeJIT class [1]_ that was used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials. We will use; the REPL code from `Chapter 7 <LangImpl07.html>`_ of that tutorial to supply the; input for our JIT: Each time the user enters an expression the REPL will add a; new IR module containing the code for that expression to the JIT. If the; expression is a top-level expression like '1+1' or 'sin(x)', the REPL will also; use the lookup method of our JIT class find and execute the code for the; expression. In later chapters of this tu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:3797,Usability,simpl,simple,3797,"l have; just two functions:. 1. ``Error addModule(std::unique_ptr<Module> M)``: Make the given IR module; available for execution.; 2. ``Expected<ExecutorSymbolDef> lookup()``: Search for pointers to; symbols (functions or variables) that have been added to the JIT. A basic use-case for this API, executing the 'main' function from a module,; will look like:. .. code-block:: c++. JIT J;; J.addModule(buildModule());; auto *Main = J.lookup(""main"").getAddress().toPtr<int(*)(int, char *[])>();; int Result = Main();. The APIs that we build in these tutorials will all be variations on this simple; theme. Behind this API we will refine the implementation of the JIT to add; support for concurrent compilation, optimization and lazy compilation.; Eventually we will extend the API itself to allow higher-level program; representations (e.g. ASTs) to be added to the JIT. KaleidoscopeJIT; ===============. In the previous section we described our API, now we examine a simple; implementation of it: The KaleidoscopeJIT class [1]_ that was used in the; `Implementing a language with LLVM <LangImpl01.html>`_ tutorials. We will use; the REPL code from `Chapter 7 <LangImpl07.html>`_ of that tutorial to supply the; input for our JIT: Each time the user enters an expression the REPL will add a; new IR module containing the code for that expression to the JIT. If the; expression is a top-level expression like '1+1' or 'sin(x)', the REPL will also; use the lookup method of our JIT class find and execute the code for the; expression. In later chapters of this tutorial we will modify the REPL to enable; new interactions with our JIT class, but for now we will take this setup for; granted and focus our attention on the implementation of our JIT itself. Our KaleidoscopeJIT class is defined in the KaleidoscopeJIT.h header. After the; usual include guards and #includes [2]_, we get to the definition of our class:. .. code-block:: c++. #ifndef LLVM_EXECUTIONENGINE_ORC_KALEIDOSCOPEJIT_H; #define LLVM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:13333,Usability,simpl,simplifying,13333,"un-mangled name, we just re-produce this mangling ourselves using our; ``Mangle`` member function object. This brings us to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter1/KaleidoscopeJIT.h; :language: c++. .. [1] Actually we use a cut-down version of KaleidoscopeJIT that makes a; simplifying assumption: symbols cannot be re-defined. This will make it; impossible to re-define symbols in the REPL, but will make our symbol; lookup logic simpler. Re-introducing support for symbol redefinition is; left as an exercise for the reader. (The KaleidoscopeJIT.h used in the; original tutorials will be a helpful reference). .. [2] +-----------------------------+-----------------------------------------------+; | File | Reason for inclusion |; +=============================+===============================================+; | CompileUtils.h | Provides the SimpleCompiler class. |; +-----------------------------+-----------------------------------------------+; | Core.h | Core utilities such as ExecutionSession and |; | | JITDylib. |; +-----------------------------+-----------------------------------------------+; | ExecutionUtils.h | Provides the DynamicLibrarySearchGenerator |; | | class. |; +-----------------------------+--------------------------------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst:13490,Usability,simpl,simpler,13490,"s to the end of Chapter 1 of Building a JIT. You now have a basic; but fully functioning JIT stack that you can use to take LLVM IR and make it; executable within the context of your JIT process. In the next chapter we'll; look at how to extend this JIT to produce better quality code, and in the; process take a deeper look at the ORC layer concept. `Next: Extending the KaleidoscopeJIT <BuildingAJIT2.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example. To build this; example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter1/KaleidoscopeJIT.h; :language: c++. .. [1] Actually we use a cut-down version of KaleidoscopeJIT that makes a; simplifying assumption: symbols cannot be re-defined. This will make it; impossible to re-define symbols in the REPL, but will make our symbol; lookup logic simpler. Re-introducing support for symbol redefinition is; left as an exercise for the reader. (The KaleidoscopeJIT.h used in the; original tutorials will be a helpful reference). .. [2] +-----------------------------+-----------------------------------------------+; | File | Reason for inclusion |; +=============================+===============================================+; | CompileUtils.h | Provides the SimpleCompiler class. |; +-----------------------------+-----------------------------------------------+; | Core.h | Core utilities such as ExecutionSession and |; | | JITDylib. |; +-----------------------------+-----------------------------------------------+; | ExecutionUtils.h | Provides the DynamicLibrarySearchGenerator |; | | class. |; +-----------------------------+-----------------------------------------------+; | IRCompileLayer.h | Provides the IRCompileLayer class. |; +-----------------------------+-----------------------",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT1.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:8860,Availability,error,error,8860,"ransformLayer, from; ``llvm/include/llvm/ExecutionEngine/Orc/IRTransformLayer.h`` and; ``llvm/lib/ExecutionEngine/Orc/IRTransformLayer.cpp``. This class is concerned; with two very simple jobs: (1) Running every IR Module that is emitted via this; layer through the transform function object, and (2) implementing the ORC; ``IRLayer`` interface (which itself conforms to the general ORC Layer concept,; more on that below). Most of the class is straightforward: a typedef for the; transform function, a constructor to initialize the members, a setter for the; transform function value, and a default no-op transform. The most important; method is ``emit`` as this is half of our IRLayer interface. The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:513,Deployability,update,updated,513,"=====================================================================; Building a JIT: Adding Optimizations -- An introduction to ORC Layers; =====================================================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 2 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:671,Deployability,update,updated,671,"=====================================================================; Building a JIT: Adding Optimizations -- An introduction to ORC Layers; =====================================================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 2 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4217,Deployability,update,update,4217,"() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:5465,Deployability,pipeline,pipeline,5465,"t MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:9647,Deployability,pipeline,pipeline,9647,"r emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11769,Deployability,configurat,configurations,11769,"nk the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :language: c++; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2692,Integrability,interface,interface,2692,"e it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we int",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:8102,Integrability,interface,interface,8102,"bility R, ThreadSafeModule TSM) override;. private:; IRLayer &BaseLayer;; TransformFunction Transform;; };. // From IRTransformLayer.cpp:. IRTransformLayer::IRTransformLayer(ExecutionSession &ES,; IRLayer &BaseLayer,; TransformFunction Transform); : IRLayer(ES), BaseLayer(BaseLayer), Transform(std::move(Transform)) {}. void IRTransformLayer::emit(MaterializationResponsibility R,; ThreadSafeModule TSM) {; assert(TSM.getModule() && ""Module must not be null"");. if (auto TransformedTSM = Transform(std::move(TSM), R)); BaseLayer.emit(std::move(R), std::move(*TransformedTSM));; else {; R.failMaterialization();; getExecutionSession().reportError(TransformedTSM.takeError());; }; }. This is the whole definition of IRTransformLayer, from; ``llvm/include/llvm/ExecutionEngine/Orc/IRTransformLayer.h`` and; ``llvm/lib/ExecutionEngine/Orc/IRTransformLayer.cpp``. This class is concerned; with two very simple jobs: (1) Running every IR Module that is emitted via this; layer through the transform function object, and (2) implementing the ORC; ``IRLayer`` interface (which itself conforms to the general ORC Layer concept,; more on that below). Most of the class is straightforward: a typedef for the; transform function, a constructor to initialize the members, a setter for the; transform function value, and a default no-op transform. The most important; method is ``emit`` as this is half of our IRLayer interface. The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDyli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:8454,Integrability,interface,interface,8454,"lity R,; ThreadSafeModule TSM) {; assert(TSM.getModule() && ""Module must not be null"");. if (auto TransformedTSM = Transform(std::move(TSM), R)); BaseLayer.emit(std::move(R), std::move(*TransformedTSM));; else {; R.failMaterialization();; getExecutionSession().reportError(TransformedTSM.takeError());; }; }. This is the whole definition of IRTransformLayer, from; ``llvm/include/llvm/ExecutionEngine/Orc/IRTransformLayer.h`` and; ``llvm/lib/ExecutionEngine/Orc/IRTransformLayer.cpp``. This class is concerned; with two very simple jobs: (1) Running every IR Module that is emitted via this; layer through the transform function object, and (2) implementing the ORC; ``IRLayer`` interface (which itself conforms to the general ORC Layer concept,; more on that below). Most of the class is straightforward: a typedef for the; transform function, a constructor to initialize the members, a setter for the; transform function value, and a default no-op transform. The most important; method is ``emit`` as this is half of our IRLayer interface. The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most laye",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:8944,Integrability,interface,interface,8944,"bs: (1) Running every IR Module that is emitted via this; layer through the transform function object, and (2) implementing the ORC; ``IRLayer`` interface (which itself conforms to the general ORC Layer concept,; more on that below). Most of the class is straightforward: a typedef for the; transform function, a constructor to initialize the members, a setter for the; transform function value, and a default no-op transform. The most important; method is ``emit`` as this is half of our IRLayer interface. The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:9315,Integrability,wrap,wrapping,9315," function value, and a default no-op transform. The most important; method is ``emit`` as this is half of our IRLayer interface. The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produce",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:9621,Integrability,wrap,wrap,9621,"r emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:9744,Integrability,interface,interface,9744,"r emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1040,Modifiability,layers,layers,1040,"========================================; Building a JIT: Adding Optimizations -- An introduction to ORC Layers; =====================================================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 2 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1682,Modifiability,config,configure,1682," Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2133,Modifiability,layers,layers,2133,"ayer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContex",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2817,Modifiability,layers,layers,2817,"e it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we int",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:3661,Modifiability,extend,extended,3661,"a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass()",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:3961,Modifiability,layers,layers,3961,":. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bot",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:6289,Modifiability,layers,layers,6289,"alizationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(TransformFunction Transform) {; this->Transform = std::move(Transform);; }. static ThreadSafeModule; identityTransform(ThreadSafeModule TSM,; const MaterializationResponsibility &R) {; return TSM;; }. void emit(MaterializationResponsibility R, ThreadSafeModule TSM) override;. private:; IRLayer &BaseLayer;; TransformFunction Transform;; };. // From IRTransformLayer.cpp:. IRTransfor",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:6421,Modifiability,layers,layers,6421,"to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(TransformFunction Transform) {; this->Transform = std::move(Transform);; }. static ThreadSafeModule; identityTransform(ThreadSafeModule TSM,; const MaterializationResponsibility &R) {; return TSM;; }. void emit(MaterializationResponsibility R, ThreadSafeModule TSM) override;. private:; IRLayer &BaseLayer;; TransformFunction Transform;; };. // From IRTransformLayer.cpp:. IRTransformLayer::IRTransformLayer(ExecutionSession &ES,; IRLayer &BaseLayer,; TransformFunction Transform); : IRLayer(ES), BaseLayer(BaseLayer), Transform(std::move(Transform)) {}. void IRTr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:8957,Modifiability,inherit,inherit,8957,"bs: (1) Running every IR Module that is emitted via this; layer through the transform function object, and (2) implementing the ORC; ``IRLayer`` interface (which itself conforms to the general ORC Layer concept,; more on that below). Most of the class is straightforward: a typedef for the; transform function, a constructor to initialize the members, a setter for the; transform function value, and a default no-op transform. The most important; method is ``emit`` as this is half of our IRLayer interface. The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:9420,Modifiability,layers,layers,9420," The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11646,Modifiability,layers,layers,11646,"nk the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :language: c++; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11769,Modifiability,config,configurations,11769,"nk the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :language: c++; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:12092,Modifiability,config,config,12092,"nk the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :language: c++; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1232,Performance,optimiz,optimization,1232,"===============. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 2 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1522,Performance,optimiz,optimizing,1522,". **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimizati",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1614,Performance,optimiz,optimize,1614," Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1709,Performance,optimiz,optimizations,1709," Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1799,Performance,optimiz,optimized,1799," Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1958,Performance,optimiz,optimized,1958,"able code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via add",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2024,Performance,optimiz,optimization,2024,"shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2169,Performance,optimiz,optimization,2169,"ayer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContex",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2353,Performance,optimiz,optimization,2353,"er 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompile",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2402,Performance,optimiz,optimize,2402,"er 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompile",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2457,Performance,optimiz,optimization,2457,"er 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompile",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2487,Performance,optimiz,optimization,2487,"rested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDy",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2840,Performance,optimiz,optimization,2840,"e it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we int",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:3431,Performance,optimiz,optimizeModule,3431,"zation managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4056,Performance,optimiz,optimizeModule,4056,"IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4394,Performance,optimiz,optimizeModule,4394,"ptimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4588,Performance,optimiz,optimizations,4588,"rySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we introduce a new member, TransformLayer, which sits; on top of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4792,Performance,optimiz,optimizations,4792,"op of our CompileLayer. We initialize our OptimizeLayer with a reference to; the ExecutionSession and output layer (standard practice for layers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth disc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4982,Performance,optimiz,optimization,4982,"ers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:4998,Performance,optimiz,optimizeModule,4998,"ers), along with; a *transform function*. For our transform function we supply our classes; optimizeModule static method. .. code-block:: c++. // ...; return cantFail(OptimizeLayer.addModule(std::move(M),; std::move(Resolver)));; // ... Next we need to update our addModule method to replace the call to; ``CompileLayer::add`` with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:5451,Performance,optimiz,optimization,5451,"t MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:5635,Performance,optimiz,optimizations,5635,"reateReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const Materializ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:5850,Performance,optimiz,optimization,5850,"ions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(Transf",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:6000,Performance,optimiz,optimizeModule,6000,"ptimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(TransformFunction Transform) {; this->Transform = std::move(Transform);; }. static ThreadSafeModule; identityTransform(ThreadSafeModule TSM,; const MaterializationRespon",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:6126,Performance,optimiz,optimizeModule,6126,"alizationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(TransformFunction Transform) {; this->Transform = std::move(Transform);; }. static ThreadSafeModule; identityTransform(ThreadSafeModule TSM,; const MaterializationResponsibility &R) {; return TSM;; }. void emit(MaterializationResponsibility R, ThreadSafeModule TSM) override;. private:; IRLayer &BaseLayer;; TransformFunction Transform;; };. // From IRTransformLayer.cpp:. IRTransfor",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:10386,Performance,optimiz,optimize,10386," Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things bec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:10491,Performance,optimiz,optimizes,10491," ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are comple",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11018,Performance,optimiz,optimize,11018,"lling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11090,Performance,optimiz,optimized,11090,"lling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11202,Performance,optimiz,optimize,11202,"jectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11352,Performance,optimiz,optimized,11352,"jectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11472,Performance,optimiz,optimizations,11472,"nk the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :language: c++; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11516,Performance,perform,performed,11516,"nk the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :language: c++; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11991,Performance,optimiz,optimization,11991,"nk the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :language: c++; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:5386,Security,access,access,5386," with a call to ``OptimizeLayer::add`` instead. .. code-block:: c++. static Expected<ThreadSafeModule>; optimizeModule(ThreadSafeModule M, const MaterializationResponsibility &R) {; // Create a function pass manager.; auto FPM = std::make_unique<legacy::FunctionPassManager>(M.get());. // Add some optimizations.; FPM->add(createInstructionCombiningPass());; FPM->add(createReassociatePass());; FPM->add(createGVNPass());; FPM->add(createCFGSimplificationPass());; FPM->doInitialization();. // Run the optimizations over all functions in the module being added to; // the JIT.; for (auto &F : *M); FPM->run(F);. return M;; }. At the bottom of our JIT we add a private method to do the actual optimization:; *optimizeModule*. This function takes the module to be transformed as input (as; a ThreadSafeModule) along with a reference to a reference to a new class:; ``MaterializationResponsibility``. The MaterializationResponsibility argument; can be used to query JIT state for the module being transformed, such as the set; of definitions in the module that JIT'd code is actively trying to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:7457,Testability,assert,assert,7457,"st layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(TransformFunction Transform) {; this->Transform = std::move(Transform);; }. static ThreadSafeModule; identityTransform(ThreadSafeModule TSM,; const MaterializationResponsibility &R) {; return TSM;; }. void emit(MaterializationResponsibility R, ThreadSafeModule TSM) override;. private:; IRLayer &BaseLayer;; TransformFunction Transform;; };. // From IRTransformLayer.cpp:. IRTransformLayer::IRTransformLayer(ExecutionSession &ES,; IRLayer &BaseLayer,; TransformFunction Transform); : IRLayer(ES), BaseLayer(BaseLayer), Transform(std::move(Transform)) {}. void IRTransformLayer::emit(MaterializationResponsibility R,; ThreadSafeModule TSM) {; assert(TSM.getModule() && ""Module must not be null"");. if (auto TransformedTSM = Transform(std::move(TSM), R)); BaseLayer.emit(std::move(R), std::move(*TransformedTSM));; else {; R.failMaterialization();; getExecutionSession().reportError(TransformedTSM.takeError());; }; }. This is the whole definition of IRTransformLayer, from; ``llvm/include/llvm/ExecutionEngine/Orc/IRTransformLayer.h`` and; ``llvm/lib/ExecutionEngine/Orc/IRTransformLayer.cpp``. This class is concerned; with two very simple jobs: (1) Running every IR Module that is emitted via this; layer through the transform function object, and (2) implementing the ORC; ``IRLayer`` interface (which itself conforms to the general ORC Layer concept,; more on that below). Most of the class is straightforward: a typedef for the; transform function, a constructor to initialize the members, a setter for the; transform function value, and a default no-op transform. The most important; method is ``emi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:8851,Testability,log,logs,8851,"ransformLayer, from; ``llvm/include/llvm/ExecutionEngine/Orc/IRTransformLayer.h`` and; ``llvm/lib/ExecutionEngine/Orc/IRTransformLayer.cpp``. This class is concerned; with two very simple jobs: (1) Running every IR Module that is emitted via this; layer through the transform function object, and (2) implementing the ORC; ``IRLayer`` interface (which itself conforms to the general ORC Layer concept,; more on that below). Most of the class is straightforward: a typedef for the; transform function, a constructor to initialize the members, a setter for the; transform function value, and a default no-op transform. The most important; method is ``emit`` as this is half of our IRLayer interface. The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDylib &JD, ThreadSafeModule TSM, VModuleKey K) {; return JD.define(std::make_unique<BasicIRLayerMaterializationUnit>(; *this, std::move(K), std::move(TSM)));; }. This code, from ``llvm/lib/ExecutionEngine/Orc/Layer.cpp``, adds a; ThreadSafeModule to a given JITDylib by wrapping it up in a; ``MaterializationUnit`` (in this case a ``BasicIRLayerMaterializationUnit``).; Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:411,Usability,feedback,feedback,411,"=====================================================================; Building a JIT: Adding Optimizations -- An introduction to ORC Layers; =====================================================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 2 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:1142,Usability,learn,learn,1142,"===============. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 2 Introduction; ======================. **Warning: This tutorial is currently being updated to account for ORC API; changes. Only Chapters 1 and 2 are up-to-date.**. **Example code from Chapters 3 to 5 will compile and run, but has not been; updated**. Welcome to Chapter 2 of the ""Building an ORC-based JIT in LLVM"" tutorial. In; `Chapter 1 <BuildingAJIT1.html>`_ of this series we examined a basic JIT; class, KaleidoscopeJIT, that could take LLVM IR modules as input and produce; executable code in memory. KaleidoscopeJIT was able to do this with relatively; little code by composing two off-the-shelf *ORC layers*: IRCompileLayer and; ObjectLinkingLayer, to do much of the heavy lifting. In this layer we'll learn more about the ORC layer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2111,Usability,learn,learn,2111,"ayer concept by using a new layer,; IRTransformLayer, to add IR optimization support to KaleidoscopeJIT. Optimizing Modules using the IRTransformLayer; =============================================. In `Chapter 4 <LangImpl04.html>`_ of the ""Implementing a language with LLVM""; tutorial series the llvm *FunctionPassManager* is introduced as a means for; optimizing LLVM IR. Interested readers may read that chapter for details, but; in short: to optimize a Module we create an llvm::FunctionPassManager; instance, configure it with a set of optimizations, then run the PassManager on; a Module to mutate it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContex",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:2705,Usability,simpl,simple,2705,"e it into a (hopefully) more optimized but semantically; equivalent form. In the original tutorial series the FunctionPassManager was; created outside the KaleidoscopeJIT and modules were optimized before being; added to it. In this Chapter we will make optimization a phase of our JIT; instead. For now this will provide us a motivation to learn more about ORC; layers, but in the long term making optimization part of our JIT will yield an; important benefit: When we begin lazily compiling code (i.e. deferring; compilation of each function until the first time it's run) having; optimization managed by our JIT will allow us to optimize lazily too, rather; than having to do all our optimization up-front. To add optimization support to our JIT we will take the KaleidoscopeJIT from; Chapter 1 and compose an ORC *IRTransformLayer* on top. We will look at how the; IRTransformLayer works in more detail below, but the interface is simple: the; constructor for this layer takes a reference to the execution session and the; layer below (as all layers do) plus an *IR optimization function* that it will; apply to each Module that is added via addModule:. .. code-block:: c++. class KaleidoscopeJIT {; private:; ExecutionSession ES;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer CompileLayer;; IRTransformLayer TransformLayer;. DataLayout DL;; MangleAndInterner Mangle;; ThreadSafeContext Ctx;. public:. KaleidoscopeJIT(JITTargetMachineBuilder JTMB, DataLayout DL); : ObjectLayer(ES,; []() { return std::make_unique<SectionMemoryManager>(); }),; CompileLayer(ES, ObjectLayer, ConcurrentIRCompiler(std::move(JTMB))),; TransformLayer(ES, CompileLayer, optimizeModule),; DL(std::move(DL)), Mangle(ES, this->DL),; Ctx(std::make_unique<LLVMContext>()) {; ES.getMainJITDylib().addGenerator(; cantFail(DynamicLibrarySearchGenerator::GetForCurrentProcess(DL.getGlobalPrefix())));; }. Our extended KaleidoscopeJIT class starts out the same as it did in Chapter 1,; but after the CompileLayer we int",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:6412,Usability,simpl,simplest,6412,"to call/access.; For now we will ignore this argument and use a standard optimization; pipeline. To do this we set up a FunctionPassManager, add some passes to it, run; it over every function in the module, and then return the mutated module. The; specific optimizations are the same ones used in `Chapter 4 <LangImpl04.html>`_; of the ""Implementing a language with LLVM"" tutorial series. Readers may visit; that chapter for a more in-depth discussion of these, and of IR optimization in; general. And that's it in terms of changes to KaleidoscopeJIT: When a module is added via; addModule the OptimizeLayer will call our optimizeModule function before passing; the transformed module on to the CompileLayer below. Of course, we could have; called optimizeModule directly in our addModule function and not gone to the; bother of using the IRTransformLayer, but doing so gives us another opportunity; to see how layers compose. It also provides a neat entry point to the *layer*; concept itself, because IRTransformLayer is one of the simplest layers that; can be implemented. .. code-block:: c++. // From IRTransformLayer.h:; class IRTransformLayer : public IRLayer {; public:; using TransformFunction = std::function<Expected<ThreadSafeModule>(; ThreadSafeModule, const MaterializationResponsibility &R)>;. IRTransformLayer(ExecutionSession &ES, IRLayer &BaseLayer,; TransformFunction Transform = identityTransform);. void setTransform(TransformFunction Transform) {; this->Transform = std::move(Transform);; }. static ThreadSafeModule; identityTransform(ThreadSafeModule TSM,; const MaterializationResponsibility &R) {; return TSM;; }. void emit(MaterializationResponsibility R, ThreadSafeModule TSM) override;. private:; IRLayer &BaseLayer;; TransformFunction Transform;; };. // From IRTransformLayer.cpp:. IRTransformLayer::IRTransformLayer(ExecutionSession &ES,; IRLayer &BaseLayer,; TransformFunction Transform); : IRLayer(ES), BaseLayer(BaseLayer), Transform(std::move(Transform)) {}. void IRTr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:7948,Usability,simpl,simple,7948,"bility R, ThreadSafeModule TSM) override;. private:; IRLayer &BaseLayer;; TransformFunction Transform;; };. // From IRTransformLayer.cpp:. IRTransformLayer::IRTransformLayer(ExecutionSession &ES,; IRLayer &BaseLayer,; TransformFunction Transform); : IRLayer(ES), BaseLayer(BaseLayer), Transform(std::move(Transform)) {}. void IRTransformLayer::emit(MaterializationResponsibility R,; ThreadSafeModule TSM) {; assert(TSM.getModule() && ""Module must not be null"");. if (auto TransformedTSM = Transform(std::move(TSM), R)); BaseLayer.emit(std::move(R), std::move(*TransformedTSM));; else {; R.failMaterialization();; getExecutionSession().reportError(TransformedTSM.takeError());; }; }. This is the whole definition of IRTransformLayer, from; ``llvm/include/llvm/ExecutionEngine/Orc/IRTransformLayer.h`` and; ``llvm/lib/ExecutionEngine/Orc/IRTransformLayer.cpp``. This class is concerned; with two very simple jobs: (1) Running every IR Module that is emitted via this; layer through the transform function object, and (2) implementing the ORC; ``IRLayer`` interface (which itself conforms to the general ORC Layer concept,; more on that below). Most of the class is straightforward: a typedef for the; transform function, a constructor to initialize the members, a setter for the; transform function value, and a default no-op transform. The most important; method is ``emit`` as this is half of our IRLayer interface. The emit method; applies our transform to each module that it is called on and, if the transform; succeeds, passes the transformed module to the base layer. If the transform; fails, our emit function calls; ``MaterializationResponsibility::failMaterialization`` (this JIT clients who; may be waiting on other threads know that the code they were waiting for has; failed to compile) and logs the error with the execution session before bailing; out. The other half of the IRLayer interface we inherit unmodified from the IRLayer; class:. .. code-block:: c++. Error IRLayer::add(JITDyli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:10371,Usability,learn,learned,10371," Most layers that derived from IRLayer can rely on this default implementation; of the ``add`` method. These two operations, ``add`` and ``emit``, together constitute the layer; concept: A layer is a way to wrap a part of a compiler pipeline (in this case; the ""opt"" phase of an LLVM compiler) whose API is opaque to ORC with an; interface that ORC can call as needed. The add method takes an; module in some input program representation (in this case an LLVM IR module); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things bec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:10939,Usability,pause,pause,10939,"odule); and stores it in the target ``JITDylib``, arranging for it to be passed back; to the layer's emit method when any symbol defined by that module is requested.; Each layer can complete its own work by calling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; ======",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11136,Usability,pause,pauses,11136,"lling the ``emit`` method of its base; layer. For example, in this tutorial our IRTransformLayer calls through to; our IRCompileLayer to compile the transformed IR, and our IRCompileLayer in; turn calls our ObjectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst:11312,Usability,pause,pauses,11312,"jectLayer to link the object file produced by our compiler. So far we have learned how to optimize and compile our LLVM IR, but we have; not focused on when compilation happens. Our current REPL optimizes and; compiles each function as soon as it is referenced by any other code,; regardless of whether it is ever called at runtime. In the next chapter we; will introduce a fully lazy compilation, in which functions are not compiled; until they are first called at run-time. At this point the trade-offs get much; more interesting: the lazier we are, the quicker we can start executing the; first function, but the more often we will have to pause to compile newly; encountered functions. If we only code-gen lazily, but optimize eagerly, we; will have a longer startup time (as everything is optimized at that time) but; relatively short pauses as each function just passes through code-gen. If we; both optimize and code-gen lazily we can start executing the first function; more quickly, but we will have longer pauses as each function has to be both; optimized and code-gen'd when it is first executed. Things become even more; interesting if we consider interprocedural optimizations like inlining, which; must be performed eagerly. These are complex trade-offs, and there is no; one-size-fits all solution to them, but by providing composable layers we leave; the decisions to the person implementing the JIT, and make it easy for them to; experiment with different configurations. `Next: Adding Per-function Lazy Compilation <BuildingAJIT3.html>`_. Full Code Listing; =================. Here is the complete code listing for our running example with an; IRTransformLayer added to enable optimization. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter2/KaleidoscopeJIT.h; :",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT2.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:573,Availability,down,down,573,"=============================================; Building a JIT: Per-function Lazy Compilation; =============================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:1612,Availability,redundant,redundant,1612,"text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:6224,Availability,down,down,6224,"::move(M));; }),; CompileCallbackManager(; orc::createLocalCompileCallbackManager(TM->getTargetTriple(), 0)),; CODLayer(OptimizeLayer,; [this](Function &F) { return std::set<Function*>({&F}); },; *CompileCallbackManager,; orc::createLocalIndirectStubsManagerBuilder(; TM->getTargetTriple())) {; llvm::sys::DynamicLibrary::LoadLibraryPermanently(nullptr);; }. Next we have to update our constructor to initialize the new members. To create; an appropriate compile callback manager we use the; createLocalCompileCallbackManager function, which takes a TargetMachine and an; ExecutorAddr to call if it receives a request to compile an unknown; function. In our simple JIT this situation is unlikely to come up, so we'll; cheat and just pass '0' here. In a production quality JIT you could give the; address of a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:458,Deployability,update,updates,458,"=============================================; Building a JIT: Per-function Lazy Compilation; =============================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:498,Deployability,update,updated,498,"=============================================; Building a JIT: Per-function Lazy Compilation; =============================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:540,Deployability,update,updated,540,"=============================================; Building a JIT: Per-function Lazy Compilation; =============================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:3232,Deployability,update,update,3232,"which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that; will trigger compilation and then execution of a function. By initializing the; function's stub to point at the function's compile callback, we enable lazy; compilation: The first attempted call to the function will follow the function; pointer and trigger the compile callback instead. The compile callback will; compile the function, update the function pointer for the stub, then execute; the function. On all subsequent calls to the function, the function pointer; will point at the already-compiled function, so there is no further overhead; from the compiler. We will look at this process in more detail in the next; chapter of this tutorial, but for now we'll trust the CompileOnDemandLayer to; set all the stubs and callbacks up for us. All we need to do is to add the; CompileOnDemandLayer to the top of our stack and we'll get the benefits of; lazy compilation. We just need a few changes to the source:. .. code-block:: c++. ...; #include ""llvm/ExecutionEngine/SectionMemoryManager.h""; #include ""llvm/ExecutionEngine/Orc/CompileOnDemandLayer.h""; #include ""llvm/ExecutionEngine/Orc/CompileUtils.h""; ... ...; class KaleidoscopeJIT {; private:; std::unique_ptr<TargetMachine> TM;; const DataLayout DL;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer<decltype(ObjectLayer), SimpleCompiler> CompileLayer;. using OptimizeFunction =; s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:5571,Deployability,update,update,5571," include the CompileOnDemandLayer.h header, then add two new; members: a std::unique_ptr<JITCompileCallbackManager> and a CompileOnDemandLayer,; to our class. The CompileCallbackManager member is used by the CompileOnDemandLayer; to create the compile callback needed for each function. .. code-block:: c++. KaleidoscopeJIT(); : TM(EngineBuilder().selectTarget()), DL(TM->createDataLayout()),; ObjectLayer([]() { return std::make_shared<SectionMemoryManager>(); }),; CompileLayer(ObjectLayer, SimpleCompiler(*TM)),; OptimizeLayer(CompileLayer,; [this](std::shared_ptr<Module> M) {; return optimizeModule(std::move(M));; }),; CompileCallbackManager(; orc::createLocalCompileCallbackManager(TM->getTargetTriple(), 0)),; CODLayer(OptimizeLayer,; [this](Function &F) { return std::set<Function*>({&F}); },; *CompileCallbackManager,; orc::createLocalIndirectStubsManagerBuilder(; TM->getTargetTriple())) {; llvm::sys::DynamicLibrary::LoadLibraryPermanently(nullptr);; }. Next we have to update our constructor to initialize the new members. To create; an appropriate compile callback manager we use the; createLocalCompileCallbackManager function, which takes a TargetMachine and an; ExecutorAddr to call if it receives a request to compile an unknown; function. In our simple JIT this situation is unlikely to come up, so we'll; cheat and just pass '0' here. In a production quality JIT you could give the; address of a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also reques",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:7426,Energy Efficiency,allocate,allocated,7426,"yer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter conclusion.**. Full Code Listing; =================. Here is the complete code listing for our running example with a CompileOnDemand; layer added to enable lazy function-at-a-time compilation. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:1780,Integrability,interface,interface,1780,"ss from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:1875,Modifiability,layers,layers,1875,"ss from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:6167,Modifiability,layers,layers,6167,"::move(M));; }),; CompileCallbackManager(; orc::createLocalCompileCallbackManager(TM->getTargetTriple(), 0)),; CODLayer(OptimizeLayer,; [this](Function &F) { return std::set<Function*>({&F}); },; *CompileCallbackManager,; orc::createLocalIndirectStubsManagerBuilder(; TM->getTargetTriple())) {; llvm::sys::DynamicLibrary::LoadLibraryPermanently(nullptr);; }. Next we have to update our constructor to initialize the new members. To create; an appropriate compile callback manager we use the; createLocalCompileCallbackManager function, which takes a TargetMachine and an; ExecutorAddr to call if it receives a request to compile an unknown; function. In our simple JIT this situation is unlikely to come up, so we'll; cheat and just pass '0' here. In a production quality JIT you could give the; address of a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:8281,Modifiability,config,config,8281,"ompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter conclusion.**. Full Code Listing; =================. Here is the complete code listing for our running example with a CompileOnDemand; layer added to enable lazy function-at-a-time compilation. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter3/KaleidoscopeJIT.h; :language: c++. `Next: Extreme Laziness -- Using Compile Callbacks to JIT directly from ASTs <BuildingAJIT4.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:931,Performance,optimiz,optimized,931,"=============================================; Building a JIT: Per-function Lazy Compilation; =============================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:1165,Performance,perform,performance,1165,"====. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:2743,Performance,optimiz,optimization,2743,"ing launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that; will trigger compilation and then execution of a function. By initializing the; function's stub to point at the function's compile callback, we enable lazy; compilation: The first attempted call to the function will follow the function; pointer and trigger the compile callback instead. The compile callback will; compile the function, update the function pointer for the stub, then execute; the function. On all subsequent calls to the function, the function pointer; will point at the already-compiled function, so there is no further overhead; from the compiler. We will look at this process in more detail in the next; chapter of this tutorial, but for now we'll trust the Compile",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:5178,Performance,optimiz,optimizeModule,5178,":function<std::shared_ptr<Module>(std::shared_ptr<Module>)>;. IRTransformLayer<decltype(CompileLayer), OptimizeFunction> OptimizeLayer;. std::unique_ptr<JITCompileCallbackManager> CompileCallbackManager;; CompileOnDemandLayer<decltype(OptimizeLayer)> CODLayer;. public:; using ModuleHandle = decltype(CODLayer)::ModuleHandleT;. First we need to include the CompileOnDemandLayer.h header, then add two new; members: a std::unique_ptr<JITCompileCallbackManager> and a CompileOnDemandLayer,; to our class. The CompileCallbackManager member is used by the CompileOnDemandLayer; to create the compile callback needed for each function. .. code-block:: c++. KaleidoscopeJIT(); : TM(EngineBuilder().selectTarget()), DL(TM->createDataLayout()),; ObjectLayer([]() { return std::make_shared<SectionMemoryManager>(); }),; CompileLayer(ObjectLayer, SimpleCompiler(*TM)),; OptimizeLayer(CompileLayer,; [this](std::shared_ptr<Module> M) {; return optimizeModule(std::move(M));; }),; CompileCallbackManager(; orc::createLocalCompileCallbackManager(TM->getTargetTriple(), 0)),; CODLayer(OptimizeLayer,; [this](Function &F) { return std::set<Function*>({&F}); },; *CompileCallbackManager,; orc::createLocalIndirectStubsManagerBuilder(; TM->getTargetTriple())) {; llvm::sys::DynamicLibrary::LoadLibraryPermanently(nullptr);; }. Next we have to update our constructor to initialize the new members. To create; an appropriate compile callback manager we use the; createLocalCompileCallbackManager function, which takes a TargetMachine and an; ExecutorAddr to call if it receives a request to compile an unknown; function. In our simple JIT this situation is unlikely to come up, so we'll; cheat and just pass '0' here. In a production quality JIT you could give the; address of a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:1612,Safety,redund,redundant,1612,"text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:2528,Safety,safe,safely,2528,"ing launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that; will trigger compilation and then execution of a function. By initializing the; function's stub to point at the function's compile callback, we enable lazy; compilation: The first attempted call to the function will follow the function; pointer and trigger the compile callback instead. The compile callback will; compile the function, update the function pointer for the stub, then execute; the function. On all subsequent calls to the function, the function pointer; will point at the already-compiled function, so there is no further overhead; from the compiler. We will look at this process in more detail in the next; chapter of this tutorial, but for now we'll trust the Compile",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:2166,Testability,stub,stub,2166,", is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that; will trigger compilation and then execution of a function. By initializing the; function's stub to point at the function's compile callback, we enable lazy; compilation: The first attempted call to the function will follow the function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:2203,Testability,stub,stub,2203,"ode to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that; will trigger compilation and then execution of a function. By initializing the; function's stub to point at the function's compile callback, we enable lazy; compilation: The first attempted call to the function will follow the function; pointer and trigger the compile callback instead. The compile callback will; compile the function, update the function pointer for the stub, then execute;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:2987,Testability,stub,stub,2987,"o this, the CompileOnDemandLayer creates two small; utilities for each function that it scans: a *stub* and a *compile; callback*. The stub is a pair of a function pointer (which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that; will trigger compilation and then execution of a function. By initializing the; function's stub to point at the function's compile callback, we enable lazy; compilation: The first attempted call to the function will follow the function; pointer and trigger the compile callback instead. The compile callback will; compile the function, update the function pointer for the stub, then execute; the function. On all subsequent calls to the function, the function pointer; will point at the already-compiled function, so there is no further overhead; from the compiler. We will look at this process in more detail in the next; chapter of this tutorial, but for now we'll trust the CompileOnDemandLayer to; set all the stubs and callbacks up for us. All we need to do is to add the; CompileOnDemandLayer to the top of our stack and we'll get the benefits of; lazy compilation. We just need a few changes to the source:. .. code-block:: c++. ...; #include ""llvm/ExecutionEngine/SectionMemoryManager.h""; #include ""llvm/ExecutionEngine/Orc/CompileOnDemandLayer.h""; #include ""llvm/ExecutionEngine/Orc/CompileUtils.h""; ... ...; class KaleidoscopeJIT {; private:; std::unique_ptr<Tar",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:3268,Testability,stub,stub,3268,"which will be pointed at; the function's implementation once the function has been compiled) and an; indirect jump through the pointer. By fixing the address of the indirect jump; for the lifetime of the program we can give the function a permanent ""effective; address"", one that can be safely used for indirection and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that; will trigger compilation and then execution of a function. By initializing the; function's stub to point at the function's compile callback, we enable lazy; compilation: The first attempted call to the function will follow the function; pointer and trigger the compile callback instead. The compile callback will; compile the function, update the function pointer for the stub, then execute; the function. On all subsequent calls to the function, the function pointer; will point at the already-compiled function, so there is no further overhead; from the compiler. We will look at this process in more detail in the next; chapter of this tutorial, but for now we'll trust the CompileOnDemandLayer to; set all the stubs and callbacks up for us. All we need to do is to add the; CompileOnDemandLayer to the top of our stack and we'll get the benefits of; lazy compilation. We just need a few changes to the source:. .. code-block:: c++. ...; #include ""llvm/ExecutionEngine/SectionMemoryManager.h""; #include ""llvm/ExecutionEngine/Orc/CompileOnDemandLayer.h""; #include ""llvm/ExecutionEngine/Orc/CompileUtils.h""; ... ...; class KaleidoscopeJIT {; private:; std::unique_ptr<TargetMachine> TM;; const DataLayout DL;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer<decltype(ObjectLayer), SimpleCompiler> CompileLayer;. using OptimizeFunction =; s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:3610,Testability,stub,stubs,3610,"ction and function pointer; comparison even if the function's implementation is never compiled, or if it is; compiled more than once (due to, for example, recompiling the function at a; higher optimization level) and changes address. The second utility, the compile; callback, represents a re-entry point from the program into the compiler that; will trigger compilation and then execution of a function. By initializing the; function's stub to point at the function's compile callback, we enable lazy; compilation: The first attempted call to the function will follow the function; pointer and trigger the compile callback instead. The compile callback will; compile the function, update the function pointer for the stub, then execute; the function. On all subsequent calls to the function, the function pointer; will point at the already-compiled function, so there is no further overhead; from the compiler. We will look at this process in more detail in the next; chapter of this tutorial, but for now we'll trust the CompileOnDemandLayer to; set all the stubs and callbacks up for us. All we need to do is to add the; CompileOnDemandLayer to the top of our stack and we'll get the benefits of; lazy compilation. We just need a few changes to the source:. .. code-block:: c++. ...; #include ""llvm/ExecutionEngine/SectionMemoryManager.h""; #include ""llvm/ExecutionEngine/Orc/CompileOnDemandLayer.h""; #include ""llvm/ExecutionEngine/Orc/CompileUtils.h""; ... ...; class KaleidoscopeJIT {; private:; std::unique_ptr<TargetMachine> TM;; const DataLayout DL;; RTDyldObjectLinkingLayer ObjectLayer;; IRCompileLayer<decltype(ObjectLayer), SimpleCompiler> CompileLayer;. using OptimizeFunction =; std::function<std::shared_ptr<Module>(std::shared_ptr<Module>)>;. IRTransformLayer<decltype(CompileLayer), OptimizeFunction> OptimizeLayer;. std::unique_ptr<JITCompileCallbackManager> CompileCallbackManager;; CompileOnDemandLayer<decltype(OptimizeLayer)> CODLayer;. public:; using ModuleHandle = decltype(CODLa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:6944,Testability,stub,stubs,6944," a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter con",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:7063,Testability,stub,stubs,7063," a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter con",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:7152,Testability,stub,stub,7152,"o the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter conclusion.**. Full Code Listing; =================. Here is the complete code listing for our running example with a CompileOnDemand; layer added to enable lazy function-at-a-time compilation. To build this ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:7238,Testability,stub,stubs,7238,"o the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter conclusion.**. Full Code Listing; =================. Here is the complete code listing for our running example with a CompileOnDemand; layer added to enable lazy function-at-a-time compilation. To build this ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:7263,Testability,stub,stubs,7263,"o the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter conclusion.**. Full Code Listing; =================. Here is the complete code listing for our running example with a CompileOnDemand; layer added to enable lazy function-at-a-time compilation. To build this ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:7375,Testability,stub,stubs,7375,"yer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter conclusion.**. Full Code Listing; =================. Here is the complete code listing for our running example with a CompileOnDemand; layer added to enable lazy function-at-a-time compilation. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:7443,Testability,stub,stubs,7443,"yer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLayer.removeModule(H);; // ... Finally, we need to replace the references to OptimizeLayer in our addModule,; findSymbol, and removeModule methods. With that, we're up and running. **To be done:**. ** Chapter conclusion.**. Full Code Listing; =================. Here is the complete code listing for our running example with a CompileOnDemand; layer added to enable lazy function-at-a-time compilation. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:339,Usability,feedback,feedback,339,"=============================================; Building a JIT: Per-function Lazy Compilation; =============================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:1135,Usability,simpl,simple,1135,"====. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 3 Introduction; ======================. **Warning: This text is currently out of date due to ORC API updates.**. **The example code has been updated and can be used. The text will be updated; once the API churn dies down.**. Welcome to Chapter 3 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter discusses lazy JITing and shows you how to enable it by adding an ORC; CompileOnDemand layer the JIT from `Chapter 2 <BuildingAJIT2.html>`_. Lazy Compilation; ================. When we add a module to the KaleidoscopeJIT class from Chapter 2 it is; immediately optimized, compiled and linked for us by the IRTransformLayer,; IRCompileLayer and RTDyldObjectLinkingLayer respectively. This scheme, where all the; work to make a Module executable is done up front, is simple to understand and; its performance characteristics are easy to reason about. However, it will lead; to very high startup times if the amount of code to be compiled is large, and; may also do a lot of unnecessary compilation if only a few compiled functions; are ever called at runtime. A truly ""just-in-time"" compiler should allow us to; defer the compilation of any given function until the moment that function is; first called, improving launch times and eliminating redundant work. In fact,; the ORC APIs provide us with a layer to lazily compile LLVM IR:; *CompileOnDemandLayer*. The CompileOnDemandLayer class conforms to the layer interface described in; Chapter 2, but its addModule method behaves quite differently from the layers; we have seen so far: rather than doing any work up front, it just scans the; Modules being added and arranges for each function in them to be compiled the; first time it is called. To do this, the CompileOnDemandLayer creates two small; utilities for e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:5854,Usability,simpl,simple,5854,"+. KaleidoscopeJIT(); : TM(EngineBuilder().selectTarget()), DL(TM->createDataLayout()),; ObjectLayer([]() { return std::make_shared<SectionMemoryManager>(); }),; CompileLayer(ObjectLayer, SimpleCompiler(*TM)),; OptimizeLayer(CompileLayer,; [this](std::shared_ptr<Module> M) {; return optimizeModule(std::move(M));; }),; CompileCallbackManager(; orc::createLocalCompileCallbackManager(TM->getTargetTriple(), 0)),; CODLayer(OptimizeLayer,; [this](Function &F) { return std::set<Function*>({&F}); },; *CompileCallbackManager,; orc::createLocalIndirectStubsManagerBuilder(; TM->getTargetTriple())) {; llvm::sys::DynamicLibrary::LoadLibraryPermanently(nullptr);; }. Next we have to update our constructor to initialize the new members. To create; an appropriate compile callback manager we use the; createLocalCompileCallbackManager function, which takes a TargetMachine and an; ExecutorAddr to call if it receives a request to compile an unknown; function. In our simple JIT this situation is unlikely to come up, so we'll; cheat and just pass '0' here. In a production quality JIT you could give the; address of a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallback",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst:6777,Usability,simpl,simple,6777," receives a request to compile an unknown; function. In our simple JIT this situation is unlikely to come up, so we'll; cheat and just pass '0' here. In a production quality JIT you could give the; address of a function that throws an exception in order to unwind the JIT'd; code's stack. Now we can construct our CompileOnDemandLayer. Following the pattern from; previous layers we start by passing a reference to the next layer down in our; stack -- the OptimizeLayer. Next we need to supply a 'partitioning function':; when a not-yet-compiled function is called, the CompileOnDemandLayer will call; this function to ask us what we would like to compile. At a minimum we need to; compile the function being called (given by the argument to the partitioning; function), but we could also request that the CompileOnDemandLayer compile other; functions that are unconditionally called (or highly likely to be called) from; the function being called. For KaleidoscopeJIT we'll keep it simple and just; request compilation of the function that was called. Next we pass a reference to; our CompileCallbackManager. Finally, we need to supply an ""indirect stubs; manager builder"": a utility function that constructs IndirectStubManagers, which; are in turn used to build the stubs for the functions in each module. The; CompileOnDemandLayer will call the indirect stub manager builder once for each; call to addModule, and use the resulting indirect stubs manager to create; stubs for all functions in all modules in the set. If/when the module set is; removed from the JIT the indirect stubs manager will be deleted, freeing any; memory allocated to the stubs. We supply this function by using the; createLocalIndirectStubsManagerBuilder utility. .. code-block:: c++. // ...; if (auto Sym = CODLayer.findSymbol(Name, false)); // ...; return cantFail(CODLayer.addModule(std::move(Ms),; std::move(Resolver)));; // ... // ...; return CODLayer.findSymbol(MangledNameStream.str(), true);; // ... // ...; CODLay",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT3.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst:925,Energy Efficiency,reduce,reduces,925,"=======================================================================; Building a JIT: Extreme Laziness - Using LazyReexports to JIT from ASTs; =======================================================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter introduces custom MaterializationUnits and Layers, and the lazy; reexports API. Together these will be used to replace the CompileOnDemandLayer; from `Chapter 3 <BuildingAJIT3.html>`_ with a custom lazy-JITing scheme that JITs; directly from Kaleidoscope ASTs. **To be done:**. **(1) Describe the drawbacks of JITing from IR (have to compile to IR first,; which reduces the benefits of laziness).**. **(2) Describe CompileCallbackManagers and IndirectStubManagers in detail.**. **(3) Run through the implementation of addFunctionAST.**. Full Code Listing; =================. Here is the complete code listing for our running example that JITs lazily from; Kaleidoscope ASTS. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter4/KaleidoscopeJIT.h; :language: c++. `Next: Remote-JITing -- Process-isolation and laziness-at-a-distance <BuildingAJIT5.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst:1325,Modifiability,config,config,1325,"=======================================================================; Building a JIT: Extreme Laziness - Using LazyReexports to JIT from ASTs; =======================================================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter introduces custom MaterializationUnits and Layers, and the lazy; reexports API. Together these will be used to replace the CompileOnDemandLayer; from `Chapter 3 <BuildingAJIT3.html>`_ with a custom lazy-JITing scheme that JITs; directly from Kaleidoscope ASTs. **To be done:**. **(1) Describe the drawbacks of JITing from IR (have to compile to IR first,; which reduces the benefits of laziness).**. **(2) Describe CompileCallbackManagers and IndirectStubManagers in detail.**. **(3) Run through the implementation of addFunctionAST.**. Full Code Listing; =================. Here is the complete code listing for our running example that JITs lazily from; Kaleidoscope ASTS. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter4/KaleidoscopeJIT.h; :language: c++. `Next: Remote-JITing -- Process-isolation and laziness-at-a-distance <BuildingAJIT5.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst:417,Usability,feedback,feedback,417,"=======================================================================; Building a JIT: Extreme Laziness - Using LazyReexports to JIT from ASTs; =======================================================================. .. contents::; :local:. **This tutorial is under active development. It is incomplete and details may; change frequently.** Nonetheless we invite you to try it out as it stands, and; we welcome any feedback. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""Building an ORC-based JIT in LLVM"" tutorial. This; chapter introduces custom MaterializationUnits and Layers, and the lazy; reexports API. Together these will be used to replace the CompileOnDemandLayer; from `Chapter 3 <BuildingAJIT3.html>`_ with a custom lazy-JITing scheme that JITs; directly from Kaleidoscope ASTs. **To be done:**. **(1) Describe the drawbacks of JITing from IR (have to compile to IR first,; which reduces the benefits of laziness).**. **(2) Describe CompileCallbackManagers and IndirectStubManagers in detail.**. **(3) Run through the implementation of addFunctionAST.**. Full Code Listing; =================. Here is the complete code listing for our running example that JITs lazily from; Kaleidoscope ASTS. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../examples/Kaleidoscope/BuildingAJIT/Chapter4/KaleidoscopeJIT.h; :language: c++. `Next: Remote-JITing -- Process-isolation and laziness-at-a-distance <BuildingAJIT5.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/BuildingAJIT4.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst:1017,Deployability,integrat,integrated,1017,"================================; LLVM Tutorial: Table of Contents; ================================. Kaleidoscope: Implementing a Language with LLVM; ===============================================. .. toctree::; :hidden:. MyFirstLanguageFrontend/index. :doc:`MyFirstLanguageFrontend/index`; This is the ""Kaleidoscope"" Language tutorial, showing how to implement a simple; language using LLVM components in C++. .. toctree::; :titlesonly:; :glob:; :numbered:. MyFirstLanguageFrontend/LangImpl*. Building a JIT in LLVM; ===============================================. .. toctree::; :titlesonly:; :glob:; :numbered:. BuildingAJIT*. External Tutorials; ==================. `Tutorial: Creating an LLVM Backend for the Cpu0 Architecture <http://jonathan2251.github.io/lbd/>`_; A step-by-step tutorial for developing an LLVM backend. Under; active development at `<https://github.com/Jonathan2251/lbd>`_ (please; contribute!). `Howto: Implementing LLVM Integrated Assembler`_; A simple guide for how to implement an LLVM integrated assembler for an; architecture. .. _`Howto: Implementing LLVM Integrated Assembler`: http://www.embecosm.com/appnotes/ean10/ean10-howto-llvmas-1.0.html. Advanced Topics; ===============. #. `Writing an Optimization for LLVM <https://llvm.org/pubs/2004-09-22-LCPCLLVMTutorial.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst:1017,Integrability,integrat,integrated,1017,"================================; LLVM Tutorial: Table of Contents; ================================. Kaleidoscope: Implementing a Language with LLVM; ===============================================. .. toctree::; :hidden:. MyFirstLanguageFrontend/index. :doc:`MyFirstLanguageFrontend/index`; This is the ""Kaleidoscope"" Language tutorial, showing how to implement a simple; language using LLVM components in C++. .. toctree::; :titlesonly:; :glob:; :numbered:. MyFirstLanguageFrontend/LangImpl*. Building a JIT in LLVM; ===============================================. .. toctree::; :titlesonly:; :glob:; :numbered:. BuildingAJIT*. External Tutorials; ==================. `Tutorial: Creating an LLVM Backend for the Cpu0 Architecture <http://jonathan2251.github.io/lbd/>`_; A step-by-step tutorial for developing an LLVM backend. Under; active development at `<https://github.com/Jonathan2251/lbd>`_ (please; contribute!). `Howto: Implementing LLVM Integrated Assembler`_; A simple guide for how to implement an LLVM integrated assembler for an; architecture. .. _`Howto: Implementing LLVM Integrated Assembler`: http://www.embecosm.com/appnotes/ean10/ean10-howto-llvmas-1.0.html. Advanced Topics; ===============. #. `Writing an Optimization for LLVM <https://llvm.org/pubs/2004-09-22-LCPCLLVMTutorial.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst:366,Usability,simpl,simple,366,"================================; LLVM Tutorial: Table of Contents; ================================. Kaleidoscope: Implementing a Language with LLVM; ===============================================. .. toctree::; :hidden:. MyFirstLanguageFrontend/index. :doc:`MyFirstLanguageFrontend/index`; This is the ""Kaleidoscope"" Language tutorial, showing how to implement a simple; language using LLVM components in C++. .. toctree::; :titlesonly:; :glob:; :numbered:. MyFirstLanguageFrontend/LangImpl*. Building a JIT in LLVM; ===============================================. .. toctree::; :titlesonly:; :glob:; :numbered:. BuildingAJIT*. External Tutorials; ==================. `Tutorial: Creating an LLVM Backend for the Cpu0 Architecture <http://jonathan2251.github.io/lbd/>`_; A step-by-step tutorial for developing an LLVM backend. Under; active development at `<https://github.com/Jonathan2251/lbd>`_ (please; contribute!). `Howto: Implementing LLVM Integrated Assembler`_; A simple guide for how to implement an LLVM integrated assembler for an; architecture. .. _`Howto: Implementing LLVM Integrated Assembler`: http://www.embecosm.com/appnotes/ean10/ean10-howto-llvmas-1.0.html. Advanced Topics; ===============. #. `Writing an Optimization for LLVM <https://llvm.org/pubs/2004-09-22-LCPCLLVMTutorial.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst:975,Usability,simpl,simple,975,"================================; LLVM Tutorial: Table of Contents; ================================. Kaleidoscope: Implementing a Language with LLVM; ===============================================. .. toctree::; :hidden:. MyFirstLanguageFrontend/index. :doc:`MyFirstLanguageFrontend/index`; This is the ""Kaleidoscope"" Language tutorial, showing how to implement a simple; language using LLVM components in C++. .. toctree::; :titlesonly:; :glob:; :numbered:. MyFirstLanguageFrontend/LangImpl*. Building a JIT in LLVM; ===============================================. .. toctree::; :titlesonly:; :glob:; :numbered:. BuildingAJIT*. External Tutorials; ==================. `Tutorial: Creating an LLVM Backend for the Cpu0 Architecture <http://jonathan2251.github.io/lbd/>`_; A step-by-step tutorial for developing an LLVM backend. Under; active development at `<https://github.com/Jonathan2251/lbd>`_ (please; contribute!). `Howto: Implementing LLVM Integrated Assembler`_; A simple guide for how to implement an LLVM integrated assembler for an; architecture. .. _`Howto: Implementing LLVM Integrated Assembler`: http://www.embecosm.com/appnotes/ean10/ean10-howto-llvmas-1.0.html. Advanced Topics; ===============. #. `Writing an Optimization for LLVM <https://llvm.org/pubs/2004-09-22-LCPCLLVMTutorial.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst:982,Usability,guid,guide,982,"================================; LLVM Tutorial: Table of Contents; ================================. Kaleidoscope: Implementing a Language with LLVM; ===============================================. .. toctree::; :hidden:. MyFirstLanguageFrontend/index. :doc:`MyFirstLanguageFrontend/index`; This is the ""Kaleidoscope"" Language tutorial, showing how to implement a simple; language using LLVM components in C++. .. toctree::; :titlesonly:; :glob:; :numbered:. MyFirstLanguageFrontend/LangImpl*. Building a JIT in LLVM; ===============================================. .. toctree::; :titlesonly:; :glob:; :numbered:. BuildingAJIT*. External Tutorials; ==================. `Tutorial: Creating an LLVM Backend for the Cpu0 Architecture <http://jonathan2251.github.io/lbd/>`_; A step-by-step tutorial for developing an LLVM backend. Under; active development at `<https://github.com/Jonathan2251/lbd>`_ (please; contribute!). `Howto: Implementing LLVM Integrated Assembler`_; A simple guide for how to implement an LLVM integrated assembler for an; architecture. .. _`Howto: Implementing LLVM Integrated Assembler`: http://www.embecosm.com/appnotes/ean10/ean10-howto-llvmas-1.0.html. Advanced Topics; ===============. #. `Writing an Optimization for LLVM <https://llvm.org/pubs/2004-09-22-LCPCLLVMTutorial.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:3606,Availability,down,down,3606,"Chapter #5: Extending the Language: Control Flow <LangImpl05.html>`_ - With; the basic language up and running, we show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes pointers to info on ""special topics"" like adding garbage; collection support, exceptions, debugging, support for ""spaghetti; stacks"", etc. By the end of the tutorial, we'll have written a bit less than 1000 lines; of (non-comment, non-blank) lines of code. With this small amount of; code, we'll have built up a nice little compiler for a non-trivial; language including a hand-written lexer, parser, AST, as well as code; generation support - both static and JIT! The breadth of this is a great; testament to the strengths of LLVM and shows ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:970,Energy Efficiency,reduce,reduces,970,"=============================================; My First Language Frontend with LLVM Tutorial; =============================================. .. toctree::; :hidden:. LangImpl01; LangImpl02; LangImpl03; LangImpl04; LangImpl05; LangImpl06; LangImpl07; LangImpl08; LangImpl09; LangImpl10. **Requirements:** This tutorial assumes you know C++, but no previous; compiler experience is necessary. Welcome to the ""My First Language Frontend with LLVM"" tutorial. Here we; run through the implementation of a simple language, showing; how fun and easy it can be. This tutorial will get you up and running; fast and show a concrete example of something that uses LLVM to generate; code. This tutorial introduces the simple ""Kaleidoscope"" language, building it; iteratively over the course of several chapters, showing how it is built; over time. This lets us cover a range of language design and LLVM-specific; ideas, showing and explaining the code for it all along the way,; and reduces the overwhelming amount of details up front. We strongly; encourage that you *work with this code* - make a copy and hack it up and; experiment. **Warning**: In order to focus on teaching compiler techniques and LLVM; specifically,; this tutorial does *not* show best practices in software engineering; principles. For example, the code uses global variables; pervasively, doesn't use; `visitors <http://en.wikipedia.org/wiki/Visitor_pattern>`_, etc... but; instead keeps things simple and focuses on the topics at hand. This tutorial is structured into chapters covering individual topics,; allowing you to skip ahead as you wish:. - `Chapter #1: Kaleidoscope language and Lexer <LangImpl01.html>`_ -; This shows where we are; going and the basic functionality that we want to build. A lexer; is also the first part of building a parser for a language, and we; use a simple C++ lexer which is easy to understand.; - `Chapter #2: Implementing a Parser and AST <LangImpl02.html>`_ -; With the lexer in place, we can talk abo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:3156,Integrability,rout,routines,3156,"tutorial describes recursive descent; parsing and operator precedence parsing.; - `Chapter #3: Code generation to LLVM IR <LangImpl03.html>`_ - with; the AST ready, we show how easy it is to generate LLVM IR, and show; a simple way to incorporate LLVM into your project.; - `Chapter #4: Adding JIT and Optimizer Support <LangImpl04.html>`_ -; One great thing about LLVM is its support for JIT compilation, so; we'll dive right into it and show you the 3 lines it takes to add JIT; support. Later chapters show how to generate .o files.; - `Chapter #5: Extending the Language: Control Flow <LangImpl05.html>`_ - With; the basic language up and running, we show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes p",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:3980,Integrability,wrap,wraps,3980,"e show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes pointers to info on ""special topics"" like adding garbage; collection support, exceptions, debugging, support for ""spaghetti; stacks"", etc. By the end of the tutorial, we'll have written a bit less than 1000 lines; of (non-comment, non-blank) lines of code. With this small amount of; code, we'll have built up a nice little compiler for a non-trivial; language including a hand-written lexer, parser, AST, as well as code; generation support - both static and JIT! The breadth of this is a great; testament to the strengths of LLVM and shows why it is such a popular; target for language designers and others who need high performance code; generation.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:1327,Modifiability,variab,variables,1327," but no previous; compiler experience is necessary. Welcome to the ""My First Language Frontend with LLVM"" tutorial. Here we; run through the implementation of a simple language, showing; how fun and easy it can be. This tutorial will get you up and running; fast and show a concrete example of something that uses LLVM to generate; code. This tutorial introduces the simple ""Kaleidoscope"" language, building it; iteratively over the course of several chapters, showing how it is built; over time. This lets us cover a range of language design and LLVM-specific; ideas, showing and explaining the code for it all along the way,; and reduces the overwhelming amount of details up front. We strongly; encourage that you *work with this code* - make a copy and hack it up and; experiment. **Warning**: In order to focus on teaching compiler techniques and LLVM; specifically,; this tutorial does *not* show best practices in software engineering; principles. For example, the code uses global variables; pervasively, doesn't use; `visitors <http://en.wikipedia.org/wiki/Visitor_pattern>`_, etc... but; instead keeps things simple and focuses on the topics at hand. This tutorial is structured into chapters covering individual topics,; allowing you to skip ahead as you wish:. - `Chapter #1: Kaleidoscope language and Lexer <LangImpl01.html>`_ -; This shows where we are; going and the basic functionality that we want to build. A lexer; is also the first part of building a parser for a language, and we; use a simple C++ lexer which is easy to understand.; - `Chapter #2: Implementing a Parser and AST <LangImpl02.html>`_ -; With the lexer in place, we can talk about parsing techniques and; basic AST construction. This tutorial describes recursive descent; parsing and operator precedence parsing.; - `Chapter #3: Code generation to LLVM IR <LangImpl03.html>`_ - with; the AST ready, we show how easy it is to generate LLVM IR, and show; a simple way to incorporate LLVM into your project.; - `Chapter",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:2724,Modifiability,extend,extend,2724,"nctionality that we want to build. A lexer; is also the first part of building a parser for a language, and we; use a simple C++ lexer which is easy to understand.; - `Chapter #2: Implementing a Parser and AST <LangImpl02.html>`_ -; With the lexer in place, we can talk about parsing techniques and; basic AST construction. This tutorial describes recursive descent; parsing and operator precedence parsing.; - `Chapter #3: Code generation to LLVM IR <LangImpl03.html>`_ - with; the AST ready, we show how easy it is to generate LLVM IR, and show; a simple way to incorporate LLVM into your project.; - `Chapter #4: Adding JIT and Optimizer Support <LangImpl04.html>`_ -; One great thing about LLVM is its support for JIT compilation, so; we'll dive right into it and show you the 3 lines it takes to add JIT; support. Later chapters show how to generate .o files.; - `Chapter #5: Extending the Language: Control Flow <LangImpl05.html>`_ - With; the basic language up and running, we show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real languag",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:2972,Modifiability,extend,extends,2972,"tutorial describes recursive descent; parsing and operator precedence parsing.; - `Chapter #3: Code generation to LLVM IR <LangImpl03.html>`_ - with; the AST ready, we show how easy it is to generate LLVM IR, and show; a simple way to incorporate LLVM into your project.; - `Chapter #4: Adding JIT and Optimizer Support <LangImpl04.html>`_ -; One great thing about LLVM is its support for JIT compilation, so; we'll dive right into it and show you the 3 lines it takes to add JIT; support. Later chapters show how to generate .o files.; - `Chapter #5: Extending the Language: Control Flow <LangImpl05.html>`_ - With; the basic language up and running, we show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes p",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:3299,Modifiability,variab,variables,3299,"o incorporate LLVM into your project.; - `Chapter #4: Adding JIT and Optimizer Support <LangImpl04.html>`_ -; One great thing about LLVM is its support for JIT compilation, so; we'll dive right into it and show you the 3 lines it takes to add JIT; support. Later chapters show how to generate .o files.; - `Chapter #5: Extending the Language: Control Flow <LangImpl05.html>`_ - With; the basic language up and running, we show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes pointers to info on ""special topics"" like adding garbage; collection support, exceptions, debugging, support for ""spaghetti; stacks"", etc. By the end of the tutorial, we'll have written a bit less than 1000 lines; of (non-comment, non",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:3867,Modifiability,variab,variables,3867,"e show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes pointers to info on ""special topics"" like adding garbage; collection support, exceptions, debugging, support for ""spaghetti; stacks"", etc. By the end of the tutorial, we'll have written a bit less than 1000 lines; of (non-comment, non-blank) lines of code. With this small amount of; code, we'll have built up a nice little compiler for a non-trivial; language including a hand-written lexer, parser, AST, as well as code; generation support - both static and JIT! The breadth of this is a great; testament to the strengths of LLVM and shows why it is such a popular; target for language designers and others who need high performance code; generation.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:4022,Modifiability,extend,extend,4022,"e show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes pointers to info on ""special topics"" like adding garbage; collection support, exceptions, debugging, support for ""spaghetti; stacks"", etc. By the end of the tutorial, we'll have written a bit less than 1000 lines; of (non-comment, non-blank) lines of code. With this small amount of; code, we'll have built up a nice little compiler for a non-trivial; language including a hand-written lexer, parser, AST, as well as code; generation support - both static and JIT! The breadth of this is a great; testament to the strengths of LLVM and shows why it is such a popular; target for language designers and others who need high performance code; generation.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:4679,Performance,perform,performance,4679,"e show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes pointers to info on ""special topics"" like adding garbage; collection support, exceptions, debugging, support for ""spaghetti; stacks"", etc. By the end of the tutorial, we'll have written a bit less than 1000 lines; of (non-comment, non-blank) lines of code. With this small amount of; code, we'll have built up a nice little compiler for a non-trivial; language including a hand-written lexer, parser, AST, as well as code; generation support - both static and JIT! The breadth of this is a great; testament to the strengths of LLVM and shows why it is such a popular; target for language designers and others who need high performance code; generation.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:4553,Testability,test,testament,4553,"e show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter talks about adding user-defined local; variables along with an assignment operator. This shows how easy it is; to construct SSA form in LLVM: LLVM does *not* require your front-end; to construct SSA form in order to use it!; - `Chapter #8: Compiling to Object Files <LangImpl08.html>`_ - This; chapter explains how to take LLVM IR and compile it down to object; files, like a static compiler does.; - `Chapter #9: Debug Information <LangImpl09.html>`_ - A real language; needs to support debuggers, so we; add debug information that allows setting breakpoints in Kaleidoscope; functions, print out argument variables, and call functions!; - `Chapter #10: Conclusion and other tidbits <LangImpl10.html>`_ - This; chapter wraps up the series by discussing ways to extend the language; and includes pointers to info on ""special topics"" like adding garbage; collection support, exceptions, debugging, support for ""spaghetti; stacks"", etc. By the end of the tutorial, we'll have written a bit less than 1000 lines; of (non-comment, non-blank) lines of code. With this small amount of; code, we'll have built up a nice little compiler for a non-trivial; language including a hand-written lexer, parser, AST, as well as code; generation support - both static and JIT! The breadth of this is a great; testament to the strengths of LLVM and shows why it is such a popular; target for language designers and others who need high performance code; generation.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:499,Usability,simpl,simple,499,"=============================================; My First Language Frontend with LLVM Tutorial; =============================================. .. toctree::; :hidden:. LangImpl01; LangImpl02; LangImpl03; LangImpl04; LangImpl05; LangImpl06; LangImpl07; LangImpl08; LangImpl09; LangImpl10. **Requirements:** This tutorial assumes you know C++, but no previous; compiler experience is necessary. Welcome to the ""My First Language Frontend with LLVM"" tutorial. Here we; run through the implementation of a simple language, showing; how fun and easy it can be. This tutorial will get you up and running; fast and show a concrete example of something that uses LLVM to generate; code. This tutorial introduces the simple ""Kaleidoscope"" language, building it; iteratively over the course of several chapters, showing how it is built; over time. This lets us cover a range of language design and LLVM-specific; ideas, showing and explaining the code for it all along the way,; and reduces the overwhelming amount of details up front. We strongly; encourage that you *work with this code* - make a copy and hack it up and; experiment. **Warning**: In order to focus on teaching compiler techniques and LLVM; specifically,; this tutorial does *not* show best practices in software engineering; principles. For example, the code uses global variables; pervasively, doesn't use; `visitors <http://en.wikipedia.org/wiki/Visitor_pattern>`_, etc... but; instead keeps things simple and focuses on the topics at hand. This tutorial is structured into chapters covering individual topics,; allowing you to skip ahead as you wish:. - `Chapter #1: Kaleidoscope language and Lexer <LangImpl01.html>`_ -; This shows where we are; going and the basic functionality that we want to build. A lexer; is also the first part of building a parser for a language, and we; use a simple C++ lexer which is easy to understand.; - `Chapter #2: Implementing a Parser and AST <LangImpl02.html>`_ -; With the lexer in place, we can talk abo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:705,Usability,simpl,simple,705,"=============================================; My First Language Frontend with LLVM Tutorial; =============================================. .. toctree::; :hidden:. LangImpl01; LangImpl02; LangImpl03; LangImpl04; LangImpl05; LangImpl06; LangImpl07; LangImpl08; LangImpl09; LangImpl10. **Requirements:** This tutorial assumes you know C++, but no previous; compiler experience is necessary. Welcome to the ""My First Language Frontend with LLVM"" tutorial. Here we; run through the implementation of a simple language, showing; how fun and easy it can be. This tutorial will get you up and running; fast and show a concrete example of something that uses LLVM to generate; code. This tutorial introduces the simple ""Kaleidoscope"" language, building it; iteratively over the course of several chapters, showing how it is built; over time. This lets us cover a range of language design and LLVM-specific; ideas, showing and explaining the code for it all along the way,; and reduces the overwhelming amount of details up front. We strongly; encourage that you *work with this code* - make a copy and hack it up and; experiment. **Warning**: In order to focus on teaching compiler techniques and LLVM; specifically,; this tutorial does *not* show best practices in software engineering; principles. For example, the code uses global variables; pervasively, doesn't use; `visitors <http://en.wikipedia.org/wiki/Visitor_pattern>`_, etc... but; instead keeps things simple and focuses on the topics at hand. This tutorial is structured into chapters covering individual topics,; allowing you to skip ahead as you wish:. - `Chapter #1: Kaleidoscope language and Lexer <LangImpl01.html>`_ -; This shows where we are; going and the basic functionality that we want to build. A lexer; is also the first part of building a parser for a language, and we; use a simple C++ lexer which is easy to understand.; - `Chapter #2: Implementing a Parser and AST <LangImpl02.html>`_ -; With the lexer in place, we can talk abo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:1457,Usability,simpl,simple,1457,"un through the implementation of a simple language, showing; how fun and easy it can be. This tutorial will get you up and running; fast and show a concrete example of something that uses LLVM to generate; code. This tutorial introduces the simple ""Kaleidoscope"" language, building it; iteratively over the course of several chapters, showing how it is built; over time. This lets us cover a range of language design and LLVM-specific; ideas, showing and explaining the code for it all along the way,; and reduces the overwhelming amount of details up front. We strongly; encourage that you *work with this code* - make a copy and hack it up and; experiment. **Warning**: In order to focus on teaching compiler techniques and LLVM; specifically,; this tutorial does *not* show best practices in software engineering; principles. For example, the code uses global variables; pervasively, doesn't use; `visitors <http://en.wikipedia.org/wiki/Visitor_pattern>`_, etc... but; instead keeps things simple and focuses on the topics at hand. This tutorial is structured into chapters covering individual topics,; allowing you to skip ahead as you wish:. - `Chapter #1: Kaleidoscope language and Lexer <LangImpl01.html>`_ -; This shows where we are; going and the basic functionality that we want to build. A lexer; is also the first part of building a parser for a language, and we; use a simple C++ lexer which is easy to understand.; - `Chapter #2: Implementing a Parser and AST <LangImpl02.html>`_ -; With the lexer in place, we can talk about parsing techniques and; basic AST construction. This tutorial describes recursive descent; parsing and operator precedence parsing.; - `Chapter #3: Code generation to LLVM IR <LangImpl03.html>`_ - with; the AST ready, we show how easy it is to generate LLVM IR, and show; a simple way to incorporate LLVM into your project.; - `Chapter #4: Adding JIT and Optimizer Support <LangImpl04.html>`_ -; One great thing about LLVM is its support for JIT compilation, s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:1846,Usability,simpl,simple,1846,"er time. This lets us cover a range of language design and LLVM-specific; ideas, showing and explaining the code for it all along the way,; and reduces the overwhelming amount of details up front. We strongly; encourage that you *work with this code* - make a copy and hack it up and; experiment. **Warning**: In order to focus on teaching compiler techniques and LLVM; specifically,; this tutorial does *not* show best practices in software engineering; principles. For example, the code uses global variables; pervasively, doesn't use; `visitors <http://en.wikipedia.org/wiki/Visitor_pattern>`_, etc... but; instead keeps things simple and focuses on the topics at hand. This tutorial is structured into chapters covering individual topics,; allowing you to skip ahead as you wish:. - `Chapter #1: Kaleidoscope language and Lexer <LangImpl01.html>`_ -; This shows where we are; going and the basic functionality that we want to build. A lexer; is also the first part of building a parser for a language, and we; use a simple C++ lexer which is easy to understand.; - `Chapter #2: Implementing a Parser and AST <LangImpl02.html>`_ -; With the lexer in place, we can talk about parsing techniques and; basic AST construction. This tutorial describes recursive descent; parsing and operator precedence parsing.; - `Chapter #3: Code generation to LLVM IR <LangImpl03.html>`_ - with; the AST ready, we show how easy it is to generate LLVM IR, and show; a simple way to incorporate LLVM into your project.; - `Chapter #4: Adding JIT and Optimizer Support <LangImpl04.html>`_ -; One great thing about LLVM is its support for JIT compilation, so; we'll dive right into it and show you the 3 lines it takes to add JIT; support. Later chapters show how to generate .o files.; - `Chapter #5: Extending the Language: Control Flow <LangImpl05.html>`_ - With; the basic language up and running, we show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst:2278,Usability,simpl,simple,2278,"software engineering; principles. For example, the code uses global variables; pervasively, doesn't use; `visitors <http://en.wikipedia.org/wiki/Visitor_pattern>`_, etc... but; instead keeps things simple and focuses on the topics at hand. This tutorial is structured into chapters covering individual topics,; allowing you to skip ahead as you wish:. - `Chapter #1: Kaleidoscope language and Lexer <LangImpl01.html>`_ -; This shows where we are; going and the basic functionality that we want to build. A lexer; is also the first part of building a parser for a language, and we; use a simple C++ lexer which is easy to understand.; - `Chapter #2: Implementing a Parser and AST <LangImpl02.html>`_ -; With the lexer in place, we can talk about parsing techniques and; basic AST construction. This tutorial describes recursive descent; parsing and operator precedence parsing.; - `Chapter #3: Code generation to LLVM IR <LangImpl03.html>`_ - with; the AST ready, we show how easy it is to generate LLVM IR, and show; a simple way to incorporate LLVM into your project.; - `Chapter #4: Adding JIT and Optimizer Support <LangImpl04.html>`_ -; One great thing about LLVM is its support for JIT compilation, so; we'll dive right into it and show you the 3 lines it takes to add JIT; support. Later chapters show how to generate .o files.; - `Chapter #5: Extending the Language: Control Flow <LangImpl05.html>`_ - With; the basic language up and running, we show how to extend; it with control flow operations ('if' statement and a 'for' loop). This; gives us a chance to talk about SSA construction and control; flow.; - `Chapter #6: Extending the Language: User-defined Operators; <LangImpl06.html>`_ - This chapter extends the language to let; users define arbitrary unary and binary operators - with assignable; precedence! This allows us to build a significant piece of the; ""language"" as library routines.; - `Chapter #7: Extending the Language: Mutable Variables; <LangImpl07.html>`_ - This chapter",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/index.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:5131,Availability,error,error,5131,"+. if (isalpha(LastChar)) { // identifier: [a-zA-Z][a-zA-Z0-9]*; IdentifierStr = LastChar;; while (isalnum((LastChar = getchar()))); IdentifierStr += LastChar;. if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; return tok_identifier;; }. Note that this code sets the '``IdentifierStr``' global whenever it; lexes an identifier. Also, since language keywords are matched by the; same loop, we handle them here inline. Numeric values are similar:. .. code-block:: c++. if (isdigit(LastChar) || LastChar == '.') { // Number: [0-9.]+; std::string NumStr;; do {; NumStr += LastChar;; LastChar = getchar();; } while (isdigit(LastChar) || LastChar == '.');. NumVal = strtod(NumStr.c_str(), 0);; return tok_number;; }. This is all pretty straightforward code for processing input. When; reading a numeric value from input, we use the C ``strtod`` function to; convert it to a numeric value that we store in ``NumVal``. Note that; this isn't doing sufficient error checking: it will incorrectly read; ""1.23.45.67"" and handle it as if you typed in ""1.23"". Feel free to; extend it! Next we handle comments:. .. code-block:: c++. if (LastChar == '#') {; // Comment until end of line.; do; LastChar = getchar();; while (LastChar != EOF && LastChar != '\n' && LastChar != '\r');. if (LastChar != EOF); return gettok();; }. We handle comments by skipping to the end of the line and then return; the next token. Finally, if the input doesn't match one of the above; cases, it is either an operator character like '+' or the end of the; file. These are handled with this code:. .. code-block:: c++. // Check for end of file. Don't eat the EOF.; if (LastChar == EOF); return tok_eof;. // Otherwise, just return the character as its ascii value.; int ThisChar = LastChar;; LastChar = getchar();; return ThisChar;; }. With this, we have the complete lexer for the basic Kaleidoscope; language (the `full code listing <LangImpl02.html#full-code-listing>`_ for the Lexer; is ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:6134,Availability,avail,available,6134,"rStr``' global whenever it; lexes an identifier. Also, since language keywords are matched by the; same loop, we handle them here inline. Numeric values are similar:. .. code-block:: c++. if (isdigit(LastChar) || LastChar == '.') { // Number: [0-9.]+; std::string NumStr;; do {; NumStr += LastChar;; LastChar = getchar();; } while (isdigit(LastChar) || LastChar == '.');. NumVal = strtod(NumStr.c_str(), 0);; return tok_number;; }. This is all pretty straightforward code for processing input. When; reading a numeric value from input, we use the C ``strtod`` function to; convert it to a numeric value that we store in ``NumVal``. Note that; this isn't doing sufficient error checking: it will incorrectly read; ""1.23.45.67"" and handle it as if you typed in ""1.23"". Feel free to; extend it! Next we handle comments:. .. code-block:: c++. if (LastChar == '#') {; // Comment until end of line.; do; LastChar = getchar();; while (LastChar != EOF && LastChar != '\n' && LastChar != '\r');. if (LastChar != EOF); return gettok();; }. We handle comments by skipping to the end of the line and then return; the next token. Finally, if the input doesn't match one of the above; cases, it is either an operator character like '+' or the end of the; file. These are handled with this code:. .. code-block:: c++. // Check for end of file. Don't eat the EOF.; if (LastChar == EOF); return tok_eof;. // Otherwise, just return the character as its ascii value.; int ThisChar = LastChar;; LastChar = getchar();; return ThisChar;; }. With this, we have the complete lexer for the basic Kaleidoscope; language (the `full code listing <LangImpl02.html#full-code-listing>`_ for the Lexer; is available in the `next chapter <LangImpl02.html>`_ of the tutorial).; Next we'll `build a simple parser that uses this to build an Abstract; Syntax Tree <LangImpl02.html>`_. When we have that, we'll include a; driver so that you can use the lexer and parser together. `Next: Implementing a Parser and AST <LangImpl02.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:700,Integrability,interface,interface,700,"=====================================================; Kaleidoscope: Kaleidoscope Introduction and the Lexer; =====================================================. .. contents::; :local:. The Kaleidoscope Language; =========================. This tutorial is illustrated with a toy language called; ""`Kaleidoscope <http://en.wikipedia.org/wiki/Kaleidoscope>`_"" (derived; from ""meaning beautiful, form, and view""). Kaleidoscope is a procedural; language that allows you to define functions, use conditionals, math,; etc. Over the course of the tutorial, we'll extend Kaleidoscope to; support the if/then/else construct, a for loop, user defined operators,; JIT compilation with a simple command line interface, debug info, etc. We want to keep things simple, so the only datatype in Kaleidoscope; is a 64-bit floating point type (aka 'double' in C parlance). As such,; all values are implicitly double precision and the language doesn't; require type declarations. This gives the language a very nice and; simple syntax. For example, the following simple example computes; `Fibonacci numbers: <http://en.wikipedia.org/wiki/Fibonacci_number>`_. ::. # Compute the x'th fibonacci number.; def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2). # This expression will compute the 40th number.; fib(40). We also allow Kaleidoscope to call into standard library functions - the; LLVM JIT makes this really easy. This means that you can use the; 'extern' keyword to define a function before you use it (this is also; useful for mutually recursive functions). For example:. ::. extern sin(arg);; extern cos(arg);; extern atan2(arg1 arg2);. atan2(sin(.4), cos(42)). A more interesting example is included in Chapter 6 where we write a; little Kaleidoscope application that `displays a Mandelbrot; Set <LangImpl06.html#kicking-the-tires>`_ at various levels of magnification. Let's dive into the implementation of this language!. The Lexer; =========. When it comes to implementing a language, the first thing n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:560,Modifiability,extend,extend,560,"=====================================================; Kaleidoscope: Kaleidoscope Introduction and the Lexer; =====================================================. .. contents::; :local:. The Kaleidoscope Language; =========================. This tutorial is illustrated with a toy language called; ""`Kaleidoscope <http://en.wikipedia.org/wiki/Kaleidoscope>`_"" (derived; from ""meaning beautiful, form, and view""). Kaleidoscope is a procedural; language that allows you to define functions, use conditionals, math,; etc. Over the course of the tutorial, we'll extend Kaleidoscope to; support the if/then/else construct, a for loop, user defined operators,; JIT compilation with a simple command line interface, debug info, etc. We want to keep things simple, so the only datatype in Kaleidoscope; is a 64-bit floating point type (aka 'double' in C parlance). As such,; all values are implicitly double precision and the language doesn't; require type declarations. This gives the language a very nice and; simple syntax. For example, the following simple example computes; `Fibonacci numbers: <http://en.wikipedia.org/wiki/Fibonacci_number>`_. ::. # Compute the x'th fibonacci number.; def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2). # This expression will compute the 40th number.; fib(40). We also allow Kaleidoscope to call into standard library functions - the; LLVM JIT makes this really easy. This means that you can use the; 'extern' keyword to define a function before you use it (this is also; useful for mutually recursive functions). For example:. ::. extern sin(arg);; extern cos(arg);; extern atan2(arg1 arg2);. atan2(sin(.4), cos(42)). A more interesting example is included in Chapter 6 where we write a; little Kaleidoscope application that `displays a Mandelbrot; Set <LangImpl06.html#kicking-the-tires>`_ at various levels of magnification. Let's dive into the implementation of this language!. The Lexer; =========. When it comes to implementing a language, the first thing n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:3008,Modifiability,variab,variable,3008,"thing needed is the; ability to process a text file and recognize what it says. The; traditional way to do this is to use a; ""`lexer <http://en.wikipedia.org/wiki/Lexical_analysis>`_"" (aka; 'scanner') to break the input up into ""tokens"". Each token returned by; the lexer includes a token code and potentially some metadata (e.g. the; numeric value of a number). First, we define the possibilities:. .. code-block:: c++. // The lexer returns tokens [0-255] if it is an unknown character, otherwise one; // of these for known things.; enum Token {; tok_eof = -1,. // commands; tok_def = -2,; tok_extern = -3,. // primary; tok_identifier = -4,; tok_number = -5,; };. static std::string IdentifierStr; // Filled in if tok_identifier; static double NumVal; // Filled in if tok_number. Each token returned by our lexer will either be one of the Token enum; values or it will be an 'unknown' character like '+', which is returned; as its ASCII value. If the current token is an identifier, the; ``IdentifierStr`` global variable holds the name of the identifier. If; the current token is a numeric literal (like 1.0), ``NumVal`` holds its; value. We use global variables for simplicity, but this is not the; best choice for a real language implementation :). The actual implementation of the lexer is a single function named; ``gettok``. The ``gettok`` function is called to return the next token; from standard input. Its definition starts as:. .. code-block:: c++. /// gettok - Return the next token from standard input.; static int gettok() {; static int LastChar = ' ';. // Skip any whitespace.; while (isspace(LastChar)); LastChar = getchar();. ``gettok`` works by calling the C ``getchar()`` function to read; characters one at a time from standard input. It eats them as it; recognizes them and stores the last character read, but not processed,; in LastChar. The first thing that it has to do is ignore whitespace; between tokens. This is accomplished with the loop above. The next thing ``gettok``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:3149,Modifiability,variab,variables,3149,"er') to break the input up into ""tokens"". Each token returned by; the lexer includes a token code and potentially some metadata (e.g. the; numeric value of a number). First, we define the possibilities:. .. code-block:: c++. // The lexer returns tokens [0-255] if it is an unknown character, otherwise one; // of these for known things.; enum Token {; tok_eof = -1,. // commands; tok_def = -2,; tok_extern = -3,. // primary; tok_identifier = -4,; tok_number = -5,; };. static std::string IdentifierStr; // Filled in if tok_identifier; static double NumVal; // Filled in if tok_number. Each token returned by our lexer will either be one of the Token enum; values or it will be an 'unknown' character like '+', which is returned; as its ASCII value. If the current token is an identifier, the; ``IdentifierStr`` global variable holds the name of the identifier. If; the current token is a numeric literal (like 1.0), ``NumVal`` holds its; value. We use global variables for simplicity, but this is not the; best choice for a real language implementation :). The actual implementation of the lexer is a single function named; ``gettok``. The ``gettok`` function is called to return the next token; from standard input. Its definition starts as:. .. code-block:: c++. /// gettok - Return the next token from standard input.; static int gettok() {; static int LastChar = ' ';. // Skip any whitespace.; while (isspace(LastChar)); LastChar = getchar();. ``gettok`` works by calling the C ``getchar()`` function to read; characters one at a time from standard input. It eats them as it; recognizes them and stores the last character read, but not processed,; in LastChar. The first thing that it has to do is ignore whitespace; between tokens. This is accomplished with the loop above. The next thing ``gettok`` needs to do is recognize identifiers and; specific keywords like ""def"". Kaleidoscope does this with this simple; loop:. .. code-block:: c++. if (isalpha(LastChar)) { // identifier: [a-zA-Z][a-zA-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:5241,Modifiability,extend,extend,5241," getchar()))); IdentifierStr += LastChar;. if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; return tok_identifier;; }. Note that this code sets the '``IdentifierStr``' global whenever it; lexes an identifier. Also, since language keywords are matched by the; same loop, we handle them here inline. Numeric values are similar:. .. code-block:: c++. if (isdigit(LastChar) || LastChar == '.') { // Number: [0-9.]+; std::string NumStr;; do {; NumStr += LastChar;; LastChar = getchar();; } while (isdigit(LastChar) || LastChar == '.');. NumVal = strtod(NumStr.c_str(), 0);; return tok_number;; }. This is all pretty straightforward code for processing input. When; reading a numeric value from input, we use the C ``strtod`` function to; convert it to a numeric value that we store in ``NumVal``. Note that; this isn't doing sufficient error checking: it will incorrectly read; ""1.23.45.67"" and handle it as if you typed in ""1.23"". Feel free to; extend it! Next we handle comments:. .. code-block:: c++. if (LastChar == '#') {; // Comment until end of line.; do; LastChar = getchar();; while (LastChar != EOF && LastChar != '\n' && LastChar != '\r');. if (LastChar != EOF); return gettok();; }. We handle comments by skipping to the end of the line and then return; the next token. Finally, if the input doesn't match one of the above; cases, it is either an operator character like '+' or the end of the; file. These are handled with this code:. .. code-block:: c++. // Check for end of file. Don't eat the EOF.; if (LastChar == EOF); return tok_eof;. // Otherwise, just return the character as its ascii value.; int ThisChar = LastChar;; LastChar = getchar();; return ThisChar;; }. With this, we have the complete lexer for the basic Kaleidoscope; language (the `full code listing <LangImpl02.html#full-code-listing>`_ for the Lexer; is available in the `next chapter <LangImpl02.html>`_ of the tutorial).; Next we'll `build a simple parser that uses this",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:680,Usability,simpl,simple,680,"=====================================================; Kaleidoscope: Kaleidoscope Introduction and the Lexer; =====================================================. .. contents::; :local:. The Kaleidoscope Language; =========================. This tutorial is illustrated with a toy language called; ""`Kaleidoscope <http://en.wikipedia.org/wiki/Kaleidoscope>`_"" (derived; from ""meaning beautiful, form, and view""). Kaleidoscope is a procedural; language that allows you to define functions, use conditionals, math,; etc. Over the course of the tutorial, we'll extend Kaleidoscope to; support the if/then/else construct, a for loop, user defined operators,; JIT compilation with a simple command line interface, debug info, etc. We want to keep things simple, so the only datatype in Kaleidoscope; is a 64-bit floating point type (aka 'double' in C parlance). As such,; all values are implicitly double precision and the language doesn't; require type declarations. This gives the language a very nice and; simple syntax. For example, the following simple example computes; `Fibonacci numbers: <http://en.wikipedia.org/wiki/Fibonacci_number>`_. ::. # Compute the x'th fibonacci number.; def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2). # This expression will compute the 40th number.; fib(40). We also allow Kaleidoscope to call into standard library functions - the; LLVM JIT makes this really easy. This means that you can use the; 'extern' keyword to define a function before you use it (this is also; useful for mutually recursive functions). For example:. ::. extern sin(arg);; extern cos(arg);; extern atan2(arg1 arg2);. atan2(sin(.4), cos(42)). A more interesting example is included in Chapter 6 where we write a; little Kaleidoscope application that `displays a Mandelbrot; Set <LangImpl06.html#kicking-the-tires>`_ at various levels of magnification. Let's dive into the implementation of this language!. The Lexer; =========. When it comes to implementing a language, the first thing n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:751,Usability,simpl,simple,751,"=====================================================; Kaleidoscope: Kaleidoscope Introduction and the Lexer; =====================================================. .. contents::; :local:. The Kaleidoscope Language; =========================. This tutorial is illustrated with a toy language called; ""`Kaleidoscope <http://en.wikipedia.org/wiki/Kaleidoscope>`_"" (derived; from ""meaning beautiful, form, and view""). Kaleidoscope is a procedural; language that allows you to define functions, use conditionals, math,; etc. Over the course of the tutorial, we'll extend Kaleidoscope to; support the if/then/else construct, a for loop, user defined operators,; JIT compilation with a simple command line interface, debug info, etc. We want to keep things simple, so the only datatype in Kaleidoscope; is a 64-bit floating point type (aka 'double' in C parlance). As such,; all values are implicitly double precision and the language doesn't; require type declarations. This gives the language a very nice and; simple syntax. For example, the following simple example computes; `Fibonacci numbers: <http://en.wikipedia.org/wiki/Fibonacci_number>`_. ::. # Compute the x'th fibonacci number.; def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2). # This expression will compute the 40th number.; fib(40). We also allow Kaleidoscope to call into standard library functions - the; LLVM JIT makes this really easy. This means that you can use the; 'extern' keyword to define a function before you use it (this is also; useful for mutually recursive functions). For example:. ::. extern sin(arg);; extern cos(arg);; extern atan2(arg1 arg2);. atan2(sin(.4), cos(42)). A more interesting example is included in Chapter 6 where we write a; little Kaleidoscope application that `displays a Mandelbrot; Set <LangImpl06.html#kicking-the-tires>`_ at various levels of magnification. Let's dive into the implementation of this language!. The Lexer; =========. When it comes to implementing a language, the first thing n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:1006,Usability,simpl,simple,1006,"=====================================================; Kaleidoscope: Kaleidoscope Introduction and the Lexer; =====================================================. .. contents::; :local:. The Kaleidoscope Language; =========================. This tutorial is illustrated with a toy language called; ""`Kaleidoscope <http://en.wikipedia.org/wiki/Kaleidoscope>`_"" (derived; from ""meaning beautiful, form, and view""). Kaleidoscope is a procedural; language that allows you to define functions, use conditionals, math,; etc. Over the course of the tutorial, we'll extend Kaleidoscope to; support the if/then/else construct, a for loop, user defined operators,; JIT compilation with a simple command line interface, debug info, etc. We want to keep things simple, so the only datatype in Kaleidoscope; is a 64-bit floating point type (aka 'double' in C parlance). As such,; all values are implicitly double precision and the language doesn't; require type declarations. This gives the language a very nice and; simple syntax. For example, the following simple example computes; `Fibonacci numbers: <http://en.wikipedia.org/wiki/Fibonacci_number>`_. ::. # Compute the x'th fibonacci number.; def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2). # This expression will compute the 40th number.; fib(40). We also allow Kaleidoscope to call into standard library functions - the; LLVM JIT makes this really easy. This means that you can use the; 'extern' keyword to define a function before you use it (this is also; useful for mutually recursive functions). For example:. ::. extern sin(arg);; extern cos(arg);; extern atan2(arg1 arg2);. atan2(sin(.4), cos(42)). A more interesting example is included in Chapter 6 where we write a; little Kaleidoscope application that `displays a Mandelbrot; Set <LangImpl06.html#kicking-the-tires>`_ at various levels of magnification. Let's dive into the implementation of this language!. The Lexer; =========. When it comes to implementing a language, the first thing n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:1048,Usability,simpl,simple,1048,"scope: Kaleidoscope Introduction and the Lexer; =====================================================. .. contents::; :local:. The Kaleidoscope Language; =========================. This tutorial is illustrated with a toy language called; ""`Kaleidoscope <http://en.wikipedia.org/wiki/Kaleidoscope>`_"" (derived; from ""meaning beautiful, form, and view""). Kaleidoscope is a procedural; language that allows you to define functions, use conditionals, math,; etc. Over the course of the tutorial, we'll extend Kaleidoscope to; support the if/then/else construct, a for loop, user defined operators,; JIT compilation with a simple command line interface, debug info, etc. We want to keep things simple, so the only datatype in Kaleidoscope; is a 64-bit floating point type (aka 'double' in C parlance). As such,; all values are implicitly double precision and the language doesn't; require type declarations. This gives the language a very nice and; simple syntax. For example, the following simple example computes; `Fibonacci numbers: <http://en.wikipedia.org/wiki/Fibonacci_number>`_. ::. # Compute the x'th fibonacci number.; def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2). # This expression will compute the 40th number.; fib(40). We also allow Kaleidoscope to call into standard library functions - the; LLVM JIT makes this really easy. This means that you can use the; 'extern' keyword to define a function before you use it (this is also; useful for mutually recursive functions). For example:. ::. extern sin(arg);; extern cos(arg);; extern atan2(arg1 arg2);. atan2(sin(.4), cos(42)). A more interesting example is included in Chapter 6 where we write a; little Kaleidoscope application that `displays a Mandelbrot; Set <LangImpl06.html#kicking-the-tires>`_ at various levels of magnification. Let's dive into the implementation of this language!. The Lexer; =========. When it comes to implementing a language, the first thing needed is the; ability to process a text file and recognize wh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:3163,Usability,simpl,simplicity,3163,"er') to break the input up into ""tokens"". Each token returned by; the lexer includes a token code and potentially some metadata (e.g. the; numeric value of a number). First, we define the possibilities:. .. code-block:: c++. // The lexer returns tokens [0-255] if it is an unknown character, otherwise one; // of these for known things.; enum Token {; tok_eof = -1,. // commands; tok_def = -2,; tok_extern = -3,. // primary; tok_identifier = -4,; tok_number = -5,; };. static std::string IdentifierStr; // Filled in if tok_identifier; static double NumVal; // Filled in if tok_number. Each token returned by our lexer will either be one of the Token enum; values or it will be an 'unknown' character like '+', which is returned; as its ASCII value. If the current token is an identifier, the; ``IdentifierStr`` global variable holds the name of the identifier. If; the current token is a numeric literal (like 1.0), ``NumVal`` holds its; value. We use global variables for simplicity, but this is not the; best choice for a real language implementation :). The actual implementation of the lexer is a single function named; ``gettok``. The ``gettok`` function is called to return the next token; from standard input. Its definition starts as:. .. code-block:: c++. /// gettok - Return the next token from standard input.; static int gettok() {; static int LastChar = ' ';. // Skip any whitespace.; while (isspace(LastChar)); LastChar = getchar();. ``gettok`` works by calling the C ``getchar()`` function to read; characters one at a time from standard input. It eats them as it; recognizes them and stores the last character read, but not processed,; in LastChar. The first thing that it has to do is ignore whitespace; between tokens. This is accomplished with the loop above. The next thing ``gettok`` needs to do is recognize identifiers and; specific keywords like ""def"". Kaleidoscope does this with this simple; loop:. .. code-block:: c++. if (isalpha(LastChar)) { // identifier: [a-zA-Z][a-zA-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:4100,Usability,simpl,simple,4100,"ral (like 1.0), ``NumVal`` holds its; value. We use global variables for simplicity, but this is not the; best choice for a real language implementation :). The actual implementation of the lexer is a single function named; ``gettok``. The ``gettok`` function is called to return the next token; from standard input. Its definition starts as:. .. code-block:: c++. /// gettok - Return the next token from standard input.; static int gettok() {; static int LastChar = ' ';. // Skip any whitespace.; while (isspace(LastChar)); LastChar = getchar();. ``gettok`` works by calling the C ``getchar()`` function to read; characters one at a time from standard input. It eats them as it; recognizes them and stores the last character read, but not processed,; in LastChar. The first thing that it has to do is ignore whitespace; between tokens. This is accomplished with the loop above. The next thing ``gettok`` needs to do is recognize identifiers and; specific keywords like ""def"". Kaleidoscope does this with this simple; loop:. .. code-block:: c++. if (isalpha(LastChar)) { // identifier: [a-zA-Z][a-zA-Z0-9]*; IdentifierStr = LastChar;; while (isalnum((LastChar = getchar()))); IdentifierStr += LastChar;. if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; return tok_identifier;; }. Note that this code sets the '``IdentifierStr``' global whenever it; lexes an identifier. Also, since language keywords are matched by the; same loop, we handle them here inline. Numeric values are similar:. .. code-block:: c++. if (isdigit(LastChar) || LastChar == '.') { // Number: [0-9.]+; std::string NumStr;; do {; NumStr += LastChar;; LastChar = getchar();; } while (isdigit(LastChar) || LastChar == '.');. NumVal = strtod(NumStr.c_str(), 0);; return tok_number;; }. This is all pretty straightforward code for processing input. When; reading a numeric value from input, we use the C ``strtod`` function to; convert it to a numeric value that we store in ``NumVal``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst:6224,Usability,simpl,simple,6224,"rStr``' global whenever it; lexes an identifier. Also, since language keywords are matched by the; same loop, we handle them here inline. Numeric values are similar:. .. code-block:: c++. if (isdigit(LastChar) || LastChar == '.') { // Number: [0-9.]+; std::string NumStr;; do {; NumStr += LastChar;; LastChar = getchar();; } while (isdigit(LastChar) || LastChar == '.');. NumVal = strtod(NumStr.c_str(), 0);; return tok_number;; }. This is all pretty straightforward code for processing input. When; reading a numeric value from input, we use the C ``strtod`` function to; convert it to a numeric value that we store in ``NumVal``. Note that; this isn't doing sufficient error checking: it will incorrectly read; ""1.23.45.67"" and handle it as if you typed in ""1.23"". Feel free to; extend it! Next we handle comments:. .. code-block:: c++. if (LastChar == '#') {; // Comment until end of line.; do; LastChar = getchar();; while (LastChar != EOF && LastChar != '\n' && LastChar != '\r');. if (LastChar != EOF); return gettok();; }. We handle comments by skipping to the end of the line and then return; the next token. Finally, if the input doesn't match one of the above; cases, it is either an operator character like '+' or the end of the; file. These are handled with this code:. .. code-block:: c++. // Check for end of file. Don't eat the EOF.; if (LastChar == EOF); return tok_eof;. // Otherwise, just return the character as its ascii value.; int ThisChar = LastChar;; LastChar = getchar();; return ThisChar;; }. With this, we have the complete lexer for the basic Kaleidoscope; language (the `full code listing <LangImpl02.html#full-code-listing>`_ for the Lexer; is available in the `next chapter <LangImpl02.html>`_ of the tutorial).; Next we'll `build a simple parser that uses this to build an Abstract; Syntax Tree <LangImpl02.html>`_. When we have that, we'll include a; driver so that you can use the lexer and parser together. `Next: Implementing a Parser and AST <LangImpl02.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl01.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6391,Availability,error,error,6391,"ng like ""x+y""; (which is returned as three tokens by the lexer) into an AST that could; be generated with calls like this:. .. code-block:: c++. auto LHS = std::make_unique<VariableExprAST>(""x"");; auto RHS = std::make_unique<VariableExprAST>(""y"");; auto Result = std::make_unique<BinaryExprAST>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6705,Availability,error,errors,6705,"T>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the curre",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6717,Availability,error,error,6717,". .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. The",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6723,Availability,recover,recovery,6723,". .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. The",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6886,Availability,error,errors,6886,"s looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. There are some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:8827,Availability,error,error,8827,"some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond to the; production and returns the lexer buffer with the next token (which is; not part of the grammar production) ready to go. This is a fairly; standard way to go for recursive descent parsers. For a better example,; the parenthesis operator is defined like this:. .. code-block:: c++. /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. ge",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:8842,Availability,error,errors,8842,"rrespond to the; production and returns the lexer buffer with the next token (which is; not part of the grammar production) ready to go. This is a fairly; standard way to go for recursive descent parsers. For a better example,; the parenthesis operator is defined like this:. .. code-block:: c++. /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableE",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:8952,Availability,error,error,8952,"rrespond to the; production and returns the lexer buffer with the next token (which is; not part of the grammar production) ready to go. This is a fairly; standard way to go for recursive descent parsers. For a better example,; the parenthesis operator is defined like this:. .. code-block:: c++. /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableE",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:10519,Availability,error,error,10519," simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST`` or; ``CallExprAST`` node as appropriate. Now that we have all of our simple expression-parsing logic in place, we; can define a helper function to wrap it together into one entry point.; We call this class of expressions ""primary"" expressions, for reasons; that will become more clear `later in the; tutorial <LangImpl06.html#user-defined-unary-operators>`_. In order to parse an arbitrary; primary expression, we need to determine what sort of expression it is:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:14106,Availability,down,down,14106,"rec = BinopPrecedence[CurTok];; if (TokPrec <= 0) return -1;; return TokPrec;; }. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest.; ...; }. For the basic form of Kaleidoscope, we will only support 4 binary; operators (this can obviously be extended by you, our brave and intrepid; reader). The ``GetTokPrecedence`` function returns the precedence for; the current token, or -1 if the token is not a binary operator. Having a; map makes it easy to add new operators and makes it clear that the; algorithm doesn't depend on the specific operators involved, but it; would be easy enough to eliminate the map and do the comparisons in the; ``GetTokPrecedence`` function. (Or just use a fixed-size array). With the helper above defined, we can now start parsing binary; expressions. The basic idea of operator precedence parsing is to break; down an expression with potentially ambiguous binary operators into; pieces. Consider, for example, the expression ""a+b+(c+d)\*e\*f+g"".; Operator precedence parsing considers this as a stream of primary; expressions separated by binary operators. As such, it will first parse; the leading primary expression ""a"", then it will see the pairs [+, b]; [+, (c+d)] [\*, e] [\*, f] and [+, g]. Note that because parentheses are; primary expressions, the binary expression parser doesn't need to worry; about nested subexpressions like (c+d) at all. To start, an expression is a primary expression potentially followed by; a sequence of [binop,primaryexpr] pairs:. .. code-block:: c++. /// expression; /// ::= primary binoprhs; ///; static std::unique_ptr<ExprAST> ParseExpression() {; auto LHS = ParsePrimary();; if (!LHS); return nullptr;. return ParseBinOpRHS(0, std::move(LHS));; }. ``ParseBinOpRHS`` is the function that parses the sequence of pairs for; us. It takes a precedence and a pointer to an expression",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:3899,Deployability,install,installment,3899,"que_ptr<ExprAST> LHS,; std::unique_ptr<ExprAST> RHS); : Op(Op), LHS(std::move(LHS)), RHS(std::move(RHS)) {}; };. /// CallExprAST - Expression class for function calls.; class CallExprAST : public ExprAST {; std::string Callee;; std::vector<std::unique_ptr<ExprAST>> Args;. public:; CallExprAST(const std::string &Callee,; std::vector<std::unique_ptr<ExprAST>> Args); : Callee(Callee), Args(std::move(Args)) {}; };. This is all (intentionally) rather straight-forward: variables capture; the variable name, binary operators capture their opcode (e.g. '+'), and; calls capture a function name as well as a list of any argument; expressions. One thing that is nice about our AST is that it captures; the language features without talking about the syntax of the language.; Note that there is no discussion about precedence of binary operators,; lexical structure, etc. For our basic language, these are all of the expression nodes we'll; define. Because it doesn't have conditional control flow, it isn't; Turing-complete; we'll fix that in a later installment. The two things; we need next are a way to talk about the interface to a function, and a; way to talk about functions themselves:. .. code-block:: c++. /// PrototypeAST - This class represents the ""prototype"" for a function,; /// which captures its name, and its argument names (thus implicitly the number; /// of arguments the function takes).; class PrototypeAST {; std::string Name;; std::vector<std::string> Args;. public:; PrototypeAST(const std::string &Name, std::vector<std::string> Args); : Name(Name), Args(std::move(Args)) {}. const std::string &getName() const { return Name; }; };. /// FunctionAST - This class represents a function definition itself.; class FunctionAST {; std::unique_ptr<PrototypeAST> Proto;; std::unique_ptr<ExprAST> Body;. public:; FunctionAST(std::unique_ptr<PrototypeAST> Proto,; std::unique_ptr<ExprAST> Body); : Proto(std::move(Proto)), Body(std::move(Body)) {}; };. In Kaleidoscope, functions are typed ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:5974,Deployability,update,updates,5974,"ch argument doesn't need to be stored anywhere. In a more; aggressive and realistic language, the ""ExprAST"" class would probably; have a type field. With this scaffolding, we can now talk about parsing expressions and; function bodies in Kaleidoscope. Parser Basics; =============. Now that we have an AST to build, we need to define the parser code to; build it. The idea here is that we want to parse something like ""x+y""; (which is returned as three tokens by the lexer) into an AST that could; be generated with calls like this:. .. code-block:: c++. auto LHS = std::make_unique<VariableExprAST>(""x"");; auto RHS = std::make_unique<VariableExprAST>(""y"");; auto Result = std::make_unique<BinaryExprAST>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:25106,Deployability,install,installment,25106,"lons.; getNextToken();; break;; case tok_def:; HandleDefinition();; break;; case tok_extern:; HandleExtern();; break;; default:; HandleTopLevelExpression();; break;; }; }; }. The most interesting part of this is that we ignore top-level; semicolons. Why is this, you ask? The basic reason is that if you type; ""4 + 5"" at the command line, the parser doesn't know whether that is the; end of what you will type or not. For example, on the next line you; could type ""def foo..."" in which case 4+5 is the end of a top-level; expression. Alternatively you could type ""\* 6"", which would continue; the expression. Having top-level semicolons allows you to type ""4+5;"",; and the parser will know you are done. Conclusions; ===========. With just under 400 lines of commented code (240 lines of non-comment,; non-blank code), we fully defined our minimal language, including a; lexer, parser, and AST builder. With this done, the executable will; validate Kaleidoscope code and tell us if it is grammatically invalid.; For example, here is a sample interaction:. .. code-block:: bash. $ ./a.out; ready> def foo(x y) x+foo(y, 4.0);; Parsed a function definition.; ready> def foo(x y) x+y y;; Parsed a function definition.; Parsed a top-level expr; ready> def foo(x y) x+y );; Parsed a function definition.; Error: unknown token when expecting an expression; ready> extern sin(a);; ready> Parsed an extern; ready> ^D; $. There is a lot of room for extension here. You can define new AST nodes,; extend the language in many ways, etc. In the `next; installment <LangImpl03.html>`_, we will describe how to generate LLVM; Intermediate Representation (IR) from the AST. Full Code Listing; =================. Here is the complete code listing for our running example. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp; # Run; ./a.out. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter2/toy.cpp; :language: c++. `Next: Implementing Code Generation to LLVM IR <LangImpl03.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:9146,Energy Efficiency,power,powerful,9146," /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Exp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:12509,Energy Efficiency,efficient,efficient,12509,"en expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; }; }. Now that you see the definition of this function, it is more obvious why; we can assume the state of CurTok in the various functions. This uses; look-ahead to determine which sort of expression is being inspected, and; then parses it with a function call. Now that basic expressions are handled, we need to handle binary; expressions. They are a bit more complex. Binary Expression Parsing; =========================. Binary expressions are significantly harder to parse because they are; often ambiguous. For example, when given the string ""x+y\*z"", the parser; can choose to parse it as either ""(x+y)\*z"" or ""x+(y\*z)"". With common; definitions from mathematics, we expect the later parse, because ""\*""; (multiplication) has higher *precedence* than ""+"" (addition). There are many ways to handle this, but an elegant and efficient way is; to use `Operator-Precedence; Parsing <http://en.wikipedia.org/wiki/Operator-precedence_parser>`_.; This parsing technique uses the precedence of binary operators to guide; recursion. To start with, we need a table of precedences:. .. code-block:: c++. /// BinopPrecedence - This holds the precedence for each binary operator that is; /// defined.; static std::map<char, int> BinopPrecedence;. /// GetTokPrecedence - Get the precedence of the pending binary operator token.; static int GetTokPrecedence() {; if (!isascii(CurTok)); return -1;. // Make sure it's a declared binop.; int TokPrec = BinopPrecedence[CurTok];; if (TokPrec <= 0) return -1;; return TokPrec;; }. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest.; ...; }. For the basic form of Kaleidoscope, we will only support 4 binary; operators (this can obviously be exten",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:3969,Integrability,interface,interface,3969,"prAST - Expression class for function calls.; class CallExprAST : public ExprAST {; std::string Callee;; std::vector<std::unique_ptr<ExprAST>> Args;. public:; CallExprAST(const std::string &Callee,; std::vector<std::unique_ptr<ExprAST>> Args); : Callee(Callee), Args(std::move(Args)) {}; };. This is all (intentionally) rather straight-forward: variables capture; the variable name, binary operators capture their opcode (e.g. '+'), and; calls capture a function name as well as a list of any argument; expressions. One thing that is nice about our AST is that it captures; the language features without talking about the syntax of the language.; Note that there is no discussion about precedence of binary operators,; lexical structure, etc. For our basic language, these are all of the expression nodes we'll; define. Because it doesn't have conditional control flow, it isn't; Turing-complete; we'll fix that in a later installment. The two things; we need next are a way to talk about the interface to a function, and a; way to talk about functions themselves:. .. code-block:: c++. /// PrototypeAST - This class represents the ""prototype"" for a function,; /// which captures its name, and its argument names (thus implicitly the number; /// of arguments the function takes).; class PrototypeAST {; std::string Name;; std::vector<std::string> Args;. public:; PrototypeAST(const std::string &Name, std::vector<std::string> Args); : Name(Name), Args(std::move(Args)) {}. const std::string &getName() const { return Name; }; };. /// FunctionAST - This class represents a function definition itself.; class FunctionAST {; std::unique_ptr<PrototypeAST> Proto;; std::unique_ptr<ExprAST> Body;. public:; FunctionAST(std::unique_ptr<PrototypeAST> Proto,; std::unique_ptr<ExprAST> Body); : Proto(std::move(Proto)), Body(std::move(Body)) {}; };. In Kaleidoscope, functions are typed with just a count of their; arguments. Since all values are double precision floating point, the; type of each argument doe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:5769,Integrability,rout,routines,5769,"<ExprAST> Body); : Proto(std::move(Proto)), Body(std::move(Body)) {}; };. In Kaleidoscope, functions are typed with just a count of their; arguments. Since all values are double precision floating point, the; type of each argument doesn't need to be stored anywhere. In a more; aggressive and realistic language, the ""ExprAST"" class would probably; have a type field. With this scaffolding, we can now talk about parsing expressions and; function bodies in Kaleidoscope. Parser Basics; =============. Now that we have an AST to build, we need to define the parser code to; build it. The idea here is that we want to parse something like ""x+y""; (which is returned as three tokens by the lexer) into an AST that could; be generated with calls like this:. .. code-block:: c++. auto LHS = std::make_unique<VariableExprAST>(""x"");; auto RHS = std::make_unique<VariableExprAST>(""y"");; auto Result = std::make_unique<BinaryExprAST>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our par",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6633,Integrability,rout,routines,6633,"T>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the curre",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6660,Integrability,rout,routines,6660,"T>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the curre",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6852,Integrability,rout,routines,6852,"s looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. There are some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6896,Integrability,rout,routines,6896,"s looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. There are some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:7547,Integrability,rout,routine,7547,";; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. There are some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond to the; production and returns the lexer buffer with the next token (which is; not part of the grammar production) ready to go. This is a fairly; standard way to go for recursive descent parsers. For a better example,; the parenthesis operator is defined like this:. .. code-block:: c++. /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:7857,Integrability,rout,routine,7857,"eturn types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. There are some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond to the; production and returns the lexer buffer with the next token (which is; not part of the grammar production) ready to go. This is a fairly; standard way to go for recursive descent parsers. For a better example,; the parenthesis operator is defined like this:. .. code-block:: c++. /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our pa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:8572,Integrability,rout,routines,8572,"very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. There are some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond to the; production and returns the lexer buffer with the next token (which is; not part of the grammar production) ready to go. This is a fairly; standard way to go for recursive descent parsers. For a better example,; the parenthesis operator is defined like this:. .. code-block:: c++. /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable re",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:10359,Integrability,rout,routine,10359,"rentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST`` or; ``CallExprAST`` node as appropriate. Now that we have all of our simple expression-parsing logic in place, we; can define a helper function to wrap it together into one entry point.; We call this class of expressions ""primary"" expressions, for reasons; that will become more clear `later in the; tutorial <LangImpl06.html#user-defined-unary-operators>`_. In order to parse an arbitrary; primary expression, we need to determine what sort of expression it is:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:10403,Integrability,rout,routines,10403,"rentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST`` or; ``CallExprAST`` node as appropriate. Now that we have all of our simple expression-parsing logic in place, we; can define a helper function to wrap it together into one entry point.; We call this class of expressions ""primary"" expressions, for reasons; that will become more clear `later in the; tutorial <LangImpl06.html#user-defined-unary-operators>`_. In order to parse an arbitrary; primary expression, we need to determine what sort of expression it is:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:10988,Integrability,wrap,wrap,10988,"::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST`` or; ``CallExprAST`` node as appropriate. Now that we have all of our simple expression-parsing logic in place, we; can define a helper function to wrap it together into one entry point.; We call this class of expressions ""primary"" expressions, for reasons; that will become more clear `later in the; tutorial <LangImpl06.html#user-defined-unary-operators>`_. In order to parse an arbitrary; primary expression, we need to determine what sort of expression it is:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; }; }. Now that you see the definition of this function, it is more obvious why; we can assume the state of CurTok in the various functions. This uses; look-ahead to determine which sort of expression is being inspected, and; then parses it with a function call. Now that bas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:13781,Integrability,depend,depend,13781,"ds the precedence for each binary operator that is; /// defined.; static std::map<char, int> BinopPrecedence;. /// GetTokPrecedence - Get the precedence of the pending binary operator token.; static int GetTokPrecedence() {; if (!isascii(CurTok)); return -1;. // Make sure it's a declared binop.; int TokPrec = BinopPrecedence[CurTok];; if (TokPrec <= 0) return -1;; return TokPrec;; }. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest.; ...; }. For the basic form of Kaleidoscope, we will only support 4 binary; operators (this can obviously be extended by you, our brave and intrepid; reader). The ``GetTokPrecedence`` function returns the precedence for; the current token, or -1 if the token is not a binary operator. Having a; map makes it easy to add new operators and makes it clear that the; algorithm doesn't depend on the specific operators involved, but it; would be easy enough to eliminate the map and do the comparisons in the; ``GetTokPrecedence`` function. (Or just use a fixed-size array). With the helper above defined, we can now start parsing binary; expressions. The basic idea of operator precedence parsing is to break; down an expression with potentially ambiguous binary operators into; pieces. Consider, for example, the expression ""a+b+(c+d)\*e\*f+g"".; Operator precedence parsing considers this as a stream of primary; expressions separated by binary operators. As such, it will first parse; the leading primary expression ""a"", then it will see the pairs [+, b]; [+, (c+d)] [\*, e] [\*, f] and [+, g]. Note that because parentheses are; primary expressions, the binary expression parser doesn't need to worry; about nested subexpressions like (c+d) at all. To start, an expression is a primary expression potentially followed by; a sequence of [binop,primaryexpr] pairs:. .. code-block:: c++. /// expression; /// ::= pr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:20361,Integrability,wrap,wraps,20361,"p of the while loop.; }. At this point, we know that the binary operator to the RHS of our; primary has higher precedence than the binop we are currently parsing.; As such, we know that any sequence of pairs whose operators are all; higher precedence than ""+"" should be parsed together and returned as; ""RHS"". To do this, we recursively invoke the ``ParseBinOpRHS`` function; specifying ""TokPrec+1"" as the minimum precedence required for it to; continue. In our example above, this will cause it to return the AST; node for ""(c+d)\*e\*f"" as RHS, which is then set as the RHS of the '+'; expression. Finally, on the next iteration of the while loop, the ""+g"" piece is; parsed and added to the AST. With this little bit of code (14; non-trivial lines), we correctly handle fully general binary expression; parsing in a very elegant way. This was a whirlwind tour of this code,; and it is somewhat subtle. I recommend running through it with a few; tough examples to see how it works. This wraps up handling of expressions. At this point, we can point the; parser at an arbitrary token stream and build an expression from it,; stopping at the first token that is not part of the expression. Next up; we need to handle function definitions, etc. Parsing the Rest; ================. The next thing missing is handling of function prototypes. In; Kaleidoscope, these are used both for 'extern' function declarations as; well as function body definitions. The code to do this is; straight-forward and not very interesting (once you've survived; expressions):. .. code-block:: c++. /// prototype; /// ::= id '(' id* ')'; static std::unique_ptr<PrototypeAST> ParsePrototype() {; if (CurTok != tok_identifier); return LogErrorP(""Expected function name in prototype"");. std::string FnName = IdentifierStr;; getNextToken();. if (CurTok != '('); return LogErrorP(""Expected '(' in prototype"");. // Read the list of argument names.; std::vector<std::string> ArgNames;; while (getNextToken() == tok_identifier); ArgNa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:2054,Modifiability,variab,variable,2054,"s; talk about the output of the parser: the Abstract Syntax Tree. The Abstract Syntax Tree (AST); ==============================. The AST for a program captures its behavior in such a way that it is; easy for later stages of the compiler (e.g. code generation) to; interpret. We basically want one object for each construct in the; language, and the AST should closely model the language. In; Kaleidoscope, we have expressions, a prototype, and a function object.; We'll start with expressions first:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; };. The code above shows the definition of the base ExprAST class and one; subclass which we use for numeric literals. The important thing to note; about this code is that the NumberExprAST class captures the numeric; value of the literal as an instance variable. This allows later phases; of the compiler to know what the stored numeric value is. Right now we only create the AST, so there are no useful accessor; methods on them. It would be very easy to add a virtual method to pretty; print the code, for example. Here are the other expression AST node; definitions that we'll use in the basic form of the Kaleidoscope; language:. .. code-block:: c++. /// VariableExprAST - Expression class for referencing a variable, like ""a"".; class VariableExprAST : public ExprAST {; std::string Name;. public:; VariableExprAST(const std::string &Name) : Name(Name) {}; };. /// BinaryExprAST - Expression class for a binary operator.; class BinaryExprAST : public ExprAST {; char Op;; std::unique_ptr<ExprAST> LHS, RHS;. public:; BinaryExprAST(char Op, std::unique_ptr<ExprAST> LHS,; std::unique_ptr<ExprAST> RHS); : Op(Op), LHS(std::move(LHS)), RHS(std::move(RHS)) {}; };. /// CallExprAST - Express",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:2513,Modifiability,variab,variable,2513,". code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; };. The code above shows the definition of the base ExprAST class and one; subclass which we use for numeric literals. The important thing to note; about this code is that the NumberExprAST class captures the numeric; value of the literal as an instance variable. This allows later phases; of the compiler to know what the stored numeric value is. Right now we only create the AST, so there are no useful accessor; methods on them. It would be very easy to add a virtual method to pretty; print the code, for example. Here are the other expression AST node; definitions that we'll use in the basic form of the Kaleidoscope; language:. .. code-block:: c++. /// VariableExprAST - Expression class for referencing a variable, like ""a"".; class VariableExprAST : public ExprAST {; std::string Name;. public:; VariableExprAST(const std::string &Name) : Name(Name) {}; };. /// BinaryExprAST - Expression class for a binary operator.; class BinaryExprAST : public ExprAST {; char Op;; std::unique_ptr<ExprAST> LHS, RHS;. public:; BinaryExprAST(char Op, std::unique_ptr<ExprAST> LHS,; std::unique_ptr<ExprAST> RHS); : Op(Op), LHS(std::move(LHS)), RHS(std::move(RHS)) {}; };. /// CallExprAST - Expression class for function calls.; class CallExprAST : public ExprAST {; std::string Callee;; std::vector<std::unique_ptr<ExprAST>> Args;. public:; CallExprAST(const std::string &Callee,; std::vector<std::unique_ptr<ExprAST>> Args); : Callee(Callee), Args(std::move(Args)) {}; };. This is all (intentionally) rather straight-forward: variables capture; the variable name, binary operators capture their opcode (e.g. '+'), and; calls capture a function name as well as a list of any argument; expressions. On",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:3321,Modifiability,variab,variables,3321,"her expression AST node; definitions that we'll use in the basic form of the Kaleidoscope; language:. .. code-block:: c++. /// VariableExprAST - Expression class for referencing a variable, like ""a"".; class VariableExprAST : public ExprAST {; std::string Name;. public:; VariableExprAST(const std::string &Name) : Name(Name) {}; };. /// BinaryExprAST - Expression class for a binary operator.; class BinaryExprAST : public ExprAST {; char Op;; std::unique_ptr<ExprAST> LHS, RHS;. public:; BinaryExprAST(char Op, std::unique_ptr<ExprAST> LHS,; std::unique_ptr<ExprAST> RHS); : Op(Op), LHS(std::move(LHS)), RHS(std::move(RHS)) {}; };. /// CallExprAST - Expression class for function calls.; class CallExprAST : public ExprAST {; std::string Callee;; std::vector<std::unique_ptr<ExprAST>> Args;. public:; CallExprAST(const std::string &Callee,; std::vector<std::unique_ptr<ExprAST>> Args); : Callee(Callee), Args(std::move(Args)) {}; };. This is all (intentionally) rather straight-forward: variables capture; the variable name, binary operators capture their opcode (e.g. '+'), and; calls capture a function name as well as a list of any argument; expressions. One thing that is nice about our AST is that it captures; the language features without talking about the syntax of the language.; Note that there is no discussion about precedence of binary operators,; lexical structure, etc. For our basic language, these are all of the expression nodes we'll; define. Because it doesn't have conditional control flow, it isn't; Turing-complete; we'll fix that in a later installment. The two things; we need next are a way to talk about the interface to a function, and a; way to talk about functions themselves:. .. code-block:: c++. /// PrototypeAST - This class represents the ""prototype"" for a function,; /// which captures its name, and its argument names (thus implicitly the number; /// of arguments the function takes).; class PrototypeAST {; std::string Name;; std::vector<std::string> Args;. publ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:3344,Modifiability,variab,variable,3344,"her expression AST node; definitions that we'll use in the basic form of the Kaleidoscope; language:. .. code-block:: c++. /// VariableExprAST - Expression class for referencing a variable, like ""a"".; class VariableExprAST : public ExprAST {; std::string Name;. public:; VariableExprAST(const std::string &Name) : Name(Name) {}; };. /// BinaryExprAST - Expression class for a binary operator.; class BinaryExprAST : public ExprAST {; char Op;; std::unique_ptr<ExprAST> LHS, RHS;. public:; BinaryExprAST(char Op, std::unique_ptr<ExprAST> LHS,; std::unique_ptr<ExprAST> RHS); : Op(Op), LHS(std::move(LHS)), RHS(std::move(RHS)) {}; };. /// CallExprAST - Expression class for function calls.; class CallExprAST : public ExprAST {; std::string Callee;; std::vector<std::unique_ptr<ExprAST>> Args;. public:; CallExprAST(const std::string &Callee,; std::vector<std::unique_ptr<ExprAST>> Args); : Callee(Callee), Args(std::move(Args)) {}; };. This is all (intentionally) rather straight-forward: variables capture; the variable name, binary operators capture their opcode (e.g. '+'), and; calls capture a function name as well as a list of any argument; expressions. One thing that is nice about our AST is that it captures; the language features without talking about the syntax of the language.; Note that there is no discussion about precedence of binary operators,; lexical structure, etc. For our basic language, these are all of the expression nodes we'll; define. Because it doesn't have conditional control flow, it isn't; Turing-complete; we'll fix that in a later installment. The two things; we need next are a way to talk about the interface to a function, and a; way to talk about functions themselves:. .. code-block:: c++. /// PrototypeAST - This class represents the ""prototype"" for a function,; /// which captures its name, and its argument names (thus implicitly the number; /// of arguments the function takes).; class PrototypeAST {; std::string Name;; std::vector<std::string> Args;. publ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:9547,Modifiability,variab,variable,9547,"ws how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interes",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:9848,Modifiability,variab,variable,9848,"se errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST``",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:10653,Modifiability,variab,variable,10653,"erexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST`` or; ``CallExprAST`` node as appropriate. Now that we have all of our simple expression-parsing logic in place, we; can define a helper function to wrap it together into one entry point.; We call this class of expressions ""primary"" expressions, for reasons; that will become more clear `later in the; tutorial <LangImpl06.html#user-defined-unary-operators>`_. In order to parse an arbitrary; primary expression, we need to determine what sort of expression it is:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:13509,Modifiability,extend,extended,13509," this, but an elegant and efficient way is; to use `Operator-Precedence; Parsing <http://en.wikipedia.org/wiki/Operator-precedence_parser>`_.; This parsing technique uses the precedence of binary operators to guide; recursion. To start with, we need a table of precedences:. .. code-block:: c++. /// BinopPrecedence - This holds the precedence for each binary operator that is; /// defined.; static std::map<char, int> BinopPrecedence;. /// GetTokPrecedence - Get the precedence of the pending binary operator token.; static int GetTokPrecedence() {; if (!isascii(CurTok)); return -1;. // Make sure it's a declared binop.; int TokPrec = BinopPrecedence[CurTok];; if (TokPrec <= 0) return -1;; return TokPrec;; }. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest.; ...; }. For the basic form of Kaleidoscope, we will only support 4 binary; operators (this can obviously be extended by you, our brave and intrepid; reader). The ``GetTokPrecedence`` function returns the precedence for; the current token, or -1 if the token is not a binary operator. Having a; map makes it easy to add new operators and makes it clear that the; algorithm doesn't depend on the specific operators involved, but it; would be easy enough to eliminate the map and do the comparisons in the; ``GetTokPrecedence`` function. (Or just use a fixed-size array). With the helper above defined, we can now start parsing binary; expressions. The basic idea of operator precedence parsing is to break; down an expression with potentially ambiguous binary operators into; pieces. Consider, for example, the expression ""a+b+(c+d)\*e\*f+g"".; Operator precedence parsing considers this as a stream of primary; expressions separated by binary operators. As such, it will first parse; the leading primary expression ""a"", then it will see the pairs [+, b]; [+, (c+d)] [\*, e] [\*, f] an",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:18853,Modifiability,variab,variable,18853,"op ..."". In our example, the current; operator is ""+"" and the next operator is ""+"", we know that they have the; same precedence. In this case we'll create the AST node for ""a+b"", and; then continue parsing:. .. code-block:: c++. ... if body omitted ...; }. // Merge LHS/RHS.; LHS = std::make_unique<BinaryExprAST>(BinOp, std::move(LHS),; std::move(RHS));; } // loop around to the top of the while loop.; }. In our example above, this will turn ""a+b+"" into ""(a+b)"" and execute the; next iteration of the loop, with ""+"" as the current token. The code; above will eat, remember, and parse ""(c+d)"" as the primary expression,; which makes the current pair equal to [+, (c+d)]. It will then evaluate; the 'if' conditional above with ""\*"" as the binop to the right of the; primary. In this case, the precedence of ""\*"" is higher than the; precedence of ""+"" so the if condition will be entered. The critical question left here is ""how can the if condition parse the; right hand side in full""? In particular, to build the AST correctly for; our example, it needs to get all of ""(c+d)\*e\*f"" as the RHS expression; variable. The code to do this is surprisingly simple (code from the; above two blocks duplicated for context):. .. code-block:: c++. // If BinOp binds less tightly with RHS than the operator after RHS, let; // the pending operator take RHS as its LHS.; int NextPrec = GetTokPrecedence();; if (TokPrec < NextPrec) {; RHS = ParseBinOpRHS(TokPrec+1, std::move(RHS));; if (!RHS); return nullptr;; }; // Merge LHS/RHS.; LHS = std::make_unique<BinaryExprAST>(BinOp, std::move(LHS),; std::move(RHS));; } // loop around to the top of the while loop.; }. At this point, we know that the binary operator to the RHS of our; primary has higher precedence than the binop we are currently parsing.; As such, we know that any sequence of pairs whose operators are all; higher precedence than ""+"" should be parsed together and returned as; ""RHS"". To do this, we recursively invoke the ``ParseBinOpRHS`` function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:25053,Modifiability,extend,extend,25053,"lons.; getNextToken();; break;; case tok_def:; HandleDefinition();; break;; case tok_extern:; HandleExtern();; break;; default:; HandleTopLevelExpression();; break;; }; }; }. The most interesting part of this is that we ignore top-level; semicolons. Why is this, you ask? The basic reason is that if you type; ""4 + 5"" at the command line, the parser doesn't know whether that is the; end of what you will type or not. For example, on the next line you; could type ""def foo..."" in which case 4+5 is the end of a top-level; expression. Alternatively you could type ""\* 6"", which would continue; the expression. Having top-level semicolons allows you to type ""4+5;"",; and the parser will know you are done. Conclusions; ===========. With just under 400 lines of commented code (240 lines of non-comment,; non-blank code), we fully defined our minimal language, including a; lexer, parser, and AST builder. With this done, the executable will; validate Kaleidoscope code and tell us if it is grammatically invalid.; For example, here is a sample interaction:. .. code-block:: bash. $ ./a.out; ready> def foo(x y) x+foo(y, 4.0);; Parsed a function definition.; ready> def foo(x y) x+y y;; Parsed a function definition.; Parsed a top-level expr; ready> def foo(x y) x+y );; Parsed a function definition.; Error: unknown token when expecting an expression; ready> extern sin(a);; ready> Parsed an extern; ready> ^D; $. There is a lot of room for extension here. You can define new AST nodes,; extend the language in many ways, etc. In the `next; installment <LangImpl03.html>`_, we will describe how to generate LLVM; Intermediate Representation (IR) from the AST. Full Code Listing; =================. Here is the complete code listing for our running example. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp; # Run; ./a.out. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter2/toy.cpp; :language: c++. `Next: Implementing Code Generation to LLVM IR <LangImpl03.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6723,Safety,recover,recovery,6723,". .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. The",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:2205,Security,access,accessor,2205," easy for later stages of the compiler (e.g. code generation) to; interpret. We basically want one object for each construct in the; language, and the AST should closely model the language. In; Kaleidoscope, we have expressions, a prototype, and a function object.; We'll start with expressions first:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; };. The code above shows the definition of the base ExprAST class and one; subclass which we use for numeric literals. The important thing to note; about this code is that the NumberExprAST class captures the numeric; value of the literal as an instance variable. This allows later phases; of the compiler to know what the stored numeric value is. Right now we only create the AST, so there are no useful accessor; methods on them. It would be very easy to add a virtual method to pretty; print the code, for example. Here are the other expression AST node; definitions that we'll use in the basic form of the Kaleidoscope; language:. .. code-block:: c++. /// VariableExprAST - Expression class for referencing a variable, like ""a"".; class VariableExprAST : public ExprAST {; std::string Name;. public:; VariableExprAST(const std::string &Name) : Name(Name) {}; };. /// BinaryExprAST - Expression class for a binary operator.; class BinaryExprAST : public ExprAST {; char Op;; std::unique_ptr<ExprAST> LHS, RHS;. public:; BinaryExprAST(char Op, std::unique_ptr<ExprAST> LHS,; std::unique_ptr<ExprAST> RHS); : Op(Op), LHS(std::move(LHS)), RHS(std::move(RHS)) {}; };. /// CallExprAST - Expression class for function calls.; class CallExprAST : public ExprAST {; std::string Callee;; std::vector<std::unique_ptr<ExprAST>> Args;. public:; CallExprAST(const std::string &Callee,; std::vector<st",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:24507,Security,validat,validate,24507,"eturn;; case ';': // ignore top-level semicolons.; getNextToken();; break;; case tok_def:; HandleDefinition();; break;; case tok_extern:; HandleExtern();; break;; default:; HandleTopLevelExpression();; break;; }; }; }. The most interesting part of this is that we ignore top-level; semicolons. Why is this, you ask? The basic reason is that if you type; ""4 + 5"" at the command line, the parser doesn't know whether that is the; end of what you will type or not. For example, on the next line you; could type ""def foo..."" in which case 4+5 is the end of a top-level; expression. Alternatively you could type ""\* 6"", which would continue; the expression. Having top-level semicolons allows you to type ""4+5;"",; and the parser will know you are done. Conclusions; ===========. With just under 400 lines of commented code (240 lines of non-comment,; non-blank code), we fully defined our minimal language, including a; lexer, parser, and AST builder. With this done, the executable will; validate Kaleidoscope code and tell us if it is grammatically invalid.; For example, here is a sample interaction:. .. code-block:: bash. $ ./a.out; ready> def foo(x y) x+foo(y, 4.0);; Parsed a function definition.; ready> def foo(x y) x+y y;; Parsed a function definition.; Parsed a top-level expr; ready> def foo(x y) x+y );; Parsed a function definition.; Error: unknown token when expecting an expression; ready> extern sin(a);; ready> Parsed an extern; ready> ^D; $. There is a lot of room for extension here. You can define new AST nodes,; extend the language in many ways, etc. In the `next; installment <LangImpl03.html>`_, we will describe how to generate LLVM; Intermediate Representation (IR) from the AST. Full Code Listing; =================. Here is the complete code listing for our running example. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp; # Run; ./a.out. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter2/toy.cpp; :language: c++. `Next: Implementing Code",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:10936,Testability,log,logic,10936,"::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST`` or; ``CallExprAST`` node as appropriate. Now that we have all of our simple expression-parsing logic in place, we; can define a helper function to wrap it together into one entry point.; We call this class of expressions ""primary"" expressions, for reasons; that will become more clear `later in the; tutorial <LangImpl06.html#user-defined-unary-operators>`_. In order to parse an arbitrary; primary expression, we need to determine what sort of expression it is:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; }; }. Now that you see the definition of this function, it is more obvious why; we can assume the state of CurTok in the various functions. This uses; look-ahead to determine which sort of expression is being inspected, and; then parses it with a function call. Now that bas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:5837,Usability,simpl,simple,5837,"ope, functions are typed with just a count of their; arguments. Since all values are double precision floating point, the; type of each argument doesn't need to be stored anywhere. In a more; aggressive and realistic language, the ""ExprAST"" class would probably; have a type field. With this scaffolding, we can now talk about parsing expressions and; function bodies in Kaleidoscope. Parser Basics; =============. Now that we have an AST to build, we need to define the parser code to; build it. The idea here is that we want to parse something like ""x+y""; (which is returned as three tokens by the lexer) into an AST that could; be generated with calls like this:. .. code-block:: c++. auto LHS = std::make_unique<VariableExprAST>(""x"");; auto RHS = std::make_unique<VariableExprAST>(""y"");; auto Result = std::make_unique<BinaryExprAST>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6105,Usability,simpl,simple,6105," this scaffolding, we can now talk about parsing expressions and; function bodies in Kaleidoscope. Parser Basics; =============. Now that we have an AST to build, we need to define the parser code to; build it. The idea here is that we want to parse something like ""x+y""; (which is returned as three tokens by the lexer) into an AST that could; be generated with calls like this:. .. code-block:: c++. auto LHS = std::make_unique<VariableExprAST>(""x"");; auto RHS = std::make_unique<VariableExprAST>(""y"");; auto Result = std::make_unique<BinaryExprAST>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ======================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6646,Usability,simpl,simple,6646,"T>('+', std::move(LHS),; std::move(RHS));. In order to do this, we'll start by defining some basic helper routines:. .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the curre",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:6790,Usability,user-friendly,user-friendly,6790,". .. code-block:: c++. /// CurTok/getNextToken - Provide a simple token buffer. CurTok is the current; /// token the parser is looking at. getNextToken reads another token from the; /// lexer and updates CurTok with its results.; static int CurTok;; static int getNextToken() {; return CurTok = gettok();; }. This implements a simple token buffer around the lexer. This allows us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. The",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:7171,Usability,simpl,simplest,7171," us; to look one token ahead at what the lexer is returning. Every function; in our parser will assume that CurTok is the current token that needs to; be parsed. .. code-block:: c++. /// LogError* - These are little helper functions for error handling.; std::unique_ptr<ExprAST> LogError(const char *Str) {; fprintf(stderr, ""Error: %s\n"", Str);; return nullptr;; }; std::unique_ptr<PrototypeAST> LogErrorP(const char *Str) {; LogError(Str);; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. There are some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond to the; production and returns the lexer buffer with the next token (which is; not part of the grammar production) ready to go. This is a fairly; standard way to go for recursive descent parsers. For a better example,; the parenthesis operator is defi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:7563,Usability,simpl,simple,7563,";; return nullptr;; }. The ``LogError`` routines are simple helper routines that our parser will; use to handle errors. The error recovery in our parser will not be the; best and is not particular user-friendly, but it will be enough for our; tutorial. These routines make it easier to handle errors in routines; that have various return types: they always return null. With these basic helper functions, we can implement the first piece of; our grammar: numeric literals. Basic Expression Parsing; ========================. We start with numeric literals, because they are the simplest to; process. For each production in our grammar, we'll define a function; which parses that production. For numeric literals, we have:. .. code-block:: c++. /// numberexpr ::= number; static std::unique_ptr<ExprAST> ParseNumberExpr() {; auto Result = std::make_unique<NumberExprAST>(NumVal);; getNextToken(); // consume the number; return std::move(Result);; }. This routine is very simple: it expects to be called when the current; token is a ``tok_number`` token. It takes the current number value,; creates a ``NumberExprAST`` node, advances the lexer to the next token,; and finally returns. There are some interesting aspects to this. The most important one is; that this routine eats all of the tokens that correspond to the; production and returns the lexer buffer with the next token (which is; not part of the grammar production) ready to go. This is a fairly; standard way to go for recursive descent parsers. For a better example,; the parenthesis operator is defined like this:. .. code-block:: c++. /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:9239,Usability,simpl,simple,9239," /// parenexpr ::= '(' expression ')'; static std::unique_ptr<ExprAST> ParseParenExpr() {; getNextToken(); // eat (.; auto V = ParseExpression();; if (!V); return nullptr;. if (CurTok != ')'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Exp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:9399,Usability,guid,guide,9399,")'); return LogError(""expected ')'"");; getNextToken(); // eat ).; return V;; }. This function illustrates a number of interesting things about the; parser:. 1) It shows how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:9513,Usability,simpl,simple,9513,"ws how we use the LogError routines. When called, this function; expects that the current token is a '(' token, but after parsing the; subexpression, it is possible that there is no ')' waiting. For example,; if the user types in ""(4 x"" instead of ""(4)"", the parser should emit an; error. Because errors can occur, the parser needs a way to indicate that; they happened: in our parser, we return null on an error. 2) Another interesting aspect of this function is that it uses recursion; by calling ``ParseExpression`` (we will soon see that; ``ParseExpression`` can call ``ParseParenExpr``). This is powerful; because it allows us to handle recursive grammars, and keeps each; production very simple. Note that parentheses do not cause construction; of AST nodes themselves. While we could do it this way, the most; important role of parentheses are to guide the parser and provide; grouping. Once the parser constructs the AST, parentheses are not; needed. The next simple production is for handling variable references and; function calls:. .. code-block:: c++. /// identifierexpr; /// ::= identifier; /// ::= identifier '(' expression* ')'; static std::unique_ptr<ExprAST> ParseIdentifierExpr() {; std::string IdName = IdentifierStr;. getNextToken(); // eat identifier. if (CurTok != '(') // Simple variable ref.; return std::make_unique<VariableExprAST>(IdName);. // Call.; getNextToken(); // eat (; std::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interes",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:10910,Usability,simpl,simple,10910,"::vector<std::unique_ptr<ExprAST>> Args;; if (CurTok != ')') {; while (true) {; if (auto Arg = ParseExpression()); Args.push_back(std::move(Arg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST`` or; ``CallExprAST`` node as appropriate. Now that we have all of our simple expression-parsing logic in place, we; can define a helper function to wrap it together into one entry point.; We call this class of expressions ""primary"" expressions, for reasons; that will become more clear `later in the; tutorial <LangImpl06.html#user-defined-unary-operators>`_. In order to parse an arbitrary; primary expression, we need to determine what sort of expression it is:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; }; }. Now that you see the definition of this function, it is more obvious why; we can assume the state of CurTok in the various functions. This uses; look-ahead to determine which sort of expression is being inspected, and; then parses it with a function call. Now that bas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:11120,Usability,clear,clear,11120,"rg));; else; return nullptr;. if (CurTok == ')'); break;. if (CurTok != ','); return LogError(""Expected ')' or ',' in argument list"");; getNextToken();; }; }. // Eat the ')'.; getNextToken();. return std::make_unique<CallExprAST>(IdName, std::move(Args));; }. This routine follows the same style as the other routines. (It expects; to be called if the current token is a ``tok_identifier`` token). It; also has recursion and error handling. One interesting aspect of this is; that it uses *look-ahead* to determine if the current identifier is a; stand alone variable reference or if it is a function call expression.; It handles this by checking to see if the token after the identifier is; a '(' token, constructing either a ``VariableExprAST`` or; ``CallExprAST`` node as appropriate. Now that we have all of our simple expression-parsing logic in place, we; can define a helper function to wrap it together into one entry point.; We call this class of expressions ""primary"" expressions, for reasons; that will become more clear `later in the; tutorial <LangImpl06.html#user-defined-unary-operators>`_. In order to parse an arbitrary; primary expression, we need to determine what sort of expression it is:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; }; }. Now that you see the definition of this function, it is more obvious why; we can assume the state of CurTok in the various functions. This uses; look-ahead to determine which sort of expression is being inspected, and; then parses it with a function call. Now that basic expressions are handled, we need to handle binary; expressions. They are a bit more complex. Binary Expression Parsing; =================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:12692,Usability,guid,guide,12692,"ParenExpr();; }; }. Now that you see the definition of this function, it is more obvious why; we can assume the state of CurTok in the various functions. This uses; look-ahead to determine which sort of expression is being inspected, and; then parses it with a function call. Now that basic expressions are handled, we need to handle binary; expressions. They are a bit more complex. Binary Expression Parsing; =========================. Binary expressions are significantly harder to parse because they are; often ambiguous. For example, when given the string ""x+y\*z"", the parser; can choose to parse it as either ""(x+y)\*z"" or ""x+(y\*z)"". With common; definitions from mathematics, we expect the later parse, because ""\*""; (multiplication) has higher *precedence* than ""+"" (addition). There are many ways to handle this, but an elegant and efficient way is; to use `Operator-Precedence; Parsing <http://en.wikipedia.org/wiki/Operator-precedence_parser>`_.; This parsing technique uses the precedence of binary operators to guide; recursion. To start with, we need a table of precedences:. .. code-block:: c++. /// BinopPrecedence - This holds the precedence for each binary operator that is; /// defined.; static std::map<char, int> BinopPrecedence;. /// GetTokPrecedence - Get the precedence of the pending binary operator token.; static int GetTokPrecedence() {; if (!isascii(CurTok)); return -1;. // Make sure it's a declared binop.; int TokPrec = BinopPrecedence[CurTok];; if (TokPrec <= 0) return -1;; return TokPrec;; }. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest.; ...; }. For the basic form of Kaleidoscope, we will only support 4 binary; operators (this can obviously be extended by you, our brave and intrepid; reader). The ``GetTokPrecedence`` function returns the precedence for; the current token, or -1 if the token is not a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:13747,Usability,clear,clear,13747,"ds the precedence for each binary operator that is; /// defined.; static std::map<char, int> BinopPrecedence;. /// GetTokPrecedence - Get the precedence of the pending binary operator token.; static int GetTokPrecedence() {; if (!isascii(CurTok)); return -1;. // Make sure it's a declared binop.; int TokPrec = BinopPrecedence[CurTok];; if (TokPrec <= 0) return -1;; return TokPrec;; }. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest.; ...; }. For the basic form of Kaleidoscope, we will only support 4 binary; operators (this can obviously be extended by you, our brave and intrepid; reader). The ``GetTokPrecedence`` function returns the precedence for; the current token, or -1 if the token is not a binary operator. Having a; map makes it easy to add new operators and makes it clear that the; algorithm doesn't depend on the specific operators involved, but it; would be easy enough to eliminate the map and do the comparisons in the; ``GetTokPrecedence`` function. (Or just use a fixed-size array). With the helper above defined, we can now start parsing binary; expressions. The basic idea of operator precedence parsing is to break; down an expression with potentially ambiguous binary operators into; pieces. Consider, for example, the expression ""a+b+(c+d)\*e\*f+g"".; Operator precedence parsing considers this as a stream of primary; expressions separated by binary operators. As such, it will first parse; the leading primary expression ""a"", then it will see the pairs [+, b]; [+, (c+d)] [\*, e] [\*, f] and [+, g]. Note that because parentheses are; primary expressions, the binary expression parser doesn't need to worry; about nested subexpressions like (c+d) at all. To start, an expression is a primary expression potentially followed by; a sequence of [binop,primaryexpr] pairs:. .. code-block:: c++. /// expression; /// ::= pr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:18899,Usability,simpl,simple,18899,"de for ""a+b"", and; then continue parsing:. .. code-block:: c++. ... if body omitted ...; }. // Merge LHS/RHS.; LHS = std::make_unique<BinaryExprAST>(BinOp, std::move(LHS),; std::move(RHS));; } // loop around to the top of the while loop.; }. In our example above, this will turn ""a+b+"" into ""(a+b)"" and execute the; next iteration of the loop, with ""+"" as the current token. The code; above will eat, remember, and parse ""(c+d)"" as the primary expression,; which makes the current pair equal to [+, (c+d)]. It will then evaluate; the 'if' conditional above with ""\*"" as the binop to the right of the; primary. In this case, the precedence of ""\*"" is higher than the; precedence of ""+"" so the if condition will be entered. The critical question left here is ""how can the if condition parse the; right hand side in full""? In particular, to build the AST correctly for; our example, it needs to get all of ""(c+d)\*e\*f"" as the RHS expression; variable. The code to do this is surprisingly simple (code from the; above two blocks duplicated for context):. .. code-block:: c++. // If BinOp binds less tightly with RHS than the operator after RHS, let; // the pending operator take RHS as its LHS.; int NextPrec = GetTokPrecedence();; if (TokPrec < NextPrec) {; RHS = ParseBinOpRHS(TokPrec+1, std::move(RHS));; if (!RHS); return nullptr;; }; // Merge LHS/RHS.; LHS = std::make_unique<BinaryExprAST>(BinOp, std::move(LHS),; std::move(RHS));; } // loop around to the top of the while loop.; }. At this point, we know that the binary operator to the RHS of our; primary has higher precedence than the binop we are currently parsing.; As such, we know that any sequence of pairs whose operators are all; higher precedence than ""+"" should be parsed together and returned as; ""RHS"". To do this, we recursively invoke the ``ParseBinOpRHS`` function; specifying ""TokPrec+1"" as the minimum precedence required for it to; continue. In our example above, this will cause it to return the AST; node for ""(c+d)\*e\*f"" a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:21629,Usability,simpl,simple,21629,"=========. The next thing missing is handling of function prototypes. In; Kaleidoscope, these are used both for 'extern' function declarations as; well as function body definitions. The code to do this is; straight-forward and not very interesting (once you've survived; expressions):. .. code-block:: c++. /// prototype; /// ::= id '(' id* ')'; static std::unique_ptr<PrototypeAST> ParsePrototype() {; if (CurTok != tok_identifier); return LogErrorP(""Expected function name in prototype"");. std::string FnName = IdentifierStr;; getNextToken();. if (CurTok != '('); return LogErrorP(""Expected '(' in prototype"");. // Read the list of argument names.; std::vector<std::string> ArgNames;; while (getNextToken() == tok_identifier); ArgNames.push_back(IdentifierStr);; if (CurTok != ')'); return LogErrorP(""Expected ')' in prototype"");. // success.; getNextToken(); // eat ')'. return std::make_unique<PrototypeAST>(FnName, std::move(ArgNames));; }. Given this, a function definition is very simple, just a prototype plus; an expression to implement the body:. .. code-block:: c++. /// definition ::= 'def' prototype expression; static std::unique_ptr<FunctionAST> ParseDefinition() {; getNextToken(); // eat def.; auto Proto = ParsePrototype();; if (!Proto) return nullptr;. if (auto E = ParseExpression()); return std::make_unique<FunctionAST>(std::move(Proto), std::move(E));; return nullptr;; }. In addition, we support 'extern' to declare functions like 'sin' and; 'cos' as well as to support forward declaration of user functions. These; 'extern's are just prototypes with no body:. .. code-block:: c++. /// external ::= 'extern' prototype; static std::unique_ptr<PrototypeAST> ParseExtern() {; getNextToken(); // eat extern.; return ParsePrototype();; }. Finally, we'll also let the user type in arbitrary top-level expressions; and evaluate them on the fly. We will handle this by defining anonymous; nullary (zero argument) functions for them:. .. code-block:: c++. /// toplevelexpr ::= expressio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst:23106,Usability,simpl,simply,23106,"rt forward declaration of user functions. These; 'extern's are just prototypes with no body:. .. code-block:: c++. /// external ::= 'extern' prototype; static std::unique_ptr<PrototypeAST> ParseExtern() {; getNextToken(); // eat extern.; return ParsePrototype();; }. Finally, we'll also let the user type in arbitrary top-level expressions; and evaluate them on the fly. We will handle this by defining anonymous; nullary (zero argument) functions for them:. .. code-block:: c++. /// toplevelexpr ::= expression; static std::unique_ptr<FunctionAST> ParseTopLevelExpr() {; if (auto E = ParseExpression()) {; // Make an anonymous proto.; auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; return std::make_unique<FunctionAST>(std::move(Proto), std::move(E));; }; return nullptr;; }. Now that we have all the pieces, let's build a little driver that will; let us actually *execute* this code we've built!. The Driver; ==========. The driver for this simply invokes all of the parsing pieces with a; top-level dispatch loop. There isn't much interesting here, so I'll just; include the top-level loop. See `below <#full-code-listing>`_ for full code in the; ""Top-Level Parsing"" section. .. code-block:: c++. /// top ::= definition | external | expression | ';'; static void MainLoop() {; while (true) {; fprintf(stderr, ""ready> "");; switch (CurTok) {; case tok_eof:; return;; case ';': // ignore top-level semicolons.; getNextToken();; break;; case tok_def:; HandleDefinition();; break;; case tok_extern:; HandleExtern();; break;; default:; HandleTopLevelExpression();; break;; }; }; }. The most interesting part of this is that we ignore top-level; semicolons. Why is this, you ask? The basic reason is that if you type; ""4 + 5"" at the command line, the parser doesn't know whether that is the; end of what you will type or not. For example, on the next line you; could type ""def foo..."" in which case 4+5 is the end of a top-level; expression. Alternatively you could type ""\",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl02.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:2772,Availability,error,errors,2772,"ic Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; it does not get a new value until (and if) the instruction re-executes.; In other words, there is no way to ""change"" an SSA value. For more; information, please read up on `Static Single; Assignment <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; - the concepts are really quite natural once you grok them. Note that instead of adding virtual methods to the ExprAST class; hierarchy, it could also make sense to use a `visitor; pattern <http://en.wikipedia.org/wiki/Visitor_pattern>`_ or some other; way to model this. Again, this tutorial won't dwell on good software; engineering practices: for our purposes, adding a virtual method is; simplest. The second thing we want is a ""LogError"" method like we used for the; parser, which will be used to report errors found during code generation; (for example, use of an undeclared parameter):. .. code-block:: c++. static std::unique_ptr<LLVMContext> TheContext;; static std::unique_ptr<IRBuilder<>> Builder(TheContext);; static std::unique_ptr<Module> TheModule;; static std::map<std::string, Value *> NamedValues;. Value *LogErrorV(const char *Str) {; LogError(Str);; return nullptr;; }. The static variables will be used during code generation. ``TheContext``; is an opaque object that owns a lot of core LLVM data structures, such as; the type and constant value tables. We don't need to understand it in; detail, we just need a single instance to pass into APIs that require it. The ``Builder`` object is a helper object that makes it easy to generate; LLVM instructions. Instances of the; `IRBuilder <https://llvm.org/doxygen/IRBuilder_8h_source.html>`_; class template keep track of the current place to insert instructions; and has methods to create new instructions. ``TheModule`` is an L",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:5911,Availability,avail,available,5911," of commented code for all four of our expression nodes.; First we'll do numeric literals:. .. code-block:: c++. Value *NumberExprAST::codegen() {; return ConstantFP::get(*TheContext, APFloat(Val));; }. In the LLVM IR, numeric constants are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheCont",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:9223,Availability,error,error,9223," in Kaleidoscope are doubles, this makes for very simple code for add,; sub and mul. On the other hand, LLVM specifies that the `fcmp; instruction <../../LangRef.html#fcmp-instruction>`_ always returns an 'i1' value (a; one bit integer). The problem with this is that Kaleidoscope wants the; value to be a 0.0 or 1.0 value. In order to get these semantics, we; combine the fcmp instruction with a `uitofp; instruction <../../LangRef.html#uitofp-to-instruction>`_. This instruction converts its; input integer into a floating point value by treating the input as an; unsigned value. In contrast, if we used the `sitofp; instruction <../../LangRef.html#sitofp-to-instruction>`_, the Kaleidoscope '<' operator; would return 0.0 and -1.0, depending on the input value. .. code-block:: c++. Value *CallExprAST::codegen() {; // Look up the name in the global module table.; Function *CalleeF = TheModule->getFunction(Callee);; if (!CalleeF); return LogErrorV(""Unknown function referenced"");. // If argument mismatch error.; if (CalleeF->arg_size() != Args.size()); return LogErrorV(""Incorrect # arguments passed"");. std::vector<Value *> ArgsV;; for (unsigned i = 0, e = Args.size(); i != e; ++i) {; ArgsV.push_back(Args[i]->codegen());; if (!ArgsV.back()); return nullptr;; }. return Builder->CreateCall(CalleeF, ArgsV, ""calltmp"");; }. Code generation for function calls is quite straightforward with LLVM. The code; above initially does a function name lookup in the LLVM Module's symbol table.; Recall that the LLVM Module is the container that holds the functions we are; JIT'ing. By giving each function the same name as what the user specifies, we; can use the LLVM symbol table to resolve function names for us. Once we have the function to call, we recursively codegen each argument; that is to be passed in, and create an LLVM `call; instruction <../../LangRef.html#call-instruction>`_. Note that LLVM uses the native C; calling conventions by default, allowing these calls to also call into; standa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:15795,Availability,error,error,15795,"nd line; then tells the builder that new instructions should be inserted into the; end of the new basic block. Basic blocks in LLVM are an important part; of functions that define the `Control Flow; Graph <http://en.wikipedia.org/wiki/Control_flow_graph>`_. Since we; don't have any control flow, our functions will only contain one block; at this point. We'll fix this in `Chapter 5 <LangImpl05.html>`_ :). Next we add the function arguments to the NamedValues map (after first clearing; it out) so that they're accessible to ``VariableExprAST`` nodes. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder->CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. return TheFunction;; }. Once the insertion point has been set up and the NamedValues map populated,; we call the ``codegen()`` method for the root expression of the function. If no; error happens, this emits code to compute the expression into the entry block; and returns the value that was computed. Assuming no error, we then create an; LLVM `ret instruction <../../LangRef.html#ret-instruction>`_, which completes the function.; Once the function is built, we call ``verifyFunction``, which is; provided by LLVM. This function does a variety of consistency checks on; the generated code, to determine if our compiler is doing everything; right. Using this is important: it can catch a lot of bugs. Once the; function is finished and validated, we return it. .. code-block:: c++. // Error reading body, remove function.; TheFunction->eraseFromParent();; return nullptr;; }. The only piece left here is handling of the error case. For simplicity,; we handle this by merely deleting the function we produced with the; ``eraseFromParent`` method. This allows the user to redefine a function; that they incorrectly typed in before: if we didn't delete it, it would; live in the symbol table, with a body, preventing future redefinition. Th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:15927,Availability,error,error,15927,"ew basic block. Basic blocks in LLVM are an important part; of functions that define the `Control Flow; Graph <http://en.wikipedia.org/wiki/Control_flow_graph>`_. Since we; don't have any control flow, our functions will only contain one block; at this point. We'll fix this in `Chapter 5 <LangImpl05.html>`_ :). Next we add the function arguments to the NamedValues map (after first clearing; it out) so that they're accessible to ``VariableExprAST`` nodes. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder->CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. return TheFunction;; }. Once the insertion point has been set up and the NamedValues map populated,; we call the ``codegen()`` method for the root expression of the function. If no; error happens, this emits code to compute the expression into the entry block; and returns the value that was computed. Assuming no error, we then create an; LLVM `ret instruction <../../LangRef.html#ret-instruction>`_, which completes the function.; Once the function is built, we call ``verifyFunction``, which is; provided by LLVM. This function does a variety of consistency checks on; the generated code, to determine if our compiler is doing everything; right. Using this is important: it can catch a lot of bugs. Once the; function is finished and validated, we return it. .. code-block:: c++. // Error reading body, remove function.; TheFunction->eraseFromParent();; return nullptr;; }. The only piece left here is handling of the error case. For simplicity,; we handle this by merely deleting the function we produced with the; ``eraseFromParent`` method. This allows the user to redefine a function; that they incorrectly typed in before: if we didn't delete it, it would; live in the symbol table, with a body, preventing future redefinition. This code does have a bug, though: If the ``FunctionAST::codegen()`` method; finds an existing IR",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:16534,Availability,error,error,16534,"al);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. return TheFunction;; }. Once the insertion point has been set up and the NamedValues map populated,; we call the ``codegen()`` method for the root expression of the function. If no; error happens, this emits code to compute the expression into the entry block; and returns the value that was computed. Assuming no error, we then create an; LLVM `ret instruction <../../LangRef.html#ret-instruction>`_, which completes the function.; Once the function is built, we call ``verifyFunction``, which is; provided by LLVM. This function does a variety of consistency checks on; the generated code, to determine if our compiler is doing everything; right. Using this is important: it can catch a lot of bugs. Once the; function is finished and validated, we return it. .. code-block:: c++. // Error reading body, remove function.; TheFunction->eraseFromParent();; return nullptr;; }. The only piece left here is handling of the error case. For simplicity,; we handle this by merely deleting the function we produced with the; ``eraseFromParent`` method. This allows the user to redefine a function; that they incorrectly typed in before: if we didn't delete it, it would; live in the symbol table, with a body, preventing future redefinition. This code does have a bug, though: If the ``FunctionAST::codegen()`` method; finds an existing IR Function, it does not validate its signature against the; definition's own prototype. This means that an earlier 'extern' declaration will; take precedence over the function definition's signature, which can cause; codegen to fail, for instance if the function arguments are named differently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; =====================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:815,Deployability,release,release,815,"========================================; Kaleidoscope: Code generation to LLVM IR; ========================================. .. contents::; :local:. Chapter 3 Introduction; ======================. Welcome to Chapter 3 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. This chapter shows you how to transform; the `Abstract Syntax Tree <LangImpl02.html>`_, built in Chapter 2, into; LLVM IR. This will teach you a little bit about how LLVM does things, as; well as demonstrate how easy it is to use. It's much more work to build; a lexer and parser than it is to generate LLVM IR code. :). **Please note**: the code in this chapter and later require LLVM 3.7 or; later. LLVM 3.6 and before will not work with it. Also note that you; need to use a version of this tutorial that matches your LLVM release:; If you are using an official LLVM release, use the version of the; documentation included with your release or on the `llvm.org releases; page <https://llvm.org/releases/>`_. Code Generation Setup; =====================. In order to generate LLVM IR, we want some simple setup to get started.; First we define virtual code generation (codegen) methods in each AST; class:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; virtual Value *codegen() = 0;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; Value *codegen() override;; };; ... The codegen() method says to emit IR for that AST node along with all; the things it depends on, and they all return an LLVM Value object.; ""Value"" is the class used to represent a ""`Static Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:859,Deployability,release,release,859,"========================================; Kaleidoscope: Code generation to LLVM IR; ========================================. .. contents::; :local:. Chapter 3 Introduction; ======================. Welcome to Chapter 3 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. This chapter shows you how to transform; the `Abstract Syntax Tree <LangImpl02.html>`_, built in Chapter 2, into; LLVM IR. This will teach you a little bit about how LLVM does things, as; well as demonstrate how easy it is to use. It's much more work to build; a lexer and parser than it is to generate LLVM IR code. :). **Please note**: the code in this chapter and later require LLVM 3.7 or; later. LLVM 3.6 and before will not work with it. Also note that you; need to use a version of this tutorial that matches your LLVM release:; If you are using an official LLVM release, use the version of the; documentation included with your release or on the `llvm.org releases; page <https://llvm.org/releases/>`_. Code Generation Setup; =====================. In order to generate LLVM IR, we want some simple setup to get started.; First we define virtual code generation (codegen) methods in each AST; class:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; virtual Value *codegen() = 0;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; Value *codegen() override;; };; ... The codegen() method says to emit IR for that AST node along with all; the things it depends on, and they all return an LLVM Value object.; ""Value"" is the class used to represent a ""`Static Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:925,Deployability,release,release,925,"========================================; Kaleidoscope: Code generation to LLVM IR; ========================================. .. contents::; :local:. Chapter 3 Introduction; ======================. Welcome to Chapter 3 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. This chapter shows you how to transform; the `Abstract Syntax Tree <LangImpl02.html>`_, built in Chapter 2, into; LLVM IR. This will teach you a little bit about how LLVM does things, as; well as demonstrate how easy it is to use. It's much more work to build; a lexer and parser than it is to generate LLVM IR code. :). **Please note**: the code in this chapter and later require LLVM 3.7 or; later. LLVM 3.6 and before will not work with it. Also note that you; need to use a version of this tutorial that matches your LLVM release:; If you are using an official LLVM release, use the version of the; documentation included with your release or on the `llvm.org releases; page <https://llvm.org/releases/>`_. Code Generation Setup; =====================. In order to generate LLVM IR, we want some simple setup to get started.; First we define virtual code generation (codegen) methods in each AST; class:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; virtual Value *codegen() = 0;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; Value *codegen() override;; };; ... The codegen() method says to emit IR for that AST node along with all; the things it depends on, and they all return an LLVM Value object.; ""Value"" is the class used to represent a ""`Static Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:953,Deployability,release,releases,953,"========================================; Kaleidoscope: Code generation to LLVM IR; ========================================. .. contents::; :local:. Chapter 3 Introduction; ======================. Welcome to Chapter 3 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. This chapter shows you how to transform; the `Abstract Syntax Tree <LangImpl02.html>`_, built in Chapter 2, into; LLVM IR. This will teach you a little bit about how LLVM does things, as; well as demonstrate how easy it is to use. It's much more work to build; a lexer and parser than it is to generate LLVM IR code. :). **Please note**: the code in this chapter and later require LLVM 3.7 or; later. LLVM 3.6 and before will not work with it. Also note that you; need to use a version of this tutorial that matches your LLVM release:; If you are using an official LLVM release, use the version of the; documentation included with your release or on the `llvm.org releases; page <https://llvm.org/releases/>`_. Code Generation Setup; =====================. In order to generate LLVM IR, we want some simple setup to get started.; First we define virtual code generation (codegen) methods in each AST; class:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; virtual Value *codegen() = 0;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; Value *codegen() override;; };; ... The codegen() method says to emit IR for that AST node along with all; the things it depends on, and they all return an LLVM Value object.; ""Value"" is the class used to represent a ""`Static Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:986,Deployability,release,releases,986,"========================================; Kaleidoscope: Code generation to LLVM IR; ========================================. .. contents::; :local:. Chapter 3 Introduction; ======================. Welcome to Chapter 3 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. This chapter shows you how to transform; the `Abstract Syntax Tree <LangImpl02.html>`_, built in Chapter 2, into; LLVM IR. This will teach you a little bit about how LLVM does things, as; well as demonstrate how easy it is to use. It's much more work to build; a lexer and parser than it is to generate LLVM IR code. :). **Please note**: the code in this chapter and later require LLVM 3.7 or; later. LLVM 3.6 and before will not work with it. Also note that you; need to use a version of this tutorial that matches your LLVM release:; If you are using an official LLVM release, use the version of the; documentation included with your release or on the `llvm.org releases; page <https://llvm.org/releases/>`_. Code Generation Setup; =====================. In order to generate LLVM IR, we want some simple setup to get started.; First we define virtual code generation (codegen) methods in each AST; class:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; virtual Value *codegen() = 0;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; Value *codegen() override;; };; ... The codegen() method says to emit IR for that AST node along with all; the things it depends on, and they all return an LLVM Value object.; ""Value"" is the class used to represent a ""`Static Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:11403,Energy Efficiency,power,power,11403,"d add some more. For example,; by browsing the `LLVM language reference <../../LangRef.html>`_ you'll find; several other interesting instructions that are really easy to plug into; our basic framework. Function Code Generation; ========================. Code generation for prototypes and functions must handle a number of; details, which make their code less beautiful than expression code; generation, but allows us to illustrate some important points. First,; let's talk about code generation for prototypes: they are used both for; function bodies and external function declarations. The code starts; with:. .. code-block:: c++. Function *PrototypeAST::codegen() {; // Make the function type: double(double,double) etc.; std::vector<Type*> Doubles(Args.size(),; Type::getDoubleTy(*TheContext));; FunctionType *FT =; FunctionType::get(Type::getDoubleTy(*TheContext), Doubles, false);. Function *F =; Function::Create(FT, Function::ExternalLinkage, Name, TheModule.get());. This code packs a lot of power into a few lines. Note first that this; function returns a ""Function\*"" instead of a ""Value\*"". Because a; ""prototype"" really talks about the external interface for a function; (not the value computed by an expression), it makes sense for it to; return the LLVM Function it corresponds to when codegen'd. The call to ``FunctionType::get`` creates the ``FunctionType`` that; should be used for a given Prototype. Since all function arguments in; Kaleidoscope are of type double, the first line creates a vector of ""N""; LLVM double types. It then uses the ``Functiontype::get`` method to; create a function type that takes ""N"" doubles as arguments, returns one; double as a result, and that is not vararg (the false parameter; indicates this). Note that Types in LLVM are uniqued just like Constants; are, so you don't ""new"" a type, you ""get"" it. The final line above actually creates the IR Function corresponding to; the Prototype. This indicates the type, linkage and name to use, as; well as",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:1658,Integrability,depend,depends,1658,"is chapter and later require LLVM 3.7 or; later. LLVM 3.6 and before will not work with it. Also note that you; need to use a version of this tutorial that matches your LLVM release:; If you are using an official LLVM release, use the version of the; documentation included with your release or on the `llvm.org releases; page <https://llvm.org/releases/>`_. Code Generation Setup; =====================. In order to generate LLVM IR, we want some simple setup to get started.; First we define virtual code generation (codegen) methods in each AST; class:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; virtual Value *codegen() = 0;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; Value *codegen() override;; };; ... The codegen() method says to emit IR for that AST node along with all; the things it depends on, and they all return an LLVM Value object.; ""Value"" is the class used to represent a ""`Static Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; it does not get a new value until (and if) the instruction re-executes.; In other words, there is no way to ""change"" an SSA value. For more; information, please read up on `Static Single; Assignment <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; - the concepts are really quite natural once you grok them. Note that instead of adding virtual methods to the ExprAST class; hierarchy, it could also make sense to use a `visitor; pattern <http://en.wikipedia.org/wiki/Visitor_pattern>`_ or some other; way to model this. Again, this tutorial won't dwell on good software; engineering practices: for our purposes, adding a virtua",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:8948,Integrability,depend,depending,8948,"ence>`_ are constrained by strict; rules: for example, the Left and Right operands of an `add; instruction <../../LangRef.html#add-instruction>`_ must have the same type, and the; result type of the add must match the operand types. Because all values; in Kaleidoscope are doubles, this makes for very simple code for add,; sub and mul. On the other hand, LLVM specifies that the `fcmp; instruction <../../LangRef.html#fcmp-instruction>`_ always returns an 'i1' value (a; one bit integer). The problem with this is that Kaleidoscope wants the; value to be a 0.0 or 1.0 value. In order to get these semantics, we; combine the fcmp instruction with a `uitofp; instruction <../../LangRef.html#uitofp-to-instruction>`_. This instruction converts its; input integer into a floating point value by treating the input as an; unsigned value. In contrast, if we used the `sitofp; instruction <../../LangRef.html#sitofp-to-instruction>`_, the Kaleidoscope '<' operator; would return 0.0 and -1.0, depending on the input value. .. code-block:: c++. Value *CallExprAST::codegen() {; // Look up the name in the global module table.; Function *CalleeF = TheModule->getFunction(Callee);; if (!CalleeF); return LogErrorV(""Unknown function referenced"");. // If argument mismatch error.; if (CalleeF->arg_size() != Args.size()); return LogErrorV(""Incorrect # arguments passed"");. std::vector<Value *> ArgsV;; for (unsigned i = 0, e = Args.size(); i != e; ++i) {; ArgsV.push_back(Args[i]->codegen());; if (!ArgsV.back()); return nullptr;; }. return Builder->CreateCall(CalleeF, ArgsV, ""calltmp"");; }. Code generation for function calls is quite straightforward with LLVM. The code; above initially does a function name lookup in the LLVM Module's symbol table.; Recall that the LLVM Module is the container that holds the functions we are; JIT'ing. By giving each function the same name as what the user specifies, we; can use the LLVM symbol table to resolve function names for us. Once we have the function to call, w",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:10290,Integrability,wrap,wraps,10290,"tor<Value *> ArgsV;; for (unsigned i = 0, e = Args.size(); i != e; ++i) {; ArgsV.push_back(Args[i]->codegen());; if (!ArgsV.back()); return nullptr;; }. return Builder->CreateCall(CalleeF, ArgsV, ""calltmp"");; }. Code generation for function calls is quite straightforward with LLVM. The code; above initially does a function name lookup in the LLVM Module's symbol table.; Recall that the LLVM Module is the container that holds the functions we are; JIT'ing. By giving each function the same name as what the user specifies, we; can use the LLVM symbol table to resolve function names for us. Once we have the function to call, we recursively codegen each argument; that is to be passed in, and create an LLVM `call; instruction <../../LangRef.html#call-instruction>`_. Note that LLVM uses the native C; calling conventions by default, allowing these calls to also call into; standard library functions like ""sin"" and ""cos"", with no additional; effort. This wraps up our handling of the four basic expressions that we have so; far in Kaleidoscope. Feel free to go in and add some more. For example,; by browsing the `LLVM language reference <../../LangRef.html>`_ you'll find; several other interesting instructions that are really easy to plug into; our basic framework. Function Code Generation; ========================. Code generation for prototypes and functions must handle a number of; details, which make their code less beautiful than expression code; generation, but allows us to illustrate some important points. First,; let's talk about code generation for prototypes: they are used both for; function bodies and external function declarations. The code starts; with:. .. code-block:: c++. Function *PrototypeAST::codegen() {; // Make the function type: double(double,double) etc.; std::vector<Type*> Doubles(Args.size(),; Type::getDoubleTy(*TheContext));; FunctionType *FT =; FunctionType::get(Type::getDoubleTy(*TheContext), Doubles, false);. Function *F =; Function::Create(FT, Functi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:11560,Integrability,interface,interface,11560,"tion Code Generation; ========================. Code generation for prototypes and functions must handle a number of; details, which make their code less beautiful than expression code; generation, but allows us to illustrate some important points. First,; let's talk about code generation for prototypes: they are used both for; function bodies and external function declarations. The code starts; with:. .. code-block:: c++. Function *PrototypeAST::codegen() {; // Make the function type: double(double,double) etc.; std::vector<Type*> Doubles(Args.size(),; Type::getDoubleTy(*TheContext));; FunctionType *FT =; FunctionType::get(Type::getDoubleTy(*TheContext), Doubles, false);. Function *F =; Function::Create(FT, Function::ExternalLinkage, Name, TheModule.get());. This code packs a lot of power into a few lines. Note first that this; function returns a ""Function\*"" instead of a ""Value\*"". Because a; ""prototype"" really talks about the external interface for a function; (not the value computed by an expression), it makes sense for it to; return the LLVM Function it corresponds to when codegen'd. The call to ``FunctionType::get`` creates the ``FunctionType`` that; should be used for a given Prototype. Since all function arguments in; Kaleidoscope are of type double, the first line creates a vector of ""N""; LLVM double types. It then uses the ``Functiontype::get`` method to; create a function type that takes ""N"" doubles as arguments, returns one; double as a result, and that is not vararg (the false parameter; indicates this). Note that Types in LLVM are uniqued just like Constants; are, so you don't ""new"" a type, you ""get"" it. The final line above actually creates the IR Function corresponding to; the Prototype. This indicates the type, linkage and name to use, as; well as which module to insert into. ""`external; linkage <../../LangRef.html#linkage>`_"" means that the function may be; defined outside the current module and/or that it is callable by; functions outside the modul",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:20855,Integrability,wrap,wraps,20855,"on Pygments' horrible `llvm` lexer. It just totally gives up; on highlighting this due to the first line. ::. ready> ^D; ; ModuleID = 'my cool jit'. define double @0() {; entry:; %addtmp = fadd double 4.000000e+00, 5.000000e+00; ret double %addtmp; }. define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. declare double @cos(double). define double @1() {; entry:; %calltmp = call double @cos(double 1.234000e+00); ret double %calltmp; }. When you quit the current demo (by sending an EOF via CTRL+D on Linux; or CTRL+Z and ENTER on Windows), it dumps out the IR for the entire; module generated. Here you can see the big picture with all the; functions referencing each other. This wraps up the third chapter of the Kaleidoscope tutorial. Up next,; we'll describe how to `add JIT codegen and optimizer; support <LangImpl04.html>`_ to this so we can actually start running; code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM code generator. Because this uses the LLVM libraries, we need; to link them in. To do this, we use the; `llvm-config <https://llvm.org/cmds/llvm-config.html>`_ tool to inform; our makefile/command line about which options to use:. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core` -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter3/toy.cpp; :language: c++. `Next: Adding JIT and Optimizer Support <LangImpl04.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:3164,Modifiability,variab,variables,3164,"ingle; Assignment <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; - the concepts are really quite natural once you grok them. Note that instead of adding virtual methods to the ExprAST class; hierarchy, it could also make sense to use a `visitor; pattern <http://en.wikipedia.org/wiki/Visitor_pattern>`_ or some other; way to model this. Again, this tutorial won't dwell on good software; engineering practices: for our purposes, adding a virtual method is; simplest. The second thing we want is a ""LogError"" method like we used for the; parser, which will be used to report errors found during code generation; (for example, use of an undeclared parameter):. .. code-block:: c++. static std::unique_ptr<LLVMContext> TheContext;; static std::unique_ptr<IRBuilder<>> Builder(TheContext);; static std::unique_ptr<Module> TheModule;; static std::map<std::string, Value *> NamedValues;. Value *LogErrorV(const char *Str) {; LogError(Str);; return nullptr;; }. The static variables will be used during code generation. ``TheContext``; is an opaque object that owns a lot of core LLVM data structures, such as; the type and constant value tables. We don't need to understand it in; detail, we just need a single instance to pass into APIs that require it. The ``Builder`` object is a helper object that makes it easy to generate; LLVM instructions. Instances of the; `IRBuilder <https://llvm.org/doxygen/IRBuilder_8h_source.html>`_; class template keep track of the current place to insert instructions; and has methods to create new instructions. ``TheModule`` is an LLVM construct that contains functions and global; variables. In many ways, it is the top-level structure that the LLVM IR; uses to contain code. It will own the memory for all of the IR that we; generate, which is why the codegen() method returns a raw Value\*,; rather than a unique_ptr<Value>. The ``NamedValues`` map keeps track of which values are defined in the; current scope and what their LLVM representation is. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:3810,Modifiability,variab,variables,3810,"found during code generation; (for example, use of an undeclared parameter):. .. code-block:: c++. static std::unique_ptr<LLVMContext> TheContext;; static std::unique_ptr<IRBuilder<>> Builder(TheContext);; static std::unique_ptr<Module> TheModule;; static std::map<std::string, Value *> NamedValues;. Value *LogErrorV(const char *Str) {; LogError(Str);; return nullptr;; }. The static variables will be used during code generation. ``TheContext``; is an opaque object that owns a lot of core LLVM data structures, such as; the type and constant value tables. We don't need to understand it in; detail, we just need a single instance to pass into APIs that require it. The ``Builder`` object is a helper object that makes it easy to generate; LLVM instructions. Instances of the; `IRBuilder <https://llvm.org/doxygen/IRBuilder_8h_source.html>`_; class template keep track of the current place to insert instructions; and has methods to create new instructions. ``TheModule`` is an LLVM construct that contains functions and global; variables. In many ways, it is the top-level structure that the LLVM IR; uses to contain code. It will own the memory for all of the IR that we; generate, which is why the codegen() method returns a raw Value\*,; rather than a unique_ptr<Value>. The ``NamedValues`` map keeps track of which values are defined in the; current scope and what their LLVM representation is. (In other words, it; is a symbol table for the code). In this form of Kaleidoscope, the only; things that can be referenced are function parameters. As such, function; parameters will be in this map when generating code for their function; body. With these basics in place, we can start talking about how to generate; code for each expression. Note that this assumes that the ``Builder``; has been set up to generate code *into* something. For now, we'll assume; that this has already been done, and we'll just use it to emit code. Expression Code Generation; ==========================. Generating",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:5611,Modifiability,variab,variable,5611,"ode *into* something. For now, we'll assume; that this has already been done, and we'll just use it to emit code. Expression Code Generation; ==========================. Generating LLVM code for expression nodes is very straightforward: less; than 45 lines of commented code for all four of our expression nodes.; First we'll do numeric literals:. .. code-block:: c++. Value *NumberExprAST::codegen() {; return ConstantFP::get(*TheContext, APFloat(Val));; }. In the LLVM IR, numeric constants are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; r",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:5700,Modifiability,variab,variable,5700,"l just use it to emit code. Expression Code Generation; ==========================. Generating LLVM code for expression nodes is very straightforward: less; than 45 lines of commented code for all four of our expression nodes.; First we'll do numeric literals:. .. code-block:: c++. Value *NumberExprAST::codegen() {; return ConstantFP::get(*TheContext, APFloat(Val));; }. In the LLVM IR, numeric constants are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:5746,Modifiability,variab,variables,5746,"=======. Generating LLVM code for expression nodes is very straightforward: less; than 45 lines of commented code for all four of our expression nodes.; First we'll do numeric literals:. .. code-block:: c++. Value *NumberExprAST::codegen() {; return ConstantFP::get(*TheContext, APFloat(Val));; }. In the LLVM IR, numeric constants are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // C",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:5849,Modifiability,variab,variable,5849," of commented code for all four of our expression nodes.; First we'll do numeric literals:. .. code-block:: c++. Value *NumberExprAST::codegen() {; return ConstantFP::get(*TheContext, APFloat(Val));; }. In the LLVM IR, numeric constants are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheCont",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:6106,Modifiability,variab,variable,6106,"s are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheContext),; ""booltmp"");; default:; return LogErrorV(""invalid binary operator"");; }; }. Binary operators start to get more interesting. The basic idea here is; that we recursively emit code for the left-hand side of the expression,; then the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:6226,Modifiability,variab,variables,6226,"oat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheContext),; ""booltmp"");; default:; return LogErrorV(""invalid binary operator"");; }; }. Binary operators start to get more interesting. The basic idea here is; that we recursively emit code for the left-hand side of the expression,; then the right-hand side, then we compute the result of the binary; expression. In this code, we do a simple switch on the op",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:6313,Modifiability,variab,variables,6313,"). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheContext),; ""booltmp"");; default:; return LogErrorV(""invalid binary operator"");; }; }. Binary operators start to get more interesting. The basic idea here is; that we recursively emit code for the left-hand side of the expression,; then the right-hand side, then we compute the result of the binary; expression. In this code, we do a simple switch on the opcode to create; the right LLVM instruction. In the example above, the LLVM builder cl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:6359,Modifiability,variab,variables,6359,"P``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheContext),; ""booltmp"");; default:; return LogErrorV(""invalid binary operator"");; }; }. Binary operators start to get more interesting. The basic idea here is; that we recursively emit code for the left-hand side of the expression,; then the right-hand side, then we compute the result of the binary; expression. In this code, we do a simple switch on the opcode to create; the right LLVM instruction. In the example above, the LLVM builder class is starting to show its; value. IRBuilder knows where to i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:7701,Modifiability,variab,variables,7701,"er->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheContext),; ""booltmp"");; default:; return LogErrorV(""invalid binary operator"");; }; }. Binary operators start to get more interesting. The basic idea here is; that we recursively emit code for the left-hand side of the expression,; then the right-hand side, then we compute the result of the binary; expression. In this code, we do a simple switch on the opcode to create; the right LLVM instruction. In the example above, the LLVM builder class is starting to show its; value. IRBuilder knows where to insert the newly created instruction,; all you have to do is specify what instruction to create (e.g. with; ``CreateFAdd``), which operands to use (``L`` and ``R`` here) and; optionally provide a name for the generated instruction. One nice thing about LLVM is that the name is just a hint. For instance,; if the code above emits multiple ""addtmp"" variables, LLVM will; automatically provide each one with an increasing, unique numeric; suffix. Local value names for instructions are purely optional, but it; makes it much easier to read the IR dumps. `LLVM instructions <../../LangRef.html#instruction-reference>`_ are constrained by strict; rules: for example, the Left and Right operands of an `add; instruction <../../LangRef.html#add-instruction>`_ must have the same type, and the; result type of the add must match the operand types. Because all values; in Kaleidoscope are doubles, this makes for very simple code for add,; sub and mul. On the other hand, LLVM specifies that the `fcmp; instruction <../../LangRef.html#fcmp-instruction>`_ always returns an 'i1' value (a; one bit integer). The problem with this is that Kaleidoscope wants the; value to be a 0.0 or 1.0 value. In order to get these semantics, we; combine the fcmp instruction with a `uitofp; instruction <../../LangRef.html#uitofp-to-instruction>`_. This instruction converts its; input intege",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:17409,Modifiability,variab,variable,17409,"Error reading body, remove function.; TheFunction->eraseFromParent();; return nullptr;; }. The only piece left here is handling of the error case. For simplicity,; we handle this by merely deleting the function we produced with the; ``eraseFromParent`` method. This allows the user to redefine a function; that they incorrectly typed in before: if we didn't delete it, it would; live in the symbol table, with a body, preventing future redefinition. This code does have a bug, though: If the ``FunctionAST::codegen()`` method; finds an existing IR Function, it does not validate its signature against the; definition's own prototype. This means that an earlier 'extern' declaration will; take precedence over the function definition's signature, which can cause; codegen to fail, for instance if the function arguments are named differently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the ne",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:21150,Modifiability,enhance,enhanced,21150,"on Pygments' horrible `llvm` lexer. It just totally gives up; on highlighting this due to the first line. ::. ready> ^D; ; ModuleID = 'my cool jit'. define double @0() {; entry:; %addtmp = fadd double 4.000000e+00, 5.000000e+00; ret double %addtmp; }. define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. declare double @cos(double). define double @1() {; entry:; %calltmp = call double @cos(double 1.234000e+00); ret double %calltmp; }. When you quit the current demo (by sending an EOF via CTRL+D on Linux; or CTRL+Z and ENTER on Windows), it dumps out the IR for the entire; module generated. Here you can see the big picture with all the; functions referencing each other. This wraps up the third chapter of the Kaleidoscope tutorial. Up next,; we'll describe how to `add JIT codegen and optimizer; support <LangImpl04.html>`_ to this so we can actually start running; code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM code generator. Because this uses the LLVM libraries, we need; to link them in. To do this, we use the; `llvm-config <https://llvm.org/cmds/llvm-config.html>`_ tool to inform; our makefile/command line about which options to use:. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core` -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter3/toy.cpp; :language: c++. `Next: Adding JIT and Optimizer Support <LangImpl04.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:21284,Modifiability,config,config,21284,"on Pygments' horrible `llvm` lexer. It just totally gives up; on highlighting this due to the first line. ::. ready> ^D; ; ModuleID = 'my cool jit'. define double @0() {; entry:; %addtmp = fadd double 4.000000e+00, 5.000000e+00; ret double %addtmp; }. define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. declare double @cos(double). define double @1() {; entry:; %calltmp = call double @cos(double 1.234000e+00); ret double %calltmp; }. When you quit the current demo (by sending an EOF via CTRL+D on Linux; or CTRL+Z and ENTER on Windows), it dumps out the IR for the entire; module generated. Here you can see the big picture with all the; functions referencing each other. This wraps up the third chapter of the Kaleidoscope tutorial. Up next,; we'll describe how to `add JIT codegen and optimizer; support <LangImpl04.html>`_ to this so we can actually start running; code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM code generator. Because this uses the LLVM libraries, we need; to link them in. To do this, we use the; `llvm-config <https://llvm.org/cmds/llvm-config.html>`_ tool to inform; our makefile/command line about which options to use:. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core` -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter3/toy.cpp; :language: c++. `Next: Adding JIT and Optimizer Support <LangImpl04.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:21319,Modifiability,config,config,21319,"on Pygments' horrible `llvm` lexer. It just totally gives up; on highlighting this due to the first line. ::. ready> ^D; ; ModuleID = 'my cool jit'. define double @0() {; entry:; %addtmp = fadd double 4.000000e+00, 5.000000e+00; ret double %addtmp; }. define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. declare double @cos(double). define double @1() {; entry:; %calltmp = call double @cos(double 1.234000e+00); ret double %calltmp; }. When you quit the current demo (by sending an EOF via CTRL+D on Linux; or CTRL+Z and ENTER on Windows), it dumps out the IR for the entire; module generated. Here you can see the big picture with all the; functions referencing each other. This wraps up the third chapter of the Kaleidoscope tutorial. Up next,; we'll describe how to `add JIT codegen and optimizer; support <LangImpl04.html>`_ to this so we can actually start running; code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM code generator. Because this uses the LLVM libraries, we need; to link them in. To do this, we use the; `llvm-config <https://llvm.org/cmds/llvm-config.html>`_ tool to inform; our makefile/command line about which options to use:. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core` -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter3/toy.cpp; :language: c++. `Next: Adding JIT and Optimizer Support <LangImpl04.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:21467,Modifiability,config,config,21467,"on Pygments' horrible `llvm` lexer. It just totally gives up; on highlighting this due to the first line. ::. ready> ^D; ; ModuleID = 'my cool jit'. define double @0() {; entry:; %addtmp = fadd double 4.000000e+00, 5.000000e+00; ret double %addtmp; }. define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. declare double @cos(double). define double @1() {; entry:; %calltmp = call double @cos(double 1.234000e+00); ret double %calltmp; }. When you quit the current demo (by sending an EOF via CTRL+D on Linux; or CTRL+Z and ENTER on Windows), it dumps out the IR for the entire; module generated. Here you can see the big picture with all the; functions referencing each other. This wraps up the third chapter of the Kaleidoscope tutorial. Up next,; we'll describe how to `add JIT codegen and optimizer; support <LangImpl04.html>`_ to this so we can actually start running; code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM code generator. Because this uses the LLVM libraries, we need; to link them in. To do this, we use the; `llvm-config <https://llvm.org/cmds/llvm-config.html>`_ tool to inform; our makefile/command line about which options to use:. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core` -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter3/toy.cpp; :language: c++. `Next: Adding JIT and Optimizer Support <LangImpl04.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:18222,Performance,optimiz,optimizations,18222,"ently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the next; chapter. ::. ready> def foo(a b) a*a + 2*a*b + b*b;; Read function definition:; define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. This shows some simple arithmetic. Notice the striking similarity to the; LLVM builder calls that we use to create the instructions. ::. ready> def bar(a) foo(a, 4.0) + bar(31337);; Read function definition:; define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. This shows some function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:18246,Performance,perform,performed,18246,"ently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the next; chapter. ::. ready> def foo(a b) a*a + 2*a*b + b*b;; Read function definition:; define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. This shows some simple arithmetic. Notice the striking similarity to the; LLVM builder calls that we use to create the instructions. ::. ready> def bar(a) foo(a, 4.0) + bar(31337);; Read function definition:; define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. This shows some function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:18321,Performance,optimiz,optimizations,18321,"a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the next; chapter. ::. ready> def foo(a b) a*a + 2*a*b + b*b;; Read function definition:; define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. This shows some simple arithmetic. Notice the striking similarity to the; LLVM builder calls that we use to create the instructions. ::. ready> def bar(a) foo(a, 4.0) + bar(31337);; Read function definition:; define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. This shows some function calls. Note that this function will take a long; time to execute if you call it. In the futu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:20965,Performance,optimiz,optimizer,20965,"on Pygments' horrible `llvm` lexer. It just totally gives up; on highlighting this due to the first line. ::. ready> ^D; ; ModuleID = 'my cool jit'. define double @0() {; entry:; %addtmp = fadd double 4.000000e+00, 5.000000e+00; ret double %addtmp; }. define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. declare double @cos(double). define double @1() {; entry:; %calltmp = call double @cos(double 1.234000e+00); ret double %calltmp; }. When you quit the current demo (by sending an EOF via CTRL+D on Linux; or CTRL+Z and ENTER on Windows), it dumps out the IR for the entire; module generated. Here you can see the big picture with all the; functions referencing each other. This wraps up the third chapter of the Kaleidoscope tutorial. Up next,; we'll describe how to `add JIT codegen and optimizer; support <LangImpl04.html>`_ to this so we can actually start running; code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM code generator. Because this uses the LLVM libraries, we need; to link them in. To do this, we use the; `llvm-config <https://llvm.org/cmds/llvm-config.html>`_ tool to inform; our makefile/command line about which options to use:. .. code-block:: bash. # Compile; clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core` -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter3/toy.cpp; :language: c++. `Next: Adding JIT and Optimizer Support <LangImpl04.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:15363,Security,access,accessible,15363,"basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()); NamedValues[std::string(Arg.getName())] = &Arg;. Now we get to the point where the ``Builder`` is set up. The first line; creates a new `basic block <http://en.wikipedia.org/wiki/Basic_block>`_; (named ""entry""), which is inserted into ``TheFunction``. The second line; then tells the builder that new instructions should be inserted into the; end of the new basic block. Basic blocks in LLVM are an important part; of functions that define the `Control Flow; Graph <http://en.wikipedia.org/wiki/Control_flow_graph>`_. Since we; don't have any control flow, our functions will only contain one block; at this point. We'll fix this in `Chapter 5 <LangImpl05.html>`_ :). Next we add the function arguments to the NamedValues map (after first clearing; it out) so that they're accessible to ``VariableExprAST`` nodes. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder->CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. return TheFunction;; }. Once the insertion point has been set up and the NamedValues map populated,; we call the ``codegen()`` method for the root expression of the function. If no; error happens, this emits code to compute the expression into the entry block; and returns the value that was computed. Assuming no error, we then create an; LLVM `ret instruction <../../LangRef.html#ret-instruction>`_, which completes the function.; Once the function is built, we call ``verifyFunction``, which is; provided by LLVM. This function does a variety of consistency checks on; the generated code, to determine if our compiler is doing everything; right. Using this is important: it can catch a lot of bugs. Once the; funct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:16350,Security,validat,validated,16350,"t) so that they're accessible to ``VariableExprAST`` nodes. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder->CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. return TheFunction;; }. Once the insertion point has been set up and the NamedValues map populated,; we call the ``codegen()`` method for the root expression of the function. If no; error happens, this emits code to compute the expression into the entry block; and returns the value that was computed. Assuming no error, we then create an; LLVM `ret instruction <../../LangRef.html#ret-instruction>`_, which completes the function.; Once the function is built, we call ``verifyFunction``, which is; provided by LLVM. This function does a variety of consistency checks on; the generated code, to determine if our compiler is doing everything; right. Using this is important: it can catch a lot of bugs. Once the; function is finished and validated, we return it. .. code-block:: c++. // Error reading body, remove function.; TheFunction->eraseFromParent();; return nullptr;; }. The only piece left here is handling of the error case. For simplicity,; we handle this by merely deleting the function we produced with the; ``eraseFromParent`` method. This allows the user to redefine a function; that they incorrectly typed in before: if we didn't delete it, it would; live in the symbol table, with a body, preventing future redefinition. This code does have a bug, though: If the ``FunctionAST::codegen()`` method; finds an existing IR Function, it does not validate its signature against the; definition's own prototype. This means that an earlier 'extern' declaration will; take precedence over the function definition's signature, which can cause; codegen to fail, for instance if the function arguments are named differently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:16969,Security,validat,validate,16969,"n create an; LLVM `ret instruction <../../LangRef.html#ret-instruction>`_, which completes the function.; Once the function is built, we call ``verifyFunction``, which is; provided by LLVM. This function does a variety of consistency checks on; the generated code, to determine if our compiler is doing everything; right. Using this is important: it can catch a lot of bugs. Once the; function is finished and validated, we return it. .. code-block:: c++. // Error reading body, remove function.; TheFunction->eraseFromParent();; return nullptr;; }. The only piece left here is handling of the error case. For simplicity,; we handle this by merely deleting the function we produced with the; ``eraseFromParent`` method. This allows the user to redefine a function; that they incorrectly typed in before: if we didn't delete it, it would; live in the symbol table, with a body, preventing future redefinition. This code does have a bug, though: If the ``FunctionAST::codegen()`` method; finds an existing IR Function, it does not validate its signature against the; definition's own prototype. This means that an earlier 'extern' declaration will; take precedence over the function definition's signature, which can cause; codegen to fail, for instance if the function arguments are named differently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:14219,Testability,assert,assert,14219,"p in the Prototype AST. At this point we have a function prototype with no body. This is how LLVM IR; represents function declarations. For extern statements in Kaleidoscope, this; is as far as we need to go. For function definitions however, we need to; codegen and attach a function body. .. code-block:: c++. Function *FunctionAST::codegen() {; // First, check for an existing function from a previous 'extern' declaration.; Function *TheFunction = TheModule->getFunction(Proto->getName());. if (!TheFunction); TheFunction = Proto->codegen();. if (!TheFunction); return nullptr;. if (!TheFunction->empty()); return (Function*)LogErrorV(""Function cannot be redefined."");. For function definitions, we start by searching TheModule's symbol table for an; existing version of this function, in case one has already been created using an; 'extern' statement. If Module::getFunction returns null then no previous version; exists, so we'll codegen one from the Prototype. In either case, we want to; assert that the function is empty (i.e. has no body yet) before we start. .. code-block:: c++. // Create a new basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()); NamedValues[std::string(Arg.getName())] = &Arg;. Now we get to the point where the ``Builder`` is set up. The first line; creates a new `basic block <http://en.wikipedia.org/wiki/Basic_block>`_; (named ""entry""), which is inserted into ``TheFunction``. The second line; then tells the builder that new instructions should be inserted into the; end of the new basic block. Basic blocks in LLVM are an important part; of functions that define the `Control Flow; Graph <http://en.wikipedia.org/wiki/Control_flow_graph>`_. Since we; don't have any control flow, our functions will only contain one block; at this point. We'll fix this in ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:17328,Testability,test,testcase,17328," can catch a lot of bugs. Once the; function is finished and validated, we return it. .. code-block:: c++. // Error reading body, remove function.; TheFunction->eraseFromParent();; return nullptr;; }. The only piece left here is handling of the error case. For simplicity,; we handle this by merely deleting the function we produced with the; ``eraseFromParent`` method. This allows the user to redefine a function; that they incorrectly typed in before: if we didn't delete it, it would; live in the symbol table, with a body, preventing future redefinition. This code does have a bug, though: If the ``FunctionAST::codegen()`` method; finds an existing IR Function, it does not validate its signature against the; definition's own prototype. This means that an earlier 'extern' declaration will; take precedence over the function definition's signature, which can cause; codegen to fail, for instance if the function arguments are named differently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:1089,Usability,simpl,simple,1089,"R; ========================================. .. contents::; :local:. Chapter 3 Introduction; ======================. Welcome to Chapter 3 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. This chapter shows you how to transform; the `Abstract Syntax Tree <LangImpl02.html>`_, built in Chapter 2, into; LLVM IR. This will teach you a little bit about how LLVM does things, as; well as demonstrate how easy it is to use. It's much more work to build; a lexer and parser than it is to generate LLVM IR code. :). **Please note**: the code in this chapter and later require LLVM 3.7 or; later. LLVM 3.6 and before will not work with it. Also note that you; need to use a version of this tutorial that matches your LLVM release:; If you are using an official LLVM release, use the version of the; documentation included with your release or on the `llvm.org releases; page <https://llvm.org/releases/>`_. Code Generation Setup; =====================. In order to generate LLVM IR, we want some simple setup to get started.; First we define virtual code generation (codegen) methods in each AST; class:. .. code-block:: c++. /// ExprAST - Base class for all expression nodes.; class ExprAST {; public:; virtual ~ExprAST() = default;; virtual Value *codegen() = 0;; };. /// NumberExprAST - Expression class for numeric literals like ""1.0"".; class NumberExprAST : public ExprAST {; double Val;. public:; NumberExprAST(double Val) : Val(Val) {}; Value *codegen() override;; };; ... The codegen() method says to emit IR for that AST node along with all; the things it depends on, and they all return an LLVM Value object.; ""Value"" is the class used to represent a ""`Static Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; it does not get a new value until (and if) the instruction re-executes.; In other",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:2655,Usability,simpl,simplest,2655,"to emit IR for that AST node along with all; the things it depends on, and they all return an LLVM Value object.; ""Value"" is the class used to represent a ""`Static Single Assignment; (SSA) <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; register"" or ""SSA value"" in LLVM. The most distinct aspect of SSA values; is that their value is computed as the related instruction executes, and; it does not get a new value until (and if) the instruction re-executes.; In other words, there is no way to ""change"" an SSA value. For more; information, please read up on `Static Single; Assignment <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; - the concepts are really quite natural once you grok them. Note that instead of adding virtual methods to the ExprAST class; hierarchy, it could also make sense to use a `visitor; pattern <http://en.wikipedia.org/wiki/Visitor_pattern>`_ or some other; way to model this. Again, this tutorial won't dwell on good software; engineering practices: for our purposes, adding a virtual method is; simplest. The second thing we want is a ""LogError"" method like we used for the; parser, which will be used to report errors found during code generation; (for example, use of an undeclared parameter):. .. code-block:: c++. static std::unique_ptr<LLVMContext> TheContext;; static std::unique_ptr<IRBuilder<>> Builder(TheContext);; static std::unique_ptr<Module> TheModule;; static std::map<std::string, Value *> NamedValues;. Value *LogErrorV(const char *Str) {; LogError(Str);; return nullptr;; }. The static variables will be used during code generation. ``TheContext``; is an opaque object that owns a lot of core LLVM data structures, such as; the type and constant value tables. We don't need to understand it in; detail, we just need a single instance to pass into APIs that require it. The ``Builder`` object is a helper object that makes it easy to generate; LLVM instructions. Instances of the; `IRBuilder <https://llvm.org/doxygen/IRBu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:5771,Usability,simpl,simple,5771,"=======. Generating LLVM code for expression nodes is very straightforward: less; than 45 lines of commented code for all four of our expression nodes.; First we'll do numeric literals:. .. code-block:: c++. Value *NumberExprAST::codegen() {; return ConstantFP::get(*TheContext, APFloat(Val));; }. In the LLVM IR, numeric constants are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // C",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:5797,Usability,simpl,simple,5797," of commented code for all four of our expression nodes.; First we'll do numeric literals:. .. code-block:: c++. Value *NumberExprAST::codegen() {; return ConstantFP::get(*TheContext, APFloat(Val));; }. In the LLVM IR, numeric constants are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheCont",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:6026,Usability,simpl,simply,6026,"s are represented with the; ``ConstantFP`` class, which holds the numeric value in an ``APFloat``; internally (``APFloat`` has the capability of holding floating point; constants of Arbitrary Precision). This code basically just creates; and returns a ``ConstantFP``. Note that in the LLVM IR that constants; are all uniqued together and shared. For this reason, the API uses the; ""foo::get(...)"" idiom instead of ""new foo(..)"" or ""foo::Create(..)"". .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; Value *V = NamedValues[Name];; if (!V); LogErrorV(""Unknown variable name"");; return V;; }. References to variables are also quite simple using LLVM. In the simple; version of Kaleidoscope, we assume that the variable has already been; emitted somewhere and its value is available. In practice, the only; values that can be in the ``NamedValues`` map are function arguments.; This code simply checks to see that the specified name is in the map (if; not, an unknown variable is being referenced) and returns the value for; it. In future chapters, we'll add support for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheContext),; ""booltmp"");; default:; return LogErrorV(""invalid binary operator"");; }; }. Binary operators start to get more interesting. The basic idea here is; that we recursively emit code for the left-hand side of the expression,; then the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:7184,Usability,simpl,simple,7184,"for `loop induction; variables <LangImpl05.html#for-loop-expression>`_ in the symbol table, and for `local; variables <LangImpl07.html#user-defined-local-variables>`_. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(TheContext),; ""booltmp"");; default:; return LogErrorV(""invalid binary operator"");; }; }. Binary operators start to get more interesting. The basic idea here is; that we recursively emit code for the left-hand side of the expression,; then the right-hand side, then we compute the result of the binary; expression. In this code, we do a simple switch on the opcode to create; the right LLVM instruction. In the example above, the LLVM builder class is starting to show its; value. IRBuilder knows where to insert the newly created instruction,; all you have to do is specify what instruction to create (e.g. with; ``CreateFAdd``), which operands to use (``L`` and ``R`` here) and; optionally provide a name for the generated instruction. One nice thing about LLVM is that the name is just a hint. For instance,; if the code above emits multiple ""addtmp"" variables, LLVM will; automatically provide each one with an increasing, unique numeric; suffix. Local value names for instructions are purely optional, but it; makes it much easier to read the IR dumps. `LLVM instructions <../../LangRef.html#instruction-reference>`_ are constrained by strict; rules: for example, the Left and Right operands of an `add; instruction <../../LangRef.html#add-instruction>`_ must have the same type, and the; result type of the add must match the operand types. Because all ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:8263,Usability,simpl,simple,8263,"tion. In the example above, the LLVM builder class is starting to show its; value. IRBuilder knows where to insert the newly created instruction,; all you have to do is specify what instruction to create (e.g. with; ``CreateFAdd``), which operands to use (``L`` and ``R`` here) and; optionally provide a name for the generated instruction. One nice thing about LLVM is that the name is just a hint. For instance,; if the code above emits multiple ""addtmp"" variables, LLVM will; automatically provide each one with an increasing, unique numeric; suffix. Local value names for instructions are purely optional, but it; makes it much easier to read the IR dumps. `LLVM instructions <../../LangRef.html#instruction-reference>`_ are constrained by strict; rules: for example, the Left and Right operands of an `add; instruction <../../LangRef.html#add-instruction>`_ must have the same type, and the; result type of the add must match the operand types. Because all values; in Kaleidoscope are doubles, this makes for very simple code for add,; sub and mul. On the other hand, LLVM specifies that the `fcmp; instruction <../../LangRef.html#fcmp-instruction>`_ always returns an 'i1' value (a; one bit integer). The problem with this is that Kaleidoscope wants the; value to be a 0.0 or 1.0 value. In order to get these semantics, we; combine the fcmp instruction with a `uitofp; instruction <../../LangRef.html#uitofp-to-instruction>`_. This instruction converts its; input integer into a floating point value by treating the input as an; unsigned value. In contrast, if we used the `sitofp; instruction <../../LangRef.html#sitofp-to-instruction>`_, the Kaleidoscope '<' operator; would return 0.0 and -1.0, depending on the input value. .. code-block:: c++. Value *CallExprAST::codegen() {; // Look up the name in the global module table.; Function *CalleeF = TheModule->getFunction(Callee);; if (!CalleeF); return LogErrorV(""Unknown function referenced"");. // If argument mismatch error.; if (CalleeF->a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:14541,Usability,clear,clear,14541,", check for an existing function from a previous 'extern' declaration.; Function *TheFunction = TheModule->getFunction(Proto->getName());. if (!TheFunction); TheFunction = Proto->codegen();. if (!TheFunction); return nullptr;. if (!TheFunction->empty()); return (Function*)LogErrorV(""Function cannot be redefined."");. For function definitions, we start by searching TheModule's symbol table for an; existing version of this function, in case one has already been created using an; 'extern' statement. If Module::getFunction returns null then no previous version; exists, so we'll codegen one from the Prototype. In either case, we want to; assert that the function is empty (i.e. has no body yet) before we start. .. code-block:: c++. // Create a new basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()); NamedValues[std::string(Arg.getName())] = &Arg;. Now we get to the point where the ``Builder`` is set up. The first line; creates a new `basic block <http://en.wikipedia.org/wiki/Basic_block>`_; (named ""entry""), which is inserted into ``TheFunction``. The second line; then tells the builder that new instructions should be inserted into the; end of the new basic block. Basic blocks in LLVM are an important part; of functions that define the `Control Flow; Graph <http://en.wikipedia.org/wiki/Control_flow_graph>`_. Since we; don't have any control flow, our functions will only contain one block; at this point. We'll fix this in `Chapter 5 <LangImpl05.html>`_ :). Next we add the function arguments to the NamedValues map (after first clearing; it out) so that they're accessible to ``VariableExprAST`` nodes. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder->CreateRet(RetVal);. // Validate the generated code, checking for consistency.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:15329,Usability,clear,clearing,15329,"basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()); NamedValues[std::string(Arg.getName())] = &Arg;. Now we get to the point where the ``Builder`` is set up. The first line; creates a new `basic block <http://en.wikipedia.org/wiki/Basic_block>`_; (named ""entry""), which is inserted into ``TheFunction``. The second line; then tells the builder that new instructions should be inserted into the; end of the new basic block. Basic blocks in LLVM are an important part; of functions that define the `Control Flow; Graph <http://en.wikipedia.org/wiki/Control_flow_graph>`_. Since we; don't have any control flow, our functions will only contain one block; at this point. We'll fix this in `Chapter 5 <LangImpl05.html>`_ :). Next we add the function arguments to the NamedValues map (after first clearing; it out) so that they're accessible to ``VariableExprAST`` nodes. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder->CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. return TheFunction;; }. Once the insertion point has been set up and the NamedValues map populated,; we call the ``codegen()`` method for the root expression of the function. If no; error happens, this emits code to compute the expression into the entry block; and returns the value that was computed. Assuming no error, we then create an; LLVM `ret instruction <../../LangRef.html#ret-instruction>`_, which completes the function.; Once the function is built, we call ``verifyFunction``, which is; provided by LLVM. This function does a variety of consistency checks on; the generated code, to determine if our compiler is doing everything; right. Using this is important: it can catch a lot of bugs. Once the; funct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:16550,Usability,simpl,simplicity,16550,"unction);. return TheFunction;; }. Once the insertion point has been set up and the NamedValues map populated,; we call the ``codegen()`` method for the root expression of the function. If no; error happens, this emits code to compute the expression into the entry block; and returns the value that was computed. Assuming no error, we then create an; LLVM `ret instruction <../../LangRef.html#ret-instruction>`_, which completes the function.; Once the function is built, we call ``verifyFunction``, which is; provided by LLVM. This function does a variety of consistency checks on; the generated code, to determine if our compiler is doing everything; right. Using this is important: it can catch a lot of bugs. Once the; function is finished and validated, we return it. .. code-block:: c++. // Error reading body, remove function.; TheFunction->eraseFromParent();; return nullptr;; }. The only piece left here is handling of the error case. For simplicity,; we handle this by merely deleting the function we produced with the; ``eraseFromParent`` method. This allows the user to redefine a function; that they incorrectly typed in before: if we didn't delete it, it would; live in the symbol table, with a body, preventing future redefinition. This code does have a bug, though: If the ``FunctionAST::codegen()`` method; finds an existing IR Function, it does not validate its signature against the; definition's own prototype. This means that an earlier 'extern' declaration will; take precedence over the function definition's signature, which can cause; codegen to fail, for instance if the function arguments are named differently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:17835,Usability,simpl,simple,17835,"reventing future redefinition. This code does have a bug, though: If the ``FunctionAST::codegen()`` method; finds an existing IR Function, it does not validate its signature against the; definition's own prototype. This means that an earlier 'extern' declaration will; take precedence over the function definition's signature, which can cause; codegen to fail, for instance if the function arguments are named differently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the next; chapter. ::. ready> def foo(a b) a*a + 2*a*b + b*b;; Read function definition:; define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. This shows some simple arithmetic. N",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:18264,Usability,simpl,simple,18264,"ently.; There are a number of ways to fix this bug, see what you can come up with! Here; is a testcase:. ::. extern foo(a); # ok, defines foo.; def foo(b) b; # Error: Unknown variable name. (decl using 'a' takes precedence). Driver Changes and Closing Thoughts; ===================================. For now, code generation to LLVM doesn't really get us much, except that; we can look at the pretty IR calls. The sample code inserts calls to; codegen into the ""``HandleDefinition``"", ""``HandleExtern``"" etc; functions, and then dumps out the LLVM IR. This gives a nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the next; chapter. ::. ready> def foo(a b) a*a + 2*a*b + b*b;; Read function definition:; define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. This shows some simple arithmetic. Notice the striking similarity to the; LLVM builder calls that we use to create the instructions. ::. ready> def bar(a) foo(a, 4.0) + bar(31337);; Read function definition:; define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. This shows some function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst:18798,Usability,simpl,simple,18798,"nice way to look; at the LLVM IR for simple functions. For example:. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Note how the parser turns the top-level expression into anonymous; functions for us. This will be handy when we add `JIT; support <LangImpl04.html#adding-a-jit-compiler>`_ in the next chapter. Also note that the; code is very literally transcribed, no optimizations are being performed; except simple constant folding done by IRBuilder. We will `add; optimizations <LangImpl04.html#trivial-constant-folding>`_ explicitly in the next; chapter. ::. ready> def foo(a b) a*a + 2*a*b + b*b;; Read function definition:; define double @foo(double %a, double %b) {; entry:; %multmp = fmul double %a, %a; %multmp1 = fmul double 2.000000e+00, %a; %multmp2 = fmul double %multmp1, %b; %addtmp = fadd double %multmp, %multmp2; %multmp3 = fmul double %b, %b; %addtmp4 = fadd double %addtmp, %multmp3; ret double %addtmp4; }. This shows some simple arithmetic. Notice the striking similarity to the; LLVM builder calls that we use to create the instructions. ::. ready> def bar(a) foo(a, 4.0) + bar(31337);; Read function definition:; define double @bar(double %a) {; entry:; %calltmp = call double @foo(double %a, double 4.000000e+00); %calltmp1 = call double @bar(double 3.133700e+04); %addtmp = fadd double %calltmp, %calltmp1; ret double %addtmp; }. This shows some function calls. Note that this function will take a long; time to execute if you call it. In the future we'll add conditional; control flow to actually make recursion useful :). ::. ready> extern cos(x);; Read extern:; declare double @cos(double). ready> cos(1.234);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @cos(double 1.234000e+00); ret double %calltmp; }. This shows an extern for the libm ""cos"" function, and a call to it. .. TODO:: Abandon Pygments' horrible `llvm` lexer. It just totally gives up; on highlighting this due to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl03.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:3154,Availability,redundant,redundant,3154,"nt checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for languages with a; macro preprocessor or that use a lot of constants). On the other hand, the ``IRBuilder`` is limited by the fact that it does; all of its analysis inline with the code as it is built. If you take a; slightly more complex example:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:9203,Availability,avail,available,9203," Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump it out in textual or binary forms, you can compile; the code to an assembly file (.s) for some target, or you can JIT; compile it. The nice thing about the LLVM IR representation is that it; is the ""common currency"" between many different parts of the compiler. In this section, we'll add JIT compiler support to our interpreter. The; basic idea that we want for Kaleidoscope is to have the user enter; function bodies as they do now, but immediately ev",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:9616,Availability,avail,available,9616," in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump it out in textual or binary forms, you can compile; the code to an assembly file (.s) for some target, or you can JIT; compile it. The nice thing about the LLVM IR representation is that it; is the ""common currency"" between many different parts of the compiler. In this section, we'll add JIT compiler support to our interpreter. The; basic idea that we want for Kaleidoscope is to have the user enter; function bodies as they do now, but immediately evaluate the top-level; expressions they type in. For example, if they type in ""1 + 2;"", we; should evaluate and print out 3. If they define a function, they should; be able to call it from the command line. In order to do this, we first prepare the environment to create code for; the current native target and declare and initialize the JIT. This is; done by calling some ``InitializeNativeTarget\*`` functions and; adding a gl",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:11893,Availability,avail,available,11893,"tall standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest. // Prime the first token.; fprintf(stderr, ""ready> "");; getNextToken();. TheJIT = std::make_unique<KaleidoscopeJIT>();. // Run the main ""interpreter loop"" now.; MainLoop();. return 0;; }. We also need to setup the data layout for the JIT:. .. code-block:: c++. void InitializeModuleAndPassManager(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""my cool jit"", TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create a new pass manager attached to it.; TheFPM = std::make_unique<legacy::FunctionPassManager>(TheModule.get());; ... The KaleidoscopeJIT class is a simple JIT built specifically for these; tutorials, available inside the LLVM source code; at `llvm-src/examples/Kaleidoscope/include/KaleidoscopeJIT.h; <https://github.com/llvm/llvm-project/blob/main/llvm/examples/Kaleidoscope/include/KaleidoscopeJIT.h>`_.; In later chapters we will look at how it works and extend it with; new features, but for now we will take it as given. Its API is very simple:; ``addModule`` adds an LLVM IR module to the JIT, making its functions; available for execution (with its memory managed by a ``ResourceTracker``); and; ``lookup`` allows us to look up pointers to the compiled code. We can take this simple API and change our code that parses top-level expressions to; look like this:. .. code-block:: c++. static ExitOnError ExitOnErr;; ...; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; if (FnAST->codegen()) {; // Create a ResourceTracker to track JIT'd memory allocated to our; // anonymous expression -- that way we ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:12315,Availability,avail,available,12315,": c++. void InitializeModuleAndPassManager(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""my cool jit"", TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create a new pass manager attached to it.; TheFPM = std::make_unique<legacy::FunctionPassManager>(TheModule.get());; ... The KaleidoscopeJIT class is a simple JIT built specifically for these; tutorials, available inside the LLVM source code; at `llvm-src/examples/Kaleidoscope/include/KaleidoscopeJIT.h; <https://github.com/llvm/llvm-project/blob/main/llvm/examples/Kaleidoscope/include/KaleidoscopeJIT.h>`_.; In later chapters we will look at how it works and extend it with; new features, but for now we will take it as given. Its API is very simple:; ``addModule`` adds an LLVM IR module to the JIT, making its functions; available for execution (with its memory managed by a ``ResourceTracker``); and; ``lookup`` allows us to look up pointers to the compiled code. We can take this simple API and change our code that parses top-level expressions to; look like this:. .. code-block:: c++. static ExitOnError ExitOnErr;; ...; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; if (FnAST->codegen()) {; // Create a ResourceTracker to track JIT'd memory allocated to our; // anonymous expression -- that way we can free it after executing.; auto RT = TheJIT->getMainJITDylib().createResourceTracker();. auto TSM = ThreadSafeModule(std::move(TheModule), std::move(TheContext));; ExitOnErr(TheJIT->addModule(std::move(TSM), RT));; InitializeModuleAndPassManager();. // Search the JIT for the __anon_expr symbol.; auto ExprSymbol = ExitOnErr(TheJIT->lookup(""__anon_expr""));; assert(ExprSymbol && ""Function not found"");. // Get the symbol's address and ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:20341,Availability,error,error,20341," this, we'll start by adding a new global, ``FunctionProtos``, that; holds the most recent prototype for each function. We'll also add a convenience; method, ``getFunction()``, to replace calls to ``TheModule->getFunction()``.; Our convenience method searches ``TheModule`` for an existing function; declaration, falling back to generating a new declaration from FunctionProtos if; it doesn't find one. In ``CallExprAST::codegen()`` we just need to replace the; call to ``TheModule->getFunction()``. In ``FunctionAST::codegen()`` we need to; update the FunctionProtos map first, then call ``getFunction()``. With this; done, we can always obtain a function declaration in the current module for any; previously declared function. We also need to update HandleDefinition and HandleExtern:. .. code-block:: c++. static void HandleDefinition() {; if (auto FnAST = ParseDefinition()) {; if (auto *FnIR = FnAST->codegen()) {; fprintf(stderr, ""Read function definition:"");; FnIR->print(errs());; fprintf(stderr, ""\n"");; ExitOnErr(TheJIT->addModule(; ThreadSafeModule(std::move(TheModule), std::move(TheContext))));; InitializeModuleAndPassManager();; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:20347,Availability,recover,recovery,20347," this, we'll start by adding a new global, ``FunctionProtos``, that; holds the most recent prototype for each function. We'll also add a convenience; method, ``getFunction()``, to replace calls to ``TheModule->getFunction()``.; Our convenience method searches ``TheModule`` for an existing function; declaration, falling back to generating a new declaration from FunctionProtos if; it doesn't find one. In ``CallExprAST::codegen()`` we just need to replace the; call to ``TheModule->getFunction()``. In ``FunctionAST::codegen()`` we need to; update the FunctionProtos map first, then call ``getFunction()``. With this; done, we can always obtain a function declaration in the current module for any; previously declared function. We also need to update HandleDefinition and HandleExtern:. .. code-block:: c++. static void HandleDefinition() {; if (auto FnAST = ParseDefinition()) {; if (auto *FnIR = FnAST->codegen()) {; fprintf(stderr, ""Read function definition:"");; FnIR->print(errs());; fprintf(stderr, ""\n"");; ExitOnErr(TheJIT->addModule(; ThreadSafeModule(std::move(TheModule), std::move(TheContext))));; InitializeModuleAndPassManager();; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:20662,Availability,error,error,20662,"FunctionProtos if; it doesn't find one. In ``CallExprAST::codegen()`` we just need to replace the; call to ``TheModule->getFunction()``. In ``FunctionAST::codegen()`` we need to; update the FunctionProtos map first, then call ``getFunction()``. With this; done, we can always obtain a function declaration in the current module for any; previously declared function. We also need to update HandleDefinition and HandleExtern:. .. code-block:: c++. static void HandleDefinition() {; if (auto FnAST = ParseDefinition()) {; if (auto *FnIR = FnAST->codegen()) {; fprintf(stderr, ""Read function definition:"");; FnIR->print(errs());; fprintf(stderr, ""\n"");; ExitOnErr(TheJIT->addModule(; ThreadSafeModule(std::move(TheModule), std::move(TheContext))));; InitializeModuleAndPassManager();; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close to the static and dynamic linker rules, including rejecting duplicate symbols. Requiring symbol names to be unique allows us to support concurrent compilation for symbols using the (unique) symbol names as keys for tracking. With these changes made, let's try our REPL again (I removed the dump of the; anonymous functions this time, you should get the idea by now ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:20668,Availability,recover,recovery,20668,"FunctionProtos if; it doesn't find one. In ``CallExprAST::codegen()`` we just need to replace the; call to ``TheModule->getFunction()``. In ``FunctionAST::codegen()`` we need to; update the FunctionProtos map first, then call ``getFunction()``. With this; done, we can always obtain a function declaration in the current module for any; previously declared function. We also need to update HandleDefinition and HandleExtern:. .. code-block:: c++. static void HandleDefinition() {; if (auto FnAST = ParseDefinition()) {; if (auto *FnIR = FnAST->codegen()) {; fprintf(stderr, ""Read function definition:"");; FnIR->print(errs());; fprintf(stderr, ""\n"");; ExitOnErr(TheJIT->addModule(; ThreadSafeModule(std::move(TheModule), std::move(TheContext))));; InitializeModuleAndPassManager();; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close to the static and dynamic linker rules, including rejecting duplicate symbols. Requiring symbol names to be unique allows us to support concurrent compilation for symbols using the (unique) symbol names as keys for tracking. With these changes made, let's try our REPL again (I removed the dump of the; anonymous functions this time, you should get the idea by now ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:22757,Availability,avail,available,22757,"lities -; check this out:. ::. ready> extern sin(x);; Read extern:; declare double @sin(double). ready> extern cos(x);; Read extern:; declare double @cos(double). ready> sin(1.0);; Read top-level expression:; define double @2() {; entry:; ret double 0x3FEAED548F090CEE; }. Evaluated to 0.841471. ready> def foo(x) sin(x)*sin(x) + cos(x)*cos(x);; Read function definition:; define double @foo(double %x) {; entry:; %calltmp = call double @sin(double %x); %multmp = fmul double %calltmp, %calltmp; %calltmp2 = call double @cos(double %x); %multmp4 = fmul double %calltmp2, %calltmp2; %addtmp = fadd double %multmp, %multmp4; ret double %addtmp; }. ready> foo(4.0);; Read top-level expression:; define double @3() {; entry:; %calltmp = call double @foo(double 4.000000e+00); ret double %calltmp; }. Evaluated to 1.000000. Whoa, how does the JIT know about sin and cos? The answer is surprisingly; simple: The KaleidoscopeJIT has a straightforward symbol resolution rule that; it uses to find symbols that aren't available in any given module: First; it searches all the modules that have already been added to the JIT, from the; most recent to the oldest, to find the newest definition. If no definition is; found inside the JIT, it falls back to calling ""``dlsym(""sin"")``"" on the; Kaleidoscope process itself. Since ""``sin``"" is defined within the JIT's; address space, it simply patches up calls in the module to call the libm; version of ``sin`` directly. But in some cases this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symb",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:23612,Availability,avail,available,23612,"s the JIT know about sin and cos? The answer is surprisingly; simple: The KaleidoscopeJIT has a straightforward symbol resolution rule that; it uses to find symbols that aren't available in any given module: First; it searches all the modules that have already been added to the JIT, from the; most recent to the oldest, to find the newest definition. If no definition is; found inside the JIT, it falls back to calling ""``dlsym(""sin"")``"" on the; Kaleidoscope process itself. Since ""``sin``"" is defined within the JIT's; address space, it simply patches up calls in the module to call the libm; version of ``sin`` directly. But in some cases this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:8563,Deployability,update,updates,8563,"on't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:19707,Deployability,update,update,19707," Value *CallExprAST::codegen() {; // Look up the name in the global module table.; Function *CalleeF = getFunction(Callee);. ... Function *FunctionAST::codegen() {; // Transfer ownership of the prototype to the FunctionProtos map, but keep a; // reference to it for use below.; auto &P = *Proto;; FunctionProtos[Proto->getName()] = std::move(Proto);; Function *TheFunction = getFunction(P.getName());; if (!TheFunction); return nullptr;. To enable this, we'll start by adding a new global, ``FunctionProtos``, that; holds the most recent prototype for each function. We'll also add a convenience; method, ``getFunction()``, to replace calls to ``TheModule->getFunction()``.; Our convenience method searches ``TheModule`` for an existing function; declaration, falling back to generating a new declaration from FunctionProtos if; it doesn't find one. In ``CallExprAST::codegen()`` we just need to replace the; call to ``TheModule->getFunction()``. In ``FunctionAST::codegen()`` we need to; update the FunctionProtos map first, then call ``getFunction()``. With this; done, we can always obtain a function declaration in the current module for any; previously declared function. We also need to update HandleDefinition and HandleExtern:. .. code-block:: c++. static void HandleDefinition() {; if (auto FnAST = ParseDefinition()) {; if (auto *FnIR = FnAST->codegen()) {; fprintf(stderr, ""Read function definition:"");; FnIR->print(errs());; fprintf(stderr, ""\n"");; ExitOnErr(TheJIT->addModule(; ThreadSafeModule(std::move(TheModule), std::move(TheContext))));; InitializeModuleAndPassManager();; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:19911,Deployability,update,update,19911,"o the FunctionProtos map, but keep a; // reference to it for use below.; auto &P = *Proto;; FunctionProtos[Proto->getName()] = std::move(Proto);; Function *TheFunction = getFunction(P.getName());; if (!TheFunction); return nullptr;. To enable this, we'll start by adding a new global, ``FunctionProtos``, that; holds the most recent prototype for each function. We'll also add a convenience; method, ``getFunction()``, to replace calls to ``TheModule->getFunction()``.; Our convenience method searches ``TheModule`` for an existing function; declaration, falling back to generating a new declaration from FunctionProtos if; it doesn't find one. In ``CallExprAST::codegen()`` we just need to replace the; call to ``TheModule->getFunction()``. In ``FunctionAST::codegen()`` we need to; update the FunctionProtos map first, then call ``getFunction()``. With this; done, we can always obtain a function declaration in the current module for any; previously declared function. We also need to update HandleDefinition and HandleExtern:. .. code-block:: c++. static void HandleDefinition() {; if (auto FnAST = ParseDefinition()) {; if (auto *FnIR = FnAST->codegen()) {; fprintf(stderr, ""Read function definition:"");; FnIR->print(errs());; fprintf(stderr, ""\n"");; ExitOnErr(TheJIT->addModule(; ThreadSafeModule(std::move(TheModule), std::move(TheContext))));; InitializeModuleAndPassManager();; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplicati",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:23126,Deployability,patch,patches,23126,"ouble @foo(double %x) {; entry:; %calltmp = call double @sin(double %x); %multmp = fmul double %calltmp, %calltmp; %calltmp2 = call double @cos(double %x); %multmp4 = fmul double %calltmp2, %calltmp2; %addtmp = fadd double %multmp, %multmp4; ret double %addtmp; }. ready> foo(4.0);; Read top-level expression:; define double @3() {; entry:; %calltmp = call double @foo(double 4.000000e+00); ret double %calltmp; }. Evaluated to 1.000000. Whoa, how does the JIT know about sin and cos? The answer is surprisingly; simple: The KaleidoscopeJIT has a straightforward symbol resolution rule that; it uses to find symbols that aren't available in any given module: First; it searches all the modules that have already been added to the JIT, from the; most recent to the oldest, to find the newest definition. If no definition is; found inside the JIT, it falls back to calling ""``dlsym(""sin"")``"" on the; Kaleidoscope process itself. Since ""``sin``"" is defined within the JIT's; address space, it simply patches up calls in the module to call the libm; version of ``sin`` directly. But in some cases this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stder",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:585,Energy Efficiency,efficient,efficient,585,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:2131,Energy Efficiency,reduce,reduce,2131,"hat would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""syntactic; overhead"" for its use (you don't have to uglify your compiler with; constant checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for languages with a; macro preprocessor or that use a lot of constants). On the other hand, the ``IRBuilder`` is limited by the fact that it does; all of its analysis inline with the code as it is built. If you take a; slightly more complex example:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:12841,Energy Efficiency,allocate,allocated,12841,"peJIT class is a simple JIT built specifically for these; tutorials, available inside the LLVM source code; at `llvm-src/examples/Kaleidoscope/include/KaleidoscopeJIT.h; <https://github.com/llvm/llvm-project/blob/main/llvm/examples/Kaleidoscope/include/KaleidoscopeJIT.h>`_.; In later chapters we will look at how it works and extend it with; new features, but for now we will take it as given. Its API is very simple:; ``addModule`` adds an LLVM IR module to the JIT, making its functions; available for execution (with its memory managed by a ``ResourceTracker``); and; ``lookup`` allows us to look up pointers to the compiled code. We can take this simple API and change our code that parses top-level expressions to; look like this:. .. code-block:: c++. static ExitOnError ExitOnErr;; ...; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; if (FnAST->codegen()) {; // Create a ResourceTracker to track JIT'd memory allocated to our; // anonymous expression -- that way we can free it after executing.; auto RT = TheJIT->getMainJITDylib().createResourceTracker();. auto TSM = ThreadSafeModule(std::move(TheModule), std::move(TheContext));; ExitOnErr(TheJIT->addModule(std::move(TSM), RT));; InitializeModuleAndPassManager();. // Search the JIT for the __anon_expr symbol.; auto ExprSymbol = ExitOnErr(TheJIT->lookup(""__anon_expr""));; assert(ExprSymbol && ""Function not found"");. // Get the symbol's address and cast it to the right type (takes no; // arguments, returns a double) so we can call it as a native function.; double (*FP)() = ExprSymbol.getAddress().toPtr<double (*)()>();; fprintf(stderr, ""Evaluated to %f\n"", FP());. // Delete the anonymous expression module from the JIT.; ExitOnErr(RT->remove());; }. If parsing and codegen succeed, the next step is to add the module containing; the top-level expression to the JIT. We do this by calling addModule, which; triggers code generation fo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:21733,Energy Efficiency,power,powerful,21733,"add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close to the static and dynamic linker rules, including rejecting duplicate symbols. Requiring symbol names to be unique allows us to support concurrent compilation for symbols using the (unique) symbol names as keys for tracking. With these changes made, let's try our REPL again (I removed the dump of the; anonymous functions this time, you should get the idea by now :) :. ::. ready> def foo(x) x + 1;; ready> foo(2);; Evaluated to 3.000000. ready> def foo(x) x + 2;; ready> foo(2);; Evaluated to 4.000000. It works!. Even with this simple code, we get some surprisingly powerful capabilities -; check this out:. ::. ready> extern sin(x);; Read extern:; declare double @sin(double). ready> extern cos(x);; Read extern:; declare double @cos(double). ready> sin(1.0);; Read top-level expression:; define double @2() {; entry:; ret double 0x3FEAED548F090CEE; }. Evaluated to 0.841471. ready> def foo(x) sin(x)*sin(x) + cos(x)*cos(x);; Read function definition:; define double @foo(double %x) {; entry:; %calltmp = call double @sin(double %x); %multmp = fmul double %calltmp, %calltmp; %calltmp2 = call double @cos(double %x); %multmp4 = fmul double %calltmp2, %calltmp2; %addtmp = fadd double %multmp, %multmp4; ret double %addtmp; }. ready> foo(4.0);; Read top-level expression:; define double @3() {; entry:; %calltmp = call double @foo(double 4.000000e+00); ret double %calltmp; }. Evaluated to 1.000000. Whoa, how does the JIT know about sin and cos? The answer is surprisingly; simple: The KaleidoscopeJIT has a straightforward symbol resolution rule that; it use",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:5104,Integrability,depend,depends,5104,"For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""KaleidoscopeJIT"", *TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create new pass and analysis managers.; TheFPM = std::make_unique<FunctionPassManager>();; TheLAM = std::make_unique<LoopAnalysisManager>(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:739,Modifiability,extend,extend,739,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:10649,Modifiability,variab,variable,10649,"VM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump it out in textual or binary forms, you can compile; the code to an assembly file (.s) for some target, or you can JIT; compile it. The nice thing about the LLVM IR representation is that it; is the ""common currency"" between many different parts of the compiler. In this section, we'll add JIT compiler support to our interpreter. The; basic idea that we want for Kaleidoscope is to have the user enter; function bodies as they do now, but immediately evaluate the top-level; expressions they type in. For example, if they type in ""1 + 2;"", we; should evaluate and print out 3. If they define a function, they should; be able to call it from the command line. In order to do this, we first prepare the environment to create code for; the current native target and declare and initialize the JIT. This is; done by calling some ``InitializeNativeTarget\*`` functions and; adding a global variable ``TheJIT``, and initializing it in; ``main``:. .. code-block:: c++. static std::unique_ptr<KaleidoscopeJIT> TheJIT;; ...; int main() {; InitializeNativeTarget();; InitializeNativeTargetAsmPrinter();; InitializeNativeTargetAsmParser();. // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest. // Prime the first token.; fprintf(stderr, ""ready> "");; getNextToken();. TheJIT = std::make_unique<KaleidoscopeJIT>();. // Run the main ""interpreter loop"" now.; MainLoop();. return 0;; }. We also need to setup the data layout for the JIT:. .. code-block:: c++. void InitializeModuleAndPassManager(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""my cool jit"", TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:12151,Modifiability,extend,extend,12151,"heJIT = std::make_unique<KaleidoscopeJIT>();. // Run the main ""interpreter loop"" now.; MainLoop();. return 0;; }. We also need to setup the data layout for the JIT:. .. code-block:: c++. void InitializeModuleAndPassManager(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""my cool jit"", TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create a new pass manager attached to it.; TheFPM = std::make_unique<legacy::FunctionPassManager>(TheModule.get());; ... The KaleidoscopeJIT class is a simple JIT built specifically for these; tutorials, available inside the LLVM source code; at `llvm-src/examples/Kaleidoscope/include/KaleidoscopeJIT.h; <https://github.com/llvm/llvm-project/blob/main/llvm/examples/Kaleidoscope/include/KaleidoscopeJIT.h>`_.; In later chapters we will look at how it works and extend it with; new features, but for now we will take it as given. Its API is very simple:; ``addModule`` adds an LLVM IR module to the JIT, making its functions; available for execution (with its memory managed by a ``ResourceTracker``); and; ``lookup`` allows us to look up pointers to the compiled code. We can take this simple API and change our code that parses top-level expressions to; look like this:. .. code-block:: c++. static ExitOnError ExitOnErr;; ...; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; if (FnAST->codegen()) {; // Create a ResourceTracker to track JIT'd memory allocated to our; // anonymous expression -- that way we can free it after executing.; auto RT = TheJIT->getMainJITDylib().createResourceTracker();. auto TSM = ThreadSafeModule(std::move(TheModule), std::move(TheContext));; ExitOnErr(TheJIT->addModule(std::move(TSM), RT));; InitializeModuleAndPassManager();. // Sear",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:23787,Modifiability,extend,extend,23787,"First; it searches all the modules that have already been added to the JIT, from the; most recent to the oldest, to find the newest definition. If no definition is; found inside the JIT, it falls back to calling ""``dlsym(""sin"")``"" on the; Kaleidoscope process itself. Since ""``sin``"" is defined within the JIT's; address space, it simply patches up calls in the module to call the libm; version of ``sin`` directly. But in some cases this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-d",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:24826,Modifiability,extend,extending,24826,"vailable to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. If you are compiling this on Linux, make sure to add the ""-rdynamic""; option as well. This makes sure that the external functions are resolved; properly at runtime. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter4/toy.cpp; :language: c++. `Next: Extending the language: control flow <LangImpl05.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:25054,Modifiability,enhance,enhanced,25054,"vailable to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. If you are compiling this on Linux, make sure to add the ""-rdynamic""; option as well. This makes sure that the external functions are resolved; properly at runtime. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter4/toy.cpp; :language: c++. `Next: Extending the language: control flow <LangImpl05.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:25184,Modifiability,config,config,25184,"vailable to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. If you are compiling this on Linux, make sure to add the ""-rdynamic""; option as well. This makes sure that the external functions are resolved; properly at runtime. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter4/toy.cpp; :language: c++. `Next: Extending the language: control flow <LangImpl05.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:464,Performance,optimiz,optimizer,464,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:845,Performance,optimiz,optimizations,845,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:1443,Performance,optimiz,optimization,1443,"ing optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""syntactic; overhead"" for its use (you don't have to uglify your compiler with; constant checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for languages with a; macro preprocessor or that use a lot of constants). On the other hand, the ``IRBuilder`` is limited by the fact that it does; all of its analysis inline with the code as it is built. If you take a; slightly more complex example:. ::. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:3226,Performance,optimiz,optimizations,3226,"o preprocessor or that use a lot of constants). On the other hand, the ``IRBuilder`` is limited by the fact that it does; all of its analysis inline with the code as it is built. If you take a; slightly more complex example:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `L",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:3355,Performance,optimiz,optimization,3355,"ode as it is built. If you take a; slightly more complex example:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We are",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:3527,Performance,optimiz,optimizations,3527,"n definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff wher",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:3667,Performance,optimiz,optimizations,3667,"ltmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""stat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:4422,Performance,optimiz,optimization,4422,"er systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassMa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:4582,Performance,optimiz,optimizations,4582," situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeMod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:4771,Performance,optimiz,optimizer,4771,"on. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; functions. For more information on passes and how they are run, see the; `How to Write a Pass <../../WritingAnLLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMC",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:5178,Performance,optimiz,optimizations,5178,"LLVMPass.html>`_ document and the; `List of LLVM Passes <../../Passes.html>`_. For Kaleidoscope, we are currently generating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""KaleidoscopeJIT"", *TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create new pass and analysis managers.; TheFPM = std::make_unique<FunctionPassManager>();; TheLAM = std::make_unique<LoopAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysis",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:5328,Performance,optimiz,optimizations,5328,"ating functions on the fly, one; at a time, as the user types them in. We aren't shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""KaleidoscopeJIT"", *TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create new pass and analysis managers.; TheFPM = std::make_unique<FunctionPassManager>();; TheLAM = std::make_unique<LoopAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysisManager>();; TheMAM = std::make_unique<ModuleAnalysisManager>();; ThePIC = std::make_unique<PassInstrumentationCallbac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:5403,Performance,optimiz,optimizations,5403,"t shooting for the; ultimate optimization experience in this setting, but we also want to; catch the easy and quick stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""KaleidoscopeJIT"", *TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create new pass and analysis managers.; TheFPM = std::make_unique<FunctionPassManager>();; TheLAM = std::make_unique<LoopAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysisManager>();; TheMAM = std::make_unique<ModuleAnalysisManager>();; ThePIC = std::make_unique<PassInstrumentationCallbacks>();; TheSI = std::make_unique<StandardInstrumentations>(*TheContext,; /*Debu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:5495,Performance,optimiz,optimize,5495,"k stuff where possible. As such, we will choose; to run a few per-function optimizations as the user types the function; in. If we wanted to make a ""static Kaleidoscope compiler"", we would use; exactly the code we have now, except that we would defer running the; optimizer until the entire file has been parsed. In addition to the distinction between function and module passes, passes can be; divided into transform and analysis passes. Transform passes mutate the IR, and; analysis passes compute information that other passes can use. In order to add; a transform pass, all analysis passes it depends upon must be registered in; advance. In order to get per-function optimizations going, we need to set up a; `FunctionPassManager <../../WritingAnLLVMPass.html#what-passmanager-doesr>`_ to hold; and organize the LLVM optimizations that we want to run. Once we have; that, we can add a set of optimizations to run. We'll need a new; FunctionPassManager for each module that we want to optimize, so we'll; add to a function created in the previous chapter (``InitializeModule()``):. .. code-block:: c++. void InitializeModuleAndManagers(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""KaleidoscopeJIT"", *TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create new pass and analysis managers.; TheFPM = std::make_unique<FunctionPassManager>();; TheLAM = std::make_unique<LoopAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysisManager>();; TheMAM = std::make_unique<ModuleAnalysisManager>();; ThePIC = std::make_unique<PassInstrumentationCallbacks>();; TheSI = std::make_unique<StandardInstrumentations>(*TheContext,; /*DebugLogging*/ true);; TheSI->registerCallbacks(*ThePIC, TheMAM.get());; ... After initializing the global module ``The",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:7072,Performance,optimiz,optimizations,7072,"opAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysisManager>();; TheMAM = std::make_unique<ModuleAnalysisManager>();; ThePIC = std::make_unique<PassInstrumentationCallbacks>();; TheSI = std::make_unique<StandardInstrumentations>(*TheContext,; /*DebugLogging*/ true);; TheSI->registerCallbacks(*ThePIC, TheMAM.get());; ... After initializing the global module ``TheModule`` and the FunctionPassManager,; we need to initialize other parts of the framework. The four AnalysisManagers; allow us to add analysis passes that run across the four levels of the IR; hierarchy. PassInstrumentationCallbacks and StandardInstrumentations are; required for the pass instrumentation framework, which allows developers to; customize what happens between passes. Once these managers are set up, we use a series of ""addPass"" calls to add a; bunch of LLVM transform passes:. .. code-block:: c++. // Add transform passes.; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->addPass(InstCombinePass());; // Reassociate expressions.; TheFPM->addPass(ReassociatePass());; // Eliminate Common SubExpressions.; TheFPM->addPass(GVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; TheFPM->addPass(SimplifyCFGPass());. In this case, we choose to add four optimization passes.; The passes we choose here are a pretty standard set; of ""cleanup"" optimizations that are useful for a wide variety of code. I won't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created fun",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:7427,Performance,optimiz,optimization,7427,">registerCallbacks(*ThePIC, TheMAM.get());; ... After initializing the global module ``TheModule`` and the FunctionPassManager,; we need to initialize other parts of the framework. The four AnalysisManagers; allow us to add analysis passes that run across the four levels of the IR; hierarchy. PassInstrumentationCallbacks and StandardInstrumentations are; required for the pass instrumentation framework, which allows developers to; customize what happens between passes. Once these managers are set up, we use a series of ""addPass"" calls to add a; bunch of LLVM transform passes:. .. code-block:: c++. // Add transform passes.; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->addPass(InstCombinePass());; // Reassociate expressions.; TheFPM->addPass(ReassociatePass());; // Eliminate Common SubExpressions.; TheFPM->addPass(GVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; TheFPM->addPass(SimplifyCFGPass());. In this case, we choose to add four optimization passes.; The passes we choose here are a pretty standard set; of ""cleanup"" optimizations that are useful for a wide variety of code. I won't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:7515,Performance,optimiz,optimizations,7515,"odule`` and the FunctionPassManager,; we need to initialize other parts of the framework. The four AnalysisManagers; allow us to add analysis passes that run across the four levels of the IR; hierarchy. PassInstrumentationCallbacks and StandardInstrumentations are; required for the pass instrumentation framework, which allows developers to; customize what happens between passes. Once these managers are set up, we use a series of ""addPass"" calls to add a; bunch of LLVM transform passes:. .. code-block:: c++. // Add transform passes.; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->addPass(InstCombinePass());; // Reassociate expressions.; TheFPM->addPass(ReassociatePass());; // Eliminate Common SubExpressions.; TheFPM->addPass(GVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; TheFPM->addPass(SimplifyCFGPass());. In this case, we choose to add four optimization passes.; The passes we choose here are a pretty standard set; of ""cleanup"" optimizations that are useful for a wide variety of code. I won't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straigh",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:8549,Performance,optimiz,optimizes,8549,"on't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:8946,Performance,optimiz,optimized,8946," PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump it out in textual or binary forms, you can compile; the code to an assembly file (.s) for some target, or you can JIT; compile it. The nice thing about the LLVM IR representation is that it; is the ""common curren",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:9074,Performance,optimiz,optimizations,9074,"s constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump it out in textual or binary forms, you can compile; the code to an assembly file (.s) for some target, or you can JIT; compile it. The nice thing about the LLVM IR representation is that it; is the ""common currency"" between many different parts of the compiler. In this section, we'll add JIT compiler support to our inter",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:9711,Performance,optimiz,optimizations,9711,"define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump it out in textual or binary forms, you can compile; the code to an assembly file (.s) for some target, or you can JIT; compile it. The nice thing about the LLVM IR representation is that it; is the ""common currency"" between many different parts of the compiler. In this section, we'll add JIT compiler support to our interpreter. The; basic idea that we want for Kaleidoscope is to have the user enter; function bodies as they do now, but immediately evaluate the top-level; expressions they type in. For example, if they type in ""1 + 2;"", we; should evaluate and print out 3. If they define a function, they should; be able to call it from the command line. In order to do this, we first prepare the environment to create code for; the current native target and declare and initialize the JIT. This is; done by calling some ``InitializeNativeTarget\*`` functions and; adding a global variable ``TheJIT``, and initializing it in; ``main``:. .. code-block:: c++. static std::unique_ptr<KaleidoscopeJIT> ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:21300,Performance,concurren,concurrent,21300,"else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close to the static and dynamic linker rules, including rejecting duplicate symbols. Requiring symbol names to be unique allows us to support concurrent compilation for symbols using the (unique) symbol names as keys for tracking. With these changes made, let's try our REPL again (I removed the dump of the; anonymous functions this time, you should get the idea by now :) :. ::. ready> def foo(x) x + 1;; ready> foo(2);; Evaluated to 3.000000. ready> def foo(x) x + 2;; ready> foo(2);; Evaluated to 4.000000. It works!. Even with this simple code, we get some surprisingly powerful capabilities -; check this out:. ::. ready> extern sin(x);; Read extern:; declare double @sin(double). ready> extern cos(x);; Read extern:; declare double @cos(double). ready> sin(1.0);; Read top-level expression:; define double @2() {; entry:; ret double 0x3FEAED548F090CEE; }. Evaluated to 0.841471. ready> def foo(x) sin(x)*sin(x) + cos(x)*cos(x);; Read function definition:; define double @foo(double %x) {; entry:; %calltmp = call double @sin(double %x); %multmp = fmul double %calltmp, %calltmp; %calltmp2 = call double @cos(double %x); %multmp4 = fmul double %calltm",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:24240,Performance,load,loader,24240," this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:24626,Performance,optimiz,optimizer,24626,"vailable to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. If you are compiling this on Linux, make sure to add the ""-rdynamic""; option as well. This makes sure that the external functions are resolved; properly at runtime. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter4/toy.cpp; :language: c++. `Next: Extending the language: control flow <LangImpl05.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:24750,Performance,optimiz,optimize,24750,"vailable to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. If you are compiling this on Linux, make sure to add the ""-rdynamic""; option as well. This makes sure that the external functions are resolved; properly at runtime. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter4/toy.cpp; :language: c++. `Next: Extending the language: control flow <LangImpl05.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:25086,Performance,optimiz,optimizer,25086,"vailable to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. If you are compiling this on Linux, make sure to add the ""-rdynamic""; option as well. This makes sure that the external functions are resolved; properly at runtime. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter4/toy.cpp; :language: c++. `Next: Extending the language: control flow <LangImpl05.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:2965,Safety,detect,detect,2965,"Builder`` when generating code like this. It has no ""syntactic; overhead"" for its use (you don't have to uglify your compiler with; constant checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for languages with a; macro preprocessor or that use a lot of constants). On the other hand, the ``IRBuilder`` is limited by the fact that it does; all of its analysis inline with the code as it is built. If you take a; slightly more complex example:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; pro",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:3154,Safety,redund,redundant,3154,"nt checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for languages with a; macro preprocessor or that use a lot of constants). On the other hand, the ``IRBuilder`` is limited by the fact that it does; all of its analysis inline with the code as it is built. If you take a; slightly more complex example:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of optimizations is right for; all languages and for all situations. LLVM allows a compiler implementor; to make complete decisions about what optimizations to use, in which; order, and in what situation. As a concrete example, LLVM supports both ""whole module"" passes, which; look across as large of body of code as they can (often a whole file,; but if run at link time, this can be a substantial portion of the whole; program). It also supports and includes ""per-function"" passes which just; operate on a single function at a time, without looking at other; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:20347,Safety,recover,recovery,20347," this, we'll start by adding a new global, ``FunctionProtos``, that; holds the most recent prototype for each function. We'll also add a convenience; method, ``getFunction()``, to replace calls to ``TheModule->getFunction()``.; Our convenience method searches ``TheModule`` for an existing function; declaration, falling back to generating a new declaration from FunctionProtos if; it doesn't find one. In ``CallExprAST::codegen()`` we just need to replace the; call to ``TheModule->getFunction()``. In ``FunctionAST::codegen()`` we need to; update the FunctionProtos map first, then call ``getFunction()``. With this; done, we can always obtain a function declaration in the current module for any; previously declared function. We also need to update HandleDefinition and HandleExtern:. .. code-block:: c++. static void HandleDefinition() {; if (auto FnAST = ParseDefinition()) {; if (auto *FnIR = FnAST->codegen()) {; fprintf(stderr, ""Read function definition:"");; FnIR->print(errs());; fprintf(stderr, ""\n"");; ExitOnErr(TheJIT->addModule(; ThreadSafeModule(std::move(TheModule), std::move(TheContext))));; InitializeModuleAndPassManager();; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:20668,Safety,recover,recovery,20668,"FunctionProtos if; it doesn't find one. In ``CallExprAST::codegen()`` we just need to replace the; call to ``TheModule->getFunction()``. In ``FunctionAST::codegen()`` we need to; update the FunctionProtos map first, then call ``getFunction()``. With this; done, we can always obtain a function declaration in the current module for any; previously declared function. We also need to update HandleDefinition and HandleExtern:. .. code-block:: c++. static void HandleDefinition() {; if (auto FnAST = ParseDefinition()) {; if (auto *FnIR = FnAST->codegen()) {; fprintf(stderr, ""Read function definition:"");; FnIR->print(errs());; fprintf(stderr, ""\n"");; ExitOnErr(TheJIT->addModule(; ThreadSafeModule(std::move(TheModule), std::move(TheContext))));; InitializeModuleAndPassManager();; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. static void HandleExtern() {; if (auto ProtoAST = ParseExtern()) {; if (auto *FnIR = ProtoAST->codegen()) {; fprintf(stderr, ""Read extern: "");; FnIR->print(errs());; fprintf(stderr, ""\n"");; FunctionProtos[ProtoAST->getName()] = std::move(ProtoAST);; }; } else {; // Skip token for error recovery.; getNextToken();; }; }. In HandleDefinition, we add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close to the static and dynamic linker rules, including rejecting duplicate symbols. Requiring symbol names to be unique allows us to support concurrent compilation for symbols using the (unique) symbol names as keys for tracking. With these changes made, let's try our REPL again (I removed the dump of the; anonymous functions this time, you should get the idea by now ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:23570,Security,secur,security,23570,"s the JIT know about sin and cos? The answer is surprisingly; simple: The KaleidoscopeJIT has a straightforward symbol resolution rule that; it uses to find symbols that aren't available in any given module: First; it searches all the modules that have already been added to the JIT, from the; most recent to the oldest, to find the newest definition. If no definition is; found inside the JIT, it falls back to calling ""``dlsym(""sin"")``"" on the; Kaleidoscope process itself. Since ""``sin``"" is defined within the JIT's; address space, it simply patches up calls in the module to call the libm; version of ``sin`` directly. But in some cases this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:903,Testability,test,test,903,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:961,Testability,test,test,961,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:1163,Testability,test,test,1163,"===. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""syntactic; overhead"" for its use (you don't have to uglify your compiler with; constant checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for lan",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:1221,Testability,test,test,1221,"===. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""syntactic; overhead"" for its use (you don't have to uglify your compiler with; constant checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for lan",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:2472,Testability,test,test,2472,"pport in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""syntactic; overhead"" for its use (you don't have to uglify your compiler with; constant checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for languages with a; macro preprocessor or that use a lot of constants). On the other hand, the ``IRBuilder`` is limited by the fact that it does; all of its analysis inline with the code as it is built. If you take a; slightly more complex example:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of opt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:2549,Testability,test,test,2549,"pport in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""syntactic; overhead"" for its use (you don't have to uglify your compiler with; constant checks everywhere) and it can dramatically reduce the amount of; LLVM IR that is generated in some cases (particular for languages with a; macro preprocessor or that use a lot of constants). On the other hand, the ``IRBuilder`` is limited by the fact that it does; all of its analysis inline with the code as it is built. If you take a; slightly more complex example:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; %addtmp1 = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp1; ret double %multmp; }. In this case, the LHS and RHS of the multiplication are the same value.; We'd really like to see this generate ""``tmp = x+3; result = tmp*tmp;``""; instead of computing ""``x+3``"" twice. Unfortunately, no amount of local analysis will be able to detect and; correct this. This requires two transformations: reassociation of; expressions (to make the add's lexically identical) and Common; Subexpression Elimination (CSE) to delete the redundant add instruction.; Fortunately, LLVM provides a broad range of optimizations that you can; use, in the form of ""passes"". LLVM Optimization Passes; ========================. LLVM provides many optimization passes, which do many different sorts of; things and have different tradeoffs. Unlike other systems, LLVM doesn't; hold to the mistaken notion that one set of opt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:8670,Testability,test,test,8670,"xt, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:8704,Testability,test,test,8704,"sis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:8781,Testability,test,test,8781,"sis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created function is constructed (in; ``FunctionAST::codegen()``), but before it is returned to the client:. .. code-block:: c++. if (Value *RetVal = Body->codegen()) {; // Finish off the function.; Builder.CreateRet(RetVal);. // Validate the generated code, checking for consistency.; verifyFunction(*TheFunction);. // Optimize the function.; TheFPM->run(*TheFunction, *TheFAM);. return TheFunction;; }. As you can see, this is pretty straightforward. The; ``FunctionPassManager`` optimizes and updates the LLVM Function\* in; place, improving (hopefully) its body. With this in place, we can try; our test above again:. ::. ready> def test(x) (1+2+x)*(x+(1+2));; ready> Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double %x, 3.000000e+00; %multmp = fmul double %addtmp, %addtmp; ret double %multmp; }. As expected, we now get our nicely optimized code, saving a floating; point add instruction from every execution of this function. LLVM provides a wide variety of optimizations that can be used in; certain circumstances. Some `documentation about the various; passes <../../Passes.html>`_ is available, but it isn't very complete.; Another good source of ideas can come from looking at the passes that; ``Clang`` runs to get started. The ""``opt``"" tool allows you to; experiment with passes from the command line, so you can see if they do; anything. Now that we have reasonable code coming out of our front-end, let's talk; about executing it!. Adding a JIT Compiler; =====================. Code that is available in LLVM IR can have a wide variety of tools; applied to it. For example, you can run optimizations on it (as we did; above), you can dump ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:13259,Testability,assert,assert,13259,"Module`` adds an LLVM IR module to the JIT, making its functions; available for execution (with its memory managed by a ``ResourceTracker``); and; ``lookup`` allows us to look up pointers to the compiled code. We can take this simple API and change our code that parses top-level expressions to; look like this:. .. code-block:: c++. static ExitOnError ExitOnErr;; ...; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; if (FnAST->codegen()) {; // Create a ResourceTracker to track JIT'd memory allocated to our; // anonymous expression -- that way we can free it after executing.; auto RT = TheJIT->getMainJITDylib().createResourceTracker();. auto TSM = ThreadSafeModule(std::move(TheModule), std::move(TheContext));; ExitOnErr(TheJIT->addModule(std::move(TSM), RT));; InitializeModuleAndPassManager();. // Search the JIT for the __anon_expr symbol.; auto ExprSymbol = ExitOnErr(TheJIT->lookup(""__anon_expr""));; assert(ExprSymbol && ""Function not found"");. // Get the symbol's address and cast it to the right type (takes no; // arguments, returns a double) so we can call it as a native function.; double (*FP)() = ExprSymbol.getAddress().toPtr<double (*)()>();; fprintf(stderr, ""Evaluated to %f\n"", FP());. // Delete the anonymous expression module from the JIT.; ExitOnErr(RT->remove());; }. If parsing and codegen succeed, the next step is to add the module containing; the top-level expression to the JIT. We do this by calling addModule, which; triggers code generation for all the functions in the module, and accepts a; ``ResourceTracker`` which can be used to remove the module from the JIT later. Once the module; has been added to the JIT it can no longer be modified, so we also open a new; module to hold subsequent code by calling ``InitializeModuleAndPassManager()``. Once we've added the module to the JIT we need to get a pointer to the final; generated code. We do this by calling the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:14391,Testability,assert,assert,14391,"rns a double) so we can call it as a native function.; double (*FP)() = ExprSymbol.getAddress().toPtr<double (*)()>();; fprintf(stderr, ""Evaluated to %f\n"", FP());. // Delete the anonymous expression module from the JIT.; ExitOnErr(RT->remove());; }. If parsing and codegen succeed, the next step is to add the module containing; the top-level expression to the JIT. We do this by calling addModule, which; triggers code generation for all the functions in the module, and accepts a; ``ResourceTracker`` which can be used to remove the module from the JIT later. Once the module; has been added to the JIT it can no longer be modified, so we also open a new; module to hold subsequent code by calling ``InitializeModuleAndPassManager()``. Once we've added the module to the JIT we need to get a pointer to the final; generated code. We do this by calling the JIT's ``lookup`` method, and passing; the name of the top-level expression function: ``__anon_expr``. Since we just; added this function, we assert that ``lookup`` returned a result. Next, we get the in-memory address of the ``__anon_expr`` function by calling; ``getAddress()`` on the symbol. Recall that we compile top-level expressions; into a self-contained LLVM function that takes no arguments and returns the; computed double. Because the LLVM JIT compiler matches the native platform ABI,; this means that you can just cast the result pointer to a function pointer of; that type and call it directly. This means, there is no difference between JIT; compiled code and native machine code that is statically linked into your; application. Finally, since we don't support re-evaluation of top-level expressions, we; remove the module from the JIT when we're done to free the associated memory.; Recall, however, that the module we created a few lines earlier (via; ``InitializeModuleAndPassManager``) is still open and waiting for new code to be; added. With just these two changes, let's see how Kaleidoscope works now!. ::. ready> 4+5;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:15784,Testability,test,testfunc,15784,"it directly. This means, there is no difference between JIT; compiled code and native machine code that is statically linked into your; application. Finally, since we don't support re-evaluation of top-level expressions, we; remove the module from the JIT when we're done to free the associated memory.; Recall, however, that the module we created a few lines earlier (via; ``InitializeModuleAndPassManager``) is still open and waiting for new code to be; added. With just these two changes, let's see how Kaleidoscope works now!. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Evaluated to 9.000000. Well this looks like it is basically working. The dump of the function; shows the ""no argument function that always returns double"" that we; synthesize for each top-level expression that is typed in. This; demonstrates very basic functionality, but can we do more?. ::. ready> def testfunc(x y) x + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:15850,Testability,test,testfunc,15850,"it directly. This means, there is no difference between JIT; compiled code and native machine code that is statically linked into your; application. Finally, since we don't support re-evaluation of top-level expressions, we; remove the module from the JIT when we're done to free the associated memory.; Recall, however, that the module we created a few lines earlier (via; ``InitializeModuleAndPassManager``) is still open and waiting for new code to be; added. With just these two changes, let's see how Kaleidoscope works now!. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Evaluated to 9.000000. Well this looks like it is basically working. The dump of the function; shows the ""no argument function that always returns double"" that we; synthesize for each top-level expression that is typed in. This; demonstrates very basic functionality, but can we do more?. ::. ready> def testfunc(x y) x + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:15997,Testability,test,testfunc,15997,"l expressions, we; remove the module from the JIT when we're done to free the associated memory.; Recall, however, that the module we created a few lines earlier (via; ``InitializeModuleAndPassManager``) is still open and waiting for new code to be; added. With just these two changes, let's see how Kaleidoscope works now!. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Evaluated to 9.000000. Well this looks like it is basically working. The dump of the function; shows the ""no argument function that always returns double"" that we; synthesize for each top-level expression that is typed in. This; demonstrates very basic functionality, but can we do more?. ::. ready> def testfunc(x y) x + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The easiest way to fix this is to put the anonymous expression in a separate; module from the rest of the function definitions. The JIT will happily resolve; function calls across module boundaries, as long as",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:16097,Testability,test,testfunc,16097,"l expressions, we; remove the module from the JIT when we're done to free the associated memory.; Recall, however, that the module we created a few lines earlier (via; ``InitializeModuleAndPassManager``) is still open and waiting for new code to be; added. With just these two changes, let's see how Kaleidoscope works now!. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Evaluated to 9.000000. Well this looks like it is basically working. The dump of the function; shows the ""no argument function that always returns double"" that we; synthesize for each top-level expression that is typed in. This; demonstrates very basic functionality, but can we do more?. ::. ready> def testfunc(x y) x + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The easiest way to fix this is to put the anonymous expression in a separate; module from the rest of the function definitions. The JIT will happily resolve; function calls across module boundaries, as long as",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:16204,Testability,test,testfunc,16204,") is still open and waiting for new code to be; added. With just these two changes, let's see how Kaleidoscope works now!. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Evaluated to 9.000000. Well this looks like it is basically working. The dump of the function; shows the ""no argument function that always returns double"" that we; synthesize for each top-level expression that is typed in. This; demonstrates very basic functionality, but can we do more?. ::. ready> def testfunc(x y) x + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The easiest way to fix this is to put the anonymous expression in a separate; module from the rest of the function definitions. The JIT will happily resolve; function calls across module boundaries, as long as each of the functions called; has a prototype, and is added to the JIT before it is called. By putting the; anonymous expression in a different module we can delete it without affecting; the rest of th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:16273,Testability,test,testfunc,16273,") is still open and waiting for new code to be; added. With just these two changes, let's see how Kaleidoscope works now!. ::. ready> 4+5;; Read top-level expression:; define double @0() {; entry:; ret double 9.000000e+00; }. Evaluated to 9.000000. Well this looks like it is basically working. The dump of the function; shows the ""no argument function that always returns double"" that we; synthesize for each top-level expression that is typed in. This; demonstrates very basic functionality, but can we do more?. ::. ready> def testfunc(x y) x + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The easiest way to fix this is to put the anonymous expression in a separate; module from the rest of the function definitions. The JIT will happily resolve; function calls across module boundaries, as long as each of the functions called; has a prototype, and is added to the JIT before it is called. By putting the; anonymous expression in a different module we can delete it without affecting; the rest of th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:16533,Testability,test,testfunc,16533,"ll this looks like it is basically working. The dump of the function; shows the ""no argument function that always returns double"" that we; synthesize for each top-level expression that is typed in. This; demonstrates very basic functionality, but can we do more?. ::. ready> def testfunc(x y) x + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The easiest way to fix this is to put the anonymous expression in a separate; module from the rest of the function definitions. The JIT will happily resolve; function calls across module boundaries, as long as each of the functions called; has a prototype, and is added to the JIT before it is called. By putting the; anonymous expression in a different module we can delete it without affecting; the rest of the functions. In fact, we're going to go a step further and put every function in its own; module. Doing so allows us to exploit a useful property of the KaleidoscopeJIT; that will make our environment more REPL-like: Functions can be added to the; JIT ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:16731,Testability,test,testfunc,16731,"ion that is typed in. This; demonstrates very basic functionality, but can we do more?. ::. ready> def testfunc(x y) x + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The easiest way to fix this is to put the anonymous expression in a separate; module from the rest of the function definitions. The JIT will happily resolve; function calls across module boundaries, as long as each of the functions called; has a prototype, and is added to the JIT before it is called. By putting the; anonymous expression in a different module we can delete it without affecting; the rest of the functions. In fact, we're going to go a step further and put every function in its own; module. Doing so allows us to exploit a useful property of the KaleidoscopeJIT; that will make our environment more REPL-like: Functions can be added to the; JIT more than once (unlike a module where every function must have a unique; definition). When you look up a symbol in KaleidoscopeJIT it will always return; the most recent defini",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:16786,Testability,test,testfunc,16786," + y*2;; Read function definition:; define double @testfunc(double %x, double %y) {; entry:; %multmp = fmul double %y, 2.000000e+00; %addtmp = fadd double %multmp, %x; ret double %addtmp; }. ready> testfunc(4, 10);; Read top-level expression:; define double @1() {; entry:; %calltmp = call double @testfunc(double 4.000000e+00, double 1.000000e+01); ret double %calltmp; }. Evaluated to 24.000000. ready> testfunc(5, 10);; ready> LLVM ERROR: Program used external function 'testfunc' which could not be resolved!. Function definitions and calls also work, but something went very wrong on that; last line. The call looks valid, so what happened? As you may have guessed from; the API a Module is a unit of allocation for the JIT, and testfunc was part; of the same module that contained anonymous expression. When we removed that; module from the JIT to free the memory for the anonymous expression, we deleted; the definition of ``testfunc`` along with it. Then, when we tried to call; testfunc a second time, the JIT could no longer find it. The easiest way to fix this is to put the anonymous expression in a separate; module from the rest of the function definitions. The JIT will happily resolve; function calls across module boundaries, as long as each of the functions called; has a prototype, and is added to the JIT before it is called. By putting the; anonymous expression in a different module we can delete it without affecting; the rest of the functions. In fact, we're going to go a step further and put every function in its own; module. Doing so allows us to exploit a useful property of the KaleidoscopeJIT; that will make our environment more REPL-like: Functions can be added to the; JIT more than once (unlike a module where every function must have a unique; definition). When you look up a symbol in KaleidoscopeJIT it will always return; the most recent definition:. ::. ready> def foo(x) x + 1;; Read function definition:; define double @foo(double %x) {; entry:; %addtmp = fa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:355,Usability,simpl,simple,355,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:874,Usability,simpl,simple,874,"==============================================; Kaleidoscope: Adding JIT and Optimizer Support; ==============================================. .. contents::; :local:. Chapter 4 Introduction; ======================. Welcome to Chapter 4 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Chapters 1-3 described the implementation; of a simple language and added support for generating LLVM IR. This; chapter describes two new techniques: adding optimizer support to your; language, and adding JIT compiler support. These additions will; demonstrate how to get nice, efficient code for the Kaleidoscope; language. Trivial Constant Folding; ========================. Our demonstration for Chapter 3 is elegant and easy to extend.; Unfortunately, it does not produce wonderful code. The IRBuilder,; however, does give us obvious optimizations when compiling simple code:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 3.000000e+00, %x; ret double %addtmp; }. This code is not a literal transcription of the AST built by parsing the; input. That would be:. ::. ready> def test(x) 1+2+x;; Read function definition:; define double @test(double %x) {; entry:; %addtmp = fadd double 2.000000e+00, 1.000000e+00; %addtmp1 = fadd double %addtmp, %x; ret double %addtmp1; }. Constant folding, as seen above, in particular, is a very common and; very important optimization: so much so that many language implementors; implement constant folding support in their AST representation. With LLVM, you don't need this support in the AST. Since all calls to; build LLVM IR go through the LLVM IR builder, the builder itself checked; to see if there was a constant folding opportunity when you call it. If; so, it just does the constant fold and return the constant instead of; creating an instruction. Well, that was easy :). In practice, we recommend always using; ``IRBuilder`` when generating code like this. It has no ""s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:7054,Usability,simpl,simple,7054,"opAnalysisManager>();; TheFAM = std::make_unique<FunctionAnalysisManager>();; TheCGAM = std::make_unique<CGSCCAnalysisManager>();; TheMAM = std::make_unique<ModuleAnalysisManager>();; ThePIC = std::make_unique<PassInstrumentationCallbacks>();; TheSI = std::make_unique<StandardInstrumentations>(*TheContext,; /*DebugLogging*/ true);; TheSI->registerCallbacks(*ThePIC, TheMAM.get());; ... After initializing the global module ``TheModule`` and the FunctionPassManager,; we need to initialize other parts of the framework. The four AnalysisManagers; allow us to add analysis passes that run across the four levels of the IR; hierarchy. PassInstrumentationCallbacks and StandardInstrumentations are; required for the pass instrumentation framework, which allows developers to; customize what happens between passes. Once these managers are set up, we use a series of ""addPass"" calls to add a; bunch of LLVM transform passes:. .. code-block:: c++. // Add transform passes.; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->addPass(InstCombinePass());; // Reassociate expressions.; TheFPM->addPass(ReassociatePass());; // Eliminate Common SubExpressions.; TheFPM->addPass(GVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; TheFPM->addPass(SimplifyCFGPass());. In this case, we choose to add four optimization passes.; The passes we choose here are a pretty standard set; of ""cleanup"" optimizations that are useful for a wide variety of code. I won't; delve into what they do but, believe me, they are a good starting place :). Next, we register the analysis passes used by the transform passes. .. code-block:: c++. // Register analysis passes used in these transform passes.; PassBuilder PB;; PB.registerModuleAnalyses(*TheMAM);; PB.registerFunctionAnalyses(*TheFAM);; PB.crossRegisterProxies(*TheLAM, *TheFAM, *TheCGAM, *TheMAM);; }. Once the PassManager is set up, we need to make use of it. We do this by; running it after our newly created fun",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:11841,Usability,simpl,simple,11841,"tall standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;; BinopPrecedence['*'] = 40; // highest. // Prime the first token.; fprintf(stderr, ""ready> "");; getNextToken();. TheJIT = std::make_unique<KaleidoscopeJIT>();. // Run the main ""interpreter loop"" now.; MainLoop();. return 0;; }. We also need to setup the data layout for the JIT:. .. code-block:: c++. void InitializeModuleAndPassManager(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""my cool jit"", TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create a new pass manager attached to it.; TheFPM = std::make_unique<legacy::FunctionPassManager>(TheModule.get());; ... The KaleidoscopeJIT class is a simple JIT built specifically for these; tutorials, available inside the LLVM source code; at `llvm-src/examples/Kaleidoscope/include/KaleidoscopeJIT.h; <https://github.com/llvm/llvm-project/blob/main/llvm/examples/Kaleidoscope/include/KaleidoscopeJIT.h>`_.; In later chapters we will look at how it works and extend it with; new features, but for now we will take it as given. Its API is very simple:; ``addModule`` adds an LLVM IR module to the JIT, making its functions; available for execution (with its memory managed by a ``ResourceTracker``); and; ``lookup`` allows us to look up pointers to the compiled code. We can take this simple API and change our code that parses top-level expressions to; look like this:. .. code-block:: c++. static ExitOnError ExitOnErr;; ...; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; if (FnAST->codegen()) {; // Create a ResourceTracker to track JIT'd memory allocated to our; // anonymous expression -- that way we ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:12235,Usability,simpl,simple,12235,": c++. void InitializeModuleAndPassManager(void) {; // Open a new context and module.; TheContext = std::make_unique<LLVMContext>();; TheModule = std::make_unique<Module>(""my cool jit"", TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create a new pass manager attached to it.; TheFPM = std::make_unique<legacy::FunctionPassManager>(TheModule.get());; ... The KaleidoscopeJIT class is a simple JIT built specifically for these; tutorials, available inside the LLVM source code; at `llvm-src/examples/Kaleidoscope/include/KaleidoscopeJIT.h; <https://github.com/llvm/llvm-project/blob/main/llvm/examples/Kaleidoscope/include/KaleidoscopeJIT.h>`_.; In later chapters we will look at how it works and extend it with; new features, but for now we will take it as given. Its API is very simple:; ``addModule`` adds an LLVM IR module to the JIT, making its functions; available for execution (with its memory managed by a ``ResourceTracker``); and; ``lookup`` allows us to look up pointers to the compiled code. We can take this simple API and change our code that parses top-level expressions to; look like this:. .. code-block:: c++. static ExitOnError ExitOnErr;; ...; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; if (FnAST->codegen()) {; // Create a ResourceTracker to track JIT'd memory allocated to our; // anonymous expression -- that way we can free it after executing.; auto RT = TheJIT->getMainJITDylib().createResourceTracker();. auto TSM = ThreadSafeModule(std::move(TheModule), std::move(TheContext));; ExitOnErr(TheJIT->addModule(std::move(TSM), RT));; InitializeModuleAndPassManager();. // Search the JIT for the __anon_expr symbol.; auto ExprSymbol = ExitOnErr(TheJIT->lookup(""__anon_expr""));; assert(ExprSymbol && ""Function not found"");. // Get the symbol's address and ca",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:12476,Usability,simpl,simple,12476,"""my cool jit"", TheContext);; TheModule->setDataLayout(TheJIT->getDataLayout());. // Create a new builder for the module.; Builder = std::make_unique<IRBuilder<>>(*TheContext);. // Create a new pass manager attached to it.; TheFPM = std::make_unique<legacy::FunctionPassManager>(TheModule.get());; ... The KaleidoscopeJIT class is a simple JIT built specifically for these; tutorials, available inside the LLVM source code; at `llvm-src/examples/Kaleidoscope/include/KaleidoscopeJIT.h; <https://github.com/llvm/llvm-project/blob/main/llvm/examples/Kaleidoscope/include/KaleidoscopeJIT.h>`_.; In later chapters we will look at how it works and extend it with; new features, but for now we will take it as given. Its API is very simple:; ``addModule`` adds an LLVM IR module to the JIT, making its functions; available for execution (with its memory managed by a ``ResourceTracker``); and; ``lookup`` allows us to look up pointers to the compiled code. We can take this simple API and change our code that parses top-level expressions to; look like this:. .. code-block:: c++. static ExitOnError ExitOnErr;; ...; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; if (FnAST->codegen()) {; // Create a ResourceTracker to track JIT'd memory allocated to our; // anonymous expression -- that way we can free it after executing.; auto RT = TheJIT->getMainJITDylib().createResourceTracker();. auto TSM = ThreadSafeModule(std::move(TheModule), std::move(TheContext));; ExitOnErr(TheJIT->addModule(std::move(TSM), RT));; InitializeModuleAndPassManager();. // Search the JIT for the __anon_expr symbol.; auto ExprSymbol = ExitOnErr(TheJIT->lookup(""__anon_expr""));; assert(ExprSymbol && ""Function not found"");. // Get the symbol's address and cast it to the right type (takes no; // arguments, returns a double) so we can call it as a native function.; double (*FP)() = ExprSymbol.getAddress().toPtr<double (*)()>();;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:21695,Usability,simpl,simple,21695,"add two lines to transfer the newly defined function to; the JIT and open a new module. In HandleExtern, we just need to add one line to; add the prototype to FunctionProtos. .. warning::; Duplication of symbols in separate modules is not allowed since LLVM-9. That means you can not redefine function in your Kaleidoscope as its shown below. Just skip this part. The reason is that the newer OrcV2 JIT APIs are trying to stay very close to the static and dynamic linker rules, including rejecting duplicate symbols. Requiring symbol names to be unique allows us to support concurrent compilation for symbols using the (unique) symbol names as keys for tracking. With these changes made, let's try our REPL again (I removed the dump of the; anonymous functions this time, you should get the idea by now :) :. ::. ready> def foo(x) x + 1;; ready> foo(2);; Evaluated to 3.000000. ready> def foo(x) x + 2;; ready> foo(2);; Evaluated to 4.000000. It works!. Even with this simple code, we get some surprisingly powerful capabilities -; check this out:. ::. ready> extern sin(x);; Read extern:; declare double @sin(double). ready> extern cos(x);; Read extern:; declare double @cos(double). ready> sin(1.0);; Read top-level expression:; define double @2() {; entry:; ret double 0x3FEAED548F090CEE; }. Evaluated to 0.841471. ready> def foo(x) sin(x)*sin(x) + cos(x)*cos(x);; Read function definition:; define double @foo(double %x) {; entry:; %calltmp = call double @sin(double %x); %multmp = fmul double %calltmp, %calltmp; %calltmp2 = call double @cos(double %x); %multmp4 = fmul double %calltmp2, %calltmp2; %addtmp = fadd double %multmp, %multmp4; ret double %addtmp; }. ready> foo(4.0);; Read top-level expression:; define double @3() {; entry:; %calltmp = call double @foo(double 4.000000e+00); ret double %calltmp; }. Evaluated to 1.000000. Whoa, how does the JIT know about sin and cos? The answer is surprisingly; simple: The KaleidoscopeJIT has a straightforward symbol resolution rule that; it use",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:22642,Usability,simpl,simple,22642,"lities -; check this out:. ::. ready> extern sin(x);; Read extern:; declare double @sin(double). ready> extern cos(x);; Read extern:; declare double @cos(double). ready> sin(1.0);; Read top-level expression:; define double @2() {; entry:; ret double 0x3FEAED548F090CEE; }. Evaluated to 0.841471. ready> def foo(x) sin(x)*sin(x) + cos(x)*cos(x);; Read function definition:; define double @foo(double %x) {; entry:; %calltmp = call double @sin(double %x); %multmp = fmul double %calltmp, %calltmp; %calltmp2 = call double @cos(double %x); %multmp4 = fmul double %calltmp2, %calltmp2; %addtmp = fadd double %multmp, %multmp4; ret double %addtmp; }. ready> foo(4.0);; Read top-level expression:; define double @3() {; entry:; %calltmp = call double @foo(double 4.000000e+00); ret double %calltmp; }. Evaluated to 1.000000. Whoa, how does the JIT know about sin and cos? The answer is surprisingly; simple: The KaleidoscopeJIT has a straightforward symbol resolution rule that; it uses to find symbols that aren't available in any given module: First; it searches all the modules that have already been added to the JIT, from the; most recent to the oldest, to find the newest definition. If no definition is; found inside the JIT, it falls back to calling ""``dlsym(""sin"")``"" on the; Kaleidoscope process itself. Since ""``sin``"" is defined within the JIT's; address space, it simply patches up calls in the module to call the libm; version of ``sin`` directly. But in some cases this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symb",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:23119,Usability,simpl,simply,23119,"ouble @foo(double %x) {; entry:; %calltmp = call double @sin(double %x); %multmp = fmul double %calltmp, %calltmp; %calltmp2 = call double @cos(double %x); %multmp4 = fmul double %calltmp2, %calltmp2; %addtmp = fadd double %multmp, %multmp4; ret double %addtmp; }. ready> foo(4.0);; Read top-level expression:; define double @3() {; entry:; %calltmp = call double @foo(double 4.000000e+00); ret double %calltmp; }. Evaluated to 1.000000. Whoa, how does the JIT know about sin and cos? The answer is surprisingly; simple: The KaleidoscopeJIT has a straightforward symbol resolution rule that; it uses to find symbols that aren't available in any given module: First; it searches all the modules that have already been added to the JIT, from the; most recent to the oldest, to find the newest definition. If no definition is; found inside the JIT, it falls back to calling ""``dlsym(""sin"")``"" on the; Kaleidoscope process itself. Since ""``sin``"" is defined within the JIT's; address space, it simply patches up calls in the module to call the libm; version of ``sin`` directly. But in some cases this even goes further:; as sin and cos are names of standard math functions, the constant folder; will directly evaluate the function calls to the correct result when called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stder",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst:24315,Usability,simpl,simple,24315,"called; with constants like in the ""``sin(1.0)``"" above. In the future we'll see how tweaking this symbol resolution rule can be used to; enable all sorts of useful features, from security (restricting the set of; symbols available to JIT'd code), to dynamic code generation based on symbol; names, and even lazy compilation. One immediate benefit of the symbol resolution rule is that we can now extend; the language by writing arbitrary C++ code to implement operations. For example,; if we add:. .. code-block:: c++. #ifdef _WIN32; #define DLLEXPORT __declspec(dllexport); #else; #define DLLEXPORT; #endif. /// putchard - putchar that takes a double and returns 0.; extern ""C"" DLLEXPORT double putchard(double X) {; fputc((char)X, stderr);; return 0;; }. Note, that for Windows we need to actually export the functions because; the dynamic symbol loader will use ``GetProcAddress`` to find the symbols. Now we can produce simple output to the console by using things like:; ""``extern putchard(x); putchard(120);``"", which prints a lowercase 'x'; on the console (120 is the ASCII code for 'x'). Similar code could be; used to implement file I/O, console input, and many other capabilities; in Kaleidoscope. This completes the JIT and optimizer chapter of the Kaleidoscope; tutorial. At this point, we can compile a non-Turing-complete; programming language, optimize and JIT compile it in a user-driven way.; Next up we'll look into `extending the language with control flow; constructs <LangImpl05.html>`_, tackling some interesting LLVM IR issues; along the way. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the LLVM JIT and optimizer. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. If you are compiling this on Linux, make sure to add the ""-rdynamic""; option as well. This makes sure that the e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl04.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:2199,Availability,down,down,2199," new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first subexpression is evaluated and returned, if the condition is; false, the second subexpression is evaluated and returned. Since; Kaleidoscope allows side-effects, this behavior is important to nail; down. Now that we know what we ""want"", let's break this down into its; constituent pieces. Lexer Extensions for If/Then/Else; ---------------------------------. The lexer extensions are straightforward. First we add new enum values; for the relevant tokens:. .. code-block:: c++. // control; tok_if = -6,; tok_then = -7,; tok_else = -8,. Once we have that, we recognize the new keywords in the lexer. This is; pretty simple stuff:. .. code-block:: c++. ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); return tok_else;; return tok_identifier;. AST Extensions for If/Then/Else; -------------------------------. To represent the new expression we add a new AST node for it:. .. code-block:: c++. /// IfExprAST - Expression class for if/then/else.; class IfExprAST : public ExprAST {; std::u",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:2255,Availability,down,down,2255," talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first subexpression is evaluated and returned, if the condition is; false, the second subexpression is evaluated and returned. Since; Kaleidoscope allows side-effects, this behavior is important to nail; down. Now that we know what we ""want"", let's break this down into its; constituent pieces. Lexer Extensions for If/Then/Else; ---------------------------------. The lexer extensions are straightforward. First we add new enum values; for the relevant tokens:. .. code-block:: c++. // control; tok_if = -6,; tok_then = -7,; tok_else = -8,. Once we have that, we recognize the new keywords in the lexer. This is; pretty simple stuff:. .. code-block:: c++. ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); return tok_else;; return tok_identifier;. AST Extensions for If/Then/Else; -------------------------------. To represent the new expression we add a new AST node for it:. .. code-block:: c++. /// IfExprAST - Expression class for if/then/else.; class IfExprAST : public ExprAST {; std::unique_ptr<ExprAST> Cond, Then, Else;. public:; IfExprAST(std::unique_ptr<ExprAST> Co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:7577,Availability,avail,available,7577," the generated code, it is fairly simple: the entry block; evaluates the conditional expression (""x"" in our case here) and compares; the result to 0.0 with the ""``fcmp one``"" instruction ('one' is ""Ordered; and Not Equal""). Based on the result of this expression, the code jumps; to either the ""then"" or ""else"" blocks, which contain the expressions for; the true/false cases. Once the then/else blocks are finished executing, they both branch back; to the 'ifcont' block to execute the code that happens after the; if/then/else. In this case the only thing left to do is to return to the; caller of the function. The question then becomes: how does the code; know which expression to return?. The answer to this question involves an important SSA operation: the; `Phi; operation <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; If you're not familiar with SSA, `the wikipedia; article <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; is a good introduction and there are various other introductions to it; available on your favorite search engine. The short version is that; ""execution"" of the Phi operation requires ""remembering"" which block; control came from. The Phi operation takes on the value corresponding to; the input control block. In this case, if control comes in from the; ""then"" block, it gets the value of ""calltmp"". If control comes from the; ""else"" block, it gets the value of ""calltmp1"". At this point, you are probably starting to think ""Oh no! This means my; simple and elegant front-end will have to start generating SSA form in; order to use LLVM!"". Fortunately, this is not the case, and we strongly; advise *not* implementing an SSA construction algorithm in your; front-end unless there is an amazingly good reason to do so. In; practice, there are two sorts of values that float around in code; written for your average imperative programming language that might need; Phi nodes:. #. Code that involves user variables: ``x = 1; x = x + 1;``; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:12472,Availability,error,error,12472,"ullptr;. Builder->CreateBr(MergeBB);; // Codegen of 'Then' can change the current block, update ThenBB for the PHI.; ThenBB = Builder->GetInsertBlock();. After the conditional branch is inserted, we move the builder to start; inserting into the ""then"" block. Strictly speaking, this call moves the; insertion point to be at the end of the specified block. However, since; the ""then"" block is empty, it also starts out by inserting at the; beginning of the block. :). Once the insertion point is set, we recursively codegen the ""then""; expression from the AST. To finish off the ""then"" block, we create an; unconditional branch to the merge block. One interesting (and very; important) aspect of the LLVM IR is that it :ref:`requires all basic; blocks to be ""terminated"" <functionstructure>` with a :ref:`control; flow instruction <terminators>` such as return or branch. This means; that all control flow, *including fall throughs* must be made explicit; in the LLVM IR. If you violate this rule, the verifier will emit an; error. The final line here is quite subtle, but is very important. The basic; issue is that when we create the Phi node in the merge block, we need to; set up the block/value pairs that indicate how the Phi will work.; Importantly, the Phi node expects to have an entry for each predecessor; of the block in the CFG. Why then, are we getting the current block when; we just set it to ThenBB 5 lines above? The problem is that the ""Then""; expression may actually itself change the block that the Builder is; emitting into if, for example, it contains a nested ""if/then/else""; expression. Because calling ``codegen()`` recursively could arbitrarily change; the notion of the current block, we are required to get an up-to-date; value for code that will set up the Phi node. .. code-block:: c++. // Emit else block.; TheFunction->insert(TheFunction->end(), ElseBB);; Builder->SetInsertPoint(ElseBB);. Value *ElseV = Else->codegen();; if (!ElseV); return nullptr;. Builder->Create",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:16852,Availability,down,down,16852," just define the loop as always returning 0.0. In the; future when we have mutable variables, it will get more useful. As before, let's talk about the changes that we need to Kaleidoscope to; support this. Lexer Extensions for the 'for' Loop; -----------------------------------. The lexer extensions are the same sort of thing as for if/then/else:. .. code-block:: c++. ... in enum Token ...; // control; tok_if = -6, tok_then = -7, tok_else = -8,; tok_for = -9, tok_in = -10. ... in gettok ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); return tok_else;; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; return tok_identifier;. AST Extensions for the 'for' Loop; ---------------------------------. The AST node is just as simple. It basically boils down to capturing the; variable name and the constituent expressions in the node. .. code-block:: c++. /// ForExprAST - Expression class for for/in.; class ForExprAST : public ExprAST {; std::string VarName;; std::unique_ptr<ExprAST> Start, End, Step, Body;. public:; ForExprAST(const std::string &VarName, std::unique_ptr<ExprAST> Start,; std::unique_ptr<ExprAST> End, std::unique_ptr<ExprAST> Step,; std::unique_ptr<ExprAST> Body); : VarName(VarName), Start(std::move(Start)), End(std::move(End)),; Step(std::move(Step)), Body(std::move(Body)) {}. Value *codegen() override;; };. Parser Extensions for the 'for' Loop; ------------------------------------. The parser code is also fairly standard. The only interesting thing here; is handling of the optional step value. The parser code handles it by; checking to see if the second comma is present. If not, it sets the step; value to null in the AST node:. .. code-block:: c++. /// forexpr ::= 'for' identifier '=' expr ',' expr (',' expr)? 'in' expression; static std::unique_pt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:22792,Availability,error,error,22792,"lder->SetInsertPoint(LoopBB);. // Start the PHI node with an entry for Start.; PHINode *Variable = Builder->CreatePHI(Type::getDoubleTy(*TheContext),; 2, VarName);; Variable->addIncoming(StartVal, PreheaderBB);. Now that the ""preheader"" for the loop is set up, we switch to emitting; code for the loop body. To begin with, we move the insertion point and; create the PHI node for the loop induction variable. Since we already; know the incoming value for the starting value, we add it to the Phi; node. Note that the Phi will eventually get a second value for the; backedge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23283,Availability,error,error,23283,"ge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value, or 1.0 if it isn't present.; '``NextVar``' will be the value of the loop variable on the next; iteration of the loop. .. cod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23298,Availability,error,error,23298,"ge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value, or 1.0 if it isn't present.; '``NextVar``' will be the value of the loop variable on the next; iteration of the loop. .. cod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:11537,Deployability,update,update,11537,"ion it is currently embedded; into). Once it has that, it creates three blocks. Note that it passes; ""TheFunction"" into the constructor for the ""then"" block. This causes the; constructor to automatically insert the new block into the end of the; specified function. The other two blocks are created, but aren't yet; inserted into the function. Once the blocks are created, we can emit the conditional branch that; chooses between them. Note that creating new blocks does not implicitly; affect the IRBuilder, so it is still inserting into the block that the; condition went into. Also note that it is creating a branch to the; ""then"" block and the ""else"" block, even though the ""else"" block isn't; inserted into the function yet. This is all ok: it is the standard way; that LLVM supports forward references. .. code-block:: c++. // Emit then value.; Builder->SetInsertPoint(ThenBB);. Value *ThenV = Then->codegen();; if (!ThenV); return nullptr;. Builder->CreateBr(MergeBB);; // Codegen of 'Then' can change the current block, update ThenBB for the PHI.; ThenBB = Builder->GetInsertBlock();. After the conditional branch is inserted, we move the builder to start; inserting into the ""then"" block. Strictly speaking, this call moves the; insertion point to be at the end of the specified block. However, since; the ""then"" block is empty, it also starts out by inserting at the; beginning of the block. :). Once the insertion point is set, we recursively codegen the ""then""; expression from the AST. To finish off the ""then"" block, we create an; unconditional branch to the merge block. One interesting (and very; important) aspect of the LLVM IR is that it :ref:`requires all basic; blocks to be ""terminated"" <functionstructure>` with a :ref:`control; flow instruction <terminators>` such as return or branch. This means; that all control flow, *including fall throughs* must be made explicit; in the LLVM IR. If you violate this rule, the verifier will emit an; error. The final line here is quite su",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:13513,Deployability,update,update,13513,"nal line here is quite subtle, but is very important. The basic; issue is that when we create the Phi node in the merge block, we need to; set up the block/value pairs that indicate how the Phi will work.; Importantly, the Phi node expects to have an entry for each predecessor; of the block in the CFG. Why then, are we getting the current block when; we just set it to ThenBB 5 lines above? The problem is that the ""Then""; expression may actually itself change the block that the Builder is; emitting into if, for example, it contains a nested ""if/then/else""; expression. Because calling ``codegen()`` recursively could arbitrarily change; the notion of the current block, we are required to get an up-to-date; value for code that will set up the Phi node. .. code-block:: c++. // Emit else block.; TheFunction->insert(TheFunction->end(), ElseBB);; Builder->SetInsertPoint(ElseBB);. Value *ElseV = Else->codegen();; if (!ElseV); return nullptr;. Builder->CreateBr(MergeBB);; // codegen of 'Else' can change the current block, update ElseBB for the PHI.; ElseBB = Builder->GetInsertBlock();. Code generation for the 'else' block is basically identical to codegen; for the 'then' block. The only significant difference is the first line,; which adds the 'else' block to the function. Recall previously that the; 'else' block was created, but not added to the function. Now that the; 'then' and 'else' blocks are emitted, we can finish up with the merge; code:. .. code-block:: c++. // Emit merge block.; TheFunction->insert(TheFunction->end(), MergeBB);; Builder->SetInsertPoint(MergeBB);; PHINode *PN =; Builder->CreatePHI(Type::getDoubleTy(*TheContext), 2, ""iftmp"");. PN->addIncoming(ThenV, ThenBB);; PN->addIncoming(ElseV, ElseBB);; return PN;; }. The first two lines here are now familiar: the first adds the ""merge""; block to the Function object (it was previously floating, like the else; block above). The second changes the insertion point so that newly; created code will go into the ""merge"" ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:694,Energy Efficiency,power,power,694,"==================================================; Kaleidoscope: Extending the Language: Control Flow; ==================================================. .. contents::; :local:. Chapter 5 Introduction; ======================. Welcome to Chapter 5 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Parts 1-4 described the implementation of; the simple Kaleidoscope language and included support for generating; LLVM IR, followed by optimizations and a JIT compiler. Unfortunately, as; presented, Kaleidoscope is mostly useless: it has no control flow other; than call and return. This means that you can't have conditional; branches in the code, significantly limiting its power. In this episode; of ""build that compiler"", we'll extend Kaleidoscope to have an; if/then/else expression plus a simple 'for' loop. If/Then/Else; ============. Extending Kaleidoscope to support if/then/else is quite straightforward.; It basically requires adding support for this ""new"" concept to the; lexer, parser, AST, and LLVM code emitter. This example is nice, because; it shows how easy it is to ""grow"" a language over time, incrementally; extending it as new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:15282,Energy Efficiency,power,powerful,15282,"The first two lines here are now familiar: the first adds the ""merge""; block to the Function object (it was previously floating, like the else; block above). The second changes the insertion point so that newly; created code will go into the ""merge"" block. Once that is done, we need; to create the PHI node and set up the block/value pairs for the PHI. Finally, the CodeGen function returns the phi node as the value computed; by the if/then/else expression. In our example above, this returned; value will feed into the code for the top-level function, which will; create the return instruction. Overall, we now have the ability to execute conditional code in; Kaleidoscope. With this extension, Kaleidoscope is a fairly complete; language that can calculate a wide variety of numeric functions. Next up; we'll add another useful expression that is familiar from non-functional; languages... 'for' Loop Expression; =====================. Now that we know how to add basic control flow constructs to the; language, we have the tools to add more powerful things. Let's add; something more aggressive, a 'for' expression:. ::. extern putchard(char);; def printstar(n); for i = 1, i < n, 1.0 in; putchard(42); # ascii 42 = '*'. # print 100 '*' characters; printstar(100);. This expression defines a new variable (""i"" in this case) which iterates; from a starting value, while the condition (""i < n"" in this case) is; true, incrementing by an optional step value (""1.0"" in this case). If; the step value is omitted, it defaults to 1.0. While the loop is true,; it executes its body expression. Because we don't have anything better; to return, we'll just define the loop as always returning 0.0. In the; future when we have mutable variables, it will get more useful. As before, let's talk about the changes that we need to Kaleidoscope to; support this. Lexer Extensions for the 'for' Loop; -----------------------------------. The lexer extensions are the same sort of thing as for if/then/else:. .. co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:750,Modifiability,extend,extend,750,"==================================================; Kaleidoscope: Extending the Language: Control Flow; ==================================================. .. contents::; :local:. Chapter 5 Introduction; ======================. Welcome to Chapter 5 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Parts 1-4 described the implementation of; the simple Kaleidoscope language and included support for generating; LLVM IR, followed by optimizations and a JIT compiler. Unfortunately, as; presented, Kaleidoscope is mostly useless: it has no control flow other; than call and return. This means that you can't have conditional; branches in the code, significantly limiting its power. In this episode; of ""build that compiler"", we'll extend Kaleidoscope to have an; if/then/else expression plus a simple 'for' loop. If/Then/Else; ============. Extending Kaleidoscope to support if/then/else is quite straightforward.; It basically requires adding support for this ""new"" concept to the; lexer, parser, AST, and LLVM code emitter. This example is nice, because; it shows how easy it is to ""grow"" a language over time, incrementally; extending it as new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:1147,Modifiability,extend,extending,1147,"======================================. .. contents::; :local:. Chapter 5 Introduction; ======================. Welcome to Chapter 5 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Parts 1-4 described the implementation of; the simple Kaleidoscope language and included support for generating; LLVM IR, followed by optimizations and a JIT compiler. Unfortunately, as; presented, Kaleidoscope is mostly useless: it has no control flow other; than call and return. This means that you can't have conditional; branches in the code, significantly limiting its power. In this episode; of ""build that compiler"", we'll extend Kaleidoscope to have an; if/then/else expression plus a simple 'for' loop. If/Then/Else; ============. Extending Kaleidoscope to support if/then/else is quite straightforward.; It basically requires adding support for this ""new"" concept to the; lexer, parser, AST, and LLVM code emitter. This example is nice, because; it shows how easy it is to ""grow"" a language over time, incrementally; extending it as new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first subexpression is evaluated and returned, if the condition is; false, the second subexpression is evaluated and retu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:8506,Modifiability,variab,variables,8506,"ood introduction and there are various other introductions to it; available on your favorite search engine. The short version is that; ""execution"" of the Phi operation requires ""remembering"" which block; control came from. The Phi operation takes on the value corresponding to; the input control block. In this case, if control comes in from the; ""then"" block, it gets the value of ""calltmp"". If control comes from the; ""else"" block, it gets the value of ""calltmp1"". At this point, you are probably starting to think ""Oh no! This means my; simple and elegant front-end will have to start generating SSA form in; order to use LLVM!"". Fortunately, this is not the case, and we strongly; advise *not* implementing an SSA construction algorithm in your; front-end unless there is an amazingly good reason to do so. In; practice, there are two sorts of values that float around in code; written for your average imperative programming language that might need; Phi nodes:. #. Code that involves user variables: ``x = 1; x = x + 1;``; #. Values that are implicit in the structure of your AST, such as the; Phi node in this case. In `Chapter 7 <LangImpl07.html>`_ of this tutorial (""mutable variables""),; we'll talk about #1 in depth. For now, just believe me that you don't; need SSA construction to handle this case. For #2, you have the choice; of using the techniques that we will describe for #1, or you can insert; Phi nodes directly, if convenient. In this case, it is really; easy to generate the Phi node, so we choose to do it directly. Okay, enough of the motivation and overview, let's generate code!. Code Generation for If/Then/Else; --------------------------------. In order to generate code for this, we implement the ``codegen`` method; for ``IfExprAST``:. .. code-block:: c++. Value *IfExprAST::codegen() {; Value *CondV = Cond->codegen();; if (!CondV); return nullptr;. // Convert condition to a bool by comparing non-equal to 0.0.; CondV = Builder->CreateFCmpONE(; CondV, ConstantFP::get",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:8695,Modifiability,variab,variables,8695,"g"" which block; control came from. The Phi operation takes on the value corresponding to; the input control block. In this case, if control comes in from the; ""then"" block, it gets the value of ""calltmp"". If control comes from the; ""else"" block, it gets the value of ""calltmp1"". At this point, you are probably starting to think ""Oh no! This means my; simple and elegant front-end will have to start generating SSA form in; order to use LLVM!"". Fortunately, this is not the case, and we strongly; advise *not* implementing an SSA construction algorithm in your; front-end unless there is an amazingly good reason to do so. In; practice, there are two sorts of values that float around in code; written for your average imperative programming language that might need; Phi nodes:. #. Code that involves user variables: ``x = 1; x = x + 1;``; #. Values that are implicit in the structure of your AST, such as the; Phi node in this case. In `Chapter 7 <LangImpl07.html>`_ of this tutorial (""mutable variables""),; we'll talk about #1 in depth. For now, just believe me that you don't; need SSA construction to handle this case. For #2, you have the choice; of using the techniques that we will describe for #1, or you can insert; Phi nodes directly, if convenient. In this case, it is really; easy to generate the Phi node, so we choose to do it directly. Okay, enough of the motivation and overview, let's generate code!. Code Generation for If/Then/Else; --------------------------------. In order to generate code for this, we implement the ``codegen`` method; for ``IfExprAST``:. .. code-block:: c++. Value *IfExprAST::codegen() {; Value *CondV = Cond->codegen();; if (!CondV); return nullptr;. // Convert condition to a bool by comparing non-equal to 0.0.; CondV = Builder->CreateFCmpONE(; CondV, ConstantFP::get(*TheContext, APFloat(0.0)), ""ifcond"");. This code is straightforward and similar to what we saw before. We emit; the expression for the condition, then compare that value to zero to get;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:15537,Modifiability,variab,variable,15537,"CodeGen function returns the phi node as the value computed; by the if/then/else expression. In our example above, this returned; value will feed into the code for the top-level function, which will; create the return instruction. Overall, we now have the ability to execute conditional code in; Kaleidoscope. With this extension, Kaleidoscope is a fairly complete; language that can calculate a wide variety of numeric functions. Next up; we'll add another useful expression that is familiar from non-functional; languages... 'for' Loop Expression; =====================. Now that we know how to add basic control flow constructs to the; language, we have the tools to add more powerful things. Let's add; something more aggressive, a 'for' expression:. ::. extern putchard(char);; def printstar(n); for i = 1, i < n, 1.0 in; putchard(42); # ascii 42 = '*'. # print 100 '*' characters; printstar(100);. This expression defines a new variable (""i"" in this case) which iterates; from a starting value, while the condition (""i < n"" in this case) is; true, incrementing by an optional step value (""1.0"" in this case). If; the step value is omitted, it defaults to 1.0. While the loop is true,; it executes its body expression. Because we don't have anything better; to return, we'll just define the loop as always returning 0.0. In the; future when we have mutable variables, it will get more useful. As before, let's talk about the changes that we need to Kaleidoscope to; support this. Lexer Extensions for the 'for' Loop; -----------------------------------. The lexer extensions are the same sort of thing as for if/then/else:. .. code-block:: c++. ... in enum Token ...; // control; tok_if = -6, tok_then = -7, tok_else = -8,; tok_for = -9, tok_in = -10. ... in gettok ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); retu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:15965,Modifiability,variab,variables,15965,"ete; language that can calculate a wide variety of numeric functions. Next up; we'll add another useful expression that is familiar from non-functional; languages... 'for' Loop Expression; =====================. Now that we know how to add basic control flow constructs to the; language, we have the tools to add more powerful things. Let's add; something more aggressive, a 'for' expression:. ::. extern putchard(char);; def printstar(n); for i = 1, i < n, 1.0 in; putchard(42); # ascii 42 = '*'. # print 100 '*' characters; printstar(100);. This expression defines a new variable (""i"" in this case) which iterates; from a starting value, while the condition (""i < n"" in this case) is; true, incrementing by an optional step value (""1.0"" in this case). If; the step value is omitted, it defaults to 1.0. While the loop is true,; it executes its body expression. Because we don't have anything better; to return, we'll just define the loop as always returning 0.0. In the; future when we have mutable variables, it will get more useful. As before, let's talk about the changes that we need to Kaleidoscope to; support this. Lexer Extensions for the 'for' Loop; -----------------------------------. The lexer extensions are the same sort of thing as for if/then/else:. .. code-block:: c++. ... in enum Token ...; // control; tok_if = -6, tok_then = -7, tok_else = -8,; tok_for = -9, tok_in = -10. ... in gettok ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); return tok_else;; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; return tok_identifier;. AST Extensions for the 'for' Loop; ---------------------------------. The AST node is just as simple. It basically boils down to capturing the; variable name and the constituent expressions in the node. .. code-block:: c++. /// ForEx",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:16875,Modifiability,variab,variable,16875," just define the loop as always returning 0.0. In the; future when we have mutable variables, it will get more useful. As before, let's talk about the changes that we need to Kaleidoscope to; support this. Lexer Extensions for the 'for' Loop; -----------------------------------. The lexer extensions are the same sort of thing as for if/then/else:. .. code-block:: c++. ... in enum Token ...; // control; tok_if = -6, tok_then = -7, tok_else = -8,; tok_for = -9, tok_in = -10. ... in gettok ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); return tok_else;; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; return tok_identifier;. AST Extensions for the 'for' Loop; ---------------------------------. The AST node is just as simple. It basically boils down to capturing the; variable name and the constituent expressions in the node. .. code-block:: c++. /// ForExprAST - Expression class for for/in.; class ForExprAST : public ExprAST {; std::string VarName;; std::unique_ptr<ExprAST> Start, End, Step, Body;. public:; ForExprAST(const std::string &VarName, std::unique_ptr<ExprAST> Start,; std::unique_ptr<ExprAST> End, std::unique_ptr<ExprAST> Step,; std::unique_ptr<ExprAST> Body); : VarName(VarName), Start(std::move(Start)), End(std::move(End)),; Step(std::move(Step)), Body(std::move(Body)) {}. Value *codegen() override;; };. Parser Extensions for the 'for' Loop; ------------------------------------. The parser code is also fairly standard. The only interesting thing here; is handling of the optional step value. The parser code handles it by; checking to see if the second comma is present. If not, it sets the step; value to null in the AST node:. .. code-block:: c++. /// forexpr ::= 'for' identifier '=' expr ',' expr (',' expr)? 'in' expression; static std::unique_pt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:20601,Modifiability,variab,variable,20601," @putchard(double). define double @printstar(double %n) {; entry:; ; initial value = 1.0 (inlined into phi); br label %loop. loop: ; preds = %loop, %entry; %i = phi double [ 1.000000e+00, %entry ], [ %nextvar, %loop ]; ; body; %calltmp = call double @putchard(double 4.200000e+01); ; increment; %nextvar = fadd double %i, 1.000000e+00. ; termination test; %cmptmp = fcmp ult double %i, %n; %booltmp = uitofp i1 %cmptmp to double; %loopcond = fcmp one double %booltmp, 0.000000e+00; br i1 %loopcond, label %loop, label %afterloop. afterloop: ; preds = %loop; ; loop always returns 0.0; ret double 0.000000e+00; }. This loop contains all the same constructs we saw before: a phi node,; several expressions, and some basic blocks. Let's see how this fits; together. Code Generation for the 'for' Loop; ----------------------------------. The first part of codegen is very simple: we just output the start; expression for the loop value:. .. code-block:: c++. Value *ForExprAST::codegen() {; // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. With this out of the way, the next step is to set up the LLVM basic; block for the start of the loop body. In the case above, the whole loop; body is one block, but remember that the body code itself could consist; of multiple blocks (e.g. if it contains an if/then/else or a for/in; expression). .. code-block:: c++. // Make the new basic block for the loop header, inserting after current; // block.; Function *TheFunction = Builder->GetInsertBlock()->getParent();; BasicBlock *PreheaderBB = Builder->GetInsertBlock();; BasicBlock *LoopBB =; BasicBlock::Create(*TheContext, ""loop"", TheFunction);. // Insert an explicit fall through from the current block to the LoopBB.; Builder->CreateBr(LoopBB);. This code is similar to what we saw for if/then/else. Because we will; need it to create the Phi node, we remember the block that falls through; into the loop. Once we have that, we ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:22156,Modifiability,variab,variable,22156,"on = Builder->GetInsertBlock()->getParent();; BasicBlock *PreheaderBB = Builder->GetInsertBlock();; BasicBlock *LoopBB =; BasicBlock::Create(*TheContext, ""loop"", TheFunction);. // Insert an explicit fall through from the current block to the LoopBB.; Builder->CreateBr(LoopBB);. This code is similar to what we saw for if/then/else. Because we will; need it to create the Phi node, we remember the block that falls through; into the loop. Once we have that, we create the actual block that starts; the loop and create an unconditional branch for the fall-through between; the two blocks. .. code-block:: c++. // Start insertion in LoopBB.; Builder->SetInsertPoint(LoopBB);. // Start the PHI node with an entry for Start.; PHINode *Variable = Builder->CreatePHI(Type::getDoubleTy(*TheContext),; 2, VarName);; Variable->addIncoming(StartVal, PreheaderBB);. Now that the ""preheader"" for the loop is set up, we switch to emitting; code for the loop body. To begin with, we move the insertion point and; create the PHI node for the loop induction variable. Since we already; know the incoming value for the starting value, we add it to the Phi; node. Note that the Phi will eventually get a second value for the; backedge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:22433,Modifiability,variab,variable,22433,"else. Because we will; need it to create the Phi node, we remember the block that falls through; into the loop. Once we have that, we create the actual block that starts; the loop and create an unconditional branch for the fall-through between; the two blocks. .. code-block:: c++. // Start insertion in LoopBB.; Builder->SetInsertPoint(LoopBB);. // Start the PHI node with an entry for Start.; PHINode *Variable = Builder->CreatePHI(Type::getDoubleTy(*TheContext),; 2, VarName);; Variable->addIncoming(StartVal, PreheaderBB);. Now that the ""preheader"" for the loop is set up, we switch to emitting; code for the loop body. To begin with, we move the insertion point and; create the PHI node for the loop induction variable. Since we already; know the incoming value for the starting value, we add it to the Phi; node. Note that the Phi will eventually get a second value for the; backedge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correct",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:22506,Modifiability,variab,variable,22506,"ock that falls through; into the loop. Once we have that, we create the actual block that starts; the loop and create an unconditional branch for the fall-through between; the two blocks. .. code-block:: c++. // Start insertion in LoopBB.; Builder->SetInsertPoint(LoopBB);. // Start the PHI node with an entry for Start.; PHINode *Variable = Builder->CreatePHI(Type::getDoubleTy(*TheContext),; 2, VarName);; Variable->addIncoming(StartVal, PreheaderBB);. Now that the ""preheader"" for the loop is set up, we switch to emitting; code for the loop body. To begin with, we move the insertion point and; create the PHI node for the loop induction variable. Since we already; know the incoming value for the starting value, we add it to the Phi; node. Note that the Phi will eventually get a second value for the; backedge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:22918,Modifiability,variab,variable,22918,"ame);; Variable->addIncoming(StartVal, PreheaderBB);. Now that the ""preheader"" for the loop is set up, we switch to emitting; code for the loop body. To begin with, we move the insertion point and; create the PHI node for the loop induction variable. Since we already; know the incoming value for the starting value, we add it to the Phi; node. Note that the Phi will eventually get a second value for the; backedge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); retur",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23032,Modifiability,variab,variables,23032,"for the loop is set up, we switch to emitting; code for the loop body. To begin with, we move the insertion point and; create the PHI node for the loop induction variable. Since we already; know the incoming value for the starting value, we add it to the Phi; node. Note that the Phi will eventually get a second value for the; backedge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23116,Modifiability,variab,variable,23116," point and; create the PHI node for the loop induction variable. Since we already; know the incoming value for the starting value, we add it to the Phi; node. Note that the Phi will eventually get a second value for the; backedge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23203,Modifiability,variab,variable,23203,"lue for the starting value, we add it to the Phi; node. Note that the Phi will eventually get a second value for the; backedge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23399,Modifiability,variab,variables,23399,"ge, but we can't set it up yet (because it doesn't exist!). .. code-block:: c++. // Within the loop, the variable is defined equal to the PHI node. If it; // shadows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value, or 1.0 if it isn't present.; '``NextVar``' will be the value of the loop variable on the next; iteration of the loop. .. cod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23562,Modifiability,variab,variable,23562,"ows an existing variable, we have to restore it, so save it now.; Value *OldVal = NamedValues[VarName];; NamedValues[VarName] = Variable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value, or 1.0 if it isn't present.; '``NextVar``' will be the value of the loop variable on the next; iteration of the loop. .. code-block:: c++. // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Convert condition to a bool by comparing non-eq",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23587,Modifiability,variab,variable,23587,"ariable;. // Emit the body of the loop. This, like any other expr, can change the; // current BB. Note that we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value, or 1.0 if it isn't present.; '``NextVar``' will be the value of the loop variable on the next; iteration of the loop. .. code-block:: c++. // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Convert condition to a bool by comparing non-equal to 0.0.; EndCond = Builder->CreateFCmpONE(; EndCond, ConstantFP::get(*TheContext, APFloat(0.0)), ""loopcond"");. Finally, we ev",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:23706,Modifiability,variab,variable,23706,"at we ignore the value computed by the body, but don't; // allow an error.; if (!Body->codegen()); return nullptr;. Now the code starts to get more interesting. Our 'for' loop introduces a; new variable to the symbol table. This means that our symbol table can; now contain either function arguments or loop variables. To handle this,; before we codegen the body of the loop, we add the loop variable as the; current value for its name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value, or 1.0 if it isn't present.; '``NextVar``' will be the value of the loop variable on the next; iteration of the loop. .. code-block:: c++. // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Convert condition to a bool by comparing non-equal to 0.0.; EndCond = Builder->CreateFCmpONE(; EndCond, ConstantFP::get(*TheContext, APFloat(0.0)), ""loopcond"");. Finally, we evaluate the exit value of the loop, to determine whether; the loop should exit. This mirrors the conditio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:24170,Modifiability,variab,variable,24170,"ts name. Note that it is possible that there is a; variable of the same name in the outer scope. It would be easy to make; this an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value, or 1.0 if it isn't present.; '``NextVar``' will be the value of the loop variable on the next; iteration of the loop. .. code-block:: c++. // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Convert condition to a bool by comparing non-equal to 0.0.; EndCond = Builder->CreateFCmpONE(; EndCond, ConstantFP::get(*TheContext, APFloat(0.0)), ""loopcond"");. Finally, we evaluate the exit value of the loop, to determine whether; the loop should exit. This mirrors the condition evaluation for the; if/then/else statement. .. code-block:: c++. // Create the ""after loop"" block and insert it.; BasicBlock *LoopEndBB = Builder->GetInsertBlock();; BasicBlock *AfterBB =; BasicBlock::Create(*TheContext, ""afterloop"", TheFunction);. // Insert the conditional branch into the end of LoopEndBB.; Builder->CreateCondBr(EndCond, LoopBB, AfterBB);. // Any new code will be inserted in AfterBB.; Builder->SetInsertPoi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:24278,Modifiability,variab,variable,24278,"is an error (emit an error and return null if there is already an; entry for VarName) but we choose to allow shadowing of variables. In; order to handle this correctly, we remember the Value that we are; potentially shadowing in ``OldVal`` (which will be null if there is no; shadowed variable). Once the loop variable is set into the symbol table, the code; recursively codegen's the body. This allows the body to use the loop; variable: any references to it will naturally find it in the symbol; table. .. code-block:: c++. // Emit the step value.; Value *StepVal = nullptr;; if (Step) {; StepVal = Step->codegen();; if (!StepVal); return nullptr;; } else {; // If not specified, use 1.0.; StepVal = ConstantFP::get(*TheContext, APFloat(1.0));; }. Value *NextVar = Builder->CreateFAdd(Variable, StepVal, ""nextvar"");. Now that the body is emitted, we compute the next value of the iteration; variable by adding the step value, or 1.0 if it isn't present.; '``NextVar``' will be the value of the loop variable on the next; iteration of the loop. .. code-block:: c++. // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Convert condition to a bool by comparing non-equal to 0.0.; EndCond = Builder->CreateFCmpONE(; EndCond, ConstantFP::get(*TheContext, APFloat(0.0)), ""loopcond"");. Finally, we evaluate the exit value of the loop, to determine whether; the loop should exit. This mirrors the condition evaluation for the; if/then/else statement. .. code-block:: c++. // Create the ""after loop"" block and insert it.; BasicBlock *LoopEndBB = Builder->GetInsertBlock();; BasicBlock *AfterBB =; BasicBlock::Create(*TheContext, ""afterloop"", TheFunction);. // Insert the conditional branch into the end of LoopEndBB.; Builder->CreateCondBr(EndCond, LoopBB, AfterBB);. // Any new code will be inserted in AfterBB.; Builder->SetInsertPoint(AfterBB);. With the code for the body of the loop complete, we just need to finish; up the control flow for it. This code",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:25764,Modifiability,variab,variable,25764,"lse statement. .. code-block:: c++. // Create the ""after loop"" block and insert it.; BasicBlock *LoopEndBB = Builder->GetInsertBlock();; BasicBlock *AfterBB =; BasicBlock::Create(*TheContext, ""afterloop"", TheFunction);. // Insert the conditional branch into the end of LoopEndBB.; Builder->CreateCondBr(EndCond, LoopBB, AfterBB);. // Any new code will be inserted in AfterBB.; Builder->SetInsertPoint(AfterBB);. With the code for the body of the loop complete, we just need to finish; up the control flow for it. This code remembers the end block (for the; phi node), then creates the block for the loop exit (""afterloop""). Based; on the value of the exit condition, it creates a conditional branch that; chooses between executing the loop again and exiting the loop. Any; future code is emitted in the ""afterloop"" block, so it sets the; insertion position to it. .. code-block:: c++. // Add a new entry to the PHI node for the backedge.; Variable->addIncoming(NextVar, LoopEndBB);. // Restore the unshadowed variable.; if (OldVal); NamedValues[VarName] = OldVal;; else; NamedValues.erase(VarName);. // for expr always returns 0.0.; return Constant::getNullValue(Type::getDoubleTy(*TheContext));; }. The final code handles various cleanups: now that we have the ""NextVar""; value, we can add the incoming value to the loop PHI node. After that,; we remove the loop variable from the symbol table, so that it isn't in; scope after the for loop. Finally, code generation of the for loop; always returns 0.0, so that is what we return from; ``ForExprAST::codegen()``. With this, we conclude the ""adding control flow to Kaleidoscope"" chapter; of the tutorial. In this chapter we added two control flow constructs,; and used them to motivate a couple of aspects of the LLVM IR that are; important for front-end implementors to know. In the next chapter of our; saga, we will get a bit crazier and add `user-defined; operators <LangImpl06.html>`_ to our poor innocent language. Full Code Listing; ==========",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:26119,Modifiability,variab,variable,26119,"SetInsertPoint(AfterBB);. With the code for the body of the loop complete, we just need to finish; up the control flow for it. This code remembers the end block (for the; phi node), then creates the block for the loop exit (""afterloop""). Based; on the value of the exit condition, it creates a conditional branch that; chooses between executing the loop again and exiting the loop. Any; future code is emitted in the ""afterloop"" block, so it sets the; insertion position to it. .. code-block:: c++. // Add a new entry to the PHI node for the backedge.; Variable->addIncoming(NextVar, LoopEndBB);. // Restore the unshadowed variable.; if (OldVal); NamedValues[VarName] = OldVal;; else; NamedValues.erase(VarName);. // for expr always returns 0.0.; return Constant::getNullValue(Type::getDoubleTy(*TheContext));; }. The final code handles various cleanups: now that we have the ""NextVar""; value, we can add the incoming value to the loop PHI node. After that,; we remove the loop variable from the symbol table, so that it isn't in; scope after the for loop. Finally, code generation of the for loop; always returns 0.0, so that is what we return from; ``ForExprAST::codegen()``. With this, we conclude the ""adding control flow to Kaleidoscope"" chapter; of the tutorial. In this chapter we added two control flow constructs,; and used them to motivate a couple of aspects of the LLVM IR that are; important for front-end implementors to know. In the next chapter of our; saga, we will get a bit crazier and add `user-defined; operators <LangImpl06.html>`_ to our poor innocent language. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the if/then/else and for expressions. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter5/toy.c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:26823,Modifiability,enhance,enhanced,26823,"h; up the control flow for it. This code remembers the end block (for the; phi node), then creates the block for the loop exit (""afterloop""). Based; on the value of the exit condition, it creates a conditional branch that; chooses between executing the loop again and exiting the loop. Any; future code is emitted in the ""afterloop"" block, so it sets the; insertion position to it. .. code-block:: c++. // Add a new entry to the PHI node for the backedge.; Variable->addIncoming(NextVar, LoopEndBB);. // Restore the unshadowed variable.; if (OldVal); NamedValues[VarName] = OldVal;; else; NamedValues.erase(VarName);. // for expr always returns 0.0.; return Constant::getNullValue(Type::getDoubleTy(*TheContext));; }. The final code handles various cleanups: now that we have the ""NextVar""; value, we can add the incoming value to the loop PHI node. After that,; we remove the loop variable from the symbol table, so that it isn't in; scope after the for loop. Finally, code generation of the for loop; always returns 0.0, so that is what we return from; ``ForExprAST::codegen()``. With this, we conclude the ""adding control flow to Kaleidoscope"" chapter; of the tutorial. In this chapter we added two control flow constructs,; and used them to motivate a couple of aspects of the LLVM IR that are; important for front-end implementors to know. In the next chapter of our; saga, we will get a bit crazier and add `user-defined; operators <LangImpl06.html>`_ to our poor innocent language. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the if/then/else and for expressions. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter5/toy.cpp; :language: c++. `Next: Extending the language: user-defined operators <LangImpl06.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:26963,Modifiability,config,config,26963,"h; up the control flow for it. This code remembers the end block (for the; phi node), then creates the block for the loop exit (""afterloop""). Based; on the value of the exit condition, it creates a conditional branch that; chooses between executing the loop again and exiting the loop. Any; future code is emitted in the ""afterloop"" block, so it sets the; insertion position to it. .. code-block:: c++. // Add a new entry to the PHI node for the backedge.; Variable->addIncoming(NextVar, LoopEndBB);. // Restore the unshadowed variable.; if (OldVal); NamedValues[VarName] = OldVal;; else; NamedValues.erase(VarName);. // for expr always returns 0.0.; return Constant::getNullValue(Type::getDoubleTy(*TheContext));; }. The final code handles various cleanups: now that we have the ""NextVar""; value, we can add the incoming value to the loop PHI node. After that,; we remove the loop variable from the symbol table, so that it isn't in; scope after the for loop. Finally, code generation of the for loop; always returns 0.0, so that is what we return from; ``ForExprAST::codegen()``. With this, we conclude the ""adding control flow to Kaleidoscope"" chapter; of the tutorial. In this chapter we added two control flow constructs,; and used them to motivate a couple of aspects of the LLVM IR that are; important for front-end implementors to know. In the next chapter of our; saga, we will get a bit crazier and add `user-defined; operators <LangImpl06.html>`_ to our poor innocent language. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the if/then/else and for expressions. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter5/toy.cpp; :language: c++. `Next: Extending the language: user-defined operators <LangImpl06.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:453,Performance,optimiz,optimizations,453,"==================================================; Kaleidoscope: Extending the Language: Control Flow; ==================================================. .. contents::; :local:. Chapter 5 Introduction; ======================. Welcome to Chapter 5 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Parts 1-4 described the implementation of; the simple Kaleidoscope language and included support for generating; LLVM IR, followed by optimizations and a JIT compiler. Unfortunately, as; presented, Kaleidoscope is mostly useless: it has no control flow other; than call and return. This means that you can't have conditional; branches in the code, significantly limiting its power. In this episode; of ""build that compiler"", we'll extend Kaleidoscope to have an; if/then/else expression plus a simple 'for' loop. If/Then/Else; ============. Extending Kaleidoscope to support if/then/else is quite straightforward.; It basically requires adding support for this ""new"" concept to the; lexer, parser, AST, and LLVM code emitter. This example is nice, because; it shows how easy it is to ""grow"" a language over time, incrementally; extending it as new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:5296,Performance,optimiz,optimizations,5296,"), std::move(Then),; std::move(Else));; }. Next we hook it up as a primary expression:. .. code-block:: c++. static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; }; }. LLVM IR for If/Then/Else; ------------------------. Now that we have it parsing and building the AST, the final piece is; adding LLVM code generation support. This is the most interesting part; of the if/then/else example, because this is where it starts to; introduce new concepts. All of the code above has been thoroughly; described in previous chapters. To motivate the code we want to produce, let's take a look at a simple; example. Consider:. ::. extern foo();; extern bar();; def baz(x) if x then foo() else bar();. If you disable optimizations, the code you'll (soon) get from; Kaleidoscope looks like this:. .. code-block:: llvm. declare double @foo(). declare double @bar(). define double @baz(double %x) {; entry:; %ifcond = fcmp one double %x, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; %calltmp = call double @foo(); br label %ifcont. else: ; preds = %entry; %calltmp1 = call double @bar(); br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ %calltmp, %then ], [ %calltmp1, %else ]; ret double %iftmp; }. To visualize the control flow graph, you can use a nifty feature of the; LLVM '`opt <https://llvm.org/cmds/opt.html>`_' tool. If you put this LLVM; IR into ""t.ll"" and run ""``llvm-as < t.ll | opt -passes=view-cfg``"", `a; window will pop up <../../ProgrammersManual.html#viewing-graphs-while-debugging-code>`_ and you'll; see this graph:. .. figure:: LangImpl05-cfg.png; :align: center; :alt: Example CFG. Example CFG. Another way to get this is to call ""``F->viewCFG()``"" or; ""``F->viewCFGOnly()",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:19500,Performance,optimiz,optimizations,19500,"(CurTok == ',') {; getNextToken();; Step = ParseExpression();; if (!Step); return nullptr;; }. if (CurTok != tok_in); return LogError(""expected 'in' after for"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<ForExprAST>(IdName, std::move(Start),; std::move(End), std::move(Step),; std::move(Body));; }. And again we hook it up as a primary expression:. .. code-block:: c++. static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; case tok_for:; return ParseForExpr();; }; }. LLVM IR for the 'for' Loop; --------------------------. Now we get to the good part: the LLVM IR we want to generate for this; thing. With the simple example above, we get this LLVM IR (note that; this dump is generated with optimizations disabled for clarity):. .. code-block:: llvm. declare double @putchard(double). define double @printstar(double %n) {; entry:; ; initial value = 1.0 (inlined into phi); br label %loop. loop: ; preds = %loop, %entry; %i = phi double [ 1.000000e+00, %entry ], [ %nextvar, %loop ]; ; body; %calltmp = call double @putchard(double 4.200000e+01); ; increment; %nextvar = fadd double %i, 1.000000e+00. ; termination test; %cmptmp = fcmp ult double %i, %n; %booltmp = uitofp i1 %cmptmp to double; %loopcond = fcmp one double %booltmp, 0.000000e+00; br i1 %loopcond, label %loop, label %afterloop. afterloop: ; preds = %loop; ; loop always returns 0.0; ret double 0.000000e+00; }. This loop contains all the same constructs we saw before: a phi node,; several expressions, and some basic blocks. Let's see how this fits; together. Code Generation for the 'for' Loop; ----------------------------------. The first part of codegen is very simple: we just output the sta",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:3662,Testability,log,logic,3662,"ff:. .. code-block:: c++. ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); return tok_else;; return tok_identifier;. AST Extensions for If/Then/Else; -------------------------------. To represent the new expression we add a new AST node for it:. .. code-block:: c++. /// IfExprAST - Expression class for if/then/else.; class IfExprAST : public ExprAST {; std::unique_ptr<ExprAST> Cond, Then, Else;. public:; IfExprAST(std::unique_ptr<ExprAST> Cond, std::unique_ptr<ExprAST> Then,; std::unique_ptr<ExprAST> Else); : Cond(std::move(Cond)), Then(std::move(Then)), Else(std::move(Else)) {}. Value *codegen() override;; };. The AST node just has pointers to the various subexpressions. Parser Extensions for If/Then/Else; ----------------------------------. Now that we have the relevant tokens coming from the lexer and we have; the AST node to build, our parsing logic is relatively straightforward.; First we define a new parsing function:. .. code-block:: c++. /// ifexpr ::= 'if' expression 'then' expression 'else' expression; static std::unique_ptr<ExprAST> ParseIfExpr() {; getNextToken(); // eat the if. // condition.; auto Cond = ParseExpression();; if (!Cond); return nullptr;. if (CurTok != tok_then); return LogError(""expected then"");; getNextToken(); // eat the then. auto Then = ParseExpression();; if (!Then); return nullptr;. if (CurTok != tok_else); return LogError(""expected else"");. getNextToken();. auto Else = ParseExpression();; if (!Else); return nullptr;. return std::make_unique<IfExprAST>(std::move(Cond), std::move(Then),; std::move(Else));; }. Next we hook it up as a primary expression:. .. code-block:: c++. static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExp",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:19924,Testability,test,test,19924,"Tok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; case tok_for:; return ParseForExpr();; }; }. LLVM IR for the 'for' Loop; --------------------------. Now we get to the good part: the LLVM IR we want to generate for this; thing. With the simple example above, we get this LLVM IR (note that; this dump is generated with optimizations disabled for clarity):. .. code-block:: llvm. declare double @putchard(double). define double @printstar(double %n) {; entry:; ; initial value = 1.0 (inlined into phi); br label %loop. loop: ; preds = %loop, %entry; %i = phi double [ 1.000000e+00, %entry ], [ %nextvar, %loop ]; ; body; %calltmp = call double @putchard(double 4.200000e+01); ; increment; %nextvar = fadd double %i, 1.000000e+00. ; termination test; %cmptmp = fcmp ult double %i, %n; %booltmp = uitofp i1 %cmptmp to double; %loopcond = fcmp one double %booltmp, 0.000000e+00; br i1 %loopcond, label %loop, label %afterloop. afterloop: ; preds = %loop; ; loop always returns 0.0; ret double 0.000000e+00; }. This loop contains all the same constructs we saw before: a phi node,; several expressions, and some basic blocks. Let's see how this fits; together. Code Generation for the 'for' Loop; ----------------------------------. The first part of codegen is very simple: we just output the start; expression for the loop value:. .. code-block:: c++. Value *ForExprAST::codegen() {; // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. With this out of the way, the next step is to set up the LLVM basic; block for the start of the loop body. In the case above, the whole loop; body is one block, but remember that the body code itself could consist; of multiple blocks (e.g. if it contains an if/then/else or a for/in; e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:366,Usability,simpl,simple,366,"==================================================; Kaleidoscope: Extending the Language: Control Flow; ==================================================. .. contents::; :local:. Chapter 5 Introduction; ======================. Welcome to Chapter 5 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Parts 1-4 described the implementation of; the simple Kaleidoscope language and included support for generating; LLVM IR, followed by optimizations and a JIT compiler. Unfortunately, as; presented, Kaleidoscope is mostly useless: it has no control flow other; than call and return. This means that you can't have conditional; branches in the code, significantly limiting its power. In this episode; of ""build that compiler"", we'll extend Kaleidoscope to have an; if/then/else expression plus a simple 'for' loop. If/Then/Else; ============. Extending Kaleidoscope to support if/then/else is quite straightforward.; It basically requires adding support for this ""new"" concept to the; lexer, parser, AST, and LLVM code emitter. This example is nice, because; it shows how easy it is to ""grow"" a language over time, incrementally; extending it as new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:813,Usability,simpl,simple,813,"==================================================; Kaleidoscope: Extending the Language: Control Flow; ==================================================. .. contents::; :local:. Chapter 5 Introduction; ======================. Welcome to Chapter 5 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. Parts 1-4 described the implementation of; the simple Kaleidoscope language and included support for generating; LLVM IR, followed by optimizations and a JIT compiler. Unfortunately, as; presented, Kaleidoscope is mostly useless: it has no control flow other; than call and return. This means that you can't have conditional; branches in the code, significantly limiting its power. In this episode; of ""build that compiler"", we'll extend Kaleidoscope to have an; if/then/else expression plus a simple 'for' loop. If/Then/Else; ============. Extending Kaleidoscope to support if/then/else is quite straightforward.; It basically requires adding support for this ""new"" concept to the; lexer, parser, AST, and LLVM code emitter. This example is nice, because; it shows how easy it is to ""grow"" a language over time, incrementally; extending it as new ideas are discovered. Before we get going on ""how"" we add this extension, let's talk about; ""what"" we want. The basic idea is that we want to be able to write this; sort of thing:. ::. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. In Kaleidoscope, every construct is an expression: there are no; statements. As such, the if/then/else expression needs to return a value; like any other. Since we're using a mostly functional form, we'll have; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:2616,Usability,simpl,simple,2616,"ave; it evaluate its conditional, then return the 'then' or 'else' value; based on how the condition was resolved. This is very similar to the C; ""?:"" expression. The semantics of the if/then/else expression is that it evaluates the; condition to a boolean equality value: 0.0 is considered to be false and; everything else is considered to be true. If the condition is true, the; first subexpression is evaluated and returned, if the condition is; false, the second subexpression is evaluated and returned. Since; Kaleidoscope allows side-effects, this behavior is important to nail; down. Now that we know what we ""want"", let's break this down into its; constituent pieces. Lexer Extensions for If/Then/Else; ---------------------------------. The lexer extensions are straightforward. First we add new enum values; for the relevant tokens:. .. code-block:: c++. // control; tok_if = -6,; tok_then = -7,; tok_else = -8,. Once we have that, we recognize the new keywords in the lexer. This is; pretty simple stuff:. .. code-block:: c++. ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); return tok_else;; return tok_identifier;. AST Extensions for If/Then/Else; -------------------------------. To represent the new expression we add a new AST node for it:. .. code-block:: c++. /// IfExprAST - Expression class for if/then/else.; class IfExprAST : public ExprAST {; std::unique_ptr<ExprAST> Cond, Then, Else;. public:; IfExprAST(std::unique_ptr<ExprAST> Cond, std::unique_ptr<ExprAST> Then,; std::unique_ptr<ExprAST> Else); : Cond(std::move(Cond)), Then(std::move(Then)), Else(std::move(Else)) {}. Value *codegen() override;; };. The AST node just has pointers to the various subexpressions. Parser Extensions for If/Then/Else; ----------------------------------. Now that we have the relevant tokens coming from the lexer a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:5179,Usability,simpl,simple,5179,"tok_else); return LogError(""expected else"");. getNextToken();. auto Else = ParseExpression();; if (!Else); return nullptr;. return std::make_unique<IfExprAST>(std::move(Cond), std::move(Then),; std::move(Else));; }. Next we hook it up as a primary expression:. .. code-block:: c++. static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; }; }. LLVM IR for If/Then/Else; ------------------------. Now that we have it parsing and building the AST, the final piece is; adding LLVM code generation support. This is the most interesting part; of the if/then/else example, because this is where it starts to; introduce new concepts. All of the code above has been thoroughly; described in previous chapters. To motivate the code we want to produce, let's take a look at a simple; example. Consider:. ::. extern foo();; extern bar();; def baz(x) if x then foo() else bar();. If you disable optimizations, the code you'll (soon) get from; Kaleidoscope looks like this:. .. code-block:: llvm. declare double @foo(). declare double @bar(). define double @baz(double %x) {; entry:; %ifcond = fcmp one double %x, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; %calltmp = call double @foo(); br label %ifcont. else: ; preds = %entry; %calltmp1 = call double @bar(); br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ %calltmp, %then ], [ %calltmp1, %else ]; ret double %iftmp; }. To visualize the control flow graph, you can use a nifty feature of the; LLVM '`opt <https://llvm.org/cmds/opt.html>`_' tool. If you put this LLVM; IR into ""t.ll"" and run ""``llvm-as < t.ll | opt -passes=view-cfg``"", `a; window will pop up <../../ProgrammersManual.html#viewing-graphs-while-debugging-code>`_ and you'll; se",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:6574,Usability,simpl,simple,6574,"ll double @foo(); br label %ifcont. else: ; preds = %entry; %calltmp1 = call double @bar(); br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ %calltmp, %then ], [ %calltmp1, %else ]; ret double %iftmp; }. To visualize the control flow graph, you can use a nifty feature of the; LLVM '`opt <https://llvm.org/cmds/opt.html>`_' tool. If you put this LLVM; IR into ""t.ll"" and run ""``llvm-as < t.ll | opt -passes=view-cfg``"", `a; window will pop up <../../ProgrammersManual.html#viewing-graphs-while-debugging-code>`_ and you'll; see this graph:. .. figure:: LangImpl05-cfg.png; :align: center; :alt: Example CFG. Example CFG. Another way to get this is to call ""``F->viewCFG()``"" or; ""``F->viewCFGOnly()``"" (where F is a ""``Function*``"") either by; inserting actual calls into the code and recompiling or by calling these; in the debugger. LLVM has many nice features for visualizing various; graphs. Getting back to the generated code, it is fairly simple: the entry block; evaluates the conditional expression (""x"" in our case here) and compares; the result to 0.0 with the ""``fcmp one``"" instruction ('one' is ""Ordered; and Not Equal""). Based on the result of this expression, the code jumps; to either the ""then"" or ""else"" blocks, which contain the expressions for; the true/false cases. Once the then/else blocks are finished executing, they both branch back; to the 'ifcont' block to execute the code that happens after the; if/then/else. In this case the only thing left to do is to return to the; caller of the function. The question then becomes: how does the code; know which expression to return?. The answer to this question involves an important SSA operation: the; `Phi; operation <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; If you're not familiar with SSA, `the wikipedia; article <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; is a good introduction and there are various other introductions to it; available on your favorite sea",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:8051,Usability,simpl,simple,8051,"en/else. In this case the only thing left to do is to return to the; caller of the function. The question then becomes: how does the code; know which expression to return?. The answer to this question involves an important SSA operation: the; `Phi; operation <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; If you're not familiar with SSA, `the wikipedia; article <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_; is a good introduction and there are various other introductions to it; available on your favorite search engine. The short version is that; ""execution"" of the Phi operation requires ""remembering"" which block; control came from. The Phi operation takes on the value corresponding to; the input control block. In this case, if control comes in from the; ""then"" block, it gets the value of ""calltmp"". If control comes from the; ""else"" block, it gets the value of ""calltmp1"". At this point, you are probably starting to think ""Oh no! This means my; simple and elegant front-end will have to start generating SSA form in; order to use LLVM!"". Fortunately, this is not the case, and we strongly; advise *not* implementing an SSA construction algorithm in your; front-end unless there is an amazingly good reason to do so. In; practice, there are two sorts of values that float around in code; written for your average imperative programming language that might need; Phi nodes:. #. Code that involves user variables: ``x = 1; x = x + 1;``; #. Values that are implicit in the structure of your AST, such as the; Phi node in this case. In `Chapter 7 <LangImpl07.html>`_ of this tutorial (""mutable variables""),; we'll talk about #1 in depth. For now, just believe me that you don't; need SSA construction to handle this case. For #2, you have the choice; of using the techniques that we will describe for #1, or you can insert; Phi nodes directly, if convenient. In this case, it is really; easy to generate the Phi node, so we choose to do it directly. Okay, eno",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:16825,Usability,simpl,simple,16825,"xpression. Because we don't have anything better; to return, we'll just define the loop as always returning 0.0. In the; future when we have mutable variables, it will get more useful. As before, let's talk about the changes that we need to Kaleidoscope to; support this. Lexer Extensions for the 'for' Loop; -----------------------------------. The lexer extensions are the same sort of thing as for if/then/else:. .. code-block:: c++. ... in enum Token ...; // control; tok_if = -6, tok_then = -7, tok_else = -8,; tok_for = -9, tok_in = -10. ... in gettok ...; if (IdentifierStr == ""def""); return tok_def;; if (IdentifierStr == ""extern""); return tok_extern;; if (IdentifierStr == ""if""); return tok_if;; if (IdentifierStr == ""then""); return tok_then;; if (IdentifierStr == ""else""); return tok_else;; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; return tok_identifier;. AST Extensions for the 'for' Loop; ---------------------------------. The AST node is just as simple. It basically boils down to capturing the; variable name and the constituent expressions in the node. .. code-block:: c++. /// ForExprAST - Expression class for for/in.; class ForExprAST : public ExprAST {; std::string VarName;; std::unique_ptr<ExprAST> Start, End, Step, Body;. public:; ForExprAST(const std::string &VarName, std::unique_ptr<ExprAST> Start,; std::unique_ptr<ExprAST> End, std::unique_ptr<ExprAST> Step,; std::unique_ptr<ExprAST> Body); : VarName(VarName), Start(std::move(Start)), End(std::move(End)),; Step(std::move(Step)), Body(std::move(Body)) {}. Value *codegen() override;; };. Parser Extensions for the 'for' Loop; ------------------------------------. The parser code is also fairly standard. The only interesting thing here; is handling of the optional step value. The parser code handles it by; checking to see if the second comma is present. If not, it sets the step; value to null in the AST node:. .. code-block:: c++. /// forexpr ::= 'for' identifier '",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:19418,Usability,simpl,simple,19418,"(CurTok == ',') {; getNextToken();; Step = ParseExpression();; if (!Step); return nullptr;; }. if (CurTok != tok_in); return LogError(""expected 'in' after for"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<ForExprAST>(IdName, std::move(Start),; std::move(End), std::move(Step),; std::move(Body));; }. And again we hook it up as a primary expression:. .. code-block:: c++. static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; case tok_for:; return ParseForExpr();; }; }. LLVM IR for the 'for' Loop; --------------------------. Now we get to the good part: the LLVM IR we want to generate for this; thing. With the simple example above, we get this LLVM IR (note that; this dump is generated with optimizations disabled for clarity):. .. code-block:: llvm. declare double @putchard(double). define double @printstar(double %n) {; entry:; ; initial value = 1.0 (inlined into phi); br label %loop. loop: ; preds = %loop, %entry; %i = phi double [ 1.000000e+00, %entry ], [ %nextvar, %loop ]; ; body; %calltmp = call double @putchard(double 4.200000e+01); ; increment; %nextvar = fadd double %i, 1.000000e+00. ; termination test; %cmptmp = fcmp ult double %i, %n; %booltmp = uitofp i1 %cmptmp to double; %loopcond = fcmp one double %booltmp, 0.000000e+00; br i1 %loopcond, label %loop, label %afterloop. afterloop: ; preds = %loop; ; loop always returns 0.0; ret double 0.000000e+00; }. This loop contains all the same constructs we saw before: a phi node,; several expressions, and some basic blocks. Let's see how this fits; together. Code Generation for the 'for' Loop; ----------------------------------. The first part of codegen is very simple: we just output the sta",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst:20443,Usability,simpl,simple,20443,"R (note that; this dump is generated with optimizations disabled for clarity):. .. code-block:: llvm. declare double @putchard(double). define double @printstar(double %n) {; entry:; ; initial value = 1.0 (inlined into phi); br label %loop. loop: ; preds = %loop, %entry; %i = phi double [ 1.000000e+00, %entry ], [ %nextvar, %loop ]; ; body; %calltmp = call double @putchard(double 4.200000e+01); ; increment; %nextvar = fadd double %i, 1.000000e+00. ; termination test; %cmptmp = fcmp ult double %i, %n; %booltmp = uitofp i1 %cmptmp to double; %loopcond = fcmp one double %booltmp, 0.000000e+00; br i1 %loopcond, label %loop, label %afterloop. afterloop: ; preds = %loop; ; loop always returns 0.0; ret double 0.000000e+00; }. This loop contains all the same constructs we saw before: a phi node,; several expressions, and some basic blocks. Let's see how this fits; together. Code Generation for the 'for' Loop; ----------------------------------. The first part of codegen is very simple: we just output the start; expression for the loop value:. .. code-block:: c++. Value *ForExprAST::codegen() {; // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. With this out of the way, the next step is to set up the LLVM basic; block for the start of the loop body. In the case above, the whole loop; body is one block, but remember that the body code itself could consist; of multiple blocks (e.g. if it contains an if/then/else or a for/in; expression). .. code-block:: c++. // Make the new basic block for the loop header, inserting after current; // block.; Function *TheFunction = Builder->GetInsertBlock()->getParent();; BasicBlock *PreheaderBB = Builder->GetInsertBlock();; BasicBlock *LoopBB =; BasicBlock::Create(*TheContext, ""loop"", TheFunction);. // Insert an explicit fall through from the current block to the LoopBB.; Builder->CreateBr(LoopBB);. This code is similar to what we saw for if/then/else. Because we ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl05.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:3122,Availability,down,down,3122,"cedence parsing, it is very easy to allow; the programmer to introduce new operators into the grammar: the grammar; is dynamically extensible as the JIT runs. The two specific features we'll add are programmable unary operators; (right now, Kaleidoscope has no unary operators at all) as well as; binary operators. An example of this is:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Define > with the same precedence as <.; def binary> 10 (LHS RHS); RHS < LHS;. # Binary ""logical or"", (note that it does not ""short circuit""); def binary| 5 (LHS RHS); if LHS then; 1; else if RHS then; 1; else; 0;. # Define = with slightly lower precedence than relationals.; def binary= 9 (LHS RHS); !(LHS < RHS | LHS > RHS);. Many languages aspire to being able to implement their standard runtime; library in the language itself. In Kaleidoscope, we can implement; significant parts of the language in the library!. We will break down implementation of these features into two parts:; implementing support for user-defined binary operators and adding unary; operators. User-defined Binary Operators; =============================. Adding support for user-defined binary operators is pretty simple with; our current framework. We'll first add support for the unary/binary; keywords:. .. code-block:: c++. enum Token {; ...; // operators; tok_binary = -11,; tok_unary = -12; };; ...; static int gettok() {; ...; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; return tok_identifier;. This just adds lexer support for the unary and binary keywords, like we; did in `previous chapters <LangImpl05.html#lexer-extensions-for-if-then-else>`_. One nice thing; about our current AST, is that we represent binary operators with full; generalisation by using their ASCII code as the opcode. For our extended; operators, we'll use this same representation, so ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:9021,Availability,down,down,9021," {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(*TheContext),; ""booltmp"");; default:; break;; }. // If it wasn't a builtin binary operator, it must be a user defined one. Emit; // a call to it.; Function *F = getFunction(std::string(""binary"") + Op);; assert(F && ""binary operator not found!"");. Value *Ops[2] = { L, R };; return Builder->CreateCall(F, Ops, ""binop"");; }. As you can see above, the new code is actually really simple. It just; does a lookup for the appropriate operator in the symbol table and; generates a function call to it. Since user-defined operators are just; built as normal functions (because the ""prototype"" boils down to a; function with the right name) everything falls into place. The final piece of code we are missing, is a bit of top-level magic:. .. code-block:: c++. Function *FunctionAST::codegen() {; // Transfer ownership of the prototype to the FunctionProtos map, but keep a; // reference to it for use below.; auto &P = *Proto;; FunctionProtos[Proto->getName()] = std::move(Proto);; Function *TheFunction = getFunction(P.getName());; if (!TheFunction); return nullptr;. // If this is an operator, install it.; if (P.isBinaryOp()); BinopPrecedence[P.getOperatorName()] = P.getBinaryPrecedence();. // Create a new basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; ... Basically, before codegening a function, if it is a user-defined; operator, we register it in the precedence table. This allows the binary; operator parsing logic we already have in place to handle it. Since we; are working on a fully-general operator precede",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:27260,Availability,avail,available,27260,".....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, you will need to specify -rdynamic or; -Wl,--export-dynamic when linking. This ensures that symbols defined in; the main executable are exported to the dynamic linker and so are; available for symbol resolution at run time. This is not needed if you; compile your support code into a shared library, although doing that; will cause problems on Windows. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter6/toy.cpp; :language: c++. `Next: Extending the language: mutable variables / SSA; construction <LangImpl07.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:9518,Deployability,install,install,9518,"ry operator, it must be a user defined one. Emit; // a call to it.; Function *F = getFunction(std::string(""binary"") + Op);; assert(F && ""binary operator not found!"");. Value *Ops[2] = { L, R };; return Builder->CreateCall(F, Ops, ""binop"");; }. As you can see above, the new code is actually really simple. It just; does a lookup for the appropriate operator in the symbol table and; generates a function call to it. Since user-defined operators are just; built as normal functions (because the ""prototype"" boils down to a; function with the right name) everything falls into place. The final piece of code we are missing, is a bit of top-level magic:. .. code-block:: c++. Function *FunctionAST::codegen() {; // Transfer ownership of the prototype to the FunctionProtos map, but keep a; // reference to it for use below.; auto &P = *Proto;; FunctionProtos[Proto->getName()] = std::move(Proto);; Function *TheFunction = getFunction(P.getName());; if (!TheFunction); return nullptr;. // If this is an operator, install it.; if (P.isBinaryOp()); BinopPrecedence[P.getOperatorName()] = P.getBinaryPrecedence();. // Create a new basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; ... Basically, before codegening a function, if it is a user-defined; operator, we register it in the precedence table. This allows the binary; operator parsing logic we already have in place to handle it. Since we; are working on a fully-general operator precedence parser, this is all; we need to do to ""extend the grammar"". Now we have useful user-defined binary operators. This builds a lot on; the previous framework we built for other operators. Adding unary; operators is a bit more challenging, because we don't have any framework; for it yet - let's see what it takes. User-defined Unary Operators; ============================. Since we don't currently support unary operators in the Kaleidoscope; language, we'll need to add everything to support them. Ab",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:863,Energy Efficiency,power,powerful,863,"============================================================; Kaleidoscope: Extending the Language: User-defined Operators; ============================================================. .. contents::; :local:. Chapter 6 Introduction; ======================. Welcome to Chapter 6 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. At this point in our tutorial, we now; have a fully functional language that is fairly minimal, but also; useful. There is still one big problem with it, however. Our language; doesn't have many useful operators (like division, logical negation, or; even any comparisons besides less-than). This chapter of the tutorial takes a wild digression into adding; user-defined operators to the simple and beautiful Kaleidoscope; language. This digression now gives us a simple and ugly language in; some ways, but also a powerful one at the same time. One of the great; things about creating your own language is that you get to decide what; is good or bad. In this tutorial we'll assume that it is okay to use; this as a way to show some interesting parsing techniques. At the end of this tutorial, we'll run through an example Kaleidoscope; application that `renders the Mandelbrot set <#kicking-the-tires>`_. This gives an; example of what you can build with Kaleidoscope and its feature set. User-defined Operators: the Idea; ================================. The ""operator overloading"" that we will add to Kaleidoscope is more; general than in languages like C++. In C++, you are only allowed to; redefine existing operators: you can't programmatically change the; grammar, introduce new operators, change precedence levels, etc. In this; chapter, we will add this capability to Kaleidoscope, which will let the; user round out the set of operators that are supported. The point of going into user-defined operators in a tutorial like this; is to show the power and flexibility of using a hand-written parser.; Thus far, the parser we have been implement",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:1903,Energy Efficiency,power,power,1903,"me time. One of the great; things about creating your own language is that you get to decide what; is good or bad. In this tutorial we'll assume that it is okay to use; this as a way to show some interesting parsing techniques. At the end of this tutorial, we'll run through an example Kaleidoscope; application that `renders the Mandelbrot set <#kicking-the-tires>`_. This gives an; example of what you can build with Kaleidoscope and its feature set. User-defined Operators: the Idea; ================================. The ""operator overloading"" that we will add to Kaleidoscope is more; general than in languages like C++. In C++, you are only allowed to; redefine existing operators: you can't programmatically change the; grammar, introduce new operators, change precedence levels, etc. In this; chapter, we will add this capability to Kaleidoscope, which will let the; user round out the set of operators that are supported. The point of going into user-defined operators in a tutorial like this; is to show the power and flexibility of using a hand-written parser.; Thus far, the parser we have been implementing uses recursive descent; for most parts of the grammar and operator precedence parsing for the; expressions. See `Chapter 2 <LangImpl02.html>`_ for details. By; using operator precedence parsing, it is very easy to allow; the programmer to introduce new operators into the grammar: the grammar; is dynamically extensible as the JIT runs. The two specific features we'll add are programmable unary operators; (right now, Kaleidoscope has no unary operators at all) as well as; binary operators. An example of this is:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Define > with the same precedence as <.; def binary> 10 (LHS RHS); RHS < LHS;. # Binary ""logical or"", (note that it does not ""short circuit""); def binary| 5 (LHS RHS); if LHS then; 1; else if RHS then; 1; else; 0;. # Define = with slightly lower precedence than relationals.; def binary= 9 (LHS RH",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:25794,Energy Efficiency,power,powerful,25794,"........... ........+++++++++++++++++++++++****; ++++++++++++++++++++++........... ..........++++++++++++++++++++++***; ++++++++++++++++++++........... .........++++++++++++++++++++++*; ++++++++++++++++++............ ...........++++++++++++++++++++; ++++++++++++++++............... .............++++++++++++++++++; ++++++++++++++................. ...............++++++++++++++++; ++++++++++++.................. .................++++++++++++++; +++++++++.................. .................+++++++++++++; ++++++........ . ......... ..++++++++++++; ++............ ...... ....++++++++++; .............. ...++++++++++; .............. ....+++++++++; .............. .....++++++++; ............. ......++++++++; ........... .......++++++++; ......... ........+++++++; ......... ........+++++++; ......... ....+++++++; ........ ...+++++++; ....... ...+++++++; ....+++++++; .....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; ================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:4124,Modifiability,extend,extended,4124,"ts:; implementing support for user-defined binary operators and adding unary; operators. User-defined Binary Operators; =============================. Adding support for user-defined binary operators is pretty simple with; our current framework. We'll first add support for the unary/binary; keywords:. .. code-block:: c++. enum Token {; ...; // operators; tok_binary = -11,; tok_unary = -12; };; ...; static int gettok() {; ...; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; return tok_identifier;. This just adds lexer support for the unary and binary keywords, like we; did in `previous chapters <LangImpl05.html#lexer-extensions-for-if-then-else>`_. One nice thing; about our current AST, is that we represent binary operators with full; generalisation by using their ASCII code as the opcode. For our extended; operators, we'll use this same representation, so we don't need any new; AST or parser support. On the other hand, we have to be able to represent the definitions of; these new operators, in the ""def binary\| 5"" part of the function; definition. In our grammar so far, the ""name"" for the function; definition is parsed as the ""prototype"" production and into the; ``PrototypeAST`` AST node. To represent our new user-defined operators; as prototypes, we have to extend the ``PrototypeAST`` AST node like; this:. .. code-block:: c++. /// PrototypeAST - This class represents the ""prototype"" for a function,; /// which captures its argument names as well as if it is an operator.; class PrototypeAST {; std::string Name;; std::vector<std::string> Args;; bool IsOperator;; unsigned Precedence; // Precedence if a binary op. public:; PrototypeAST(const std::string &Name, std::vector<std::string> Args,; bool IsOperator = false, unsigned Prec = 0); : Name(Name), Args(std::move(Args)), IsOperator(IsOperator),; Precedence(Prec) {}. Function *c",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:4595,Modifiability,extend,extend,4595,"t gettok() {; ...; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; return tok_identifier;. This just adds lexer support for the unary and binary keywords, like we; did in `previous chapters <LangImpl05.html#lexer-extensions-for-if-then-else>`_. One nice thing; about our current AST, is that we represent binary operators with full; generalisation by using their ASCII code as the opcode. For our extended; operators, we'll use this same representation, so we don't need any new; AST or parser support. On the other hand, we have to be able to represent the definitions of; these new operators, in the ""def binary\| 5"" part of the function; definition. In our grammar so far, the ""name"" for the function; definition is parsed as the ""prototype"" production and into the; ``PrototypeAST`` AST node. To represent our new user-defined operators; as prototypes, we have to extend the ``PrototypeAST`` AST node like; this:. .. code-block:: c++. /// PrototypeAST - This class represents the ""prototype"" for a function,; /// which captures its argument names as well as if it is an operator.; class PrototypeAST {; std::string Name;; std::vector<std::string> Args;; bool IsOperator;; unsigned Precedence; // Precedence if a binary op. public:; PrototypeAST(const std::string &Name, std::vector<std::string> Args,; bool IsOperator = false, unsigned Prec = 0); : Name(Name), Args(std::move(Args)), IsOperator(IsOperator),; Precedence(Prec) {}. Function *codegen();; const std::string &getName() const { return Name; }. bool isUnaryOp() const { return IsOperator && Args.size() == 1; }; bool isBinaryOp() const { return IsOperator && Args.size() == 2; }. char getOperatorName() const {; assert(isUnaryOp() || isBinaryOp());; return Name[Name.size() - 1];; }. unsigned getBinaryPrecedence() const { return Precedence; }; };. Basically, in addition to knowing a name for",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:10050,Modifiability,extend,extend,10050,"type"" boils down to a; function with the right name) everything falls into place. The final piece of code we are missing, is a bit of top-level magic:. .. code-block:: c++. Function *FunctionAST::codegen() {; // Transfer ownership of the prototype to the FunctionProtos map, but keep a; // reference to it for use below.; auto &P = *Proto;; FunctionProtos[Proto->getName()] = std::move(Proto);; Function *TheFunction = getFunction(P.getName());; if (!TheFunction); return nullptr;. // If this is an operator, install it.; if (P.isBinaryOp()); BinopPrecedence[P.getOperatorName()] = P.getBinaryPrecedence();. // Create a new basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; ... Basically, before codegening a function, if it is a user-defined; operator, we register it in the precedence table. This allows the binary; operator parsing logic we already have in place to handle it. Since we; are working on a fully-general operator precedence parser, this is all; we need to do to ""extend the grammar"". Now we have useful user-defined binary operators. This builds a lot on; the previous framework we built for other operators. Adding unary; operators is a bit more challenging, because we don't have any framework; for it yet - let's see what it takes. User-defined Unary Operators; ============================. Since we don't currently support unary operators in the Kaleidoscope; language, we'll need to add everything to support them. Above, we added; simple support for the 'unary' keyword to the lexer. In addition to; that, we need an AST node:. .. code-block:: c++. /// UnaryExprAST - Expression class for a unary operator.; class UnaryExprAST : public ExprAST {; char Opcode;; std::unique_ptr<ExprAST> Operand;. public:; UnaryExprAST(char Opcode, std::unique_ptr<ExprAST> Operand); : Opcode(Opcode), Operand(std::move(Operand)) {}. Value *codegen() override;; };. This AST node is very simple and obvious by now. It directly mirrors",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:12908,Modifiability,extend,extend,12908,"""!!x""). Note that; unary operators can't have ambiguous parses like binary operators can,; so there is no need for precedence information. The problem with this function, is that we need to call ParseUnary from; somewhere. To do this, we change previous callers of ParsePrimary to; call ParseUnary instead:. .. code-block:: c++. /// binoprhs; /// ::= ('+' unary)*; static std::unique_ptr<ExprAST> ParseBinOpRHS(int ExprPrec,; std::unique_ptr<ExprAST> LHS) {; ...; // Parse the unary expression after the binary operator.; auto RHS = ParseUnary();; if (!RHS); return nullptr;; ...; }; /// expression; /// ::= unary binoprhs; ///; static std::unique_ptr<ExprAST> ParseExpression() {; auto LHS = ParseUnary();; if (!LHS); return nullptr;. return ParseBinOpRHS(0, std::move(LHS));; }. With these two simple changes, we are now able to parse unary operators; and build the AST for them. Next up, we need to add parser support for; prototypes, to parse the unary operator prototype. We extend the binary; operator code above with:. .. code-block:: c++. /// prototype; /// ::= id '(' id* ')'; /// ::= binary LETTER number? (id, id); /// ::= unary LETTER (id); static std::unique_ptr<PrototypeAST> ParsePrototype() {; std::string FnName;. unsigned Kind = 0; // 0 = identifier, 1 = unary, 2 = binary.; unsigned BinaryPrecedence = 30;. switch (CurTok) {; default:; return LogErrorP(""Expected function name in prototype"");; case tok_identifier:; FnName = IdentifierStr;; Kind = 0;; getNextToken();; break;; case tok_unary:; getNextToken();; if (!isascii(CurTok)); return LogErrorP(""Expected unary operator"");; FnName = ""unary"";; FnName += (char)CurTok;; Kind = 1;; getNextToken();; break;; case tok_binary:; ... As with binary operators, we name unary operators with a name that; includes the operator character. This assists us at code generation; time. Speaking of, the final piece we need to add is codegen support for; unary operators. It looks like this:. .. code-block:: c++. Value *UnaryExprAST::codegen()",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:26043,Modifiability,extend,extend,26043,"++++++++++................. ...............++++++++++++++++; ++++++++++++.................. .................++++++++++++++; +++++++++.................. .................+++++++++++++; ++++++........ . ......... ..++++++++++++; ++............ ...... ....++++++++++; .............. ...++++++++++; .............. ....+++++++++; .............. .....++++++++; ............. ......++++++++; ........... .......++++++++; ......... ........+++++++; ......... ........+++++++; ......... ....+++++++; ........ ...+++++++; ....... ...+++++++; ....+++++++; .....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, yo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:26362,Modifiability,variab,variable,26362,"......... ..++++++++++++; ++............ ...... ....++++++++++; .............. ...++++++++++; .............. ....+++++++++; .............. .....++++++++; ............. ......++++++++; ........... .......++++++++; ......... ........+++++++; ......... ........+++++++; ......... ....+++++++; ........ ...+++++++; ....... ...+++++++; ....+++++++; .....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, you will need to specify -rdynamic or; -Wl,--export-dynamic when linking. This ensures that symbols defined in; the main executable are exported to the dynamic linker and so are; available for symbol reso",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:26391,Modifiability,variab,variable,26391,".++++++++; ........... .......++++++++; ......... ........+++++++; ......... ........+++++++; ......... ....+++++++; ........ ...+++++++; ....... ...+++++++; ....+++++++; .....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, you will need to specify -rdynamic or; -Wl,--export-dynamic when linking. This ensures that symbols defined in; the main executable are exported to the dynamic linker and so are; available for symbol resolution at run time. This is not needed if you; compile your support code into a shared library, although doing that; will cause problems on Windows. Here is the code:. .. li",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:26515,Modifiability,variab,variables,26515,".++++++++; ........... .......++++++++; ......... ........+++++++; ......... ........+++++++; ......... ....+++++++; ........ ...+++++++; ....... ...+++++++; ....+++++++; .....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, you will need to specify -rdynamic or; -Wl,--export-dynamic when linking. This ensures that symbols defined in; the main executable are exported to the dynamic linker and so are; available for symbol resolution at run time. This is not needed if you; compile your support code into a shared library, although doing that; will cause problems on Windows. Here is the code:. .. li",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:26670,Modifiability,variab,variable,26670,".....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, you will need to specify -rdynamic or; -Wl,--export-dynamic when linking. This ensures that symbols defined in; the main executable are exported to the dynamic linker and so are; available for symbol resolution at run time. This is not needed if you; compile your support code into a shared library, although doing that; will cause problems on Windows. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter6/toy.cpp; :language: c++. `Next: Extending the language: mutable variables / SSA; construction <LangImpl07.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:26825,Modifiability,enhance,enhanced,26825,".....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, you will need to specify -rdynamic or; -Wl,--export-dynamic when linking. This ensures that symbols defined in; the main executable are exported to the dynamic linker and so are; available for symbol resolution at run time. This is not needed if you; compile your support code into a shared library, although doing that; will cause problems on Windows. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter6/toy.cpp; :language: c++. `Next: Extending the language: mutable variables / SSA; construction <LangImpl07.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:26967,Modifiability,config,config,26967,".....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, you will need to specify -rdynamic or; -Wl,--export-dynamic when linking. This ensures that symbols defined in; the main executable are exported to the dynamic linker and so are; available for symbol resolution at run time. This is not needed if you; compile your support code into a shared library, although doing that; will cause problems on Windows. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter6/toy.cpp; :language: c++. `Next: Extending the language: mutable variables / SSA; construction <LangImpl07.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:27577,Modifiability,variab,variables,27577,".....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, you will need to specify -rdynamic or; -Wl,--export-dynamic when linking. This ensures that symbols defined in; the main executable are exported to the dynamic linker and so are; available for symbol resolution at run time. This is not needed if you; compile your support code into a shared library, although doing that; will cause problems on Windows. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter6/toy.cpp; :language: c++. `Next: Extending the language: mutable variables / SSA; construction <LangImpl07.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:577,Testability,log,logical,577,"============================================================; Kaleidoscope: Extending the Language: User-defined Operators; ============================================================. .. contents::; :local:. Chapter 6 Introduction; ======================. Welcome to Chapter 6 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. At this point in our tutorial, we now; have a fully functional language that is fairly minimal, but also; useful. There is still one big problem with it, however. Our language; doesn't have many useful operators (like division, logical negation, or; even any comparisons besides less-than). This chapter of the tutorial takes a wild digression into adding; user-defined operators to the simple and beautiful Kaleidoscope; language. This digression now gives us a simple and ugly language in; some ways, but also a powerful one at the same time. One of the great; things about creating your own language is that you get to decide what; is good or bad. In this tutorial we'll assume that it is okay to use; this as a way to show some interesting parsing techniques. At the end of this tutorial, we'll run through an example Kaleidoscope; application that `renders the Mandelbrot set <#kicking-the-tires>`_. This gives an; example of what you can build with Kaleidoscope and its feature set. User-defined Operators: the Idea; ================================. The ""operator overloading"" that we will add to Kaleidoscope is more; general than in languages like C++. In C++, you are only allowed to; redefine existing operators: you can't programmatically change the; grammar, introduce new operators, change precedence levels, etc. In this; chapter, we will add this capability to Kaleidoscope, which will let the; user round out the set of operators that are supported. The point of going into user-defined operators in a tutorial like this; is to show the power and flexibility of using a hand-written parser.; Thus far, the parser we have been implement",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:2678,Testability,log,logical,2678,"ope, which will let the; user round out the set of operators that are supported. The point of going into user-defined operators in a tutorial like this; is to show the power and flexibility of using a hand-written parser.; Thus far, the parser we have been implementing uses recursive descent; for most parts of the grammar and operator precedence parsing for the; expressions. See `Chapter 2 <LangImpl02.html>`_ for details. By; using operator precedence parsing, it is very easy to allow; the programmer to introduce new operators into the grammar: the grammar; is dynamically extensible as the JIT runs. The two specific features we'll add are programmable unary operators; (right now, Kaleidoscope has no unary operators at all) as well as; binary operators. An example of this is:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Define > with the same precedence as <.; def binary> 10 (LHS RHS); RHS < LHS;. # Binary ""logical or"", (note that it does not ""short circuit""); def binary| 5 (LHS RHS); if LHS then; 1; else if RHS then; 1; else; 0;. # Define = with slightly lower precedence than relationals.; def binary= 9 (LHS RHS); !(LHS < RHS | LHS > RHS);. Many languages aspire to being able to implement their standard runtime; library in the language itself. In Kaleidoscope, we can implement; significant parts of the language in the library!. We will break down implementation of these features into two parts:; implementing support for user-defined binary operators and adding unary; operators. User-defined Binary Operators; =============================. Adding support for user-defined binary operators is pretty simple with; our current framework. We'll first add support for the unary/binary; keywords:. .. code-block:: c++. enum Token {; ...; // operators; tok_binary = -11,; tok_unary = -12; };; ...; static int gettok() {; ...; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:5403,Testability,assert,assert,5403,"for the function; definition is parsed as the ""prototype"" production and into the; ``PrototypeAST`` AST node. To represent our new user-defined operators; as prototypes, we have to extend the ``PrototypeAST`` AST node like; this:. .. code-block:: c++. /// PrototypeAST - This class represents the ""prototype"" for a function,; /// which captures its argument names as well as if it is an operator.; class PrototypeAST {; std::string Name;; std::vector<std::string> Args;; bool IsOperator;; unsigned Precedence; // Precedence if a binary op. public:; PrototypeAST(const std::string &Name, std::vector<std::string> Args,; bool IsOperator = false, unsigned Prec = 0); : Name(Name), Args(std::move(Args)), IsOperator(IsOperator),; Precedence(Prec) {}. Function *codegen();; const std::string &getName() const { return Name; }. bool isUnaryOp() const { return IsOperator && Args.size() == 1; }; bool isBinaryOp() const { return IsOperator && Args.size() == 2; }. char getOperatorName() const {; assert(isUnaryOp() || isBinaryOp());; return Name[Name.size() - 1];; }. unsigned getBinaryPrecedence() const { return Precedence; }; };. Basically, in addition to knowing a name for the prototype, we now keep; track of whether it was an operator, and if it was, what precedence; level the operator is at. The precedence is only used for binary; operators (as you'll see below, it just doesn't apply for unary; operators). Now that we have a way to represent the prototype for a; user-defined operator, we need to parse it:. .. code-block:: c++. /// prototype; /// ::= id '(' id* ')'; /// ::= binary LETTER number? (id, id); static std::unique_ptr<PrototypeAST> ParsePrototype() {; std::string FnName;. unsigned Kind = 0; // 0 = identifier, 1 = unary, 2 = binary.; unsigned BinaryPrecedence = 30;. switch (CurTok) {; default:; return LogErrorP(""Expected function name in prototype"");; case tok_identifier:; FnName = IdentifierStr;; Kind = 0;; getNextToken();; break;; case tok_binary:; getNextToken();; if (!isas",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:8633,Testability,assert,assert,8633," the fact that symbol names in the; LLVM symbol table are allowed to have any character in them, including; embedded nul characters. The next interesting thing to add, is codegen support for these binary; operators. Given our current structure, this is a simple addition of a; default case for our existing binary operator node:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(*TheContext),; ""booltmp"");; default:; break;; }. // If it wasn't a builtin binary operator, it must be a user defined one. Emit; // a call to it.; Function *F = getFunction(std::string(""binary"") + Op);; assert(F && ""binary operator not found!"");. Value *Ops[2] = { L, R };; return Builder->CreateCall(F, Ops, ""binop"");; }. As you can see above, the new code is actually really simple. It just; does a lookup for the appropriate operator in the symbol table and; generates a function call to it. Since user-defined operators are just; built as normal functions (because the ""prototype"" boils down to a; function with the right name) everything falls into place. The final piece of code we are missing, is a bit of top-level magic:. .. code-block:: c++. Function *FunctionAST::codegen() {; // Transfer ownership of the prototype to the FunctionProtos map, but keep a; // reference to it for use below.; auto &P = *Proto;; FunctionProtos[Proto->getName()] = std::move(Proto);; Function *TheFunction = getFunction(P.getName());; if (!TheFunction); return nullptr;. // If this is an operator, install it.; if (P.isBinaryOp()); BinopPrecedence[P.getOperatorName()] = P.getBinaryPrecedence();. // Create",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:9905,Testability,log,logic,9905,"nction call to it. Since user-defined operators are just; built as normal functions (because the ""prototype"" boils down to a; function with the right name) everything falls into place. The final piece of code we are missing, is a bit of top-level magic:. .. code-block:: c++. Function *FunctionAST::codegen() {; // Transfer ownership of the prototype to the FunctionProtos map, but keep a; // reference to it for use below.; auto &P = *Proto;; FunctionProtos[Proto->getName()] = std::move(Proto);; Function *TheFunction = getFunction(P.getName());; if (!TheFunction); return nullptr;. // If this is an operator, install it.; if (P.isBinaryOp()); BinopPrecedence[P.getOperatorName()] = P.getBinaryPrecedence();. // Create a new basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; ... Basically, before codegening a function, if it is a user-defined; operator, we register it in the precedence table. This allows the binary; operator parsing logic we already have in place to handle it. Since we; are working on a fully-general operator precedence parser, this is all; we need to do to ""extend the grammar"". Now we have useful user-defined binary operators. This builds a lot on; the previous framework we built for other operators. Adding unary; operators is a bit more challenging, because we don't have any framework; for it yet - let's see what it takes. User-defined Unary Operators; ============================. Since we don't currently support unary operators in the Kaleidoscope; language, we'll need to add everything to support them. Above, we added; simple support for the 'unary' keyword to the lexer. In addition to; that, we need an AST node:. .. code-block:: c++. /// UnaryExprAST - Expression class for a unary operator.; class UnaryExprAST : public ExprAST {; char Opcode;; std::unique_ptr<ExprAST> Operand;. public:; UnaryExprAST(char Opcode, std::unique_ptr<ExprAST> Operand); : Opcode(Opcode), Operand(std::move(Operand)) ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:11116,Testability,log,logic,11116,"ned binary operators. This builds a lot on; the previous framework we built for other operators. Adding unary; operators is a bit more challenging, because we don't have any framework; for it yet - let's see what it takes. User-defined Unary Operators; ============================. Since we don't currently support unary operators in the Kaleidoscope; language, we'll need to add everything to support them. Above, we added; simple support for the 'unary' keyword to the lexer. In addition to; that, we need an AST node:. .. code-block:: c++. /// UnaryExprAST - Expression class for a unary operator.; class UnaryExprAST : public ExprAST {; char Opcode;; std::unique_ptr<ExprAST> Operand;. public:; UnaryExprAST(char Opcode, std::unique_ptr<ExprAST> Operand); : Opcode(Opcode), Operand(std::move(Operand)) {}. Value *codegen() override;; };. This AST node is very simple and obvious by now. It directly mirrors the; binary operator AST node, except that it only has one child. With this,; we need to add the parsing logic. Parsing a unary operator is pretty; simple: we'll add a new function to do it:. .. code-block:: c++. /// unary; /// ::= primary; /// ::= '!' unary; static std::unique_ptr<ExprAST> ParseUnary() {; // If the current token is not an operator, it must be a primary expr.; if (!isascii(CurTok) || CurTok == '(' || CurTok == ','); return ParsePrimary();. // If this is a unary operator, read it.; int Opc = CurTok;; getNextToken();; if (auto Operand = ParseUnary()); return std::make_unique<UnaryExprAST>(Opc, std::move(Operand));; return nullptr;; }. The grammar we add is pretty straightforward here. If we see a unary; operator when parsing a primary operator, we eat the operator as a; prefix and parse the remaining piece as another unary operator. This; allows us to handle multiple unary operators (e.g. ""!!x""). Note that; unary operators can't have ambiguous parses like binary operators can,; so there is no need for precedence information. The problem with this function, ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:15267,Testability,log,logical,15267,"oesn't need to handle any; predefined operators. Kicking the Tires; =================. It is somewhat hard to believe, but with a few simple extensions we've; covered in the last chapters, we have grown a real-ish language. With; this, we can do a lot of interesting things, including I/O, math, and a; bunch of other things. For example, we can now add a nice sequencing; operator (printd is defined to print out the specified value and a; newline):. ::. ready> extern printd(x);; Read extern:; declare double @printd(double). ready> def binary : 1 (x y) 0; # Low-precedence operator that ignores operands.; ...; ready> printd(123) : printd(456) : printd(789);; 123.000000; 456.000000; 789.000000; Evaluated to 0.000000. We can also define a bunch of other ""primitive"" operations, such as:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Unary negate.; def unary-(v); 0-v;. # Define > with the same precedence as <.; def binary> 10 (LHS RHS); RHS < LHS;. # Binary logical or, which does not short circuit.; def binary| 5 (LHS RHS); if LHS then; 1; else if RHS then; 1; else; 0;. # Binary logical and, which does not short circuit.; def binary& 6 (LHS RHS); if !LHS then; 0; else; !!RHS;. # Define = with slightly lower precedence than relationals.; def binary = 9 (LHS RHS); !(LHS < RHS | LHS > RHS);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. Given the previous if/then/else support, we can also define interesting; functions for I/O. For example, the following prints out a character; whose ""density"" reflects the value passed in: the lower the value, the; denser the character:. ::. ready> extern putchard(char);; ...; ready> def printdensity(d); if d > 8 then; putchard(32) # ' '; else if d > 4 then; putchard(46) # '.'; else if d > 2 then; putchard(43) # '+'; else; putchard(42); # '*'; ...; ready> printdensity(1): printdensity(2): printdensity(3):; printdensity(4): printdensity(5",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:15391,Testability,log,logical,15391,"th a few simple extensions we've; covered in the last chapters, we have grown a real-ish language. With; this, we can do a lot of interesting things, including I/O, math, and a; bunch of other things. For example, we can now add a nice sequencing; operator (printd is defined to print out the specified value and a; newline):. ::. ready> extern printd(x);; Read extern:; declare double @printd(double). ready> def binary : 1 (x y) 0; # Low-precedence operator that ignores operands.; ...; ready> printd(123) : printd(456) : printd(789);; 123.000000; 456.000000; 789.000000; Evaluated to 0.000000. We can also define a bunch of other ""primitive"" operations, such as:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Unary negate.; def unary-(v); 0-v;. # Define > with the same precedence as <.; def binary> 10 (LHS RHS); RHS < LHS;. # Binary logical or, which does not short circuit.; def binary| 5 (LHS RHS); if LHS then; 1; else if RHS then; 1; else; 0;. # Binary logical and, which does not short circuit.; def binary& 6 (LHS RHS); if !LHS then; 0; else; !!RHS;. # Define = with slightly lower precedence than relationals.; def binary = 9 (LHS RHS); !(LHS < RHS | LHS > RHS);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. Given the previous if/then/else support, we can also define interesting; functions for I/O. For example, the following prints out a character; whose ""density"" reflects the value passed in: the lower the value, the; denser the character:. ::. ready> extern putchard(char);; ...; ready> def printdensity(d); if d > 8 then; putchard(32) # ' '; else if d > 4 then; putchard(46) # '.'; else if d > 2 then; putchard(43) # '+'; else; putchard(42); # '*'; ...; ready> printdensity(1): printdensity(2): printdensity(3):; printdensity(4): printdensity(5): printdensity(9):; putchard(10);; **++.; Evaluated to 0.000000. Based on these simple primitive operations, we can start t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:736,Usability,simpl,simple,736,"============================================================; Kaleidoscope: Extending the Language: User-defined Operators; ============================================================. .. contents::; :local:. Chapter 6 Introduction; ======================. Welcome to Chapter 6 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. At this point in our tutorial, we now; have a fully functional language that is fairly minimal, but also; useful. There is still one big problem with it, however. Our language; doesn't have many useful operators (like division, logical negation, or; even any comparisons besides less-than). This chapter of the tutorial takes a wild digression into adding; user-defined operators to the simple and beautiful Kaleidoscope; language. This digression now gives us a simple and ugly language in; some ways, but also a powerful one at the same time. One of the great; things about creating your own language is that you get to decide what; is good or bad. In this tutorial we'll assume that it is okay to use; this as a way to show some interesting parsing techniques. At the end of this tutorial, we'll run through an example Kaleidoscope; application that `renders the Mandelbrot set <#kicking-the-tires>`_. This gives an; example of what you can build with Kaleidoscope and its feature set. User-defined Operators: the Idea; ================================. The ""operator overloading"" that we will add to Kaleidoscope is more; general than in languages like C++. In C++, you are only allowed to; redefine existing operators: you can't programmatically change the; grammar, introduce new operators, change precedence levels, etc. In this; chapter, we will add this capability to Kaleidoscope, which will let the; user round out the set of operators that are supported. The point of going into user-defined operators in a tutorial like this; is to show the power and flexibility of using a hand-written parser.; Thus far, the parser we have been implement",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:812,Usability,simpl,simple,812,"============================================================; Kaleidoscope: Extending the Language: User-defined Operators; ============================================================. .. contents::; :local:. Chapter 6 Introduction; ======================. Welcome to Chapter 6 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. At this point in our tutorial, we now; have a fully functional language that is fairly minimal, but also; useful. There is still one big problem with it, however. Our language; doesn't have many useful operators (like division, logical negation, or; even any comparisons besides less-than). This chapter of the tutorial takes a wild digression into adding; user-defined operators to the simple and beautiful Kaleidoscope; language. This digression now gives us a simple and ugly language in; some ways, but also a powerful one at the same time. One of the great; things about creating your own language is that you get to decide what; is good or bad. In this tutorial we'll assume that it is okay to use; this as a way to show some interesting parsing techniques. At the end of this tutorial, we'll run through an example Kaleidoscope; application that `renders the Mandelbrot set <#kicking-the-tires>`_. This gives an; example of what you can build with Kaleidoscope and its feature set. User-defined Operators: the Idea; ================================. The ""operator overloading"" that we will add to Kaleidoscope is more; general than in languages like C++. In C++, you are only allowed to; redefine existing operators: you can't programmatically change the; grammar, introduce new operators, change precedence levels, etc. In this; chapter, we will add this capability to Kaleidoscope, which will let the; user round out the set of operators that are supported. The point of going into user-defined operators in a tutorial like this; is to show the power and flexibility of using a hand-written parser.; Thus far, the parser we have been implement",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:3382,Usability,simpl,simple,3382,"e'll add are programmable unary operators; (right now, Kaleidoscope has no unary operators at all) as well as; binary operators. An example of this is:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Define > with the same precedence as <.; def binary> 10 (LHS RHS); RHS < LHS;. # Binary ""logical or"", (note that it does not ""short circuit""); def binary| 5 (LHS RHS); if LHS then; 1; else if RHS then; 1; else; 0;. # Define = with slightly lower precedence than relationals.; def binary= 9 (LHS RHS); !(LHS < RHS | LHS > RHS);. Many languages aspire to being able to implement their standard runtime; library in the language itself. In Kaleidoscope, we can implement; significant parts of the language in the library!. We will break down implementation of these features into two parts:; implementing support for user-defined binary operators and adding unary; operators. User-defined Binary Operators; =============================. Adding support for user-defined binary operators is pretty simple with; our current framework. We'll first add support for the unary/binary; keywords:. .. code-block:: c++. enum Token {; ...; // operators; tok_binary = -11,; tok_unary = -12; };; ...; static int gettok() {; ...; if (IdentifierStr == ""for""); return tok_for;; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; return tok_identifier;. This just adds lexer support for the unary and binary keywords, like we; did in `previous chapters <LangImpl05.html#lexer-extensions-for-if-then-else>`_. One nice thing; about our current AST, is that we represent binary operators with full; generalisation by using their ASCII code as the opcode. For our extended; operators, we'll use this same representation, so we don't need any new; AST or parser support. On the other hand, we have to be able to represent the definitions of; these new operators, in the ""def binary\| 5"" part of the function; de",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:7880,Usability,simpl,simple,7880," == tok_identifier); ArgNames.push_back(IdentifierStr);; if (CurTok != ')'); return LogErrorP(""Expected ')' in prototype"");. // success.; getNextToken(); // eat ')'. // Verify right number of names for operator.; if (Kind && ArgNames.size() != Kind); return LogErrorP(""Invalid number of operands for operator"");. return std::make_unique<PrototypeAST>(FnName, std::move(ArgNames), Kind != 0,; BinaryPrecedence);; }. This is all fairly straightforward parsing code, and we have already; seen a lot of similar code in the past. One interesting part about the; code above is the couple lines that set up ``FnName`` for binary; operators. This builds names like ""binary@"" for a newly defined ""@""; operator. It then takes advantage of the fact that symbol names in the; LLVM symbol table are allowed to have any character in them, including; embedded nul characters. The next interesting thing to add, is codegen support for these binary; operators. Given our current structure, this is a simple addition of a; default case for our existing binary operator node:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(*TheContext),; ""booltmp"");; default:; break;; }. // If it wasn't a builtin binary operator, it must be a user defined one. Emit; // a call to it.; Function *F = getFunction(std::string(""binary"") + Op);; assert(F && ""binary operator not found!"");. Value *Ops[2] = { L, R };; return Builder->CreateCall(F, Ops, ""binop"");; }. As you can see above, the new code is actually really simple. It just; does a lookup for the appropriate operator in the symbol table and; gener",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:8807,Usability,simpl,simple,8807,"g to add, is codegen support for these binary; operators. Given our current structure, this is a simple addition of a; default case for our existing binary operator node:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; Value *L = LHS->codegen();; Value *R = RHS->codegen();; if (!L || !R); return nullptr;. switch (Op) {; case '+':; return Builder->CreateFAdd(L, R, ""addtmp"");; case '-':; return Builder->CreateFSub(L, R, ""subtmp"");; case '*':; return Builder->CreateFMul(L, R, ""multmp"");; case '<':; L = Builder->CreateFCmpULT(L, R, ""cmptmp"");; // Convert bool 0/1 to double 0.0 or 1.0; return Builder->CreateUIToFP(L, Type::getDoubleTy(*TheContext),; ""booltmp"");; default:; break;; }. // If it wasn't a builtin binary operator, it must be a user defined one. Emit; // a call to it.; Function *F = getFunction(std::string(""binary"") + Op);; assert(F && ""binary operator not found!"");. Value *Ops[2] = { L, R };; return Builder->CreateCall(F, Ops, ""binop"");; }. As you can see above, the new code is actually really simple. It just; does a lookup for the appropriate operator in the symbol table and; generates a function call to it. Since user-defined operators are just; built as normal functions (because the ""prototype"" boils down to a; function with the right name) everything falls into place. The final piece of code we are missing, is a bit of top-level magic:. .. code-block:: c++. Function *FunctionAST::codegen() {; // Transfer ownership of the prototype to the FunctionProtos map, but keep a; // reference to it for use below.; auto &P = *Proto;; FunctionProtos[Proto->getName()] = std::move(Proto);; Function *TheFunction = getFunction(P.getName());; if (!TheFunction); return nullptr;. // If this is an operator, install it.; if (P.isBinaryOp()); BinopPrecedence[P.getOperatorName()] = P.getBinaryPrecedence();. // Create a new basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; ... Basically, before codegening a func",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:10525,Usability,simpl,simple,10525,"aryOp()); BinopPrecedence[P.getOperatorName()] = P.getBinaryPrecedence();. // Create a new basic block to start insertion into.; BasicBlock *BB = BasicBlock::Create(*TheContext, ""entry"", TheFunction);; ... Basically, before codegening a function, if it is a user-defined; operator, we register it in the precedence table. This allows the binary; operator parsing logic we already have in place to handle it. Since we; are working on a fully-general operator precedence parser, this is all; we need to do to ""extend the grammar"". Now we have useful user-defined binary operators. This builds a lot on; the previous framework we built for other operators. Adding unary; operators is a bit more challenging, because we don't have any framework; for it yet - let's see what it takes. User-defined Unary Operators; ============================. Since we don't currently support unary operators in the Kaleidoscope; language, we'll need to add everything to support them. Above, we added; simple support for the 'unary' keyword to the lexer. In addition to; that, we need an AST node:. .. code-block:: c++. /// UnaryExprAST - Expression class for a unary operator.; class UnaryExprAST : public ExprAST {; char Opcode;; std::unique_ptr<ExprAST> Operand;. public:; UnaryExprAST(char Opcode, std::unique_ptr<ExprAST> Operand); : Opcode(Opcode), Operand(std::move(Operand)) {}. Value *codegen() override;; };. This AST node is very simple and obvious by now. It directly mirrors the; binary operator AST node, except that it only has one child. With this,; we need to add the parsing logic. Parsing a unary operator is pretty; simple: we'll add a new function to do it:. .. code-block:: c++. /// unary; /// ::= primary; /// ::= '!' unary; static std::unique_ptr<ExprAST> ParseUnary() {; // If the current token is not an operator, it must be a primary expr.; if (!isascii(CurTok) || CurTok == '(' || CurTok == ','); return ParsePrimary();. // If this is a unary operator, read it.; int Opc = CurTok;; getNextTo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:10964,Usability,simpl,simple,10964,"orking on a fully-general operator precedence parser, this is all; we need to do to ""extend the grammar"". Now we have useful user-defined binary operators. This builds a lot on; the previous framework we built for other operators. Adding unary; operators is a bit more challenging, because we don't have any framework; for it yet - let's see what it takes. User-defined Unary Operators; ============================. Since we don't currently support unary operators in the Kaleidoscope; language, we'll need to add everything to support them. Above, we added; simple support for the 'unary' keyword to the lexer. In addition to; that, we need an AST node:. .. code-block:: c++. /// UnaryExprAST - Expression class for a unary operator.; class UnaryExprAST : public ExprAST {; char Opcode;; std::unique_ptr<ExprAST> Operand;. public:; UnaryExprAST(char Opcode, std::unique_ptr<ExprAST> Operand); : Opcode(Opcode), Operand(std::move(Operand)) {}. Value *codegen() override;; };. This AST node is very simple and obvious by now. It directly mirrors the; binary operator AST node, except that it only has one child. With this,; we need to add the parsing logic. Parsing a unary operator is pretty; simple: we'll add a new function to do it:. .. code-block:: c++. /// unary; /// ::= primary; /// ::= '!' unary; static std::unique_ptr<ExprAST> ParseUnary() {; // If the current token is not an operator, it must be a primary expr.; if (!isascii(CurTok) || CurTok == '(' || CurTok == ','); return ParsePrimary();. // If this is a unary operator, read it.; int Opc = CurTok;; getNextToken();; if (auto Operand = ParseUnary()); return std::make_unique<UnaryExprAST>(Opc, std::move(Operand));; return nullptr;; }. The grammar we add is pretty straightforward here. If we see a unary; operator when parsing a primary operator, we eat the operator as a; prefix and parse the remaining piece as another unary operator. This; allows us to handle multiple unary operators (e.g. ""!!x""). Note that; unary operators can",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:11159,Usability,simpl,simple,11159,"ork we built for other operators. Adding unary; operators is a bit more challenging, because we don't have any framework; for it yet - let's see what it takes. User-defined Unary Operators; ============================. Since we don't currently support unary operators in the Kaleidoscope; language, we'll need to add everything to support them. Above, we added; simple support for the 'unary' keyword to the lexer. In addition to; that, we need an AST node:. .. code-block:: c++. /// UnaryExprAST - Expression class for a unary operator.; class UnaryExprAST : public ExprAST {; char Opcode;; std::unique_ptr<ExprAST> Operand;. public:; UnaryExprAST(char Opcode, std::unique_ptr<ExprAST> Operand); : Opcode(Opcode), Operand(std::move(Operand)) {}. Value *codegen() override;; };. This AST node is very simple and obvious by now. It directly mirrors the; binary operator AST node, except that it only has one child. With this,; we need to add the parsing logic. Parsing a unary operator is pretty; simple: we'll add a new function to do it:. .. code-block:: c++. /// unary; /// ::= primary; /// ::= '!' unary; static std::unique_ptr<ExprAST> ParseUnary() {; // If the current token is not an operator, it must be a primary expr.; if (!isascii(CurTok) || CurTok == '(' || CurTok == ','); return ParsePrimary();. // If this is a unary operator, read it.; int Opc = CurTok;; getNextToken();; if (auto Operand = ParseUnary()); return std::make_unique<UnaryExprAST>(Opc, std::move(Operand));; return nullptr;; }. The grammar we add is pretty straightforward here. If we see a unary; operator when parsing a primary operator, we eat the operator as a; prefix and parse the remaining piece as another unary operator. This; allows us to handle multiple unary operators (e.g. ""!!x""). Note that; unary operators can't have ambiguous parses like binary operators can,; so there is no need for precedence information. The problem with this function, is that we need to call ParseUnary from; somewhere. To do this,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:12724,Usability,simpl,simple,12724,"ng a primary operator, we eat the operator as a; prefix and parse the remaining piece as another unary operator. This; allows us to handle multiple unary operators (e.g. ""!!x""). Note that; unary operators can't have ambiguous parses like binary operators can,; so there is no need for precedence information. The problem with this function, is that we need to call ParseUnary from; somewhere. To do this, we change previous callers of ParsePrimary to; call ParseUnary instead:. .. code-block:: c++. /// binoprhs; /// ::= ('+' unary)*; static std::unique_ptr<ExprAST> ParseBinOpRHS(int ExprPrec,; std::unique_ptr<ExprAST> LHS) {; ...; // Parse the unary expression after the binary operator.; auto RHS = ParseUnary();; if (!RHS); return nullptr;; ...; }; /// expression; /// ::= unary binoprhs; ///; static std::unique_ptr<ExprAST> ParseExpression() {; auto LHS = ParseUnary();; if (!LHS); return nullptr;. return ParseBinOpRHS(0, std::move(LHS));; }. With these two simple changes, we are now able to parse unary operators; and build the AST for them. Next up, we need to add parser support for; prototypes, to parse the unary operator prototype. We extend the binary; operator code above with:. .. code-block:: c++. /// prototype; /// ::= id '(' id* ')'; /// ::= binary LETTER number? (id, id); /// ::= unary LETTER (id); static std::unique_ptr<PrototypeAST> ParsePrototype() {; std::string FnName;. unsigned Kind = 0; // 0 = identifier, 1 = unary, 2 = binary.; unsigned BinaryPrecedence = 30;. switch (CurTok) {; default:; return LogErrorP(""Expected function name in prototype"");; case tok_identifier:; FnName = IdentifierStr;; Kind = 0;; getNextToken();; break;; case tok_unary:; getNextToken();; if (!isascii(CurTok)); return LogErrorP(""Expected unary operator"");; FnName = ""unary"";; FnName += (char)CurTok;; Kind = 1;; getNextToken();; break;; case tok_binary:; ... As with binary operators, we name unary operators with a name that; includes the operator character. This assists us at code gener",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:14200,Usability,simpl,simpler,14200," 2 = binary.; unsigned BinaryPrecedence = 30;. switch (CurTok) {; default:; return LogErrorP(""Expected function name in prototype"");; case tok_identifier:; FnName = IdentifierStr;; Kind = 0;; getNextToken();; break;; case tok_unary:; getNextToken();; if (!isascii(CurTok)); return LogErrorP(""Expected unary operator"");; FnName = ""unary"";; FnName += (char)CurTok;; Kind = 1;; getNextToken();; break;; case tok_binary:; ... As with binary operators, we name unary operators with a name that; includes the operator character. This assists us at code generation; time. Speaking of, the final piece we need to add is codegen support for; unary operators. It looks like this:. .. code-block:: c++. Value *UnaryExprAST::codegen() {; Value *OperandV = Operand->codegen();; if (!OperandV); return nullptr;. Function *F = getFunction(std::string(""unary"") + Opcode);; if (!F); return LogErrorV(""Unknown unary operator"");. return Builder->CreateCall(F, OperandV, ""unop"");; }. This code is similar to, but simpler than, the code for binary; operators. It is simpler primarily because it doesn't need to handle any; predefined operators. Kicking the Tires; =================. It is somewhat hard to believe, but with a few simple extensions we've; covered in the last chapters, we have grown a real-ish language. With; this, we can do a lot of interesting things, including I/O, math, and a; bunch of other things. For example, we can now add a nice sequencing; operator (printd is defined to print out the specified value and a; newline):. ::. ready> extern printd(x);; Read extern:; declare double @printd(double). ready> def binary : 1 (x y) 0; # Low-precedence operator that ignores operands.; ...; ready> printd(123) : printd(456) : printd(789);; 123.000000; 456.000000; 789.000000; Evaluated to 0.000000. We can also define a bunch of other ""primitive"" operations, such as:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Unary negate.; def unary-(v); 0-v;. # Define > with the same preced",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:14252,Usability,simpl,simpler,14252,"rn LogErrorP(""Expected function name in prototype"");; case tok_identifier:; FnName = IdentifierStr;; Kind = 0;; getNextToken();; break;; case tok_unary:; getNextToken();; if (!isascii(CurTok)); return LogErrorP(""Expected unary operator"");; FnName = ""unary"";; FnName += (char)CurTok;; Kind = 1;; getNextToken();; break;; case tok_binary:; ... As with binary operators, we name unary operators with a name that; includes the operator character. This assists us at code generation; time. Speaking of, the final piece we need to add is codegen support for; unary operators. It looks like this:. .. code-block:: c++. Value *UnaryExprAST::codegen() {; Value *OperandV = Operand->codegen();; if (!OperandV); return nullptr;. Function *F = getFunction(std::string(""unary"") + Opcode);; if (!F); return LogErrorV(""Unknown unary operator"");. return Builder->CreateCall(F, OperandV, ""unop"");; }. This code is similar to, but simpler than, the code for binary; operators. It is simpler primarily because it doesn't need to handle any; predefined operators. Kicking the Tires; =================. It is somewhat hard to believe, but with a few simple extensions we've; covered in the last chapters, we have grown a real-ish language. With; this, we can do a lot of interesting things, including I/O, math, and a; bunch of other things. For example, we can now add a nice sequencing; operator (printd is defined to print out the specified value and a; newline):. ::. ready> extern printd(x);; Read extern:; declare double @printd(double). ready> def binary : 1 (x y) 0; # Low-precedence operator that ignores operands.; ...; ready> printd(123) : printd(456) : printd(789);; 123.000000; 456.000000; 789.000000; Evaluated to 0.000000. We can also define a bunch of other ""primitive"" operations, such as:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Unary negate.; def unary-(v); 0-v;. # Define > with the same precedence as <.; def binary> 10 (LHS RHS); RHS < LHS;. # Binary logical or, which doe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:14416,Usability,simpl,simple,14416,"ry:; getNextToken();; if (!isascii(CurTok)); return LogErrorP(""Expected unary operator"");; FnName = ""unary"";; FnName += (char)CurTok;; Kind = 1;; getNextToken();; break;; case tok_binary:; ... As with binary operators, we name unary operators with a name that; includes the operator character. This assists us at code generation; time. Speaking of, the final piece we need to add is codegen support for; unary operators. It looks like this:. .. code-block:: c++. Value *UnaryExprAST::codegen() {; Value *OperandV = Operand->codegen();; if (!OperandV); return nullptr;. Function *F = getFunction(std::string(""unary"") + Opcode);; if (!F); return LogErrorV(""Unknown unary operator"");. return Builder->CreateCall(F, OperandV, ""unop"");; }. This code is similar to, but simpler than, the code for binary; operators. It is simpler primarily because it doesn't need to handle any; predefined operators. Kicking the Tires; =================. It is somewhat hard to believe, but with a few simple extensions we've; covered in the last chapters, we have grown a real-ish language. With; this, we can do a lot of interesting things, including I/O, math, and a; bunch of other things. For example, we can now add a nice sequencing; operator (printd is defined to print out the specified value and a; newline):. ::. ready> extern printd(x);; Read extern:; declare double @printd(double). ready> def binary : 1 (x y) 0; # Low-precedence operator that ignores operands.; ...; ready> printd(123) : printd(456) : printd(789);; 123.000000; 456.000000; 789.000000; Evaluated to 0.000000. We can also define a bunch of other ""primitive"" operations, such as:. ::. # Logical unary not.; def unary!(v); if v then; 0; else; 1;. # Unary negate.; def unary-(v); 0-v;. # Define > with the same precedence as <.; def binary> 10 (LHS RHS); RHS < LHS;. # Binary logical or, which does not short circuit.; def binary| 5 (LHS RHS); if LHS then; 1; else if RHS then; 1; else; 0;. # Binary logical and, which does not short circuit.; de",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:16364,Usability,simpl,simple,16364,"ical and, which does not short circuit.; def binary& 6 (LHS RHS); if !LHS then; 0; else; !!RHS;. # Define = with slightly lower precedence than relationals.; def binary = 9 (LHS RHS); !(LHS < RHS | LHS > RHS);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. Given the previous if/then/else support, we can also define interesting; functions for I/O. For example, the following prints out a character; whose ""density"" reflects the value passed in: the lower the value, the; denser the character:. ::. ready> extern putchard(char);; ...; ready> def printdensity(d); if d > 8 then; putchard(32) # ' '; else if d > 4 then; putchard(46) # '.'; else if d > 2 then; putchard(43) # '+'; else; putchard(42); # '*'; ...; ready> printdensity(1): printdensity(2): printdensity(3):; printdensity(4): printdensity(5): printdensity(9):; putchard(10);; **++.; Evaluated to 0.000000. Based on these simple primitive operations, we can start to define more; interesting things. For example, here's a little function that determines; the number of iterations it takes for a certain function in the complex; plane to diverge:. ::. # Determine whether the specific location diverges.; # Solve for z = z^2 + c in the complex plane.; def mandelconverger(real imag iters creal cimag); if iters > 255 | (real*real + imag*imag > 4) then; iters; else; mandelconverger(real*real - imag*imag + creal,; 2*real*imag + cimag,; iters+1, creal, cimag);. # Return the number of iterations required for the iteration to escape; def mandelconverge(real imag); mandelconverger(real, imag, 0, real, imag);. This ""``z = z2 + c``"" function is a beautiful little creature that is; the basis for computation of the `Mandelbrot; Set <http://en.wikipedia.org/wiki/Mandelbrot_set>`_. Our; ``mandelconverge`` function returns the number of iterations that it; takes for a complex orbit to escape, saturating to 255. This is not a; very useful function by itsel",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst:26130,Usability,simpl,simple,26130,"++++++++++................. ...............++++++++++++++++; ++++++++++++.................. .................++++++++++++++; +++++++++.................. .................+++++++++++++; ++++++........ . ......... ..++++++++++++; ++............ ...... ....++++++++++; .............. ...++++++++++; .............. ....+++++++++; .............. .....++++++++; ............. ......++++++++; ........... .......++++++++; ......... ........+++++++; ......... ........+++++++; ......... ....+++++++; ........ ...+++++++; ....... ...+++++++; ....+++++++; .....+++++++; ....+++++++; ....+++++++; ....+++++++; Evaluated to 0.000000; ready> ^D. At this point, you may be starting to realize that Kaleidoscope is a; real and powerful language. It may not be self-similar :), but it can be; used to plot things that are!. With this, we conclude the ""adding user-defined operators"" chapter of; the tutorial. We have successfully augmented our language, adding the; ability to extend the language in the library, and we have shown how; this can be used to build a simple but interesting end-user application; in Kaleidoscope. At this point, Kaleidoscope can build a variety of; applications that are functional and can call functions with; side-effects, but it can't actually define and mutate a variable itself. Strikingly, variable mutation is an important feature of some languages,; and it is not at all obvious how to `add support for mutable; variables <LangImpl07.html>`_ without having to add an ""SSA construction""; phase to your front-end. In the next chapter, we will describe how you; can add variable mutation without building SSA in your front-end. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; the support for user-defined operators. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. On some platforms, yo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl06.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:21105,Availability,error,error,21105,". New Assignment Operator; =======================. With our current framework, adding a new assignment operator is really; simple. We will parse it just like any other binary operator, but handle; it internally (instead of allowing the user to define it). The first; step is to set a precedence:. .. code-block:: c++. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['='] = 2;; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;. Now that the parser knows the precedence of the binary operator, it; takes care of all the parsing and AST generation. We just need to; implement codegen for the assignment operator. This looks like:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; // Special case '=' because we don't want to emit the LHS as an expression.; if (Op == '=') {; // This assume we're building without RTTI because LLVM builds that way by; // default. If you build LLVM with RTTI this can be changed to a; // dynamic_cast for automatic error checking.; VariableExprAST *LHSE = static_cast<VariableExprAST*>(LHS.get());; if (!LHSE); return LogErrorV(""destination of '=' must be a variable"");. Unlike the rest of the binary operators, our assignment operator doesn't; follow the ""emit LHS, emit RHS, do computation"" model. As such, it is; handled as a special case before the other binary operators are handled.; The other strange thing is that it requires the LHS to be a variable. It; is invalid to have ""(x+1) = expr"" - only things like ""x = expr"" are; allowed. .. code-block:: c++. // Codegen the RHS.; Value *Val = RHS->codegen();; if (!Val); return nullptr;. // Look up the name.; Value *Variable = NamedValues[LHSE->getName()];; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the compute",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6323,Deployability,update,update,6323,"n is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:12670,Deployability,update,update,12670,"by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:14216,Deployability,update,update,14216,"ryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... Thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:20004,Deployability,update,updated,20004," double %iftmp; }. This is a trivial case for mem2reg, since there are no redefinitions of; the variable. The point of showing this is to calm your tension about; inserting such blatant inefficiencies :). After the rest of the optimizers run, we get:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp ueq double %booltmp, 0.000000e+00; br i1 %ifcond, label %else, label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; ret double %addtmp. ifcont:; ret double 1.000000e+00; }. Here we see that the simplifycfg pass decided to clone the return; instruction into the end of the 'else' block. This allowed it to; eliminate some branches and the PHI node. Now that all symbol table references are updated to use stack variables,; we'll add the assignment operator. New Assignment Operator; =======================. With our current framework, adding a new assignment operator is really; simple. We will parse it just like any other binary operator, but handle; it internally (instead of allowing the user to define it). The first; step is to set a precedence:. .. code-block:: c++. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['='] = 2;; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;. Now that the parser knows the precedence of the binary operator, it; takes care of all the parsing and AST generation. We just need to; implement codegen for the assignment operator. This looks like:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; // Special case '=' because we don't want to emit the LHS as an expression.; if (Op == '=') {; // This assume we're building without RTTI because LLVM builds that way by; // defau",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:26977,Deployability,install,installing,26977,"rTok != tok_identifier); return LogError(""expected identifier list after var"");; }. Once all the variables are parsed, we then parse the body and create the; AST node:. .. code-block:: c++. // At this point, we have to have 'in'.; if (CurTok != tok_in); return LogError(""expected 'in' keyword after 'var'"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<VarExprAST>(std::move(VarNames),; std::move(Body));; }. Now that we can parse and represent the code, we need to support; emission of LLVM IR for it. This code starts out with:. .. code-block:: c++. Value *VarExprAST::codegen() {; std::vector<AllocaInst *> OldBindings;. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Register all variables and emit their initializer.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i) {; const std::string &VarName = VarNames[i].first;; ExprAST *Init = VarNames[i].second.get();. Basically it loops over all the variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:27983,Deployability,update,update,27983,"variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28051,Deployability,install,installed,28051,"evious value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5309,Energy Efficiency,allocate,allocated,5309,"tore instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:8644,Energy Efficiency,power,powerful,8644,"that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:1751,Integrability,depend,depends,1751,"hat it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7670,Integrability,depend,depend,7670,"operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:1164,Modifiability,variab,variables,1164,"nding the Language: Mutable Variables; =======================================================. .. contents::; :local:. Chapter 7 Introduction; ======================. Welcome to Chapter 7 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 6, we've built a; very respectable, albeit simple, `functional programming; language <http://en.wikipedia.org/wiki/Functional_programming>`_. In our; journey, we learned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:1486,Modifiability,variab,variables,1486,"earned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:1725,Modifiability,variab,variable,1725,"hat it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:2461,Modifiability,variab,variables,2461,"use complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:3199,Modifiability,variab,variables,3199,"ndition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLVM; ==============. The 'trick' here is that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:4133,Modifiability,variab,variable,4133," the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLVM; ==============. The 'trick' here is that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:4316,Modifiability,variab,variables,4316," there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLVM; ==============. The 'trick' here is that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:4518,Modifiability,variab,variables,4518,"s that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:4565,Modifiability,variab,variable,4565,"s that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:4744,Modifiability,variab,variables,4744," other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_fal",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:4825,Modifiability,variab,variable,4825," other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_fal",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5271,Modifiability,variab,variable,5271,"antage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack alloc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5450,Modifiability,variab,variables,5450,"tore instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5498,Modifiability,rewrite,rewrite,5498,"ariables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6144,Modifiability,variab,variables,6144,"ncrement it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, l",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6216,Modifiability,variab,variable,6216,"how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6274,Modifiability,variab,variable,6274,"he LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6337,Modifiability,variab,variable,6337,"n is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6404,Modifiability,variab,variable,6404,"nctions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7625,Modifiability,variab,variables,7625,"operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7717,Modifiability,variab,variables,7717," optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7878,Modifiability,variab,variables,7878," example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9279,Modifiability,variab,variables,9279,"nny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; ================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9373,Modifiability,variab,variables,9373,"orks on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see wha",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9597,Modifiability,variab,variables,9597,"an promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for indu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9646,Modifiability,variab,variables,9646,"an promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for indu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9895,Modifiability,variab,variable,9895,"h this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Def",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:10147,Modifiability,extend,extend,10147,"ason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:10181,Modifiability,variab,variables,10181,"ason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:10467,Modifiability,variab,variables,10467,"as a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:10530,Modifiability,variab,variables,10530," common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:10606,Modifiability,variab,variables,10606," a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation ti",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:10664,Modifiability,variab,variables,10664," a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation ti",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:10751,Modifiability,variab,variables,10751,"ebug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In o",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:11288,Modifiability,variab,variables,11288,"ckle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:11330,Modifiability,variab,variables,11330,"ckle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll al",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:11421,Modifiability,extend,extend,11421," add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these wil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:11456,Modifiability,variab,variable,11456," add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these wil",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:11759,Modifiability,variab,variable,11759,"). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:11893,Modifiability,variab,variable,11893,"ivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:11943,Modifiability,refactor,refactoring,11943,"ust returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(x); if (x < 3) then; 1; else; fib(x-1)+fib(x-2);. # Iterative fib.; def fibi(x); var a = 1, b = 1, c in; (for i = 3, i < x in; c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca ins",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:12190,Modifiability,variab,variables,12190," c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEnt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:12268,Modifiability,variab,variable,12268," c = a + b :; a = b :; b = c) :; b;. # Call it.; fibi(10);. In order to mutate variables, we have to change our existing variables; to use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEnt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:12341,Modifiability,variab,variables,12341," use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. T",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:12385,Modifiability,variab,variables,12385," use the ""alloca trick"". Once we have that, we'll add our new; operator, then extend Kaleidoscope to support new variable definitions. Adjusting Existing Variables for Mutation; =========================================. The symbol table in Kaleidoscope is managed at code generation time by; the '``NamedValues``' map. This map currently keeps track of the LLVM; ""Value\*"" that holds the double value for the named variable. In order; to support mutation, we need to change this slightly, so that; ``NamedValues`` holds the *memory location* of the variable in question.; Note that this change is a refactoring: it changes the structure of the; code, but does not (by itself) change the behavior of the compiler. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. T",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:13060,Modifiability,variab,variables,13060,"r. All; of these changes are isolated in the Kaleidoscope code generator. At this point in Kaleidoscope's development, it only supports variables; for two things: incoming arguments to functions and the induction; variable of 'for' loops. For consistency, we'll allow mutation of these; variables in addition to other user-defined variables. This means that; these will both need memory locations. To start our transformation of Kaleidoscope, we'll change the; ``NamedValues`` map so that it maps to AllocaInst\* instead of Value\*. Once; we do this, the C++ compiler will tell us what parts of the code we need; to update:. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable na",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:13706,Modifiability,variab,variable,13706,":. .. code-block:: c++. static std::map<std::string, AllocaInst*> NamedValues;. Also, since we will need to create these allocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *Start",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:13746,Modifiability,variab,variables,13746,"llocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:13942,Modifiability,variab,variable,13942," c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increme",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:14043,Modifiability,variab,variable,14043,"; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mut",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:14251,Modifiability,variab,variables,14251,"ryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... Thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:14506,Modifiability,variab,variable,14506,"expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:14643,Modifiability,variab,variable,14643,"this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15024,Modifiability,variab,variable,15024,"lues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15311,Modifiability,variab,variables,15311,"'ll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15483,Modifiability,variab,variable,15483,"uilder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15531,Modifiability,variab,variables,15531,"aInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15866,Modifiability,variab,variable,15866," end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:16053,Modifiability,variab,variable,16053,"ilder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-bloc",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17872,Modifiability,variab,variable,17872,"optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then:; br label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:18108,Modifiability,variab,variable,18108,"a double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then:; br label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. This is a trivial case for mem2reg, since there are no redefinitions of; the vari",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:19112,Modifiability,variab,variable,19112,"into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then:; br label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. This is a trivial case for mem2reg, since there are no redefinitions of; the variable. The point of showing this is to calm your tension about; inserting such blatant inefficiencies :). After the rest of the optimizers run, we get:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp ueq double %booltmp, 0.000000e+00; br i1 %ifcond, label %else, label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; ret double %addtmp. ifcont:; ret double 1.000000e+00; }. Here we see that the simplifycfg pass decided to clone the return; instruction into the end of the 'else' block. This allowed it to; eliminate some branches and the PHI node. Now that all symbol table references are updated to use stack variables,; we'll add the assignment operator. New As",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:20025,Modifiability,variab,variables,20025," double %iftmp; }. This is a trivial case for mem2reg, since there are no redefinitions of; the variable. The point of showing this is to calm your tension about; inserting such blatant inefficiencies :). After the rest of the optimizers run, we get:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp ueq double %booltmp, 0.000000e+00; br i1 %ifcond, label %else, label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; ret double %addtmp. ifcont:; ret double 1.000000e+00; }. Here we see that the simplifycfg pass decided to clone the return; instruction into the end of the 'else' block. This allowed it to; eliminate some branches and the PHI node. Now that all symbol table references are updated to use stack variables,; we'll add the assignment operator. New Assignment Operator; =======================. With our current framework, adding a new assignment operator is really; simple. We will parse it just like any other binary operator, but handle; it internally (instead of allowing the user to define it). The first; step is to set a precedence:. .. code-block:: c++. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['='] = 2;; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;. Now that the parser knows the precedence of the binary operator, it; takes care of all the parsing and AST generation. We just need to; implement codegen for the assignment operator. This looks like:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; // Special case '=' because we don't want to emit the LHS as an expression.; if (Op == '=') {; // This assume we're building without RTTI because LLVM builds that way by; // defau",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:21248,Modifiability,variab,variable,21248,"ust like any other binary operator, but handle; it internally (instead of allowing the user to define it). The first; step is to set a precedence:. .. code-block:: c++. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['='] = 2;; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;. Now that the parser knows the precedence of the binary operator, it; takes care of all the parsing and AST generation. We just need to; implement codegen for the assignment operator. This looks like:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; // Special case '=' because we don't want to emit the LHS as an expression.; if (Op == '=') {; // This assume we're building without RTTI because LLVM builds that way by; // default. If you build LLVM with RTTI this can be changed to a; // dynamic_cast for automatic error checking.; VariableExprAST *LHSE = static_cast<VariableExprAST*>(LHS.get());; if (!LHSE); return LogErrorV(""destination of '=' must be a variable"");. Unlike the rest of the binary operators, our assignment operator doesn't; follow the ""emit LHS, emit RHS, do computation"" model. As such, it is; handled as a special case before the other binary operators are handled.; The other strange thing is that it requires the LHS to be a variable. It; is invalid to have ""(x+1) = expr"" - only things like ""x = expr"" are; allowed. .. code-block:: c++. // Codegen the RHS.; Value *Val = RHS->codegen();; if (!Val); return nullptr;. // Look up the name.; Value *Variable = NamedValues[LHSE->getName()];; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:21540,Modifiability,variab,variable,21540,"'<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;. Now that the parser knows the precedence of the binary operator, it; takes care of all the parsing and AST generation. We just need to; implement codegen for the assignment operator. This looks like:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; // Special case '=' because we don't want to emit the LHS as an expression.; if (Op == '=') {; // This assume we're building without RTTI because LLVM builds that way by; // default. If you build LLVM with RTTI this can be changed to a; // dynamic_cast for automatic error checking.; VariableExprAST *LHSE = static_cast<VariableExprAST*>(LHS.get());; if (!LHSE); return LogErrorV(""destination of '=' must be a variable"");. Unlike the rest of the binary operators, our assignment operator doesn't; follow the ""emit LHS, emit RHS, do computation"" model. As such, it is; handled as a special case before the other binary operators are handled.; The other strange thing is that it requires the LHS to be a variable. It; is invalid to have ""(x+1) = expr"" - only things like ""x = expr"" are; allowed. .. code-block:: c++. // Codegen the RHS.; Value *Val = RHS->codegen();; if (!Val); return nullptr;. // Look up the name.; Value *Variable = NamedValues[LHSE->getName()];; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:21845,Modifiability,variab,variable,21845,"e *BinaryExprAST::codegen() {; // Special case '=' because we don't want to emit the LHS as an expression.; if (Op == '=') {; // This assume we're building without RTTI because LLVM builds that way by; // default. If you build LLVM with RTTI this can be changed to a; // dynamic_cast for automatic error checking.; VariableExprAST *LHSE = static_cast<VariableExprAST*>(LHS.get());; if (!LHSE); return LogErrorV(""destination of '=' must be a variable"");. Unlike the rest of the binary operators, our assignment operator doesn't; follow the ""emit LHS, emit RHS, do computation"" model. As such, it is; handled as a special case before the other binary operators are handled.; The other strange thing is that it requires the LHS to be a variable. It; is invalid to have ""(x+1) = expr"" - only things like ""x = expr"" are; allowed. .. code-block:: c++. // Codegen the RHS.; Value *Val = RHS->codegen();; if (!Val); return nullptr;. // Look up the name.; Value *Variable = NamedValues[LHSE->getName()];; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);. test(123);. When run, this example prints ""123"" and then ""4"", showing that we did; actually mutate the value! Okay, we have now officially implemented our; goal: getting this to work requires SSA construction in the general; case. However, to be really useful, we want the ability to define ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:21938,Modifiability,variab,variable,21938,"t way by; // default. If you build LLVM with RTTI this can be changed to a; // dynamic_cast for automatic error checking.; VariableExprAST *LHSE = static_cast<VariableExprAST*>(LHS.get());; if (!LHSE); return LogErrorV(""destination of '=' must be a variable"");. Unlike the rest of the binary operators, our assignment operator doesn't; follow the ""emit LHS, emit RHS, do computation"" model. As such, it is; handled as a special case before the other binary operators are handled.; The other strange thing is that it requires the LHS to be a variable. It; is invalid to have ""(x+1) = expr"" - only things like ""x = expr"" are; allowed. .. code-block:: c++. // Codegen the RHS.; Value *Val = RHS->codegen();; if (!Val); return nullptr;. // Look up the name.; Value *Variable = NamedValues[LHSE->getName()];; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);. test(123);. When run, this example prints ""123"" and then ""4"", showing that we did; actually mutate the value! Okay, we have now officially implemented our; goal: getting this to work requires SSA construction in the general; case. However, to be really useful, we want the ability to define our; own local variables, let's add this next!. User-defined Local Variables; ============================. Adding var/in is just like any other extension we made to; Kaleidoscope: we extend t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:22209,Modifiability,variab,variables,22209," (!LHSE); return LogErrorV(""destination of '=' must be a variable"");. Unlike the rest of the binary operators, our assignment operator doesn't; follow the ""emit LHS, emit RHS, do computation"" model. As such, it is; handled as a special case before the other binary operators are handled.; The other strange thing is that it requires the LHS to be a variable. It; is invalid to have ""(x+1) = expr"" - only things like ""x = expr"" are; allowed. .. code-block:: c++. // Codegen the RHS.; Value *Val = RHS->codegen();; if (!Val); return nullptr;. // Look up the name.; Value *Variable = NamedValues[LHSE->getName()];; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);. test(123);. When run, this example prints ""123"" and then ""4"", showing that we did; actually mutate the value! Okay, we have now officially implemented our; goal: getting this to work requires SSA construction in the general; case. However, to be really useful, we want the ability to define our; own local variables, let's add this next!. User-defined Local Variables; ============================. Adding var/in is just like any other extension we made to; Kaleidoscope: we extend the lexer, the parser, the AST and the code; generator. The first step for adding our new 'var/in' construct is to; extend the lexer. As before, this is pretty trivial, the code looks like; thi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:22822,Modifiability,variab,variables,22822,";; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);. test(123);. When run, this example prints ""123"" and then ""4"", showing that we did; actually mutate the value! Okay, we have now officially implemented our; goal: getting this to work requires SSA construction in the general; case. However, to be really useful, we want the ability to define our; own local variables, let's add this next!. User-defined Local Variables; ============================. Adding var/in is just like any other extension we made to; Kaleidoscope: we extend the lexer, the parser, the AST and the code; generator. The first step for adding our new 'var/in' construct is to; extend the lexer. As before, this is pretty trivial, the code looks like; this:. .. code-block:: c++. enum Token {; ...; // var definition; tok_var = -13; ...; }; ...; static int gettok() {; ...; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; if (IdentifierStr == ""var""); return tok_var;; return tok_identifier;; ... The next step is to define the AST node that we will construct. For; var/in, it looks like this:. .. code-block:: c++. /// VarExprAST - Expression class for var/in; class VarExprAST : public ExprAST {; std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNam",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:22991,Modifiability,extend,extend,22991,"ightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);. test(123);. When run, this example prints ""123"" and then ""4"", showing that we did; actually mutate the value! Okay, we have now officially implemented our; goal: getting this to work requires SSA construction in the general; case. However, to be really useful, we want the ability to define our; own local variables, let's add this next!. User-defined Local Variables; ============================. Adding var/in is just like any other extension we made to; Kaleidoscope: we extend the lexer, the parser, the AST and the code; generator. The first step for adding our new 'var/in' construct is to; extend the lexer. As before, this is pretty trivial, the code looks like; this:. .. code-block:: c++. enum Token {; ...; // var definition; tok_var = -13; ...; }; ...; static int gettok() {; ...; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; if (IdentifierStr == ""var""); return tok_var;; return tok_identifier;; ... The next step is to define the AST node that we will construct. For; var/in, it looks like this:. .. code-block:: c++. /// VarExprAST - Expression class for var/in; class VarExprAST : public ExprAST {; std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames;; std::unique_ptr<ExprAST> Body;. public:; VarExprAST(std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames,; std::unique_ptr<ExprAST> Body); : VarNames(std::move(V",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:23114,Modifiability,extend,extend,23114,"alue allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);. test(123);. When run, this example prints ""123"" and then ""4"", showing that we did; actually mutate the value! Okay, we have now officially implemented our; goal: getting this to work requires SSA construction in the general; case. However, to be really useful, we want the ability to define our; own local variables, let's add this next!. User-defined Local Variables; ============================. Adding var/in is just like any other extension we made to; Kaleidoscope: we extend the lexer, the parser, the AST and the code; generator. The first step for adding our new 'var/in' construct is to; extend the lexer. As before, this is pretty trivial, the code looks like; this:. .. code-block:: c++. enum Token {; ...; // var definition; tok_var = -13; ...; }; ...; static int gettok() {; ...; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; if (IdentifierStr == ""var""); return tok_var;; return tok_identifier;; ... The next step is to define the AST node that we will construct. For; var/in, it looks like this:. .. code-block:: c++. /// VarExprAST - Expression class for var/in; class VarExprAST : public ExprAST {; std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames;; std::unique_ptr<ExprAST> Body;. public:; VarExprAST(std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames,; std::unique_ptr<ExprAST> Body); : VarNames(std::move(VarNames)), Body(std::move(Body)) {}. Value *codegen() override;; };. var/in allows a list of names to be def",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:24290,Modifiability,variab,variables,24290,"; ...; static int gettok() {; ...; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; if (IdentifierStr == ""var""); return tok_var;; return tok_identifier;; ... The next step is to define the AST node that we will construct. For; var/in, it looks like this:. .. code-block:: c++. /// VarExprAST - Expression class for var/in; class VarExprAST : public ExprAST {; std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames;; std::unique_ptr<ExprAST> Body;. public:; VarExprAST(std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames,; std::unique_ptr<ExprAST> Body); : VarNames(std::move(VarNames)), Body(std::move(Body)) {}. Value *codegen() override;; };. var/in allows a list of names to be defined all at once, and each name; can optionally have an initializer value. As such, we capture this; information in the VarNames vector. Also, var/in has a body, this body; is allowed to access the variables defined by the var/in. With this in place, we can define the parser pieces. The first thing we; do is add it as a primary expression:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; /// ::= ifexpr; /// ::= forexpr; /// ::= varexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; case tok_for:; return ParseForExpr();; case tok_var:; return ParseVarExpr();; }; }. Next we define ParseVarExpr:. .. code-block:: c++. /// varexpr ::= 'var' identifier ('=' expression)?; // (',' identifier ('=' expression)?)* 'in' expression; static std::unique_ptr<ExprAST> ParseVarExpr() {; getNextToken(); // eat the var. std::vector<std::pair<std::string, std::unique_ptr<Ex",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:25311,Modifiability,variab,variable,25311,"var/in. With this in place, we can define the parser pieces. The first thing we; do is add it as a primary expression:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; /// ::= ifexpr; /// ::= forexpr; /// ::= varexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; case tok_for:; return ParseForExpr();; case tok_var:; return ParseVarExpr();; }; }. Next we define ParseVarExpr:. .. code-block:: c++. /// varexpr ::= 'var' identifier ('=' expression)?; // (',' identifier ('=' expression)?)* 'in' expression; static std::unique_ptr<ExprAST> ParseVarExpr() {; getNextToken(); // eat the var. std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames;. // At least one variable name is required.; if (CurTok != tok_identifier); return LogError(""expected identifier after var"");. The first part of this code parses the list of identifier/expr pairs; into the local ``VarNames`` vector. .. code-block:: c++. while (true) {; std::string Name = IdentifierStr;; getNextToken(); // eat identifier. // Read the optional initializer.; std::unique_ptr<ExprAST> Init;; if (CurTok == '=') {; getNextToken(); // eat the '='. Init = ParseExpression();; if (!Init) return nullptr;; }. VarNames.push_back(std::make_pair(Name, std::move(Init)));. // End of var list, exit loop.; if (CurTok != ',') break;; getNextToken(); // eat the ','. if (CurTok != tok_identifier); return LogError(""expected identifier list after var"");; }. Once all the variables are parsed, we then parse the body and create the; AST node:. .. code-block:: c++. // At this point, we have to have 'in'.; if (CurTok != tok_in); return LogError(""expected 'in' keyword after 'var'"");; getNextToken(); // eat 'in'. auto Bod",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:26067,Modifiability,variab,variables,26067,"ntifier ('=' expression)?)* 'in' expression; static std::unique_ptr<ExprAST> ParseVarExpr() {; getNextToken(); // eat the var. std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames;. // At least one variable name is required.; if (CurTok != tok_identifier); return LogError(""expected identifier after var"");. The first part of this code parses the list of identifier/expr pairs; into the local ``VarNames`` vector. .. code-block:: c++. while (true) {; std::string Name = IdentifierStr;; getNextToken(); // eat identifier. // Read the optional initializer.; std::unique_ptr<ExprAST> Init;; if (CurTok == '=') {; getNextToken(); // eat the '='. Init = ParseExpression();; if (!Init) return nullptr;; }. VarNames.push_back(std::make_pair(Name, std::move(Init)));. // End of var list, exit loop.; if (CurTok != ',') break;; getNextToken(); // eat the ','. if (CurTok != tok_identifier); return LogError(""expected identifier list after var"");; }. Once all the variables are parsed, we then parse the body and create the; AST node:. .. code-block:: c++. // At this point, we have to have 'in'.; if (CurTok != tok_in); return LogError(""expected 'in' keyword after 'var'"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<VarExprAST>(std::move(VarNames),; std::move(Body));; }. Now that we can parse and represent the code, we need to support; emission of LLVM IR for it. This code starts out with:. .. code-block:: c++. Value *VarExprAST::codegen() {; std::vector<AllocaInst *> OldBindings;. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Register all variables and emit their initializer.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i) {; const std::string &VarName = VarNames[i].first;; ExprAST *Init = VarNames[i].second.get();. Basically it loops over all the variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:26745,Modifiability,variab,variables,26745,"Init = ParseExpression();; if (!Init) return nullptr;; }. VarNames.push_back(std::make_pair(Name, std::move(Init)));. // End of var list, exit loop.; if (CurTok != ',') break;; getNextToken(); // eat the ','. if (CurTok != tok_identifier); return LogError(""expected identifier list after var"");; }. Once all the variables are parsed, we then parse the body and create the; AST node:. .. code-block:: c++. // At this point, we have to have 'in'.; if (CurTok != tok_in); return LogError(""expected 'in' keyword after 'var'"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<VarExprAST>(std::move(VarNames),; std::move(Body));; }. Now that we can parse and represent the code, we need to support; emission of LLVM IR for it. This code starts out with:. .. code-block:: c++. Value *VarExprAST::codegen() {; std::vector<AllocaInst *> OldBindings;. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Register all variables and emit their initializer.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i) {; const std::string &VarName = VarNames[i].first;; ExprAST *Init = VarNames[i].second.get();. Basically it loops over all the variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrec",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:26966,Modifiability,variab,variables,26966,"rTok != tok_identifier); return LogError(""expected identifier list after var"");; }. Once all the variables are parsed, we then parse the body and create the; AST node:. .. code-block:: c++. // At this point, we have to have 'in'.; if (CurTok != tok_in); return LogError(""expected 'in' keyword after 'var'"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<VarExprAST>(std::move(VarNames),; std::move(Body));; }. Now that we can parse and represent the code, we need to support; emission of LLVM IR for it. This code starts out with:. .. code-block:: c++. Value *VarExprAST::codegen() {; std::vector<AllocaInst *> OldBindings;. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Register all variables and emit their initializer.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i) {; const std::string &VarName = VarNames[i].first;; ExprAST *Init = VarNames[i].second.get();. Basically it loops over all the variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:27018,Modifiability,variab,variable,27018,"he variables are parsed, we then parse the body and create the; AST node:. .. code-block:: c++. // At this point, we have to have 'in'.; if (CurTok != tok_in); return LogError(""expected 'in' keyword after 'var'"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<VarExprAST>(std::move(VarNames),; std::move(Body));; }. Now that we can parse and represent the code, we need to support; emission of LLVM IR for it. This code starts out with:. .. code-block:: c++. Value *VarExprAST::codegen() {; std::vector<AllocaInst *> OldBindings;. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Register all variables and emit their initializer.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i) {; const std::string &VarName = VarNames[i].first;; ExprAST *Init = VarNames[i].second.get();. Basically it loops over all the variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:27184,Modifiability,variab,variable,27184,"xpected 'in' keyword after 'var'"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<VarExprAST>(std::move(VarNames),; std::move(Body));; }. Now that we can parse and represent the code, we need to support; emission of LLVM IR for it. This code starts out with:. .. code-block:: c++. Value *VarExprAST::codegen() {; std::vector<AllocaInst *> OldBindings;. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Register all variables and emit their initializer.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i) {; const std::string &VarName = VarNames[i].first;; ExprAST *Init = VarNames[i].second.get();. Basically it loops over all the variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:27258,Modifiability,variab,variable,27258,"xpected 'in' keyword after 'var'"");; getNextToken(); // eat 'in'. auto Body = ParseExpression();; if (!Body); return nullptr;. return std::make_unique<VarExprAST>(std::move(VarNames),; std::move(Body));; }. Now that we can parse and represent the code, we need to support; emission of LLVM IR for it. This code starts out with:. .. code-block:: c++. Value *VarExprAST::codegen() {; std::vector<AllocaInst *> OldBindings;. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Register all variables and emit their initializer.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i) {; const std::string &VarName = VarNames[i].first;; ExprAST *Init = VarNames[i].second.get();. Basically it loops over all the variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:27686,Modifiability,variab,variable,27686,")->getParent();. // Register all variables and emit their initializer.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i) {; const std::string &VarName = VarNames[i].first;; ExprAST *Init = VarNames[i].second.get();. Basically it loops over all the variables, installing them one at a; time. For each variable we put into the symbol table, we remember the; previous value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28037,Modifiability,variab,variables,28037,"evious value that we replace in OldBindings. .. code-block:: c++. // Emit the initializer before adding the variable to scope, this prevents; // the initializer from referencing the variable itself, and permits stuff; // like this:; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is th",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28324,Modifiability,variab,variable,28324,"; // var a = 1 in; // var a = a in ... # refers to outer 'a'.; Value *InitVal;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28380,Modifiability,variab,variables,28380,"al;; if (Init) {; InitVal = Init->codegen();; if (!InitVal); return nullptr;; } else { // If not specified, use 0.0.; InitVal = ConstantFP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28623,Modifiability,variab,variable,28623,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28865,Modifiability,variab,variables,28865,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:29126,Modifiability,enhance,enhanced,29126,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:29149,Modifiability,variab,variables,29149,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:29266,Modifiability,config,config,29266,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:620,Performance,optimiz,optimize,620,"=======================================================; Kaleidoscope: Extending the Language: Mutable Variables; =======================================================. .. contents::; :local:. Chapter 7 Introduction; ======================. Welcome to Chapter 7 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 6, we've built a; very respectable, albeit simple, `functional programming; language <http://en.wikipedia.org/wiki/Functional_programming>`_. In our; journey, we learned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:1306,Performance,tune,tuned,1306," language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 6, we've built a; very respectable, albeit simple, `functional programming; language <http://en.wikipedia.org/wiki/Functional_programming>`_. In our; journey, we learned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i3",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:2221,Performance,load,load,2221,"no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:2281,Performance,load,load,2281," highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ss",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:2431,Performance,load,loads,2431,"use complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:3709,Performance,load,loads,3709,"oming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLVM; ==============. The 'trick' here is that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that spa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:4374,Performance,load,load,4374," is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLVM; ==============. The 'trick' here is that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5050,Performance,load,load,5050,"ind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5070,Performance,load,load,5070,"ind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5843,Performance,load,load,5843,"eclared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5939,Performance,load,load,5939,"lock:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6034,Performance,load,load,6034,"oad i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6293,Performance,load,load,6293,"he LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6639,Performance,perform,performance,6639," using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6690,Performance,optimiz,optimizer,6690,"s i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6713,Performance,tune,tuned,6713,"s i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6719,Performance,optimiz,optimization,6719,"s i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7176,Performance,load,load,7176," all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7236,Performance,load,load,7236," #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny po",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7495,Performance,optimiz,optimizations,7495,"is solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), an",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7568,Performance,optimiz,optimization,7568,"operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:8156,Performance,load,loads,8156,"label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9028,Performance,optimiz,optimization,9028,"try block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:13843,Performance,load,load,13843,"llocas, we'll use a helper; function that ensures that the allocas are created in the entry block of; the function:. .. code-block:: c++. /// CreateEntryBlockAlloca - Create an alloca instruction in the entry block of; /// the function. This is used for mutable variables etc.; static AllocaInst *CreateEntryBlockAlloca(Function *TheFunction,; const std::string &VarName) {; IRBuilder<> TmpB(&TheFunction->getEntryBlock(),; TheFunction->getEntryBlock().begin());; return TmpB.CreateAlloca(Type::getDoubleTy(*TheContext), nullptr,; VarName);; }. This funny looking code creates an IRBuilder object that is pointing at; the first instruction (.begin()) of the entry block. It then creates an; alloca with the expected name and returns it. Because all values in; Kaleidoscope are doubles, there is no need to pass in a type to use. With this in place, the first functionality change we want to make belongs to; variable references. In our new scheme, variables live on the stack, so; code generating a reference to them actually needs to produce a load; from the stack slot:. .. code-block:: c++. Value *VariableExprAST::codegen() {; // Look this variable up in the function.; AllocaInst *A = NamedValues[Name];; if (!A); return LogErrorV(""Unknown variable name"");. // Load the value.; return Builder->CreateLoad(A->getAllocatedType(), A, Name.c_str());; }. As you can see, this is pretty straightforward. Now we need to update; the things that define the variables to set up the alloca. We'll start; with ``ForExprAST::codegen()`` (see the `full code listing <#id1>`_ for; the unabridged code):. .. code-block:: c++. Function *TheFunction = Builder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartV",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15457,Performance,load,load,15457,"uilder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:16680,Performance,optimiz,optimizations,16680,"onAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %su",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:16921,Performance,optimiz,optimization,16921,"Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the i",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17027,Performance,optimiz,optimization,17027,"a);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17162,Performance,load,load,17162,"RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17436,Performance,load,load,17436,"The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17554,Performance,load,load,17554,".. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:19243,Performance,optimiz,optimizers,19243,"While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then:; br label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. This is a trivial case for mem2reg, since there are no redefinitions of; the variable. The point of showing this is to calm your tension about; inserting such blatant inefficiencies :). After the rest of the optimizers run, we get:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp ueq double %booltmp, 0.000000e+00; br i1 %ifcond, label %else, label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; ret double %addtmp. ifcont:; ret double 1.000000e+00; }. Here we see that the simplifycfg pass decided to clone the return; instruction into the end of the 'else' block. This allowed it to; eliminate some branches and the PHI node. Now that all symbol table references are updated to use stack variables,; we'll add the assignment operator. New Assignment Operator; =======================. With our current framework, adding a new assignment operator is really; simple. We will parse it just like any other binar",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28838,Performance,optimiz,optimizes,28838,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5549,Safety,avoid,avoid,5549,"ariables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced;",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9000,Safety,avoid,avoiding,9000,"try block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9713,Safety,avoid,avoid,9713,"an promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for indu",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:3740,Security,access,accesses,3740,"oming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLVM; ==============. The 'trick' here is that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that spa",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:4347,Security,access,accesses,4347," is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLVM; ==============. The 'trick' here is that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; and it is carefully designed not to have (or need) an ""address-of""; operator. Notice how the type of the @G/@H global variables is actually; ""i32\*"" even though the variable is defined as ""i32"". What this means is; that @G defines *space* for an i32 in the global data area, but its; *name* actually refers to the address for that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9904,Security,expose,exposed,9904,"h this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Def",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15471,Security,access,access,15471,"uilder->GetInsertBlock()->getParent();. // Create an alloca for the variable in the entry block.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);. // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:24279,Security,access,access,24279,"; ...; static int gettok() {; ...; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; if (IdentifierStr == ""var""); return tok_var;; return tok_identifier;; ... The next step is to define the AST node that we will construct. For; var/in, it looks like this:. .. code-block:: c++. /// VarExprAST - Expression class for var/in; class VarExprAST : public ExprAST {; std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames;; std::unique_ptr<ExprAST> Body;. public:; VarExprAST(std::vector<std::pair<std::string, std::unique_ptr<ExprAST>>> VarNames,; std::unique_ptr<ExprAST> Body); : VarNames(std::move(VarNames)), Body(std::move(Body)) {}. Value *codegen() override;; };. var/in allows a list of names to be defined all at once, and each name; can optionally have an initializer value. As such, we capture this; information in the VarNames vector. Also, var/in has a body, this body; is allowed to access the variables defined by the var/in. With this in place, we can define the parser pieces. The first thing we; do is add it as a primary expression:. .. code-block:: c++. /// primary; /// ::= identifierexpr; /// ::= numberexpr; /// ::= parenexpr; /// ::= ifexpr; /// ::= forexpr; /// ::= varexpr; static std::unique_ptr<ExprAST> ParsePrimary() {; switch (CurTok) {; default:; return LogError(""unknown token when expecting an expression"");; case tok_identifier:; return ParseIdentifierExpr();; case tok_number:; return ParseNumberExpr();; case '(':; return ParseParenExpr();; case tok_if:; return ParseIfExpr();; case tok_for:; return ParseForExpr();; case tok_var:; return ParseVarExpr();; }; }. Next we define ParseVarExpr:. .. code-block:: c++. /// varexpr ::= 'var' identifier ('=' expression)?; // (',' identifier ('=' expression)?)* 'in' expression; static std::unique_ptr<ExprAST> ParseVarExpr() {; getNextToken(); // eat the var. std::vector<std::pair<std::string, std::unique_ptr<Ex",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:1322,Testability,test,tested,1322," language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 6, we've built a; very respectable, albeit simple, `functional programming; language <http://en.wikipedia.org/wiki/Functional_programming>`_. In our; journey, we learned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i3",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:1614,Testability,test,test,1614,"as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:2116,Testability,test,test,2116,"ith mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the p",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:3478,Testability,log,logic,3478,"2 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in the then/else branches of the; if statement (cond\_true/cond\_false). In order to merge the incoming; values, the X.2 phi node in the cond\_next block selects the right value; to use based on where control flow is coming from: if control flow comes; from the cond\_false block, X.2 gets the value of X.1. Alternatively, if; control flow comes from cond\_true, it gets the value of X.0. The intent; of this chapter is not to explain the details of SSA form. For more; information, see one of the many `online; references <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_. The question for this article is ""who places the phi nodes when lowering; assignments to mutable variables?"". The issue here is that LLVM; *requires* that its IR be in SSA form: there is no ""non-ssa"" mode for; it. However, SSA construction requires non-trivial algorithms and data; structures, so it is inconvenient and wasteful for every front-end to; have to reproduce this logic. Memory in LLVM; ==============. The 'trick' here is that while LLVM does require all register values to; be in SSA form, it does not require (or permit) memory objects to be in; SSA form. In the example above, note that the loads from G and H are; direct accesses to G and H: they are not renamed or versioned. This; differs from some other compiler systems, which do try to version memory; objects. In LLVM, instead of encoding dataflow analysis of memory into; the LLVM IR, it is handled with `Analysis; Passes <../../WritingAnLLVMPass.html>`_ which are computed on demand. With this in mind, the high-level idea is that we want to make a stack; variable (which lives in memory, because it is on the stack) for each; mutable object in a function. To take advantage of this trick, we need; to talk about how LLVM represents stack variables. In LLVM, all memory accesses are explicit with load/store instructions,; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:5699,Testability,test,test,5699,"that space. Stack variables; work the same way, except that instead of being declared with global; variable definitions, they are declared with the `LLVM alloca; instruction <../../LangRef.html#alloca-instruction>`_:. .. code-block:: llvm. define i32 @example() {; entry:; %X = alloca i32 ; type of %X is i32*.; ...; %tmp = load i32, i32* %X ; load the stack value %X from the stack.; %tmp2 = add i32 %tmp, 1 ; increment it; store i32 %tmp2, i32* %X ; store it back; ... This code shows an example of how you can declare and manipulate a stack; variable in the LLVM IR. Stack memory allocated with the alloca; instruction is fully general: you can pass the address of the stack slot; to functions, you can store it in other variables, etc. In our example; above, we could rewrite the example to use the alloca technique to avoid; using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimiza",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:7071,Testability,test,test,7071,"d a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:9226,Testability,test,tested,9226,"nny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass? In short, we; strongly recommend that you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; ================",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:22473,Testability,test,test,22473,"trange thing is that it requires the LHS to be a variable. It; is invalid to have ""(x+1) = expr"" - only things like ""x = expr"" are; allowed. .. code-block:: c++. // Codegen the RHS.; Value *Val = RHS->codegen();; if (!Val); return nullptr;. // Look up the name.; Value *Variable = NamedValues[LHSE->getName()];; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);. test(123);. When run, this example prints ""123"" and then ""4"", showing that we did; actually mutate the value! Okay, we have now officially implemented our; goal: getting this to work requires SSA construction in the general; case. However, to be really useful, we want the ability to define our; own local variables, let's add this next!. User-defined Local Variables; ============================. Adding var/in is just like any other extension we made to; Kaleidoscope: we extend the lexer, the parser, the AST and the code; generator. The first step for adding our new 'var/in' construct is to; extend the lexer. As before, this is pretty trivial, the code looks like; this:. .. code-block:: c++. enum Token {; ...; // var definition; tok_var = -13; ...; }; ...; static int gettok() {; ...; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; if (IdentifierStr == ""var""); return ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:22516,Testability,test,test,22516,"es the LHS to be a variable. It; is invalid to have ""(x+1) = expr"" - only things like ""x = expr"" are; allowed. .. code-block:: c++. // Codegen the RHS.; Value *Val = RHS->codegen();; if (!Val); return nullptr;. // Look up the name.; Value *Variable = NamedValues[LHSE->getName()];; if (!Variable); return LogErrorV(""Unknown variable name"");. Builder->CreateStore(Val, Variable);; return Val;; }; ... Once we have the variable, codegen'ing the assignment is; straightforward: we emit the RHS of the assignment, create a store, and; return the computed value. Returning a value allows for chained; assignments like ""X = (Y = Z)"". Now that we have an assignment operator, we can mutate loop variables; and arguments. For example, we can now run code like this:. ::. # Function to print a double.; extern printd(x);. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. def test(x); printd(x) :; x = 4 :; printd(x);. test(123);. When run, this example prints ""123"" and then ""4"", showing that we did; actually mutate the value! Okay, we have now officially implemented our; goal: getting this to work requires SSA construction in the general; case. However, to be really useful, we want the ability to define our; own local variables, let's add this next!. User-defined Local Variables; ============================. Adding var/in is just like any other extension we made to; Kaleidoscope: we extend the lexer, the parser, the AST and the code; generator. The first step for adding our new 'var/in' construct is to; extend the lexer. As before, this is pretty trivial, the code looks like; this:. .. code-block:: c++. enum Token {; ...; // var definition; tok_var = -13; ...; }; ...; static int gettok() {; ...; if (IdentifierStr == ""in""); return tok_in;; if (IdentifierStr == ""binary""); return tok_binary;; if (IdentifierStr == ""unary""); return tok_unary;; if (IdentifierStr == ""var""); return tok_var;; return tok_identifi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:399,Usability,simpl,simple,399,"=======================================================; Kaleidoscope: Extending the Language: Mutable Variables; =======================================================. .. contents::; :local:. Chapter 7 Introduction; ======================. Welcome to Chapter 7 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 6, we've built a; very respectable, albeit simple, `functional programming; language <http://en.wikipedia.org/wiki/Functional_programming>`_. In our; journey, we learned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:518,Usability,learn,learned,518,"=======================================================; Kaleidoscope: Extending the Language: Mutable Variables; =======================================================. .. contents::; :local:. Chapter 7 Introduction; ======================. Welcome to Chapter 7 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 6, we've built a; very respectable, albeit simple, `functional programming; language <http://en.wikipedia.org/wiki/Functional_programming>`_. In our; journey, we learned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:1561,Usability,simpl,simple,1561,"earned some parsing techniques, how to build and represent; an AST, how to build LLVM IR, and how to optimize the resultant code as; well as JIT compile it. While Kaleidoscope is interesting as a functional language, the fact; that it is functional makes it ""too easy"" to generate LLVM IR for it. In; particular, a functional language makes it very easy to build LLVM IR; directly in `SSA; form <http://en.wikipedia.org/wiki/Static_single_assignment_form>`_.; Since LLVM requires that the input code be in SSA form, this is a very; nice property and it is often unclear to newcomers how to generate code; for an imperative language with mutable variables. The short (and happy) summary of this chapter is that there is no need; for your front-end to build SSA form: LLVM provides highly tuned and; well tested support for this, though the way it works is a bit; unexpected for some. Why is this a hard problem?; ===========================. To understand why mutable variables cause complexities in SSA; construction, consider this extremely simple C example:. .. code-block:: c. int G, H;; int test(_Bool Condition) {; int X;; if (Condition); X = G;; else; X = H;; return X;; }. In this case, we have the variable ""X"", whose value depends on the path; executed in the program. Because there are two different possible values; for X before the return instruction, a PHI node is inserted to merge the; two values. The LLVM IR that we want for this example looks like this:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.2 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.2; }. In this example, the loads from the G and H global variables are; explicit in the LLVM IR, and they live in t",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:6601,Usability,simpl,simple,6601," using a PHI node:. .. code-block:: llvm. @G = weak global i32 0 ; type of @G is i32*; @H = weak global i32 0 ; type of @H is i32*. define i32 @test(i1 %Condition) {; entry:; %X = alloca i32 ; type of %X is i32*.; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; store i32 %X.0, i32* %X ; Update X; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; store i32 %X.1, i32* %X ; Update X; br label %cond_next. cond_next:; %X.2 = load i32, i32* %X ; Read X; ret i32 %X.2; }. With this, we have discovered a way to handle arbitrary mutable; variables without the need to create Phi nodes at all:. #. Each mutable variable becomes a stack allocation.; #. Each read of the variable becomes a load from the stack.; #. Each update of the variable becomes a store to the stack.; #. Taking the address of a variable just uses the stack address; directly. While this solution has solved our immediate problem, it introduced; another one: we have now apparently introduced a lot of stack traffic; for very simple and common operations, a major performance problem.; Fortunately for us, the LLVM optimizer has a highly-tuned optimization; pass named ""mem2reg"" that handles this case, promoting allocas like this; into SSA registers, inserting Phi nodes as appropriate. If you run this; example through the pass, for example, you'll get:. .. code-block:: bash. $ llvm-as < example.ll | opt -passes=mem2reg | llvm-dis; @G = weak global i32 0; @H = weak global i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:8091,Usability,simpl,simpler,8091,"lobal i32 0. define i32 @test(i1 %Condition) {; entry:; br i1 %Condition, label %cond_true, label %cond_false. cond_true:; %X.0 = load i32, i32* @G; br label %cond_next. cond_false:; %X.1 = load i32, i32* @H; br label %cond_next. cond_next:; %X.01 = phi i32 [ %X.1, %cond_false ], [ %X.0, %cond_true ]; ret i32 %X.01; }. The mem2reg pass implements the standard ""iterated dominance frontier""; algorithm for constructing SSA form and has a number of optimizations; that speed up (very common) degenerate cases. The mem2reg optimization; pass is the answer to dealing with mutable variables, and we highly; recommend that you depend on it. Note that mem2reg only works on; variables in certain circumstances:. #. mem2reg is alloca-driven: it looks for allocas and if it can handle; them, it promotes them. It does not apply to global variables or heap; allocations.; #. mem2reg only looks for alloca instructions in the entry block of the; function. Being in the entry block guarantees that the alloca is only; executed once, which makes analysis simpler.; #. mem2reg only promotes allocas whose uses are direct loads and stores.; If the address of the stack object is passed to a function, or if any; funny pointer arithmetic is involved, the alloca will not be; promoted.; #. mem2reg only works on allocas of `first; class <../../LangRef.html#first-class-types>`_ values (such as pointers,; scalars and vectors), and only if the array size of the allocation is; 1 (or missing in the .ll file). mem2reg is not capable of promoting; structs or arrays to registers. Note that the ""sroa"" pass is; more powerful and can promote structs, ""unions"", and arrays in many; cases. All of these properties are easy to satisfy for most imperative; languages, and we'll illustrate it below with Kaleidoscope. The final; question you may be asking is: should I bother with this nonsense for my; front-end? Wouldn't it be better if I just did SSA construction; directly, avoiding use of the mem2reg optimization pass?",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:10120,Usability,simpl,simple,10120,"at you use this technique for building SSA form,; unless there is an extremely good reason not to. Using this technique; is:. - Proven and well tested: clang uses this technique; for local mutable variables. As such, the most common clients of LLVM; are using this to handle a bulk of their variables. You can be sure; that bugs are found fast and fixed early.; - Extremely Fast: mem2reg has a number of special cases that make it; fast in common cases as well as fully general. For example, it has; fast-paths for variables that are only used in a single block,; variables that only have one assignment point, good heuristics to; avoid insertion of unneeded phi nodes, etc.; - Needed for debug info generation: `Debug information in; LLVM <../../SourceLevelDebugging.html>`_ relies on having the address of; the variable exposed so that debug info can be attached to it. This; technique dovetails very naturally with this style of debug info. If nothing else, this makes it much easier to get your front-end up and; running, and is very simple to implement. Let's extend Kaleidoscope with; mutable variables now!. Mutable Variables in Kaleidoscope; =================================. Now that we know the sort of problem we want to tackle, let's see what; this looks like in the context of our little Kaleidoscope language.; We're going to add two features:. #. The ability to mutate variables with the '=' operator.; #. The ability to define new variables. While the first item is really what this is about, we only have; variables for incoming arguments as well as for induction variables, and; redefining those only goes so far :). Also, the ability to define new; variables is a useful thing regardless of whether you will be mutating; them. Here's a motivating example that shows how we could use these:. ::. # Define ':' for sequencing: as a low-precedence operator that ignores operands; # and just returns the RHS.; def binary : 1 (x y) y;. # Recursive fib, we could do this before.; def fib(",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15615,Usability,simpl,simple,15615,". // Emit the start code first, without 'variable' in scope.; Value *StartVal = Start->codegen();; if (!StartVal); return nullptr;. // Store the value into the alloca.; Builder->CreateStore(StartVal, Alloca);; ... // Compute the end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:15786,Usability,clear,clear,15786," end condition.; Value *EndCond = End->codegen();; if (!EndCond); return nullptr;. // Reload, increment, and restore the alloca. This handles the case where; // the body of the loop mutates the variable.; Value *CurVar = Builder->CreateLoad(Alloca->getAllocatedType(), Alloca,; VarName.c_str());; Value *NextVar = Builder->CreateFAdd(CurVar, StepVal, ""nextvar"");; Builder->CreateStore(NextVar, Alloca);; ... This code is virtually identical to the code `before we allowed mutable; variables <LangImpl05.html#code-generation-for-the-for-loop>`_. The big difference is that we; no longer have to construct a PHI node, and we use load/store to access; the variable as needed. To support mutable argument variables, we need to also make allocas for; them. The code for this is also pretty simple:. .. code-block:: c++. Function *FunctionAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:16662,Usability,simpl,simple,16662,"onAST::codegen() {; ...; Builder->SetInsertPoint(BB);. // Record the function arguments in the NamedValues map.; NamedValues.clear();; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. if (Value *RetVal = Body->codegen()) {; ... For each argument, we make an alloca, store the input value to the; function into the alloca, and register the alloca as the memory location; for the argument. This method gets invoked by ``FunctionAST::codegen()``; right after it sets up the entry block for the function. The final missing piece is adding the mem2reg pass, which allows us to; get good codegen once again:. .. code-block:: c++. // Promote allocas to registers.; TheFPM->add(createPromoteMemoryToRegisterPass());; // Do simple ""peephole"" optimizations and bit-twiddling optzns.; TheFPM->add(createInstructionCombiningPass());; // Reassociate expressions.; TheFPM->add(createReassociatePass());; ... It is interesting to see what the code looks like before and after the; mem2reg optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %su",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:17942,Usability,simpl,simple-minded,17942,"optimization runs. For example, this is the before/after code; for our recursive fib function. Before the optimization:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %x1 = alloca double; store double %x, double* %x1; %x2 = load double, double* %x1; %cmptmp = fcmp ult double %x2, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then: ; preds = %entry; br label %ifcont. else: ; preds = %entry; %x3 = load double, double* %x1; %subtmp = fsub double %x3, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %x4 = load double, double* %x1; %subtmp5 = fsub double %x4, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. Here there is only one variable (x, the input argument) but you can; still see the extremely simple-minded code generation strategy we are; using. In the entry block, an alloca is created, and the initial input; value is stored into it. Each reference to the variable does a reload; from the stack. Also, note that we didn't modify the if/then/else; expression, so it still inserts a PHI node. While we could make an; alloca for it, it is actually easier to create a PHI node for it, so we; still just make the PHI. Here is the code after the mem2reg pass runs:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp one double %booltmp, 0.000000e+00; br i1 %ifcond, label %then, label %else. then:; br label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:19809,Usability,simpl,simplifycfg,19809," %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; br label %ifcont. ifcont: ; preds = %else, %then; %iftmp = phi double [ 1.000000e+00, %then ], [ %addtmp, %else ]; ret double %iftmp; }. This is a trivial case for mem2reg, since there are no redefinitions of; the variable. The point of showing this is to calm your tension about; inserting such blatant inefficiencies :). After the rest of the optimizers run, we get:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp ueq double %booltmp, 0.000000e+00; br i1 %ifcond, label %else, label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; ret double %addtmp. ifcont:; ret double 1.000000e+00; }. Here we see that the simplifycfg pass decided to clone the return; instruction into the end of the 'else' block. This allowed it to; eliminate some branches and the PHI node. Now that all symbol table references are updated to use stack variables,; we'll add the assignment operator. New Assignment Operator; =======================. With our current framework, adding a new assignment operator is really; simple. We will parse it just like any other binary operator, but handle; it internally (instead of allowing the user to define it). The first; step is to set a precedence:. .. code-block:: c++. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['='] = 2;; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;. Now that the parser knows the precedence of the binary operator, it; takes care of all the parsing and AST generation. We just need to; implement codegen for the assignment operator. This looks like:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; // Spe",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:20194,Usability,simpl,simple,20194,"ur tension about; inserting such blatant inefficiencies :). After the rest of the optimizers run, we get:. .. code-block:: llvm. define double @fib(double %x) {; entry:; %cmptmp = fcmp ult double %x, 3.000000e+00; %booltmp = uitofp i1 %cmptmp to double; %ifcond = fcmp ueq double %booltmp, 0.000000e+00; br i1 %ifcond, label %else, label %ifcont. else:; %subtmp = fsub double %x, 1.000000e+00; %calltmp = call double @fib(double %subtmp); %subtmp5 = fsub double %x, 2.000000e+00; %calltmp6 = call double @fib(double %subtmp5); %addtmp = fadd double %calltmp, %calltmp6; ret double %addtmp. ifcont:; ret double 1.000000e+00; }. Here we see that the simplifycfg pass decided to clone the return; instruction into the end of the 'else' block. This allowed it to; eliminate some branches and the PHI node. Now that all symbol table references are updated to use stack variables,; we'll add the assignment operator. New Assignment Operator; =======================. With our current framework, adding a new assignment operator is really; simple. We will parse it just like any other binary operator, but handle; it internally (instead of allowing the user to define it). The first; step is to set a precedence:. .. code-block:: c++. int main() {; // Install standard binary operators.; // 1 is lowest precedence.; BinopPrecedence['='] = 2;; BinopPrecedence['<'] = 10;; BinopPrecedence['+'] = 20;; BinopPrecedence['-'] = 20;. Now that the parser knows the precedence of the binary operator, it; takes care of all the parsing and AST generation. We just need to; implement codegen for the assignment operator. This looks like:. .. code-block:: c++. Value *BinaryExprAST::codegen() {; // Special case '=' because we don't want to emit the LHS as an expression.; if (Op == '=') {; // This assume we're building without RTTI because LLVM builds that way by; // default. If you build LLVM with RTTI this can be changed to a; // dynamic_cast for automatic error checking.; VariableExprAST *LHSE = static_cast<Var",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst:28956,Usability,simpl,simple,28956,"FP::get(*TheContext, APFloat(0.0));; }. AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, VarName);; Builder->CreateStore(InitVal, Alloca);. // Remember the old variable binding so that we can restore the binding when; // we unrecurse.; OldBindings.push_back(NamedValues[VarName]);. // Remember this binding.; NamedValues[VarName] = Alloca;; }. There are more comments here than code. The basic idea is that we emit; the initializer, create the alloca, then update the symbol table to; point to it. Once all the variables are installed in the symbol table,; we evaluate the body of the var/in expression:. .. code-block:: c++. // Codegen the body, now that all vars are in scope.; Value *BodyVal = Body->codegen();; if (!BodyVal); return nullptr;. Finally, before returning, we restore the previous variable bindings:. .. code-block:: c++. // Pop all our variables from scope.; for (unsigned i = 0, e = VarNames.size(); i != e; ++i); NamedValues[VarNames[i].first] = OldBindings[i];. // Return the body computation.; return BodyVal;; }. The end result of all of this is that we get properly scoped variable; definitions, and we even (trivially) allow mutation of them :). With this, we completed what we set out to do. Our nice iterative fib; example from the intro compiles and runs just fine. The mem2reg pass; optimizes all of our stack variables into SSA registers, inserting PHI; nodes where needed, and our front-end remains simple: no ""iterated; dominance frontier"" computation anywhere in sight. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; mutable variables and var/in support. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter7/toy.cpp; :language: c++. `Next: Compiling to Object Code <LangImpl08.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl07.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:339,Availability,down,down,339,"========================================; Kaleidoscope: Compiling to Object Code; ========================================. .. contents::; :local:. Chapter 8 Introduction; ======================. Welcome to Chapter 8 of the ""`Implementing a language with LLVM; <index.html>`_"" tutorial. This chapter describes how to compile our; language down to object files. Choosing a target; =================. LLVM has native support for cross-compilation. You can compile to the; architecture of your current machine, or just as easily compile for; other architectures. In this tutorial, we'll target the current; machine. To specify the architecture that you want to target, we use a string; called a ""target triple"". This takes the form; ``<arch><sub>-<vendor>-<sys>-<abi>`` (see the `cross compilation docs; <https://clang.llvm.org/docs/CrossCompilation.html#target-triple>`_). As an example, we can see what clang thinks is our current target; triple:. ::. $ clang --version | grep Target; Target: x86_64-unknown-linux-gnu. Running this command may show something different on your machine as; you might be using a different architecture or operating system to me. Fortunately, we don't need to hard-code a target triple to target the; current machine. LLVM provides ``sys::getDefaultTargetTriple``, which; returns the target triple of the current machine. .. code-block:: c++. auto TargetTriple = sys::getDefaultTargetTriple();. LLVM doesn't require us to link in all the target; functionality. For example, if we're just using the JIT, we don't need; the assembly printers. Similarly, if we're only targeting certain; architectures, we can only link in the functionality for those; architectures. For this example, we'll initialize all the targets for emitting object; code. .. code-block:: c++. InitializeAllTargetInfos();; InitializeAllTargets();; InitializeAllTargetMCs();; InitializeAllAsmParsers();; InitializeAllAsmPrinters();. We can now use our target triple to get a ``Target``:. .. code-block:: ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:2104,Availability,error,error,2104,"ecture or operating system to me. Fortunately, we don't need to hard-code a target triple to target the; current machine. LLVM provides ``sys::getDefaultTargetTriple``, which; returns the target triple of the current machine. .. code-block:: c++. auto TargetTriple = sys::getDefaultTargetTriple();. LLVM doesn't require us to link in all the target; functionality. For example, if we're just using the JIT, we don't need; the assembly printers. Similarly, if we're only targeting certain; architectures, we can only link in the functionality for those; architectures. For this example, we'll initialize all the targets for emitting object; code. .. code-block:: c++. InitializeAllTargetInfos();; InitializeAllTargets();; InitializeAllTargetMCs();; InitializeAllAsmParsers();; InitializeAllAsmPrinters();. We can now use our target triple to get a ``Target``:. .. code-block:: c++. std::string Error;; auto Target = TargetRegistry::lookupTarget(TargetTriple, Error);. // Print an error and exit if we couldn't find the requested target.; // This generally occurs if we've forgotten to initialise the; // TargetRegistry or we have a bogus target triple.; if (!Target) {; errs() << Error;; return 1;; }. Target Machine; ==============. We will also need a ``TargetMachine``. This class provides a complete; machine description of the machine we're targeting. If we want to; target a specific feature (such as SSE) or a specific CPU (such as; Intel's Sandylake), we do so now. To see which features and CPUs that LLVM knows about, we can use; ``llc``. For example, let's look at x86:. ::. $ llvm-as < /dev/null | llc -march=x86 -mattr=help; Available CPUs for this target:. amdfam10 - Select the amdfam10 processor.; athlon - Select the athlon processor.; athlon-4 - Select the athlon-4 processor.; ... Available features for this target:. 16bit-mode - 16-bit mode (i8086).; 32bit-mode - 32-bit mode (80386).; 3dnow - Enable 3DNow! instructions.; 3dnowa - Enable 3DNow! Athlon instructions.; ... For our e",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:4140,Integrability,message,message,4140,"ric CPU without any additional feature or; target option. .. code-block:: c++. auto CPU = ""generic"";; auto Features = """";. TargetOptions opt;; auto TargetMachine = Target->createTargetMachine(TargetTriple, CPU, Features, opt, Reloc::PIC_);. Configuring the Module; ======================. We're now ready to configure our module, to specify the target and; data layout. This isn't strictly necessary, but the `frontend; performance guide <../../Frontend/PerformanceTips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; =======================. Does it work? Let's give it a try. We need to compile our code, but; note that the arguments to ``llvm-config`` are different to the previous chapters. ::. $ clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs all` -o toy. Let's run it, and define a simple ``average`` function. Press Ctrl-D; when you're done. ::. $ ./toy; ready> def average(x y) (x + y) * 0.5;; ^D; Wrote output.o. We have an object file! To test it, let's write a simple program and; link it with our output. Here's the source code:. .. code-block:: c++. #include <iostream>. extern ""C"" ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:3460,Modifiability,config,configure,3460," If we want to; target a specific feature (such as SSE) or a specific CPU (such as; Intel's Sandylake), we do so now. To see which features and CPUs that LLVM knows about, we can use; ``llc``. For example, let's look at x86:. ::. $ llvm-as < /dev/null | llc -march=x86 -mattr=help; Available CPUs for this target:. amdfam10 - Select the amdfam10 processor.; athlon - Select the athlon processor.; athlon-4 - Select the athlon-4 processor.; ... Available features for this target:. 16bit-mode - 16-bit mode (i8086).; 32bit-mode - 32-bit mode (80386).; 3dnow - Enable 3DNow! instructions.; 3dnowa - Enable 3DNow! Athlon instructions.; ... For our example, we'll use the generic CPU without any additional feature or; target option. .. code-block:: c++. auto CPU = ""generic"";; auto Features = """";. TargetOptions opt;; auto TargetMachine = Target->createTargetMachine(TargetTriple, CPU, Features, opt, Reloc::PIC_);. Configuring the Module; ======================. We're now ready to configure our module, to specify the target and; data layout. This isn't strictly necessary, but the `frontend; performance guide <../../Frontend/PerformanceTips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }.",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:4673,Modifiability,config,config,4673,"Tips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; =======================. Does it work? Let's give it a try. We need to compile our code, but; note that the arguments to ``llvm-config`` are different to the previous chapters. ::. $ clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs all` -o toy. Let's run it, and define a simple ``average`` function. Press Ctrl-D; when you're done. ::. $ ./toy; ready> def average(x y) (x + y) * 0.5;; ^D; Wrote output.o. We have an object file! To test it, let's write a simple program and; link it with our output. Here's the source code:. .. code-block:: c++. #include <iostream>. extern ""C"" {; double average(double, double);; }. int main() {; std::cout << ""average of 3.0 and 4.0: "" << average(3.0, 4.0) << std::endl;; }. We link our program to output.o and check the result is what we; expected:. ::. $ clang++ main.cpp output.o -o main; $ ./main; average of 3.0 and 4.0: 3.5. Full Code Listing; =================. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter8/toy.cpp; :language: c++. `Next: Adding Debug Information <LangImpl09.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:4757,Modifiability,config,config,4757,"Tips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; =======================. Does it work? Let's give it a try. We need to compile our code, but; note that the arguments to ``llvm-config`` are different to the previous chapters. ::. $ clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs all` -o toy. Let's run it, and define a simple ``average`` function. Press Ctrl-D; when you're done. ::. $ ./toy; ready> def average(x y) (x + y) * 0.5;; ^D; Wrote output.o. We have an object file! To test it, let's write a simple program and; link it with our output. Here's the source code:. .. code-block:: c++. #include <iostream>. extern ""C"" {; double average(double, double);; }. int main() {; std::cout << ""average of 3.0 and 4.0: "" << average(3.0, 4.0) << std::endl;; }. We link our program to output.o and check the result is what we; expected:. ::. $ clang++ main.cpp output.o -o main; $ ./main; average of 3.0 and 4.0: 3.5. Full Code Listing; =================. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter8/toy.cpp; :language: c++. `Next: Adding Debug Information <LangImpl09.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:3572,Performance,perform,performance,3572,"uch as; Intel's Sandylake), we do so now. To see which features and CPUs that LLVM knows about, we can use; ``llc``. For example, let's look at x86:. ::. $ llvm-as < /dev/null | llc -march=x86 -mattr=help; Available CPUs for this target:. amdfam10 - Select the amdfam10 processor.; athlon - Select the athlon processor.; athlon-4 - Select the athlon-4 processor.; ... Available features for this target:. 16bit-mode - 16-bit mode (i8086).; 32bit-mode - 32-bit mode (80386).; 3dnow - Enable 3DNow! instructions.; 3dnowa - Enable 3DNow! Athlon instructions.; ... For our example, we'll use the generic CPU without any additional feature or; target option. .. code-block:: c++. auto CPU = ""generic"";; auto Features = """";. TargetOptions opt;; auto TargetMachine = Target->createTargetMachine(TargetTriple, CPU, Features, opt, Reloc::PIC_);. Configuring the Module; ======================. We're now ready to configure our module, to specify the target and; data layout. This isn't strictly necessary, but the `frontend; performance guide <../../Frontend/PerformanceTips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; ============",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:5007,Testability,test,test,5007,"Tips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; =======================. Does it work? Let's give it a try. We need to compile our code, but; note that the arguments to ``llvm-config`` are different to the previous chapters. ::. $ clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs all` -o toy. Let's run it, and define a simple ``average`` function. Press Ctrl-D; when you're done. ::. $ ./toy; ready> def average(x y) (x + y) * 0.5;; ^D; Wrote output.o. We have an object file! To test it, let's write a simple program and; link it with our output. Here's the source code:. .. code-block:: c++. #include <iostream>. extern ""C"" {; double average(double, double);; }. int main() {; std::cout << ""average of 3.0 and 4.0: "" << average(3.0, 4.0) << std::endl;; }. We link our program to output.o and check the result is what we; expected:. ::. $ clang++ main.cpp output.o -o main; $ ./main; average of 3.0 and 4.0: 3.5. Full Code Listing; =================. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter8/toy.cpp; :language: c++. `Next: Adding Debug Information <LangImpl09.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:3584,Usability,guid,guide,3584,"uch as; Intel's Sandylake), we do so now. To see which features and CPUs that LLVM knows about, we can use; ``llc``. For example, let's look at x86:. ::. $ llvm-as < /dev/null | llc -march=x86 -mattr=help; Available CPUs for this target:. amdfam10 - Select the amdfam10 processor.; athlon - Select the athlon processor.; athlon-4 - Select the athlon-4 processor.; ... Available features for this target:. 16bit-mode - 16-bit mode (i8086).; 32bit-mode - 32-bit mode (80386).; 3dnow - Enable 3DNow! instructions.; 3dnowa - Enable 3DNow! Athlon instructions.; ... For our example, we'll use the generic CPU without any additional feature or; target option. .. code-block:: c++. auto CPU = ""generic"";; auto Features = """";. TargetOptions opt;; auto TargetMachine = Target->createTargetMachine(TargetTriple, CPU, Features, opt, Reloc::PIC_);. Configuring the Module; ======================. We're now ready to configure our module, to specify the target and; data layout. This isn't strictly necessary, but the `frontend; performance guide <../../Frontend/PerformanceTips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; ============",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:4846,Usability,simpl,simple,4846,"Tips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; =======================. Does it work? Let's give it a try. We need to compile our code, but; note that the arguments to ``llvm-config`` are different to the previous chapters. ::. $ clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs all` -o toy. Let's run it, and define a simple ``average`` function. Press Ctrl-D; when you're done. ::. $ ./toy; ready> def average(x y) (x + y) * 0.5;; ^D; Wrote output.o. We have an object file! To test it, let's write a simple program and; link it with our output. Here's the source code:. .. code-block:: c++. #include <iostream>. extern ""C"" {; double average(double, double);; }. int main() {; std::cout << ""average of 3.0 and 4.0: "" << average(3.0, 4.0) << std::endl;; }. We link our program to output.o and check the result is what we; expected:. ::. $ clang++ main.cpp output.o -o main; $ ./main; average of 3.0 and 4.0: 3.5. Full Code Listing; =================. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter8/toy.cpp; :language: c++. `Next: Adding Debug Information <LangImpl09.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst:5030,Usability,simpl,simple,5030,"Tips.html>`_ recommends; this. Optimizations benefit from knowing about the target and data; layout. .. code-block:: c++. TheModule->setDataLayout(TargetMachine->createDataLayout());; TheModule->setTargetTriple(TargetTriple);. Emit Object Code; ================. We're ready to emit object code! Let's define where we want to write; our file to:. .. code-block:: c++. auto Filename = ""output.o"";; std::error_code EC;; raw_fd_ostream dest(Filename, EC, sys::fs::OF_None);. if (EC) {; errs() << ""Could not open file: "" << EC.message();; return 1;; }. Finally, we define a pass that emits object code, then we run that; pass:. .. code-block:: c++. legacy::PassManager pass;; auto FileType = CodeGenFileType::ObjectFile;. if (TargetMachine->addPassesToEmitFile(pass, dest, nullptr, FileType)) {; errs() << ""TargetMachine can't emit a file of this type"";; return 1;; }. pass.run(*TheModule);; dest.flush();. Putting It All Together; =======================. Does it work? Let's give it a try. We need to compile our code, but; note that the arguments to ``llvm-config`` are different to the previous chapters. ::. $ clang++ -g -O3 toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs all` -o toy. Let's run it, and define a simple ``average`` function. Press Ctrl-D; when you're done. ::. $ ./toy; ready> def average(x y) (x + y) * 0.5;; ^D; Wrote output.o. We have an object file! To test it, let's write a simple program and; link it with our output. Here's the source code:. .. code-block:: c++. #include <iostream>. extern ""C"" {; double average(double, double);; }. int main() {; std::cout << ""average of 3.0 and 4.0: "" << average(3.0, 4.0) << std::endl;; }. We link our program to output.o and check the result is what we; expected:. ::. $ clang++ main.cpp output.o -o main; $ ./main; average of 3.0 and 4.0: 3.5. Full Code Listing; =================. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter8/toy.cpp; :language: c++. `Next: Adding Debug Information <LangImpl09.html>`_; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl08.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:1065,Availability,down,down,1065,"eidoscope: Adding Debug Information; ======================================. .. contents::; :local:. Chapter 9 Introduction; ======================. Welcome to Chapter 9 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 8, we've built a; decent little programming language with functions and variables.; What happens if something goes wrong though, how do you debug your; program?. Source level debugging uses formatted data that helps a debugger; translate from binary and the state of the machine back to the; source that the programmer wrote. In LLVM we generally use a format; called `DWARF <http://dwarfstd.org>`_. DWARF is a compact encoding; that represents types, source locations, and variable locations. The short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructio",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:3817,Availability,error,error,3817,"y the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-block:: udiff. @@ -1129,7 +1129,6 @@ static void HandleTopLevelExpression() {; /// top ::= definition | external | expression | ';'; static void MainLoop() {; while (true) {; - fprintf(stderr, ""ready> "");; switch (CurTok) {; case tok_eof:; return;; @@ -1184,7 +1183,6 @@ int main() {; BinopPrecedence['*'] = 40; // highest. // Prime the first token.; - fprintf(stderr, ""ready> "");; getNextToken();. Lastly we're going to disable all of the optimization passes and the JIT so; that the only thing that happens after we're done parsing and generating; code is that the LLVM IR goes to standard error:. .. code-block:: udiff. @@ -1108,17 +1108,8 @@ static void HandleExtern() {; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; - if (auto *FnIR = FnAST->codegen()) {; - // We're just doing this to make sure it executes.; - TheExecutionEngine->finalizeObject();; - // JIT the function, returning a function pointer.; - void *FPtr = TheExecutionEngine->getPointerToFunction(FnIR);; -; - // Cast it to the right type (takes no arguments, returns a double) so we; - // can call it as a native function.; - double (*FP)() = (double (*)())(intptr_t)FPtr;; - // Ignore the return value for this.; - (void)FP;; + if (!FnAST->codegen()) {; + fprintf(stderr, ""Error generating code for top level expr"");; }; } else {; // Skip token for error recovery.; @@ -1439,11 +1459,11 @@ int main() {; // target lays out dat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:4647,Availability,error,error,4647,"(stderr, ""ready> "");; getNextToken();. Lastly we're going to disable all of the optimization passes and the JIT so; that the only thing that happens after we're done parsing and generating; code is that the LLVM IR goes to standard error:. .. code-block:: udiff. @@ -1108,17 +1108,8 @@ static void HandleExtern() {; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; - if (auto *FnIR = FnAST->codegen()) {; - // We're just doing this to make sure it executes.; - TheExecutionEngine->finalizeObject();; - // JIT the function, returning a function pointer.; - void *FPtr = TheExecutionEngine->getPointerToFunction(FnIR);; -; - // Cast it to the right type (takes no arguments, returns a double) so we; - // can call it as a native function.; - double (*FP)() = (double (*)())(intptr_t)FPtr;; - // Ignore the return value for this.; - (void)FP;; + if (!FnAST->codegen()) {; + fprintf(stderr, ""Error generating code for top level expr"");; }; } else {; // Skip token for error recovery.; @@ -1439,11 +1459,11 @@ int main() {; // target lays out data structures.; TheModule->setDataLayout(TheExecutionEngine->getDataLayout());; OurFPM.add(new DataLayoutPass());; +#if 0; OurFPM.add(createBasicAliasAnalysisPass());; // Promote allocas to registers.; OurFPM.add(createPromoteMemoryToRegisterPass());; @@ -1218,7 +1210,7 @@ int main() {; OurFPM.add(createGVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; OurFPM.add(createCFGSimplificationPass());; -; + #endif; OurFPM.doInitialization();. // Set the global so the code gen can use this. This relatively small set of changes get us to the point that we can compile; our piece of Kaleidoscope language down to an executable program via this; command line:. .. code-block:: bash. Kaleidoscope-Ch9 < fib.ks | & clang -x ir -. which gives an a.out/a.exe in the current working directory. Compile Unit; ============. The top level co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:4653,Availability,recover,recovery,4653,"(stderr, ""ready> "");; getNextToken();. Lastly we're going to disable all of the optimization passes and the JIT so; that the only thing that happens after we're done parsing and generating; code is that the LLVM IR goes to standard error:. .. code-block:: udiff. @@ -1108,17 +1108,8 @@ static void HandleExtern() {; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; - if (auto *FnIR = FnAST->codegen()) {; - // We're just doing this to make sure it executes.; - TheExecutionEngine->finalizeObject();; - // JIT the function, returning a function pointer.; - void *FPtr = TheExecutionEngine->getPointerToFunction(FnIR);; -; - // Cast it to the right type (takes no arguments, returns a double) so we; - // can call it as a native function.; - double (*FP)() = (double (*)())(intptr_t)FPtr;; - // Ignore the return value for this.; - (void)FP;; + if (!FnAST->codegen()) {; + fprintf(stderr, ""Error generating code for top level expr"");; }; } else {; // Skip token for error recovery.; @@ -1439,11 +1459,11 @@ int main() {; // target lays out data structures.; TheModule->setDataLayout(TheExecutionEngine->getDataLayout());; OurFPM.add(new DataLayoutPass());; +#if 0; OurFPM.add(createBasicAliasAnalysisPass());; // Promote allocas to registers.; OurFPM.add(createPromoteMemoryToRegisterPass());; @@ -1218,7 +1210,7 @@ int main() {; OurFPM.add(createGVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; OurFPM.add(createCFGSimplificationPass());; -; + #endif; OurFPM.doInitialization();. // Set the global so the code gen can use this. This relatively small set of changes get us to the point that we can compile; our piece of Kaleidoscope language down to an executable program via this; command line:. .. code-block:: bash. Kaleidoscope-Ch9 < fib.ks | & clang -x ir -. which gives an a.out/a.exe in the current working directory. Compile Unit; ============. The top level co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:5359,Availability,down,down,5359," type (takes no arguments, returns a double) so we; - // can call it as a native function.; - double (*FP)() = (double (*)())(intptr_t)FPtr;; - // Ignore the return value for this.; - (void)FP;; + if (!FnAST->codegen()) {; + fprintf(stderr, ""Error generating code for top level expr"");; }; } else {; // Skip token for error recovery.; @@ -1439,11 +1459,11 @@ int main() {; // target lays out data structures.; TheModule->setDataLayout(TheExecutionEngine->getDataLayout());; OurFPM.add(new DataLayoutPass());; +#if 0; OurFPM.add(createBasicAliasAnalysisPass());; // Promote allocas to registers.; OurFPM.add(createPromoteMemoryToRegisterPass());; @@ -1218,7 +1210,7 @@ int main() {; OurFPM.add(createGVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; OurFPM.add(createCFGSimplificationPass());; -; + #endif; OurFPM.doInitialization();. // Set the global so the code gen can use this. This relatively small set of changes get us to the point that we can compile; our piece of Kaleidoscope language down to an executable program via this; command line:. .. code-block:: bash. Kaleidoscope-Ch9 < fib.ks | & clang -x ir -. which gives an a.out/a.exe in the current working directory. Compile Unit; ============. The top level container for a section of code in DWARF is a compile unit.; This contains the type and function data for an individual translation unit; (read: one file of source code). So the first thing we need to do is; construct one for our fib.ks file. DWARF Emission Setup; ====================. Similar to the ``IRBuilder`` class we have a; `DIBuilder <https://llvm.org/doxygen/classllvm_1_1DIBuilder.html>`_ class; that helps in constructing debug metadata for an LLVM IR file. It; corresponds 1:1 similarly to ``IRBuilder`` and LLVM IR, but with nicer names.; Using it does require that you be more familiar with DWARF terminology than; you needed to be with ``IRBuilder`` and ``Instruction`` names, but if you; read through the general documentation ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:11230,Availability,down,down,11230,"tChar = getchar();. if (LastChar == '\n' || LastChar == '\r') {; LexLoc.Line++;; LexLoc.Col = 0;; } else; LexLoc.Col++;; return LastChar;; }. In this set of code we've added some functionality on how to keep track of the; line and column of the ""source file"". As we lex every token we set our current; current ""lexical location"" to the assorted line and column for the beginning; of the token. We do this by overriding all of the previous calls to; ``getchar()`` with our new ``advance()`` that keeps track of the information; and then we have added to all of our AST classes a source location:. .. code-block:: c++. class ExprAST {; SourceLocation Loc;. public:; ExprAST(SourceLocation Loc = CurLoc) : Loc(Loc) {}; virtual ~ExprAST() {}; virtual Value* codegen() = 0;; int getLine() const { return Loc.Line; }; int getCol() const { return Loc.Col; }; virtual raw_ostream &dump(raw_ostream &out, int ind) {; return out << ':' << getLine() << ':' << getCol() << '\n';; }. that we pass down through when we create a new expression:. .. code-block:: c++. LHS = std::make_unique<BinaryExprAST>(BinLoc, BinOp, std::move(LHS),; std::move(RHS));. giving us locations for each of our expressions and variables. To make sure that every instruction gets proper source location information,; we have to tell ``Builder`` whenever we're at a new source location.; We use a small helper function for this:. .. code-block:: c++. void DebugInfo::emitLocation(ExprAST *AST) {; if (!AST); return Builder->SetCurrentDebugLocation(DebugLoc());; DIScope *Scope;; if (LexicalBlocks.empty()); Scope = TheCU;; else; Scope = LexicalBlocks.back();; Builder->SetCurrentDebugLocation(; DILocation::get(Scope->getContext(), AST->getLine(), AST->getCol(), Scope));; }. This both tells the main ``IRBuilder`` where we are, but also what scope; we're in. The scope can either be on compile-unit level or be the nearest; enclosing lexical block like the current function.; To represent this we create a stack of scopes in ``DebugInfo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2415,Deployability,patch,patches,2415,"tion that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-block:: udiff. @@ -1129,7 +1129,6 @@ static void HandleTopLevelExpression() {; /// top ::= definition | external | expression | ';'; stat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:1427,Energy Efficiency,reduce,reduce,1427,"if something goes wrong though, how do you debug your; program?. Source level debugging uses formatted data that helps a debugger; translate from binary and the state of the machine back to the; source that the programmer wrote. In LLVM we generally use a format; called `DWARF <http://dwarfstd.org>`_. DWARF is a compact encoding; that represents types, source locations, and variable locations. The short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:377,Modifiability,variab,variables,377,"======================================; Kaleidoscope: Adding Debug Information; ======================================. .. contents::; :local:. Chapter 9 Introduction; ======================. Welcome to Chapter 9 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 8, we've built a; decent little programming language with functions and variables.; What happens if something goes wrong though, how do you debug your; program?. Source level debugging uses formatted data that helps a debugger; translate from binary and the state of the machine back to the; source that the programmer wrote. In LLVM we generally use a format; called `DWARF <http://dwarfstd.org>`_. DWARF is a compact encoding; that represents types, source locations, and variable locations. The short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly cr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:779,Modifiability,variab,variable,779,"======================================; Kaleidoscope: Adding Debug Information; ======================================. .. contents::; :local:. Chapter 9 Introduction; ======================. Welcome to Chapter 9 of the ""`Implementing a language with; LLVM <index.html>`_"" tutorial. In chapters 1 through 8, we've built a; decent little programming language with functions and variables.; What happens if something goes wrong though, how do you debug your; program?. Source level debugging uses formatted data that helps a debugger; translate from binary and the state of the machine back to the; source that the programmer wrote. In LLVM we generally use a format; called `DWARF <http://dwarfstd.org>`_. DWARF is a compact encoding; that represents types, source locations, and variable locations. The short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly cr",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2189,Modifiability,variab,variables,2189,"that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-blo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2267,Modifiability,variab,variables,2267,"that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-blo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:6651,Modifiability,variab,variable,6651,"tains the type and function data for an individual translation unit; (read: one file of source code). So the first thing we need to do is; construct one for our fib.ks file. DWARF Emission Setup; ====================. Similar to the ``IRBuilder`` class we have a; `DIBuilder <https://llvm.org/doxygen/classllvm_1_1DIBuilder.html>`_ class; that helps in constructing debug metadata for an LLVM IR file. It; corresponds 1:1 similarly to ``IRBuilder`` and LLVM IR, but with nicer names.; Using it does require that you be more familiar with DWARF terminology than; you needed to be with ``IRBuilder`` and ``Instruction`` names, but if you; read through the general documentation on the; `Metadata Format <https://llvm.org/docs/SourceLevelDebugging.html>`_ it; should be a little more clear. We'll be using this class to construct all; of our IR level descriptions. Construction for it takes a module so we; need to construct it shortly after we construct our module. We've left it; as a global static variable to make it a bit easier to use. Next we're going to create a small container to cache some of our frequent; data. The first will be our compile unit, but we'll also write a bit of; code for our one type since we won't have to worry about multiple typed; expressions:. .. code-block:: c++. static std::unique_ptr<DIBuilder> DBuilder;. struct DebugInfo {; DICompileUnit *TheCU;; DIType *DblTy;. DIType *getDoubleTy();; } KSDbgInfo;. DIType *DebugInfo::getDoubleTy() {; if (DblTy); return DblTy;. DblTy = DBuilder->createBasicType(""double"", 64, dwarf::DW_ATE_float);; return DblTy;; }. And then later on in ``main`` when we're constructing our module:. .. code-block:: c++. DBuilder = std::make_unique<DIBuilder>(*TheModule);. KSDbgInfo.TheCU = DBuilder->createCompileUnit(; dwarf::DW_LANG_C, DBuilder->createFile(""fib.ks"", "".""),; ""Kaleidoscope Compiler"", false, """", 0);. There are a couple of things to note here. First, while we're producing a; compile unit for a language called Kaleidoscope we",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:11438,Modifiability,variab,variables,11438,"ded some functionality on how to keep track of the; line and column of the ""source file"". As we lex every token we set our current; current ""lexical location"" to the assorted line and column for the beginning; of the token. We do this by overriding all of the previous calls to; ``getchar()`` with our new ``advance()`` that keeps track of the information; and then we have added to all of our AST classes a source location:. .. code-block:: c++. class ExprAST {; SourceLocation Loc;. public:; ExprAST(SourceLocation Loc = CurLoc) : Loc(Loc) {}; virtual ~ExprAST() {}; virtual Value* codegen() = 0;; int getLine() const { return Loc.Line; }; int getCol() const { return Loc.Col; }; virtual raw_ostream &dump(raw_ostream &out, int ind) {; return out << ':' << getLine() << ':' << getCol() << '\n';; }. that we pass down through when we create a new expression:. .. code-block:: c++. LHS = std::make_unique<BinaryExprAST>(BinLoc, BinOp, std::move(LHS),; std::move(RHS));. giving us locations for each of our expressions and variables. To make sure that every instruction gets proper source location information,; we have to tell ``Builder`` whenever we're at a new source location.; We use a small helper function for this:. .. code-block:: c++. void DebugInfo::emitLocation(ExprAST *AST) {; if (!AST); return Builder->SetCurrentDebugLocation(DebugLoc());; DIScope *Scope;; if (LexicalBlocks.empty()); Scope = TheCU;; else; Scope = LexicalBlocks.back();; Builder->SetCurrentDebugLocation(; DILocation::get(Scope->getContext(), AST->getLine(), AST->getCol(), Scope));; }. This both tells the main ``IRBuilder`` where we are, but also what scope; we're in. The scope can either be on compile-unit level or be the nearest; enclosing lexical block like the current function.; To represent this we create a stack of scopes in ``DebugInfo``:. .. code-block:: c++. std::vector<DIScope *> LexicalBlocks;. and push the scope (function) to the top of the stack when we start; generating the code for each function",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:12986,Modifiability,variab,variables,12986,"l(), Scope));; }. This both tells the main ``IRBuilder`` where we are, but also what scope; we're in. The scope can either be on compile-unit level or be the nearest; enclosing lexical block like the current function.; To represent this we create a stack of scopes in ``DebugInfo``:. .. code-block:: c++. std::vector<DIScope *> LexicalBlocks;. and push the scope (function) to the top of the stack when we start; generating the code for each function:. .. code-block:: c++. KSDbgInfo.LexicalBlocks.push_back(SP);. Also, we may not forget to pop the scope back off of the scope stack at the; end of the code generation for the function:. .. code-block:: c++. // Pop off the lexical block for the function since we added it; // unconditionally.; KSDbgInfo.LexicalBlocks.pop_back();. Then we make sure to emit the location every time we start to generate code; for a new AST object:. .. code-block:: c++. KSDbgInfo.emitLocation(this);. Variables; =========. Now that we have functions, we need to be able to print out the variables; we have in scope. Let's get our function arguments set up so we can get; decent backtraces and see how our functions are being called. It isn't; a lot of code, and we generally handle it when we're creating the; argument allocas in ``FunctionAST::codegen``. .. code-block:: c++. // Record the function arguments in the NamedValues map.; NamedValues.clear();; unsigned ArgIdx = 0;; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Create a debug descriptor for the variable.; DILocalVariable *D = DBuilder->createParameterVariable(; SP, Arg.getName(), ++ArgIdx, Unit, LineNo, KSDbgInfo.getDoubleTy(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to varia",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:13448,Modifiability,variab,variable,13448,"or each function:. .. code-block:: c++. KSDbgInfo.LexicalBlocks.push_back(SP);. Also, we may not forget to pop the scope back off of the scope stack at the; end of the code generation for the function:. .. code-block:: c++. // Pop off the lexical block for the function since we added it; // unconditionally.; KSDbgInfo.LexicalBlocks.pop_back();. Then we make sure to emit the location every time we start to generate code; for a new AST object:. .. code-block:: c++. KSDbgInfo.emitLocation(this);. Variables; =========. Now that we have functions, we need to be able to print out the variables; we have in scope. Let's get our function arguments set up so we can get; decent backtraces and see how our functions are being called. It isn't; a lot of code, and we generally handle it when we're creating the; argument allocas in ``FunctionAST::codegen``. .. code-block:: c++. // Record the function arguments in the NamedValues map.; NamedValues.clear();; unsigned ArgIdx = 0;; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Create a debug descriptor for the variable.; DILocalVariable *D = DBuilder->createParameterVariable(; SP, Arg.getName(), ++ArgIdx, Unit, LineNo, KSDbgInfo.getDoubleTy(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beg",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:13570,Modifiability,variab,variable,13570,"e; end of the code generation for the function:. .. code-block:: c++. // Pop off the lexical block for the function since we added it; // unconditionally.; KSDbgInfo.LexicalBlocks.pop_back();. Then we make sure to emit the location every time we start to generate code; for a new AST object:. .. code-block:: c++. KSDbgInfo.emitLocation(this);. Variables; =========. Now that we have functions, we need to be able to print out the variables; we have in scope. Let's get our function arguments set up so we can get; decent backtraces and see how our functions are being called. It isn't; a lot of code, and we generally handle it when we're creating the; argument allocas in ``FunctionAST::codegen``. .. code-block:: c++. // Record the function arguments in the NamedValues map.; NamedValues.clear();; unsigned ArgIdx = 0;; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Create a debug descriptor for the variable.; DILocalVariable *D = DBuilder->createParameterVariable(; SP, Arg.getName(), ++ArgIdx, Unit, LineNo, KSDbgInfo.getDoubleTy(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:13963,Modifiability,variab,variable,13963," able to print out the variables; we have in scope. Let's get our function arguments set up so we can get; decent backtraces and see how our functions are being called. It isn't; a lot of code, and we generally handle it when we're creating the; argument allocas in ``FunctionAST::codegen``. .. code-block:: c++. // Record the function arguments in the NamedValues map.; NamedValues.clear();; unsigned ArgIdx = 0;; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Create a debug descriptor for the variable.; DILocalVariable *D = DBuilder->createParameterVariable(; SP, Arg.getName(), ++ArgIdx, Unit, LineNo, KSDbgInfo.getDoubleTy(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:14071,Modifiability,variab,variable,14071,"e being called. It isn't; a lot of code, and we generally handle it when we're creating the; argument allocas in ``FunctionAST::codegen``. .. code-block:: c++. // Record the function arguments in the NamedValues map.; NamedValues.clear();; unsigned ArgIdx = 0;; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Create a debug descriptor for the variable.; DILocalVariable *D = DBuilder->createParameterVariable(; SP, Arg.getName(), ++ArgIdx, Unit, LineNo, KSDbgInfo.getDoubleTy(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // location in a function are considered part of the prologue and the debugger; // will run past them when breaking on a function); KSDbgInfo.emitLocation(n",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:14284,Modifiability,variab,variable,14284,"p.; NamedValues.clear();; unsigned ArgIdx = 0;; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Create a debug descriptor for the variable.; DILocalVariable *D = DBuilder->createParameterVariable(; SP, Arg.getName(), ++ArgIdx, Unit, LineNo, KSDbgInfo.getDoubleTy(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // location in a function are considered part of the prologue and the debugger; // will run past them when breaking on a function); KSDbgInfo.emitLocation(nullptr);. and then emit a new location when we actually start generating code for the; body of the function:. .. code-block:: c++. KSDbgInfo.emitLocation(Body.get());. With this we have enough debug information to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:14349,Modifiability,variab,variable,14349,"p.; NamedValues.clear();; unsigned ArgIdx = 0;; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Create a debug descriptor for the variable.; DILocalVariable *D = DBuilder->createParameterVariable(; SP, Arg.getName(), ++ArgIdx, Unit, LineNo, KSDbgInfo.getDoubleTy(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // location in a function are considered part of the prologue and the debugger; // will run past them when breaking on a function); KSDbgInfo.emitLocation(nullptr);. and then emit a new location when we actually start generating code for the; body of the function:. .. code-block:: c++. KSDbgInfo.emitLocation(Body.get());. With this we have enough debug information to",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:15381,Modifiability,variab,variables,15381," arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // location in a function are considered part of the prologue and the debugger; // will run past them when breaking on a function); KSDbgInfo.emitLocation(nullptr);. and then emit a new location when we actually start generating code for the; body of the function:. .. code-block:: c++. KSDbgInfo.emitLocation(Body.get());. With this we have enough debug information to set breakpoints in functions,; print out argument variables, and call functions. Not too bad for just a; few simple lines of code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; debug information. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter9/toy.cpp; :language: c++. `Next: Conclusion and other useful LLVM tidbits <LangImpl10.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:15560,Modifiability,enhance,enhanced,15560," arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // location in a function are considered part of the prologue and the debugger; // will run past them when breaking on a function); KSDbgInfo.emitLocation(nullptr);. and then emit a new location when we actually start generating code for the; body of the function:. .. code-block:: c++. KSDbgInfo.emitLocation(Body.get());. With this we have enough debug information to set breakpoints in functions,; print out argument variables, and call functions. Not too bad for just a; few simple lines of code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; debug information. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter9/toy.cpp; :language: c++. `Next: Conclusion and other useful LLVM tidbits <LangImpl10.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:15681,Modifiability,config,config,15681," arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // location in a function are considered part of the prologue and the debugger; // will run past them when breaking on a function); KSDbgInfo.emitLocation(nullptr);. and then emit a new location when we actually start generating code for the; body of the function:. .. code-block:: c++. KSDbgInfo.emitLocation(Body.get());. With this we have enough debug information to set breakpoints in functions,; print out argument variables, and call functions. Not too bad for just a; few simple lines of code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; debug information. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter9/toy.cpp; :language: c++. `Next: Conclusion and other useful LLVM tidbits <LangImpl10.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:1752,Performance,optimiz,optimized,1752,"a compact encoding; that represents types, source locations, and variable locations. The short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:1775,Performance,optimiz,optimization,1775,"e short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug,",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2136,Performance,optimiz,optimized,2136,"compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2166,Performance,optimiz,optimization,2166,"that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-blo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2223,Performance,optimiz,optimized,2223,"that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-blo",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2357,Performance,optimiz,optimization,2357,"tion that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-block:: udiff. @@ -1129,7 +1129,6 @@ static void HandleTopLevelExpression() {; /// top ::= definition | external | expression | ';'; stat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:3665,Performance,optimiz,optimization,3665,"y the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-block:: udiff. @@ -1129,7 +1129,6 @@ static void HandleTopLevelExpression() {; /// top ::= definition | external | expression | ';'; static void MainLoop() {; while (true) {; - fprintf(stderr, ""ready> "");; switch (CurTok) {; case tok_eof:; return;; @@ -1184,7 +1183,6 @@ int main() {; BinopPrecedence['*'] = 40; // highest. // Prime the first token.; - fprintf(stderr, ""ready> "");; getNextToken();. Lastly we're going to disable all of the optimization passes and the JIT so; that the only thing that happens after we're done parsing and generating; code is that the LLVM IR goes to standard error:. .. code-block:: udiff. @@ -1108,17 +1108,8 @@ static void HandleExtern() {; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; - if (auto *FnIR = FnAST->codegen()) {; - // We're just doing this to make sure it executes.; - TheExecutionEngine->finalizeObject();; - // JIT the function, returning a function pointer.; - void *FPtr = TheExecutionEngine->getPointerToFunction(FnIR);; -; - // Cast it to the right type (takes no arguments, returns a double) so we; - // can call it as a native function.; - double (*FP)() = (double (*)())(intptr_t)FPtr;; - // Ignore the return value for this.; - (void)FP;; + if (!FnAST->codegen()) {; + fprintf(stderr, ""Error generating code for top level expr"");; }; } else {; // Skip token for error recovery.; @@ -1439,11 +1459,11 @@ int main() {; // target lays out dat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:6740,Performance,cache,cache,6740," file of source code). So the first thing we need to do is; construct one for our fib.ks file. DWARF Emission Setup; ====================. Similar to the ``IRBuilder`` class we have a; `DIBuilder <https://llvm.org/doxygen/classllvm_1_1DIBuilder.html>`_ class; that helps in constructing debug metadata for an LLVM IR file. It; corresponds 1:1 similarly to ``IRBuilder`` and LLVM IR, but with nicer names.; Using it does require that you be more familiar with DWARF terminology than; you needed to be with ``IRBuilder`` and ``Instruction`` names, but if you; read through the general documentation on the; `Metadata Format <https://llvm.org/docs/SourceLevelDebugging.html>`_ it; should be a little more clear. We'll be using this class to construct all; of our IR level descriptions. Construction for it takes a module so we; need to construct it shortly after we construct our module. We've left it; as a global static variable to make it a bit easier to use. Next we're going to create a small container to cache some of our frequent; data. The first will be our compile unit, but we'll also write a bit of; code for our one type since we won't have to worry about multiple typed; expressions:. .. code-block:: c++. static std::unique_ptr<DIBuilder> DBuilder;. struct DebugInfo {; DICompileUnit *TheCU;; DIType *DblTy;. DIType *getDoubleTy();; } KSDbgInfo;. DIType *DebugInfo::getDoubleTy() {; if (DblTy); return DblTy;. DblTy = DBuilder->createBasicType(""double"", 64, dwarf::DW_ATE_float);; return DblTy;; }. And then later on in ``main`` when we're constructing our module:. .. code-block:: c++. DBuilder = std::make_unique<DIBuilder>(*TheModule);. KSDbgInfo.TheCU = DBuilder->createCompileUnit(; dwarf::DW_LANG_C, DBuilder->createFile(""fib.ks"", "".""),; ""Kaleidoscope Compiler"", false, """", 0);. There are a couple of things to note here. First, while we're producing a; compile unit for a language called Kaleidoscope we used the language; constant for C. This is because a debugger wouldn't necess",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2351,Safety,avoid,avoid,2351,"tion that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-block:: udiff. @@ -1129,7 +1129,6 @@ static void HandleTopLevelExpression() {; /// top ::= definition | external | expression | ';'; stat",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:4653,Safety,recover,recovery,4653,"(stderr, ""ready> "");; getNextToken();. Lastly we're going to disable all of the optimization passes and the JIT so; that the only thing that happens after we're done parsing and generating; code is that the LLVM IR goes to standard error:. .. code-block:: udiff. @@ -1108,17 +1108,8 @@ static void HandleExtern() {; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; - if (auto *FnIR = FnAST->codegen()) {; - // We're just doing this to make sure it executes.; - TheExecutionEngine->finalizeObject();; - // JIT the function, returning a function pointer.; - void *FPtr = TheExecutionEngine->getPointerToFunction(FnIR);; -; - // Cast it to the right type (takes no arguments, returns a double) so we; - // can call it as a native function.; - double (*FP)() = (double (*)())(intptr_t)FPtr;; - // Ignore the return value for this.; - (void)FP;; + if (!FnAST->codegen()) {; + fprintf(stderr, ""Error generating code for top level expr"");; }; } else {; // Skip token for error recovery.; @@ -1439,11 +1459,11 @@ int main() {; // target lays out data structures.; TheModule->setDataLayout(TheExecutionEngine->getDataLayout());; OurFPM.add(new DataLayoutPass());; +#if 0; OurFPM.add(createBasicAliasAnalysisPass());; // Promote allocas to registers.; OurFPM.add(createPromoteMemoryToRegisterPass());; @@ -1218,7 +1210,7 @@ int main() {; OurFPM.add(createGVNPass());; // Simplify the control flow graph (deleting unreachable blocks, etc).; OurFPM.add(createCFGSimplificationPass());; -; + #endif; OurFPM.doInitialization();. // Set the global so the code gen can use this. This relatively small set of changes get us to the point that we can compile; our piece of Kaleidoscope language down to an executable program via this; command line:. .. code-block:: bash. Kaleidoscope-Ch9 < fib.ks | & clang -x ir -. which gives an a.out/a.exe in the current working directory. Compile Unit; ============. The top level co",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:14656,Safety,avoid,avoid,14656,"(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // location in a function are considered part of the prologue and the debugger; // will run past them when breaking on a function); KSDbgInfo.emitLocation(nullptr);. and then emit a new location when we actually start generating code for the; body of the function:. .. code-block:: c++. KSDbgInfo.emitLocation(Body.get());. With this we have enough debug information to set breakpoints in functions,; print out argument variables, and call functions. Not too bad for just a; few simple lines of code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; debug information. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ld",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:1265,Usability,simpl,simple,1265,"orial. In chapters 1 through 8, we've built a; decent little programming language with functions and variables.; What happens if something goes wrong though, how do you debug your; program?. Source level debugging uses formatted data that helps a debugger; translate from binary and the state of the machine back to the; source that the programmer wrote. In LLVM we generally use a format; called `DWARF <http://dwarfstd.org>`_. DWARF is a compact encoding; that represents types, source locations, and variable locations. The short summary of this chapter is that we'll go through the; various things you have to add to a programming language to; support debug info, and how you translate that into DWARF. Caveat: For now we can't debug via the JIT, so we'll need to compile; our program down to something small and standalone. As part of this; we'll make a few modifications to the running of the language and; how programs are compiled. This means that we'll have a source file; with a simple program written in Kaleidoscope rather than the; interactive JIT. It does involve a limitation that we can only; have one ""top level"" command at a time to reduce the number of; changes necessary. Here's the sample program we'll be compiling:. .. code-block:: python. def fib(x); if x < 3 then; 1; else; fib(x-1)+fib(x-2);. fib(10). Why is this a hard problem?; ===========================. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:2746,Usability,simpl,simple,2746,"=======. Debug information is a hard problem for a few different reasons - mostly; centered around optimized code. First, optimization makes keeping source; locations more difficult. In LLVM IR we keep the original source location; for each IR level instruction on the instruction. Optimization passes; should keep the source locations for newly created instructions, but merged; instructions only get to keep a single location - this can cause jumping; around when stepping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-block:: udiff. @@ -1129,7 +1129,6 @@ static void HandleTopLevelExpression() {; /// top ::= definition | external | expression | ';'; static void MainLoop() {; while (true) {; - fprintf(stderr, ""ready> "");; switch (CurTok) {; case tok_eof:; return;; @@ -1184,7 +1183,6 @@ int main() {; BinopPrecedence['*'] = 40; // highest. // Prime the first token.; - fprintf(stderr, ""ready> "");; getNextToken();. Lastly we're going to disable",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:3112,Usability,simpl,simple,3112,"epping through optimized programs. Secondly, optimization; can move variables in ways that are either optimized out, shared in memory; with other variables, or difficult to track. For the purposes of this; tutorial we're going to avoid optimization (as you'll see with one of the; next sets of patches). Ahead-of-Time Compilation Mode; ==============================. To highlight only the aspects of adding debug information to a source; language without needing to worry about the complexities of JIT debugging; we're going to make a few changes to Kaleidoscope to support compiling; the IR emitted by the front end into a simple standalone program that; you can execute, debug, and see results. First we make our anonymous function that contains our top level; statement be our ""main"":. .. code-block:: udiff. - auto Proto = std::make_unique<PrototypeAST>("""", std::vector<std::string>());; + auto Proto = std::make_unique<PrototypeAST>(""main"", std::vector<std::string>());. just with the simple change of giving it a name. Then we're going to remove the command line code wherever it exists:. .. code-block:: udiff. @@ -1129,7 +1129,6 @@ static void HandleTopLevelExpression() {; /// top ::= definition | external | expression | ';'; static void MainLoop() {; while (true) {; - fprintf(stderr, ""ready> "");; switch (CurTok) {; case tok_eof:; return;; @@ -1184,7 +1183,6 @@ int main() {; BinopPrecedence['*'] = 40; // highest. // Prime the first token.; - fprintf(stderr, ""ready> "");; getNextToken();. Lastly we're going to disable all of the optimization passes and the JIT so; that the only thing that happens after we're done parsing and generating; code is that the LLVM IR goes to standard error:. .. code-block:: udiff. @@ -1108,17 +1108,8 @@ static void HandleExtern() {; static void HandleTopLevelExpression() {; // Evaluate a top-level expression into an anonymous function.; if (auto FnAST = ParseTopLevelExpr()) {; - if (auto *FnIR = FnAST->codegen()) {; - // We're just doing this to make",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:6434,Usability,clear,clear,6434,"de-block:: bash. Kaleidoscope-Ch9 < fib.ks | & clang -x ir -. which gives an a.out/a.exe in the current working directory. Compile Unit; ============. The top level container for a section of code in DWARF is a compile unit.; This contains the type and function data for an individual translation unit; (read: one file of source code). So the first thing we need to do is; construct one for our fib.ks file. DWARF Emission Setup; ====================. Similar to the ``IRBuilder`` class we have a; `DIBuilder <https://llvm.org/doxygen/classllvm_1_1DIBuilder.html>`_ class; that helps in constructing debug metadata for an LLVM IR file. It; corresponds 1:1 similarly to ``IRBuilder`` and LLVM IR, but with nicer names.; Using it does require that you be more familiar with DWARF terminology than; you needed to be with ``IRBuilder`` and ``Instruction`` names, but if you; read through the general documentation on the; `Metadata Format <https://llvm.org/docs/SourceLevelDebugging.html>`_ it; should be a little more clear. We'll be using this class to construct all; of our IR level descriptions. Construction for it takes a module so we; need to construct it shortly after we construct our module. We've left it; as a global static variable to make it a bit easier to use. Next we're going to create a small container to cache some of our frequent; data. The first will be our compile unit, but we'll also write a bit of; code for our one type since we won't have to worry about multiple typed; expressions:. .. code-block:: c++. static std::unique_ptr<DIBuilder> DBuilder;. struct DebugInfo {; DICompileUnit *TheCU;; DIType *DblTy;. DIType *getDoubleTy();; } KSDbgInfo;. DIType *DebugInfo::getDoubleTy() {; if (DblTy); return DblTy;. DblTy = DBuilder->createBasicType(""double"", 64, dwarf::DW_ATE_float);; return DblTy;; }. And then later on in ``main`` when we're constructing our module:. .. code-block:: c++. DBuilder = std::make_unique<DIBuilder>(*TheModule);. KSDbgInfo.TheCU = DBuilder->createC",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:13346,Usability,clear,clear,13346,"or each function:. .. code-block:: c++. KSDbgInfo.LexicalBlocks.push_back(SP);. Also, we may not forget to pop the scope back off of the scope stack at the; end of the code generation for the function:. .. code-block:: c++. // Pop off the lexical block for the function since we added it; // unconditionally.; KSDbgInfo.LexicalBlocks.pop_back();. Then we make sure to emit the location every time we start to generate code; for a new AST object:. .. code-block:: c++. KSDbgInfo.emitLocation(this);. Variables; =========. Now that we have functions, we need to be able to print out the variables; we have in scope. Let's get our function arguments set up so we can get; decent backtraces and see how our functions are being called. It isn't; a lot of code, and we generally handle it when we're creating the; argument allocas in ``FunctionAST::codegen``. .. code-block:: c++. // Record the function arguments in the NamedValues map.; NamedValues.clear();; unsigned ArgIdx = 0;; for (auto &Arg : TheFunction->args()) {; // Create an alloca for this variable.; AllocaInst *Alloca = CreateEntryBlockAlloca(TheFunction, Arg.getName());. // Create a debug descriptor for the variable.; DILocalVariable *D = DBuilder->createParameterVariable(; SP, Arg.getName(), ++ArgIdx, Unit, LineNo, KSDbgInfo.getDoubleTy(),; true);. DBuilder->insertDeclare(Alloca, D, DBuilder->createExpression(),; DILocation::get(SP->getContext(), LineNo, 0, SP),; Builder->GetInsertBlock());. // Store the initial value into the alloca.; Builder->CreateStore(&Arg, Alloca);. // Add arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beg",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst:15440,Usability,simpl,simple,15440," arguments to variable symbol table.; NamedValues[std::string(Arg.getName())] = Alloca;; }. Here we're first creating the variable, giving it the scope (``SP``),; the name, source location, type, and since it's an argument, the argument; index. Next, we create an ``lvm.dbg.declare`` call to indicate at the IR; level that we've got a variable in an alloca (and it gives a starting; location for the variable), and setting a source location for the; beginning of the scope on the declare. One interesting thing to note at this point is that various debuggers have; assumptions based on how code and debug information was generated for them; in the past. In this case we need to do a little bit of a hack to avoid; generating line information for the function prologue so that the debugger; knows to skip over those instructions when setting a breakpoint. So in; ``FunctionAST::CodeGen`` we add some more lines:. .. code-block:: c++. // Unset the location for the prologue emission (leading instructions with no; // location in a function are considered part of the prologue and the debugger; // will run past them when breaking on a function); KSDbgInfo.emitLocation(nullptr);. and then emit a new location when we actually start generating code for the; body of the function:. .. code-block:: c++. KSDbgInfo.emitLocation(Body.get());. With this we have enough debug information to set breakpoints in functions,; print out argument variables, and call functions. Not too bad for just a; few simple lines of code!. Full Code Listing; =================. Here is the complete code listing for our running example, enhanced with; debug information. To build this example, use:. .. code-block:: bash. # Compile; clang++ -g toy.cpp `llvm-config --cxxflags --ldflags --system-libs --libs core orcjit native` -O3 -o toy; # Run; ./toy. Here is the code:. .. literalinclude:: ../../../examples/Kaleidoscope/Chapter9/toy.cpp; :language: c++. `Next: Conclusion and other useful LLVM tidbits <LangImpl10.html>`_. ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl09.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:3919,Availability,error,error,3919,"ou extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make explicit use of setjmp/longjmp. There are; many different ways to go here.; - **object orientation, generics, database access, complex numbers,; geometric programming, ...** - Really, there is no end of crazy; features that you can add to the language.; - **unusual domains** - We've been talking about applying LLVM to a; domain that many people are interested in: building a compiler for a; specific language. However, there are many other domains that can use; compiler technology that are not typically considered. For example,; LLVM has been used to implement OpenGL graphics acceleration,; translate C++ code to ActionScript, and many other cute and clever; things. Maybe you will be the first to JIT compile a regular; expression interpreter into native code with LLVM?. Have fun - try doing something crazy and unusual. Building a language; like everyone else always has, is much less fun than trying something a; litt",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:9287,Availability,down,down,9287,"ety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Language-Specific Optimizations; -------------------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:9687,Availability,down,down,9687,"----------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)""",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:3646,Deployability,update,update,3646,"ion>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make explicit use of setjmp/longjmp. There are; many different ways to go here.; - **object orientation, generics, database access, complex numbers,; geometric programming, ...** - Really, there is no end of crazy; features that you can add to the language.; - **unusual domains** - We've been talking about applying LLVM to a; domain that many people are interested in: building a compiler for a; specific language. However, there are many other domains that can use; compiler technology that are not typically considered. For example,; LLVM has been used to implement OpenGL graphics acceleration,; translate C++ code to ActionScript, and many other cute a",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:3336,Energy Efficiency,allocate,allocate,3336,"e type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make explicit use of setjmp/longjmp. There are; many different ways to go here.; - **object orientation, generics, database access, complex numbers,; geometric programming, ...** - Really, there is no end of crazy; features that you can add to the language.; - **unusual domains** - We've been talking about applying LLVM to a; domain that many people are interested in: building a compiler for a; specific la",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:11879,Energy Efficiency,allocate,allocates,11879,"simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if you are trying to keep the code; generated by your compiler ""target independent"", is that you often need; to know the size of some LLVM type or the offset of some field in an; llvm structure. For example, you might need to pass the size of a type; into a function that allocates memory. Unfortunately, this can vary widely across targets: for example the; width of a pointer is trivially target-specific. However, there is a; `clever way to use the getelementptr; instruction <http://nondot.org/sabre/LLVMNotes/SizeOf-OffsetOf-VariableSizedStructs.txt>`_; that allows you to compute this in a portable way. Garbage Collected Stack Frames; ------------------------------. Some languages want to explicitly manage their stack frames, often so; that they are garbage collected or to allow easy implementation of; closures. There are often better ways to implement these features than; explicit stack frames, but `LLVM does support; them, <http://nondot.org/sabre/LLVMNotes/ExplicitlyManagedStackFrames.txt>`_; if you want. It requires your front-end to convert the code into; `Continuation Passing; Style <http://en.wikipedia.org/wiki/Continuation-passing_style>`_ and; the use of tail calls (which LLVM also supports). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:3153,Integrability,rout,routines,3153,"s to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make explicit use of setjmp/longjmp. There are; many different ways to go here.; - **object orientation, generics, database access, complex numbers,; geometric programming",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:3410,Integrability,interface,interface,3410,"e type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make explicit use of setjmp/longjmp. There are; many different ways to go here.; - **object orientation, generics, database access, complex numbers,; geometric programming, ...** - Really, there is no end of crazy; features that you can add to the language.; - **unusual domains** - We've been talking about applying LLVM to a; domain that many people are interested in: building a compiler for a; specific la",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:1358,Modifiability,variab,variables,1358," semi-interesting (but probably still useless) toy. :). It is interesting to see how far we've come, and how little code it has; taken. We built the entire lexer, parser, AST, code generator, an; interactive run-loop (with a JIT!), and emitted debug information in; standalone executables - all in under 1000 lines of (non-comment/non-blank); code. Our little language supports a couple of interesting features: it; supports user defined binary and unary operators, it uses JIT; compilation for immediate evaluation, and it supports a few control flow; constructs with SSA construction. Part of the idea of this tutorial was to show you how easy and fun it; can be to define, build, and play with languages. Building a compiler; need not be a scary or mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:1385,Modifiability,variab,variables,1385," semi-interesting (but probably still useless) toy. :). It is interesting to see how far we've come, and how little code it has; taken. We built the entire lexer, parser, AST, code generator, an; interactive run-loop (with a JIT!), and emitted debug information in; standalone executables - all in under 1000 lines of (non-comment/non-blank); code. Our little language supports a couple of interesting features: it; supports user defined binary and unary operators, it uses JIT; compilation for immediate evaluation, and it supports a few control flow; constructs with SSA construction. Part of the idea of this tutorial was to show you how easy and fun it; can be to define, build, and play with languages. Building a compiler; need not be a scary or mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:1621,Modifiability,variab,variables,1621,"ive run-loop (with a JIT!), and emitted debug information in; standalone executables - all in under 1000 lines of (non-comment/non-blank); code. Our little language supports a couple of interesting features: it; supports user defined binary and unary operators, it uses JIT; compilation for immediate evaluation, and it supports a few control flow; constructs with SSA construction. Part of the idea of this tutorial was to show you how easy and fun it; can be to define, build, and play with languages. Building a compiler; need not be a scary or mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:1686,Modifiability,variab,variable,1686,"ive run-loop (with a JIT!), and emitted debug information in; standalone executables - all in under 1000 lines of (non-comment/non-blank); code. Our little language supports a couple of interesting features: it; supports user defined binary and unary operators, it uses JIT; compilation for immediate evaluation, and it supports a few control flow; constructs with SSA construction. Part of the idea of this tutorial was to show you how easy and fun it; can be to define, build, and play with languages. Building a compiler; need not be a scary or mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:1712,Modifiability,variab,variable,1712,"ive run-loop (with a JIT!), and emitted debug information in; standalone executables - all in under 1000 lines of (non-comment/non-blank); code. Our little language supports a couple of interesting features: it; supports user defined binary and unary operators, it uses JIT; compilation for immediate evaluation, and it supports a few control flow; constructs with SSA construction. Part of the idea of this tutorial was to show you how easy and fun it; can be to define, build, and play with languages. Building a compiler; need not be a scary or mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:1779,Modifiability,variab,variable,1779,"Our little language supports a couple of interesting features: it; supports user defined binary and unary operators, it uses JIT; compilation for immediate evaluation, and it supports a few control flow; constructs with SSA construction. Part of the idea of this tutorial was to show you how easy and fun it; can be to define, build, and play with languages. Building a compiler; need not be a scary or mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to ac",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:1856,Modifiability,variab,variables,1856,"binary and unary operators, it uses JIT; compilation for immediate evaluation, and it supports a few control flow; constructs with SSA construction. Part of the idea of this tutorial was to show you how easy and fun it; can be to define, build, and play with languages. Building a compiler; need not be a scary or mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard""",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:1907,Modifiability,variab,variables,1907,"binary and unary operators, it uses JIT; compilation for immediate evaluation, and it supports a few control flow; constructs with SSA construction. Part of the idea of this tutorial was to show you how easy and fun it; can be to define, build, and play with languages. Building a compiler; need not be a scary or mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard""",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:2184,Modifiability,variab,variable,2184," mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:2233,Modifiability,variab,variable,2233," mystical process! Now that you've seen some of; the basics, I strongly encourage you to take the code and hack on it.; For example, try adding:. - **global variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:2361,Modifiability,extend,extending,2361,"obal variables** - While global variables have questionable value; in modern software engineering, they are often useful when putting; together quick little hacks like the Kaleidoscope compiler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; me",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:2898,Modifiability,extend,extend,2898,"gance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make expli",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:5647,Modifiability,portab,portable,5647," regular; expression interpreter into native code with LLVM?. Have fun - try doing something crazy and unusual. Building a language; like everyone else always has, is much less fun than trying something a; little crazy or off the wall and seeing how it turns out. If you get; stuck or want to talk about it, please post on the `LLVM forums ; <https://discourse.llvm.org>`_: it has lots of people who are interested; in languages and are often willing to help out. Before we end this tutorial, I want to talk about some ""tips and tricks""; for generating LLVM IR. These are some of the more subtle things that; may not be obvious, but are very useful if you want to take advantage of; LLVM's capabilities. Properties of the LLVM IR; =========================. We have a couple of common questions about code in the LLVM IR form -; let's just get these out of the way right now, shall we?. Target Independence; -------------------. Kaleidoscope is an example of a ""portable language"": any program written; in Kaleidoscope will work the same way on any target that it runs on.; Many other languages have this property, e.g. lisp, java, haskell,; javascript, python, etc (note that while these languages are portable,; not all their libraries are). One nice aspect of LLVM is that it is often capable of preserving target; independence in the IR: you can take the LLVM IR for a; Kaleidoscope-compiled program and run it on any target that LLVM; supports, even emitting C code and compiling that on targets that LLVM; doesn't support natively. You can trivially tell that the Kaleidoscope; compiler generates target-independent code because it never queries for; any target-specific information when generating code. The fact that LLVM provides a compact, target-independent,; representation for code gets a lot of people excited. Unfortunately,; these people are usually thinking about C or a language from the C; family when they are asking questions about language portability. I say; ""unfortunately"", b",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:5888,Modifiability,portab,portable,5888,"than trying something a; little crazy or off the wall and seeing how it turns out. If you get; stuck or want to talk about it, please post on the `LLVM forums ; <https://discourse.llvm.org>`_: it has lots of people who are interested; in languages and are often willing to help out. Before we end this tutorial, I want to talk about some ""tips and tricks""; for generating LLVM IR. These are some of the more subtle things that; may not be obvious, but are very useful if you want to take advantage of; LLVM's capabilities. Properties of the LLVM IR; =========================. We have a couple of common questions about code in the LLVM IR form -; let's just get these out of the way right now, shall we?. Target Independence; -------------------. Kaleidoscope is an example of a ""portable language"": any program written; in Kaleidoscope will work the same way on any target that it runs on.; Many other languages have this property, e.g. lisp, java, haskell,; javascript, python, etc (note that while these languages are portable,; not all their libraries are). One nice aspect of LLVM is that it is often capable of preserving target; independence in the IR: you can take the LLVM IR for a; Kaleidoscope-compiled program and run it on any target that LLVM; supports, even emitting C code and compiling that on targets that LLVM; doesn't support natively. You can trivially tell that the Kaleidoscope; compiler generates target-independent code because it never queries for; any target-specific information when generating code. The fact that LLVM provides a compact, target-independent,; representation for code gets a lot of people excited. Unfortunately,; these people are usually thinking about C or a language from the C; family when they are asking questions about language portability. I say; ""unfortunately"", because there is really no way to make (fully general); C code portable, other than shipping the source code around (and of; course, C source code is not actually portable in general",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:6647,Modifiability,portab,portability,6647,"endence; -------------------. Kaleidoscope is an example of a ""portable language"": any program written; in Kaleidoscope will work the same way on any target that it runs on.; Many other languages have this property, e.g. lisp, java, haskell,; javascript, python, etc (note that while these languages are portable,; not all their libraries are). One nice aspect of LLVM is that it is often capable of preserving target; independence in the IR: you can take the LLVM IR for a; Kaleidoscope-compiled program and run it on any target that LLVM; supports, even emitting C code and compiling that on targets that LLVM; doesn't support natively. You can trivially tell that the Kaleidoscope; compiler generates target-independent code because it never queries for; any target-specific information when generating code. The fact that LLVM provides a compact, target-independent,; representation for code gets a lot of people excited. Unfortunately,; these people are usually thinking about C or a language from the C; family when they are asking questions about language portability. I say; ""unfortunately"", because there is really no way to make (fully general); C code portable, other than shipping the source code around (and of; course, C source code is not actually portable in general either - ever; port a really old application from 32- to 64-bits?). The problem with C (again, in its full generality) is that it is heavily; laden with target specific assumptions. As one simple example, the; preprocessor often destructively removes target-independence from the; code when it processes the input text:. .. code-block:: c. #ifdef __i386__; int X = 1;; #else; int X = 42;; #endif. While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed si",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:6747,Modifiability,portab,portable,6747,"ty, e.g. lisp, java, haskell,; javascript, python, etc (note that while these languages are portable,; not all their libraries are). One nice aspect of LLVM is that it is often capable of preserving target; independence in the IR: you can take the LLVM IR for a; Kaleidoscope-compiled program and run it on any target that LLVM; supports, even emitting C code and compiling that on targets that LLVM; doesn't support natively. You can trivially tell that the Kaleidoscope; compiler generates target-independent code because it never queries for; any target-specific information when generating code. The fact that LLVM provides a compact, target-independent,; representation for code gets a lot of people excited. Unfortunately,; these people are usually thinking about C or a language from the C; family when they are asking questions about language portability. I say; ""unfortunately"", because there is really no way to make (fully general); C code portable, other than shipping the source code around (and of; course, C source code is not actually portable in general either - ever; port a really old application from 32- to 64-bits?). The problem with C (again, in its full generality) is that it is heavily; laden with target specific assumptions. As one simple example, the; preprocessor often destructively removes target-independence from the; code when it processes the input text:. .. code-block:: c. #ifdef __i386__; int X = 1;; #else; int X = 42;; #endif. While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:6847,Modifiability,portab,portable,6847,"ty, e.g. lisp, java, haskell,; javascript, python, etc (note that while these languages are portable,; not all their libraries are). One nice aspect of LLVM is that it is often capable of preserving target; independence in the IR: you can take the LLVM IR for a; Kaleidoscope-compiled program and run it on any target that LLVM; supports, even emitting C code and compiling that on targets that LLVM; doesn't support natively. You can trivially tell that the Kaleidoscope; compiler generates target-independent code because it never queries for; any target-specific information when generating code. The fact that LLVM provides a compact, target-independent,; representation for code gets a lot of people excited. Unfortunately,; these people are usually thinking about C or a language from the C; family when they are asking questions about language portability. I say; ""unfortunately"", because there is really no way to make (fully general); C code portable, other than shipping the source code around (and of; course, C source code is not actually portable in general either - ever; port a really old application from 32- to 64-bits?). The problem with C (again, in its full generality) is that it is heavily; laden with target specific assumptions. As one simple example, the; preprocessor often destructively removes target-independence from the; code when it processes the input text:. .. code-block:: c. #ifdef __i386__; int X = 1;; #else; int X = 42;; #endif. While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for s",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:7518,Modifiability,portab,portable,7518," of people excited. Unfortunately,; these people are usually thinking about C or a language from the C; family when they are asking questions about language portability. I say; ""unfortunately"", because there is really no way to make (fully general); C code portable, other than shipping the source code around (and of; course, C source code is not actually portable in general either - ever; port a really old application from 32- to 64-bits?). The problem with C (again, in its full generality) is that it is heavily; laden with target specific assumptions. As one simple example, the; preprocessor often destructively removes target-independence from the; code when it processes the input text:. .. code-block:: c. #ifdef __i386__; int X = 1;; #else; int X = 42;; #endif. While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for specialized domains; such as an in-kernel language. Safety Guarantees; -----------------. Many of the languages above are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:7757,Modifiability,portab,portable,7757,"rtability. I say; ""unfortunately"", because there is really no way to make (fully general); C code portable, other than shipping the source code around (and of; course, C source code is not actually portable in general either - ever; port a really old application from 32- to 64-bits?). The problem with C (again, in its full generality) is that it is heavily; laden with target specific assumptions. As one simple example, the; preprocessor often destructively removes target-independence from the; code when it processes the input text:. .. code-block:: c. #ifdef __i386__; int X = 1;; #else; int X = 42;; #endif. While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for specialized domains; such as an in-kernel language. Safety Guarantees; -----------------. Many of the languages above are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Lan",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:9856,Modifiability,enhance,enhance,9856,"ific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple li",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10004,Modifiability,extend,extend,10004,"ion"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10111,Modifiability,extend,extended,10111,", there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code ge",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10177,Modifiability,enhance,enhancements,10177,"n ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10281,Modifiability,extend,extend,10281,"n ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific A",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:11505,Modifiability,portab,portable,11505,"mpiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if you are trying to keep the code; generated by your compiler ""target independent"", is that you often need; to know the size of some LLVM type or the offset of some field in an; llvm structure. For example, you might need to pass the size of a type; into a function that allocates memory. Unfortunately, this can vary widely across targets: for example the; width of a pointer is trivially target-specific. However, there is a; `clever way to use the getelementptr; instruction <http://nondot.org/sabre/LLVMNotes/SizeOf-OffsetOf-VariableSizedStructs.txt>`_; that allows you to compute this in a portable way. Garbage Collected Stack Frames; ------------------------------. Some languages want to explicitly manage their stack frames, often so; that they are garbage collected or to allow easy implementation of; closures. There are often better ways to implement these features than; explicit stack frames, but `LLVM does ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:12203,Modifiability,portab,portable,12203,"simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if you are trying to keep the code; generated by your compiler ""target independent"", is that you often need; to know the size of some LLVM type or the offset of some field in an; llvm structure. For example, you might need to pass the size of a type; into a function that allocates memory. Unfortunately, this can vary widely across targets: for example the; width of a pointer is trivially target-specific. However, there is a; `clever way to use the getelementptr; instruction <http://nondot.org/sabre/LLVMNotes/SizeOf-OffsetOf-VariableSizedStructs.txt>`_; that allows you to compute this in a portable way. Garbage Collected Stack Frames; ------------------------------. Some languages want to explicitly manage their stack frames, often so; that they are garbage collected or to allow easy implementation of; closures. There are often better ways to implement these features than; explicit stack frames, but `LLVM does support; them, <http://nondot.org/sabre/LLVMNotes/ExplicitlyManagedStackFrames.txt>`_; if you want. It requires your front-end to convert the code into; `Continuation Passing; Style <http://en.wikipedia.org/wiki/Continuation-passing_style>`_ and; the use of tail calls (which LLVM also supports). ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:8902,Performance,perform,performing,8902,"bove are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Language-Specific Optimizations; -------------------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In add",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:8943,Performance,optimiz,optimization,8943,"bove are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Language-Specific Optimizations; -------------------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In add",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10056,Performance,optimiz,optimization,10056,"ion"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10350,Performance,optimiz,optimizations,10350,"m is lost. The more general issue; here, is that the LLVM type system uses ""structural equivalence"" instead; of ""name equivalence"". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/wi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10477,Performance,optimiz,optimization,10477,". Another place this surprises people is if you; have two types in a high-level language that have the same structure; (e.g. two different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10595,Performance,optimiz,optimization,10595,"wo different structs that have a single int field): these types; will compile down into a single LLVM type and it will be impossible to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10729,Performance,optimiz,optimize,10729,"ble to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if you are trying to keep the code; generated by your compiler ""target independent"", is that you often need; to know the size of som",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:11157,Performance,optimiz,optimizations,11157,"ut pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if you are trying to keep the code; generated by your compiler ""target independent"", is that you often need; to know the size of some LLVM type or the offset of some field in an; llvm structure. For example, you might need to pass the size of a type; into a function that allocates memory. Unfortunately, this can vary widely across targets: for example the; width of a pointer is trivially target-specific. However, there is a; `clever way to use the getelementptr; instruction <http://nondot.org/sabre/LLVMNotes/SizeOf-OffsetOf-",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:7924,Safety,safe,safe,7924,"rality) is that it is heavily; laden with target specific assumptions. As one simple example, the; preprocessor often destructively removes target-independence from the; code when it processes the input text:. .. code-block:: c. #ifdef __i386__; int X = 1;; #else; int X = 42;; #endif. While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for specialized domains; such as an in-kernel language. Safety Guarantees; -----------------. Many of the languages above are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Language-Specific Optimizations; -------------------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; ",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:8250,Safety,safe,safe,8250,"While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for specialized domains; such as an in-kernel language. Safety Guarantees; -----------------. Many of the languages above are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Language-Specific Optimizations; -------------------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:8312,Safety,safe,safety,8312,"While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for specialized domains; such as an in-kernel language. Safety Guarantees; -----------------. Many of the languages above are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Language-Specific Optimizations; -------------------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:8339,Safety,unsafe,unsafe,8339,"full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for specialized domains; such as an in-kernel language. Safety Guarantees; -----------------. Many of the languages above are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requires a combination of language design, runtime; support, and often operating system support. It is certainly possible to implement a safe language in LLVM, but LLVM; IR does not itself guarantee safety. The LLVM IR allows unsafe pointer; casts, use after free bugs, buffer over-runs, and a variety of other; problems. Safety needs to be implemented as a layer on top of LLVM and,; conveniently, several groups have investigated this. Ask on the `LLVM; forums <https://discourse.llvm.org>`_ if you are interested in more details. Language-Specific Optimizations; -------------------------------. One thing about LLVM that turns off many people is that it does not; solve all the world's problems in one system. One specific; complaint is that people perceive LLVM as being incapable of performing; high-level language-specific optimization: LLVM ""loses too much; information"". Here are a few observations about this:. First, you're right that LLVM does lose information. For example, as of; this writing, there is no way to distinguish in the LLVM IR whether an; SSA-value came from a C ""int"" or a C ""long"" on an ILP32 machine (other; than debug info). Both get compiled down to an 'i32' value and the; information about what it came from is lost. The more gene",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10721,Safety,safe,safe,10721,"ble to; tell what it came from. Second, while LLVM does lose information, LLVM is not a fixed target: we; continue to enhance and improve it in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if you are trying to keep the code; generated by your compiler ""target independent"", is that you often need; to know the size of som",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:2797,Security,access,access,2797,"of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate wit",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:3083,Security,hash,hash,3083,"s to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make explicit use of setjmp/longjmp. There are; many different ways to go here.; - **object orientation, generics, database access, complex numbers,; geometric programming",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:3263,Security,access,access,3263," table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make explicit use of setjmp/longjmp. There are; many different ways to go here.; - **object orientation, generics, database access, complex numbers,; geometric programming, ...** - Really, there is no end of crazy; features that you can add to the language.; - **unusual domains** - We've be",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:4088,Security,access,access,4088,"bles to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage; Collection <../../GarbageCollection.html>`_ including algorithms that; move objects and need to scan/update the stack.; - **exception handling support** - LLVM supports generation of `zero; cost exceptions <../../ExceptionHandling.html>`_ which interoperate with; code compiled in other languages. You could also generate code by; implicitly making every function return an error value and checking; it. You could also make explicit use of setjmp/longjmp. There are; many different ways to go here.; - **object orientation, generics, database access, complex numbers,; geometric programming, ...** - Really, there is no end of crazy; features that you can add to the language.; - **unusual domains** - We've been talking about applying LLVM to a; domain that many people are interested in: building a compiler for a; specific language. However, there are many other domains that can use; compiler technology that are not typically considered. For example,; LLVM has been used to implement OpenGL graphics acceleration,; translate C++ code to ActionScript, and many other cute and clever; things. Maybe you will be the first to JIT compile a regular; expression interpreter into native code with LLVM?. Have fun - try doing something crazy and unusual. Building a language; like everyone else always has, is much less fun than trying something a; little crazy or off the wall and seeing how it turns out. If you get; stuck or want to talk about it, please post on the `LLVM forums ; <https://discourse.llvm.org>`_: it has lots of people who are in",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:2542,Usability,learn,learning,2542,"ler itself.; Fortunately, our current setup makes it very easy to add global; variables: just have value lookup check to see if an unresolved; variable is in the global variable symbol table before rejecting it.; To create a new global variable, make an instance of the LLVM; ``GlobalVariable`` class.; - **typed variables** - Kaleidoscope currently only supports variables; of type double. This gives the language a very nice elegance, because; only supporting one type means that you never have to specify types.; Different languages have different ways of handling this. The easiest; way is to require the user to specify types for every variable; definition, and record the type of the variable in the symbol table; along with its Value\*.; - **arrays, structs, vectors, etc** - Once you add types, you can start; extending the type system in all sorts of interesting ways. Simple; arrays are very easy and are quite useful for many different; applications. Adding them is mostly an exercise in learning how the; LLVM `getelementptr <../../LangRef.html#getelementptr-instruction>`_ instruction; works: it is so nifty/unconventional, it `has its own; FAQ <../../GetElementPtr.html>`_!; - **standard runtime** - Our current language allows the user to access; arbitrary external functions, and we use it for things like ""printd""; and ""putchard"". As you extend the language to add higher-level; constructs, often these constructs make the most sense if they are; lowered to calls into a language-supplied runtime. For example, if; you add hash tables to the language, it would probably make sense to; add the routines to a runtime, instead of inlining them all the way.; - **memory management** - Currently we can only access the stack in; Kaleidoscope. It would also be useful to be able to allocate heap; memory, either with calls to the standard libc malloc/free interface; or with a garbage collector. If you would like to use garbage; collection, note that LLVM fully supports `Accurate Garbage",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:7056,Usability,simpl,simple,7056,"t LLVM; supports, even emitting C code and compiling that on targets that LLVM; doesn't support natively. You can trivially tell that the Kaleidoscope; compiler generates target-independent code because it never queries for; any target-specific information when generating code. The fact that LLVM provides a compact, target-independent,; representation for code gets a lot of people excited. Unfortunately,; these people are usually thinking about C or a language from the C; family when they are asking questions about language portability. I say; ""unfortunately"", because there is really no way to make (fully general); C code portable, other than shipping the source code around (and of; course, C source code is not actually portable in general either - ever; port a really old application from 32- to 64-bits?). The problem with C (again, in its full generality) is that it is heavily; laden with target specific assumptions. As one simple example, the; preprocessor often destructively removes target-independence from the; code when it processes the input text:. .. code-block:: c. #ifdef __i386__; int X = 1;; #else; int X = 42;; #endif. While it is possible to engineer more and more complex solutions to; problems like this, it cannot be solved in full generality in a way that; is better than shipping the actual source code. That said, there are interesting subsets of C that can be made portable.; If you are willing to fix primitive types to a fixed size (say int =; 32-bits, and long = 64-bits), don't care about ABI compatibility with; existing binaries, and are willing to give up some other minor features,; you can have portable code. This can make sense for specialized domains; such as an in-kernel language. Safety Guarantees; -----------------. Many of the languages above are also ""safe"" languages: it is impossible; for a program written in Java to corrupt its address space and crash the; process (assuming the JVM has no bugs). Safety is an interesting; property that requi",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst:10827,Usability,simpl,simple,10827,"t in many different ways. In addition; to adding new features (LLVM did not always support exceptions or debug; info), we also extend the IR to capture important information for; optimization (e.g. whether an argument is sign or zero extended,; information about pointers aliasing, etc). Many of the enhancements are; user-driven: people want LLVM to include some specific feature, so they; go ahead and extend it. Third, it is *possible and easy* to add language-specific optimizations,; and you have a number of choices in how to do it. As one trivial; example, it is easy to add language-specific optimization passes that; ""know"" things about code compiled for a language. In the case of the C; family, there is an optimization pass that ""knows"" about the standard C; library functions. If you call ""exit(0)"" in main(), it knows that it is; safe to optimize that into ""return 0;"" because C specifies what the; 'exit' function does. In addition to simple library knowledge, it is possible to embed a; variety of other language-specific information into the LLVM IR. If you; have a specific need and run into a wall, please bring the topic up on; the llvm-dev list. At the very worst, you can always treat LLVM as if it; were a ""dumb code generator"" and implement the high-level optimizations; you desire in your front-end, on the language-specific AST. Tips and Tricks; ===============. There is a variety of useful tips and tricks that you come to know after; working on/with LLVM that aren't obvious at first glance. Instead of; letting everyone rediscover them, this section talks about some of these; issues. Implementing portable offsetof/sizeof; -------------------------------------. One interesting thing that comes up, if you are trying to keep the code; generated by your compiler ""target independent"", is that you often need; to know the size of some LLVM type or the offset of some field in an; llvm structure. For example, you might need to pass the size of a type; into a function that",MatchSource.DOCS,interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/docs/tutorial/MyFirstLanguageFrontend/LangImpl10.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:240,Availability,failure,failures,240,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:580,Availability,avail,available,580,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:692,Availability,avail,available,692,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1042,Deployability,patch,patches,1042,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1727,Deployability,install,installed,1727,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1780,Deployability,install,install,1780,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:324,Integrability,interface,interface,324,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1246,Integrability,depend,depend,1246,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1613,Integrability,wrap,wrapper,1613,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:122,Modifiability,portab,portable,122,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:171,Testability,test,test,171,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:289,Testability,test,testing,289,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:392,Testability,test,test,392,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:420,Testability,test,test,420,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:460,Testability,test,test,460,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:477,Testability,test,test,477,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1059,Testability,test,test,1059,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1217,Testability,test,tests,1217,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1240,Testability,test,tests,1240,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1361,Testability,test,test,1361,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1447,Testability,test,testing,1447,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1551,Testability,test,test,1551,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1686,Testability,test,tests,1686,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst:1839,Testability,test,tests,1839,"===============================; lit - A Software Testing Tool; ===============================. About; =====. *lit* is a portable tool for executing LLVM and Clang style test suites,; summarizing their results, and providing indication of failures. *lit* is; designed to be a lightweight testing tool with as simple a user interface as; possible. Features; ========. * Portable!; * Flexible test discovery.; * Parallel test execution.; * Support for multiple test formats and test suite designs. Documentation; =============. The official *lit* documentation is in the man page, available online at the LLVM; Command Guide: http://llvm.org/cmds/lit.html. Source; ======. The *lit* source is available as part of LLVM, in the LLVM source repository:; https://github.com/llvm/llvm-project/tree/main/llvm/utils/lit. Contributing to lit; ===================. Please browse the issues labeled *tools:llvm-lit* in LLVM's issue tracker for; ideas on what to work on:; https://github.com/llvm/llvm-project/labels/tools%3Allvm-lit. Before submitting patches, run the test suite to ensure nothing has regressed::. # From within your LLVM source directory.; utils/lit/lit.py \; --path /path/to/your/llvm/build/bin \; utils/lit/tests. Note that lit's tests depend on ``not`` and ``FileCheck``, LLVM utilities.; You will need to have built LLVM tools in order to run lit's test suite; successfully. You'll also want to confirm that lit continues to work when testing LLVM.; Follow the instructions in http://llvm.org/docs/TestingGuide.html to run the; regression test suite:. make check-llvm. And be sure to run the llvm-lit wrapper script as well:. /path/to/your/llvm/build/bin/llvm-lit utils/lit/tests. Finally, make sure lit works when installed via setuptools:. python utils/lit/setup.py install; lit --path /path/to/your/llvm/build/bin utils/lit/tests. ",MatchSource.DOCS,interpreter/llvm-project/llvm/utils/lit/README.rst,root-project,root,v6-32-06,https://root.cern,https://github.com/root-project/root/tree/v6-32-06/interpreter/llvm-project/llvm/utils/lit/README.rst
