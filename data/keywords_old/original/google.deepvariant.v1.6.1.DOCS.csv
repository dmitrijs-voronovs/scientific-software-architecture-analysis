id,quality_attribute,keyword,matched_word,match_idx,sentence,source,filename,author,repo,version,wiki,url
https://github.com/google/deepvariant/tree/v1.6.1/CONTRIBUTING.md:504,Deployability,release,release,504,"# How to Contribute. We cannot merge external pull requests into the DeepVariant repository at this; time. The source of truth for DeepVariant lives in an internal Google codebase,; and changes must first be made internally. However, we still welcome community contributions! Please feel free to fork the; DeepVariant repository and open a pull request or issue with suggested edits. To; incorporate your contributions, we will make the changes internally and then; push them to GitHub in the subsequent release. We will attribute the changes to; you in the commit description and release notes.; ",MatchSource.DOCS,CONTRIBUTING.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/CONTRIBUTING.md
https://github.com/google/deepvariant/tree/v1.6.1/CONTRIBUTING.md:581,Deployability,release,release,581,"# How to Contribute. We cannot merge external pull requests into the DeepVariant repository at this; time. The source of truth for DeepVariant lives in an internal Google codebase,; and changes must first be made internally. However, we still welcome community contributions! Please feel free to fork the; DeepVariant repository and open a pull request or issue with suggested edits. To; incorporate your contributions, we will make the changes internally and then; push them to GitHub in the subsequent release. We will attribute the changes to; you in the commit description and release notes.; ",MatchSource.DOCS,CONTRIBUTING.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/CONTRIBUTING.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:4463,Availability,avail,available,4463,"iant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:4807,Availability,down,download,4807," free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:5306,Availability,avail,available,5306,"ed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:9921,Availability,error,error-correction,9921,"s. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:61,Deployability,release,release,61,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:99,Deployability,release,release-,99,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:172,Deployability,release,releases,172,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:7768,Deployability,pipeline,pipelines-on-noisy-wgs-data,7768,"sionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for the latest accuracy numbers on each of the; sequencing types.; * **Flexibility** - Out-of-the-box use for; [PCR-positive](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html); samples and; [low quality sequencing runs](https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/),; and easy adjustments for; [different sequencing technologies](https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/); and; [non-human species](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).; * **Ease of use** - No filtering is needed beyond setting your preferred; minimum quality threshold.; * **Cost effectiveness** - With a single non-preemptible n1-standard-16; machine on Google Cloud, it costs ~$11.8 to call a 30x whole genome and; ~$0.89 to call an exome. With preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:9480,Deployability,integrat,integration,9480,"h preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from sou",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:11281,Deployability,release,release,11281,"------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucl",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:11334,Deployability,release,releases,11334,"----; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucleus); * [numpy](http://www.numpy.org/); * [SSW Library](https://github.com/mengyao/",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:114,Energy Efficiency,green,green,114,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:2491,Energy Efficiency,power,power,2491,"rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:9480,Integrability,integrat,integration,9480,"h preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from sou",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:2426,Modifiability,extend,extends,2426,"rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:6428,Performance,scalab,scalable,6428,"h more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally, if you are generating multi-sample calls using our; [DeepVariant and GLnexus Best Practices](docs/trio-merge-case-study.md), please; cite:. [Accurate, scalable cohort variant calls using DeepVariant and GLnexus.; _Bioinformatics_ (2021).](https://doi.org/10.1093/bioinformatics/btaa1081)<br/>; Taedong Yun, Helen Li, Pi-Chuan Chang, Michael F. Lin, Andrew Carroll, and Cory; Y. McLean.<br/>; doi: https://doi.org/10.1093/bioinformatics/btaa1081. ## Why Use DeepVariant?. * **High accuracy** - DeepVariant won 2020; [PrecisionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:7749,Performance,perform,performance-of-ngs-pipelines-on-noisy-wgs-data,7749,"sionFDA Truth Challenge V2](https://precision.fda.gov/challenges/10/results); for All Benchmark Regions for ONT, PacBio, and Multiple Technologies; categories, and 2016; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results); for best SNP Performance. DeepVariant maintains high accuracy across data; from different sequencing technologies, prep methods, and species. For; [lower coverage](https://google.github.io/deepvariant/posts/2019-09-10-twenty-is-the-new-thirty-comparing-current-and-historical-wgs-accuracy-across-coverage/),; using DeepVariant makes an especially great difference. See; [metrics](docs/metrics.md) for the latest accuracy numbers on each of the; sequencing types.; * **Flexibility** - Out-of-the-box use for; [PCR-positive](https://ai.googleblog.com/2018/04/deepvariant-accuracy-improvements-for.html); samples and; [low quality sequencing runs](https://blog.dnanexus.com/2018-01-16-evaluating-the-performance-of-ngs-pipelines-on-noisy-wgs-data/),; and easy adjustments for; [different sequencing technologies](https://google.github.io/deepvariant/posts/2019-01-14-highly-accurate-snp-and-indel-calling-on-pacbio-ccs-with-deepvariant/); and; [non-human species](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/).; * **Ease of use** - No filtering is needed beyond setting your preferred; minimum quality threshold.; * **Cost effectiveness** - With a single non-preemptible n1-standard-16; machine on Google Cloud, it costs ~$11.8 to call a 30x whole genome and; ~$0.89 to call an exome. With preemptible pricing, the cost is $2.84 for a; 30x whole genome and $0.21 for whole exome (not considering preemption).; * **Speed** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:2519,Safety,predict,predict,2519,"rshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mo",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:120,Testability,log,logo,120,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:3869,Testability,log,logs,3869,".; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](ht",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:3903,Testability,log,log,3903,"dy](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=/input/YOUR_REF \; --reads=/input/YOUR_BAM \; --output_vcf=/output/YOUR_OUTPUT_VCF \; --output_gvcf=/output/YOUR_OUTPUT_GVCF \; --num_shards=$(nproc) \ **This will use all your cores to run make_examples. Feel free to change.**; --logging_dir=/output/logs \ **Optional. This saves the log output for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-stu",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:4917,Testability,test,testdata,4917," for each stage separately.; --haploid_contigs=""chrX,chrY"" \ **Optional. Heterozygous variants in these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:5015,Testability,test,testdata,5015,"these contigs will be re-genotyped as the most likely of reference or homozygous alternates. For a sample with karyotype XY, it should be set to ""chrX,chrY"" for GRCh38 and ""X,Y"" for GRCh37. For a sample with karyotype XX, this should not be used.; --par_regions_bed=""/input/GRCh3X_par.bed"" \ **Optional. If --haploid_contigs is set, then this can be used to provide PAR regions to be excluded from genotype adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, P",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:10453,Testability,test,test,10453,"mats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel ",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:11201,Testability,test,test,11201,"------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-based systems, you will need to modify these scripts.; Prebuilt Binaries | Available at [`gs://deepvariant/`](https://console.cloud.google.com/storage/browser/deepvariant). These are compiled to use SSE4 and AVX instructions, so you will need a CPU (such as Intel Sandy Bridge) that supports them. You can check the `/proc/cpuinfo` file on your computer, which lists these features under ""flags"". ## Contribution Guidelines. Please [open a pull request](https://github.com/google/deepvariant/compare) if; you wish to contribute to DeepVariant. Note, we have not set up the; infrastructure to merge pull requests externally. If you agree, we will test and; submit the changes internally and mention your contributions in our; [release notes](https://github.com/google/deepvariant/releases). We apologize; for any inconvenience. If you have any difficulty using DeepVariant, feel free to; [open an issue](https://github.com/google/deepvariant/issues/new). If you have; general questions not specific to DeepVariant, we recommend that you post on a; community discussion forum such as [BioStars](https://www.biostars.org/). ## License. [BSD-3-Clause license](LICENSE). ## Acknowledgements. DeepVariant happily makes use of many open source packages. We would like to; specifically call out a few key ones:. * [Boost Graph Library](http://www.boost.org/doc/libs/1_65_1/libs/graph/doc/index.html); * [abseil-cpp](https://github.com/abseil/abseil-cpp) and; [abseil-py](https://github.com/abseil/abseil-py); * [CLIF](https://github.com/google/clif); * [GNU Parallel](https://www.gnu.org/software/parallel/); * [htslib & samtools](http://www.htslib.org/); * [Nucleus](https://github.com/google/nucl",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:416,Usability,learn,learning-based,416,"<img src=""docs/images/dv_logo.png"" width=50% height=50%>. [![release](https://img.shields.io/badge/release-v1.6.1-green?logo=github)](https://github.com/google/deepvariant/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The mod",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:1185,Usability,simpl,simplex-case-study,1185,"t/releases); [![announcements](https://img.shields.io/badge/announcements-blue)](https://groups.google.com/d/forum/deepvariant-announcements); [![blog](https://img.shields.io/badge/blog-orange)](https://goo.gl/deepvariant). DeepVariant is a deep learning-based variant caller that takes aligned reads (in; BAM or CRAM format), produces pileup image tensors from them, classifies each; tensor using a convolutional neural network, and finally reports the results in; a standard VCF or gVCF file. DeepVariant supports germline variant-calling in diploid organisms. * NGS (Illumina or Element) data for either a; [whole genome](docs/deepvariant-case-study.md) or; [whole exome](docs/deepvariant-exome-case-study.md).; * [RNA-seq Case Study](docs/deepvariant-rnaseq-case-study.md) for Illumina; RNA-seq.; * PacBio HiFi data, see the; [PacBio case study](docs/deepvariant-pacbio-model-case-study.md).; * Oxford Nanopore R10.4.1 Simplex or Duplex data, see the; [ONT R10.4.1 Simplex case study](docs/deepvariant-ont-r104-simplex-case-study.md); and; [ONT R10.4.1 Duplex case study](docs/deepvariant-ont-r104-duplex-case-study.md).; * Hybrid PacBio HiFi + Illumina WGS, see the; [hybrid case study](docs/deepvariant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:2352,Usability,learn,learning-based,2352,"iant-hybrid-case-study.md).; * Oxford Nanopore R9.4.1 data by using; [PEPPER-DeepVariant](https://github.com/kishwarshafin/pepper).; * To map using a pangenome to improve accuracy, use this; [vg case study](docs/deepvariant-vg-case-study.md).; * Complete Genomics data:; [T7 case study](docs/deepvariant-complete-t7-case-study.md);; [G400 case study](docs/deepvariant-complete-g400-case-study.md). Please also note:. * For somatic data or any other samples where the genotypes go beyond two; copies of DNA, DeepVariant will not work out of the box because the only; genotypes supported are hom-alt, het, and hom-ref.; * The models included with DeepVariant are only trained on human data. For; other organisms, see the; [blog post on non-human variant-calling](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/); for some possible pitfalls and how to handle them. ## DeepTrio. DeepTrio is a deep learning-based trio variant caller built on top of; DeepVariant. DeepTrio extends DeepVariant's functionality, allowing it to; utilize the power of neural networks to predict genomic variants in trios or; duos. See [this page](docs/deeptrio-details.md) for more details and; instructions on how to run DeepTrio. DeepTrio supports germline variant-calling in diploid organisms for the; following types of input data:. * NGS (Illumina) data for either; [whole genome](docs/deeptrio-wgs-case-study.md) or whole exome.; * PacBio HiFi data, see the; [PacBio case study](docs/deeptrio-pacbio-case-study.md). Please also note:. * All DeepTrio models were trained on human data.; * It is possible to use DeepTrio with only 2 samples (child, and one parent).; * External tool [GLnexus](https://github.com/dnanexus-rnd/GLnexus) is used to; merge output VCFs. ## How to run DeepVariant. We recommend using our Docker solution. The command will look like this:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; ",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:5413,Usability,guid,guide,5413," adjustment. Download links to this files are available in this page.; --dry_run=false **Default is false. If set to true, commands will be printed out but not executed.; ```. For details on X,Y support, please see; [DeepVariant haploid support](docs/deepvariant-haploid-support.md) and the case; study in; [DeepVariant X, Y case study](docs/deepvariant-xy-calling-case-study.md). You; can download the PAR bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). To see all flags you can use, run: `docker run; google/deepvariant:""${BIN_VERSION}""`. If you're using GPUs, or want to use Singularity instead, see; [Quick Start](docs/deepvariant-quick-start.md) for more details or see all the; [setup options](#deepvariant_setup) available. For more information, also see:. * [Full documentation list](docs/README.md); * [Detailed usage guide](docs/deepvariant-details.md) with more information on; the input and output file formats and how to work with them.; * [Best practices for multi-sample variant calling with DeepVariant](docs/trio-merge-case-study.md); * [(Advanced) Training tutorial](docs/deepvariant-training-case-study.md); * [DeepVariant's Frequently Asked Questions, FAQ](docs/FAQ.md). ## How to cite. If you're using DeepVariant in your work, please cite:. [A universal SNP and small-indel variant caller using deep neural networks. *Nature Biotechnology* 36, 983–987 (2018).](https://rdcu.be/7Dhl) <br/>; Ryan Poplin, Pi-Chuan Chang, David Alexander, Scott Schwartz, Thomas Colthurst, Alexander Ku, Dan Newburger, Jojo Dijamco, Nam Nguyen, Pegah T. Afshar, Sam S. Gross, Lizzie Dorfman, Cory Y. McLean, and Mark A. DePristo.<br/>; doi: https://doi.org/10.1038/nbt.4235. Additionally, if you are generating multi-sample calls using our; [DeepVariant and GLnexus Best Practices](docs/trio-merge-case-study.md), please; cite:. [",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/README.md:9552,Usability,learn,learning,9552,"d** - See [metrics](docs/metrics.md) for the runtime of all supported; datatypes on a 64-core CPU-only machine</sup>. Multiple options for; acceleration exist.; * **Usage options** - DeepVariant can be run via Docker or binaries, using; both on-premise hardware or in the cloud, with support for hardware; accelerators like GPUs and TPUs. <a name=""myfootnote1"">(1)</a>: Time estimates do not include mapping. ## How DeepVariant works. ![Stages in DeepVariant](docs/images/inference_flow_diagram.svg). For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). DeepVariant relies on [Nucleus](https://github.com/google/nucleus), a library of; Python and C++ code for reading and writing data in common genomics file formats; (like SAM and VCF) designed for painless integration with the; [TensorFlow](https://www.tensorflow.org/) machine learning framework. Nucleus; was built with DeepVariant in mind and open-sourced separately so it can be used; by anyone in the genomics research community for other projects. See this blog; post on; [Using Nucleus and TensorFlow for DNA Sequencing Error Correction](https://google.github.io/deepvariant/posts/2019-01-31-using-nucleus-and-tensorflow-for-dna-sequencing-error-correction/). ## DeepVariant Setup. ### Prerequisites. * Unix-like operating system (cannot run on Windows); * Python 3.8. ### Official Solutions. Below are the official solutions provided by the; [Genomics team in Google Health](https://health.google/health-research/). Name | Description; :-------------------------------------------------------------------------------------------------: | -----------; [Docker](docs/deepvariant-quick-start.md) | This is the recommended method.; [Build from source](docs/deepvariant-build-test.md) | DeepVariant comes with scripts to build it on Ubuntu 20.04. To build and run on other Unix-",MatchSource.DOCS,README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/README.md
https://github.com/google/deepvariant/tree/v1.6.1/deeptrio/README.md:56,Deployability,release,released,56,## DeepTrio is under development. Documentation will be released in the next release. We don't provide any support; to DeepTrio codebase right now.; ,MatchSource.DOCS,deeptrio/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/deeptrio/README.md
https://github.com/google/deepvariant/tree/v1.6.1/deeptrio/README.md:77,Deployability,release,release,77,## DeepTrio is under development. Documentation will be released in the next release. We don't provide any support; to DeepTrio codebase right now.; ,MatchSource.DOCS,deeptrio/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/deeptrio/README.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-build-test.md:226,Testability,test,test,226,"## Building DeepTrio from sources. DeepTrio is a part of DeepVariant. DeepVariant comes with scripts to build it; from the source code. For more details please refer to; [""Building DeepVariant from sources""](deepvariant-build-test.md); ",MatchSource.DOCS,docs/deeptrio-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details-training-data.md:4462,Deployability,update,updated,4462,",893,180<sup>[(4)](#vfootnote4)</sup> |; | 1.3.0 | 2 HG005/HG006/HG007 trio <br>10 HG002/HG003/HG004 trios | 539,382,124<sup>[(5)](#vfootnote5)</sup> |; | 1.4.0 | (Same model as 1.3.0) | |; | 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |; | Parent model | | |; | 1.1.0 | 1 HG005/HG006/HG007 trio <br> 8 HG002/HG003/HG004 trios | 386,418,918 |; | 1.2.0 | 1 HG005/HG006/HG007 trio <br>8 HG002/HG003/HG004 trios | 392,749,204<sup>[(4)](#vfootnote4)</sup> |; | 1.3.0 | 2 HG005/HG006/HG007 trio <br>10 HG002/HG003/HG004 trios | 533,353,050<sup>[(5)](#vfootnote5)</sup> |; | 1.4.0 | (Same model as 1.3.0) | |; | 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ### ONT models<sup>[(2)](#vfootnote2)</sup><sup>[(3)](#vfootnote3)</sup>; | version | Replicates | #examples |; | ------------ | ---------------------------------- | ----------- |; | 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; | Parent model | | |; | 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; <a name=""vfootnote1"">(1)</a>: We include HG002/HG003/HG004 for training WGS; model, but only using examples from the region of NIST truth confident region; v4.2 subtracting v3.3.2. <a name=""vfootnote2"">(2)</a>: We use the entire HG002/HG003/HG004 trio for; PacBio model training. <a name=""vfootnote3"">(3)</a>: PacBio and ONT training data contains training; examples with haplotag sorted images. <a name=""vfootnote4"">(4)</a>: In v1.2.0, we updated the NIST truth versions we; used for training. <a name=""vfootnote5"">(5)</a>: In v1.3.0, we included PacBio Sequel II Chemistry; v2.2 data in the training dataset. And we updated to NIST truth version to; v4.2.1. <a name=""vfootnote6"">(6)</a>: Starting in v1.5.0, for clarity, we report the; number of unique BAM files used. Note that this doesn't mean all the trios were; paired together to produce training data.; ",MatchSource.DOCS,docs/deeptrio-details-training-data.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details-training-data.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details-training-data.md:4640,Deployability,update,updated,4640,",893,180<sup>[(4)](#vfootnote4)</sup> |; | 1.3.0 | 2 HG005/HG006/HG007 trio <br>10 HG002/HG003/HG004 trios | 539,382,124<sup>[(5)](#vfootnote5)</sup> |; | 1.4.0 | (Same model as 1.3.0) | |; | 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 890,016,014<sup>[(5)](#vfootnote5)</sup> |; | Parent model | | |; | 1.1.0 | 1 HG005/HG006/HG007 trio <br> 8 HG002/HG003/HG004 trios | 386,418,918 |; | 1.2.0 | 1 HG005/HG006/HG007 trio <br>8 HG002/HG003/HG004 trios | 392,749,204<sup>[(4)](#vfootnote4)</sup> |; | 1.3.0 | 2 HG005/HG006/HG007 trio <br>10 HG002/HG003/HG004 trios | 533,353,050<sup>[(5)](#vfootnote5)</sup> |; | 1.4.0 | (Same model as 1.3.0) | |; | 1.6.0 | 9 HG002, 5 HG003, 5 HG004, 1 HG005, 1 HG006, 1 HG007 | 838,515,085<sup>[(5)](#vfootnote5)</sup> |. ### ONT models<sup>[(2)](#vfootnote2)</sup><sup>[(3)](#vfootnote3)</sup>; | version | Replicates | #examples |; | ------------ | ---------------------------------- | ----------- |; | 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 50,249,704<sup>[(5)](#vfootnote5)</sup> |; | Parent model | | |; | 1.6.0 | 1 HG002, 1 HG002, 1 HG004 | 99,675,190<sup>[(5)](#vfootnote5)</sup> |; <a name=""vfootnote1"">(1)</a>: We include HG002/HG003/HG004 for training WGS; model, but only using examples from the region of NIST truth confident region; v4.2 subtracting v3.3.2. <a name=""vfootnote2"">(2)</a>: We use the entire HG002/HG003/HG004 trio for; PacBio model training. <a name=""vfootnote3"">(3)</a>: PacBio and ONT training data contains training; examples with haplotag sorted images. <a name=""vfootnote4"">(4)</a>: In v1.2.0, we updated the NIST truth versions we; used for training. <a name=""vfootnote5"">(5)</a>: In v1.3.0, we included PacBio Sequel II Chemistry; v2.2 data in the training dataset. And we updated to NIST truth version to; v4.2.1. <a name=""vfootnote6"">(6)</a>: Starting in v1.5.0, for clarity, we report the; number of unique BAM files used. Note that this doesn't mean all the trios were; paired together to produce training data.; ",MatchSource.DOCS,docs/deeptrio-details-training-data.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details-training-data.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:2779,Deployability,pipeline,pipeline,2779,"GS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot p",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:5022,Deployability,pipeline,pipeline,5022,"contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; ",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:4763,Integrability,depend,dependency,4763,"contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; ",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:167,Modifiability,inherit,inheritance,167,"# DeepTrio. ## Overview. DeepTrio is built on top of DeepVariant. It is intended for variant calling of; trios or duos. The main advantage of DeepTrio is that genetic inheritance is; considered by a neural network for calling variants in trio samples. Also,; variant candidates are generated from all samples at once, which ensures a; genotype call is made for any position in the trio with a variant. Since; DeepTrio is built on top of DeepVariant,; [general information](deepvariant-details.md) for DeepVariant also applies to; DeepTrio. At the highest level, a user needs to provide the following:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads files for child and one or two parents in; [BAM](http://genome.sph.umich.edu/wiki/BAM) format and its corresponding; index file (.bai). The reads must be aligned to the reference genome; described above. The output of DeepTrio is a set of variants in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format representing the; child and one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](dee",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:2176,Performance,optimiz,optimized,2176,"one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples)",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:2509,Performance,scalab,scalable,2509,"n the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" v",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:3790,Performance,perform,performed,3790,"peline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTri",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:3969,Performance,perform,perform,3969,"omosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant c",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:3295,Testability,test,test,3295,"se; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--r",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:4583,Testability,benchmark,benchmarks,4583,"contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; ",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:1991,Usability,guid,guide,1991,"aligned to the reference genome; described above. The output of DeepTrio is a set of variants in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format representing the; child and one or two parents. Similar to DeepVariant, DeepTrio is composed of three stages: `make_examples`,; `call_variants`, and `postprocess_variants`. Some of the components (; `call_variants`, `postprocess_variants`) are shared with DeepVariant, and; `make_examples` is specialized for DeepTrio. More details about each program are; described in detail in the; [Inputs and outputs](deepvariant-details.md#inputs-and-outputs) section of the; DeepVariant documentation. DeepTrio comes with three models for different types of input data:. * Illumina whole genome data (WGS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:2754,Usability,simpl,simplifies,2754,"GS).; * Illumina whole exome data (WES).; * PacBio HiFi whole genome data (PacBio WGS). ## Running DeepTrio. The easiest and recommended way to run DeepTrio is using; `google/deepvariant:deeptrio-latest` docker image. Please refer to the; [quick start guide](deeptrio-quick-start.md) for more details on how to run; DeepTrio using docker. Merging VCFs can be done using; [GLnexus](https://github.com/dnanexus-rnd/GLnexus) which has been optimized for; use with DeepVariant gVCFs. The process is described in the DeepTrio case; studies; ([DeepTrio whole genome sequencing case study](deeptrio-wgs-case-study.md) and; [Using DeepTrio for small variant calling from the trio sequenced with PacBio; HiFi](deeptrio-pacbio-case-study.md)), and in the manuscript,; [""Accurate, scalable cohort variant calls using DeepVariant and GLnexus""](https://www.biorxiv.org/content/10.1101/2020.02.10.942086v2). Please note that DeepTrio can be run with a `run_deeptrio.py` script that; automates all DeepTrio steps and thus greatly simplifies the inference pipeline.; The details of using this script can be found in the section below as well as in; the DeepTrio case studies. Also please note: for the non-PAR regions of the sex chromosomes (X and Y), we; recommend running these providing only the parent who contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot p",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md:4885,Usability,guid,guide,4885,"contributed the child's; chromosome (e.g. for chromosomeX, only the mother and son samples and for; chromosomeY only the father and son samples). If needed, DeepTrio can be built from source. For more details please refer to; [Building DeeepTrio](deeptrio-build-test.md). ## DeepTrio Input assumptions. The reference genome FASTA, passed in using the `--ref` flag, must be indexed; and can either be uncompressed or compressed with `bgzip`. All BAM files should be aligned to a ""compatible"" version of the genome; reference provided as the `--ref`. DeepTrio will only process contigs shared by; both the BAM and reference. BAM files must be also sorted and indexed. They must; exist on disk, so you cannot pipe them into DeepTrio. Duplicate marking may be; performed. In our analyses, there is almost no difference in accuracy with and; without duplicate marking except at lower (<20x) coverages. Finally, we; recommend that you do not perform BQSR. Running BQSR has a small decrease on; accuracy. If you are providing `--regions` or other similar arguments, these should refer; to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. ## Training data. DeepTrio models are trained using the latest publicly avavilable GIAB; benchmarks. You can find more details about the training data for each DeepTrio; model in the; [DeepTrio Training Data document](deeptrio-details-training-data.md). ## DeepVariant dependency. DeepTrio is built on top of DeepVariant and they share most of the components.; Please see [DeepVariant usage guide](deepvariant-details.md) for a full; description of DeepVariant components as well as other consideration for running; DeepVariant pipeline.; ",MatchSource.DOCS,docs/deeptrio-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2877,Availability,avail,available,2877,"noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfd",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1293,Deployability,release,release,1293,"lation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:3975,Deployability,pipeline,pipeline,3975,"_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus; is run as a separate command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent1 /input/HG003.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent2 /input/HG004.pfda_challenge.grch38.phased.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:4354,Deployability,update,update,4354,".phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus; is run as a separate command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent1 /input/HG003.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent2 /input/HG004.pfda_challenge.grch38.phased.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir \; --output_gvcf_child /output/HG002.g.vcf.gz \; --output_gvcf_par",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:4378,Deployability,install,install,4378,".phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus; is run as a separate command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type PACBIO \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent1 /input/HG003.pfda_challenge.grch38.phased.chr20.bam \; --reads_parent2 /input/HG004.pfda_challenge.grch38.phased.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output.vcf.gz \; --output_vcf_parent2 /output/HG004.output.vcf.gz \; --sample_name_child 'HG002' \; --sample_name_parent1 'HG003' \; --sample_name_parent2 'HG004' \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir \; --output_gvcf_child /output/HG002.g.vcf.gz \; --output_gvcf_par",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:6918,Deployability,install,install,6918,"tputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md) or; [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; sudo docker pull quay.io/mlin/glnexus:v1.2.7. # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate Mendelian Violation rate. ```bash; sudo docker pull realtimegenomics/rtg-tools. sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/i",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:7058,Modifiability,config,config,7058,"ry:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md) or; [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; sudo docker pull quay.io/mlin/glnexus:v1.2.7. # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate Mendelian Violation rate. ```bash; sudo docker pull realtimegenomics/rtg-tools. sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id in",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1086,Testability,benchmark,benchmark,1086,"dy, we describe applying [DeepTrio](deeptrio-details.md) to a; real PacBio WGS trio. Then we assess the quality of the DeepTrio variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1170,Testability,benchmark,benchmarks,1170,"acBio WGS trio. Then we assess the quality of the DeepTrio variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh3",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1233,Testability,benchmark,benchmark,1233," variant calls; with `hap.py`. In addition we evaluate a Mendelian violation rate for a merged; VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmar",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1424,Testability,benchmark,benchmark,1424,"ols. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noin",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1584,Testability,benchmark,benchmark,1584,"ll be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.v",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1736,Testability,benchmark,benchmark,1736,"h38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmar",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:1903,Testability,benchmark,benchmark,1903,"l ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2066,Testability,benchmark,benchmark,2066,"e will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/P",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2221,Testability,benchmark,benchmark,2221,"benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfd",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2388,Testability,benchmark,benchmark,2388,"rk_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG00",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2551,Testability,benchmark,benchmark,2551,"_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; cu",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:2706,Testability,benchmark,benchmark,2706,"mark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challen",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:3199,Testability,test,testdata,3199,"1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 PacBio HiFi WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10).; These reads have been aligned to the GRCh38_no_alt_analysis reference using; [pbmm2](https://github.com/PacificBiosciences/pbmm2). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/pacbio-case-study-testdata. curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam > input/HG002.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG002.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam > input/HG003.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG003.pfda_challenge.grch38.phased.chr20.bam.bai. curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam > input/HG004.pfda_challenge.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG004.pfda_challenge.grch38.phased.chr20.bam.bai > input/HG004.pfda_challenge.grch38.phased.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run the first; three steps with one command using the `run_deeptrio` script. GLnexus;",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9440,Testability,benchmark,benchmark,9440,"iant.input_rtg_output.txt; ```. As a result we should get the following output:. ```bash; Checking: /output/HG002_trio_merged.vcf.gz; Family: [HG003 + HG004] -> [HG002]; 222 non-pass records were skipped; Concordance HG002: F:166005/169476 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/i",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9453,Testability,benchmark,benchmark,9453,"iant.input_rtg_output.txt; ```. As a result we should get the following output:. ```bash; Checking: /output/HG002_trio_merged.vcf.gz; Family: [HG003 + HG004] -> [HG002]; 222 non-pass records were skipped; Concordance HG002: F:166005/169476 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/i",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9647,Testability,benchmark,benchmark,9647," HG004] -> [HG002]; 222 non-pass records were skipped; Concordance HG002: F:166005/169476 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9737,Testability,benchmark,benchmark,9737,"6 (97.95%) M:166074/168579 (98.51%) F+M:159317/164363 (96.93%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/188247 (0.00%) records did not conform to expected call ploidy; 176481/188247 (93.75%) records were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.ou",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9955,Testability,benchmark,benchmark,9955," were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TR",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:9968,Testability,benchmark,benchmark,9968," were variant in at least 1 family member and checked for Mendelian constraints; 10169/176481 (5.76%) records had indeterminate consistency status due to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TR",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10162,Testability,benchmark,benchmark,10162," to incomplete calls; 6610/176481 (3.75%) records contained a violation of Mendelian constraints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10252,Testability,benchmark,benchmark,10252,"straints; ```. ### Benchmark variant calls against 4.2.1 truth set with hap.py. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 112",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10470,Testability,benchmark,benchmark,10470,"/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10483,Testability,benchmark,benchmark,10483,"/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10677,Testability,benchmark,benchmark,10677,"rk.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270; SNP PASS 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md:10767,Testability,benchmark,benchmark,10767,"mark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; INDEL PASS 11256 11215 41 23348 85 11580 30 50 0.996357 0.992777 0.495974 0.994564 NaN NaN 1.561710 2.133416; SNP ALL 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270; SNP PASS 71333 71303 30 108157 20 36757 16 4 0.999579 0.999720 0.339849 0.999650 2.314904 1.745105 1.715978 1.773270. Benchmarking Summary for HG003:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY",MatchSource.DOCS,docs/deeptrio-pacbio-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-pacbio-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:429,Availability,down,download,429,"# DeepTrio quick start. This document explains how to quickly start using; [DeepTrio](deeptrio-details.md) to generate variant calls for trio samples. This; tutorial does not cover all possible settings of DeepTrio. It is intended to be; a starting point for using DeepTrio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test b",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1932,Availability,down,downloaded,1932,"e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1357,Deployability,update,update,1357,"; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1381,Deployability,install,install,1381,"; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:2130,Deployability,release,release,2130," that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vc",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:8094,Deployability,install,install,8094,"vcf.gz; HG002.output.vcf.gz.tbi; HG002.output.visual_report.html; HG003.g.vcf.gz; HG003.g.vcf.gz.tbi; HG003.output.vcf.gz; HG003.output.vcf.gz.tbi; HG003.output.visual_report.html; HG004.g.vcf.gz; HG004.g.vcf.gz.tbi; HG004.output.vcf.gz; HG004.output.vcf.gz.tbi; HG004.output.visual_report.html; intermediate_results_dir; ```. The directory ""intermediate_results_dir"" exists because; `--intermediate_results_dir /output/intermediate_results_dir` is specified. This; directory contains the intermediate output of make_examples and call_variants; steps. For more information about the `HG00*.output.visual_report.html` files, see the; [VCF stats report documentation](deepvariant-vcf-stats-report.md). ## Notes on GPU image. If you are using GPUs, you can pull the GPU version, and make sure you run with; `--gpus 1`. `call_variants` is the only step that uses the GPU, and can only use; one at a time. `make_examples` and `postprocess_variants` do not run on GPU. For an example to install GPU driver and docker, see [install_nvidia_docker.sh]. ```; sudo docker run --gpus 1 \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:deeptrio-""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; ...; ```. ## Notes on Singularity. ### CPU version. ```; # Pull the image.; singularity pull docker://google/deepvariant:deeptrio-""${BIN_VERSION}"". # Run DeepTrio.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type=WGS \; --ref=""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta \; --reads_child=""${INPUT_DIR}""/HG002.chr20.10_10p1mb.bam \; --reads_parent1=""${INPUT_DIR}""/HG003.chr20.10_10p1mb.bam \; --reads_parent2=""${INPUT_DIR}""/HG004.chr20.10_10p1mb.bam \; --output_vcf_child ""${OUTPUT_DIR}""/HG002.output.vcf.gz \; --output_vcf_parent1 ""${OUTPUT_DIR}""/HG003.output.vcf.gz \; --output_vcf_parent2 ""${OUTPUT_DIR}""/HG004.output.vcf.gz \; --sample_",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:346,Testability,test,test,346,"# DeepTrio quick start. This document explains how to quickly start using; [DeepTrio](deeptrio-details.md) to generate variant calls for trio samples. This; tutorial does not cover all possible settings of DeepTrio. It is intended to be; a starting point for using DeepTrio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test b",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1282,Testability,test,test,1282,"rio. ## Background. To get started, we've provided a Docker image, and some test data in a bucket on; Google Cloud Storage. The instructions below show how to download the data; through the corresponding public URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1481,Testability,test,test,1481,"c URLs. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepTrio in one command. Although DeepTrio can be built from a source, we provide a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1868,Testability,test,test,1868,"e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1995,Testability,test,test,1995,"VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:2047,Testability,test,testdata,2047,"e reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRC",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:3707,Testability,test,testdata,3707,"/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. HTTPDIR=https://storage.googleapis.com/deepvariant/quickstart-testdata. wget -P ${INPUT_DIR} ""${HTTPDIR}""/HG002.chr20.10_10p1mb.bam; curl ""${HTTPDIR}/HG002.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG002.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG002.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG002.chr20.10_10p1mb.bam.bai"". curl ""${HTTPDIR}/HG003.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG003.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG003.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG003.chr20.10_10p1mb.bam.bai"". curl ""${HTTPDIR}/HG004.chr20.10_10p1mb.bam"" > ""${INPUT_DIR}/HG004.chr20.10_10p1mb.bam""; curl ""${HTTPDIR}/HG004.chr20.10_10p1mb.bam.bai"" > ""${INPUT_DIR}/HG004.chr20.10_10p1mb.bam.bai"". FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ""${INPUT_DIR}""/GRCh38_no_alt_analysis_set.fasta.",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md:1913,Usability,guid,guide,1913,"e a docker image that; allows to run through all steps in one command to generate VCF/gVCF output files; from input BAM files and the reference. If you want to compile the binaries for yourself, we also have a [Dockerfile]; that you can use to build your own Docker image. You can read the [docker build]; documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}""; ```. ### Download test data. Before you start, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. For each sample, one aligned reads file in [BAM] format and its; corresponding index file (.bai). You get this by aligning the reads from a; sequencing instrument, using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; mkdir -p ${INPUT_DIR}. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ""${INPUT_DIR}""/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ""${INPUT_DIR}""/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark",MatchSource.DOCS,docs/deeptrio-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2805,Availability,avail,available,2805,"k_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1224,Deployability,release,release,1224,"olation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:3908,Deployability,pipeline,pipeline,3908,"pis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postprocess_variants` and `GLnexus merge`. It is possible to run DeepTrio with; one command using the `run_deepvariant` script. GLnexus is run as a separate; command. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:deeptrio-""${BIN_VERSION}"". time sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:deeptrio-""${BIN_VERSION}"" \; /opt/deepvariant/bin/deeptrio/run_deeptrio \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads_child /input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --reads_parent1 /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --reads_parent2 /input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf_child /output/HG002.output.vcf.gz \; --output_vcf_parent1 /output/HG003.output",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:6691,Deployability,install,install,6691,"needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate mendelian violation rate. ```bash; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male;",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:6831,Modifiability,config,config,6831,"les` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output_child.tfrecord.gz; call_variants_output_parent1.tfrecord.gz; call_variants_output_parent2.tfrecord.gz. gvcf_child.tfrecord-?????-of-?????.gz; gvcf_parent1.tfrecord-?????-of-?????.gz; gvcf_parent2.tfrecord-?????-of-?????.gz. make_examples_child.tfrecord-?????-of-?????.gz; make_examples_parent1.tfrecord-?????-of-?????.gz; make_examples_parent2.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Merge VCFs using GLnexus. At this step we take all 3 VCFs generated in the previous step and merge them; using GLnexus. ```bash; # bcftools and bgzip are now included in our docker images.; # You can also install them separately.; sudo docker run \; -v ""${PWD}/output"":""/output"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariant_unfiltered \; /output/HG002.g.vcf.gz \; /output/HG003.g.vcf.gz \; /output/HG004.g.vcf.gz \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bcftools view - \; | sudo docker run -i google/deepvariant:deeptrio-""${BIN_VERSION}"" \; bgzip -c > output/HG002_trio_merged.vcf.gz; ```. After completion of GLnexus command we should have a new merged VCF file in the; output directory. ```; HG002_trio_merged.vcf.gz; ```. ## Benchmark on chr20. ### Calculate mendelian violation rate. ```bash; sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/reference"":""/reference"" \; realtimegenomics/rtg-tools format \; -o /reference/GRCh38_no_alt_analysis_set.sdf ""/reference/GRCh38_no_alt_analysis_set.fasta"". FILE=""reference/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex phen; 1 HG002 HG003 HG",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1017,Testability,benchmark,benchmark,1017,"ng case study. In this case study, we describe applying DeepTrio to a real WGS trio. Then we; assess the quality of the DeepTrio variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1101,Testability,benchmark,benchmarks,1101," real WGS trio. Then we; assess the quality of the DeepTrio variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh3",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1164,Testability,benchmark,benchmark,1164,"o variant calls with `hap.py`. In addition we; evaluate a mendelian violation rate for a merged VCF. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmar",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1355,Testability,benchmark,benchmark,1355,"ols. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepTrio and; [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noin",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1515,Testability,benchmark,benchmark,1515,"ll be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.v",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1667,Testability,benchmark,benchmark,1667,"h38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmar",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1834,Testability,benchmark,benchmark,1834,"l ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:1997,Testability,benchmark,benchmark,1997,"e will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002, HG003, and HG004 trio. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPD",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2152,Testability,benchmark,benchmark,2152,"benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio. curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novase",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2319,Testability,benchmark,benchmark,2319,"rk_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2482,Testability,benchmark,benchmark,2482,"_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_NA24385_son/NISTv4.2.1/GRCh38/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2637,Testability,benchmark,benchmark,2637,"mark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:2988,Testability,test,testdata,2988,"enchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_NA24149_father/NISTv4.2.1/GRCh38/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG004_NA24143_mother/NISTv4.2.1/GRCh38/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG002, HG003, and HG004 BAM files. We'll use HG002, HG003, HG004 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG002.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai. curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG004.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepTrio with one command. DeepTrio pipeline consists of 4 steps: `make_examples`, `call_variants`,; `postp",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9159,Testability,benchmark,benchmark,9159,"t/deepvariant.input_rtg_output.txt; ```. As a result we should get the following output:. ```bash; Checking: /output/HG002_trio_merged.vcf.gz; Family: [HG003 + HG004] -> [HG002]; 95 non-pass records were skipped; Concordance HG002: F:137908/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/i",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9172,Testability,benchmark,benchmark,9172,"t/deepvariant.input_rtg_output.txt; ```. As a result we should get the following output:. ```bash; Checking: /output/HG002_trio_merged.vcf.gz; Family: [HG003 + HG004] -> [HG002]; 95 non-pass records were skipped; Concordance HG002: F:137908/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/i",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9366,Testability,benchmark,benchmark,9366," [HG003 + HG004] -> [HG002]; 95 non-pass records were skipped; Concordance HG002: F:137908/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9456,Testability,benchmark,benchmark,9456,"08/139703 (98.72%) M:137988/139909 (98.63%) F+M:134596/137968 (97.56%); Sample HG002 has less than 99.0 concordance with both parents. Check for incorrect pedigree or sample mislabelling.; 0/146013 (0.00%) records did not conform to expected call ploidy; 143704/146013 (98.42%) records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.ou",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9674,Testability,benchmark,benchmark,9674," records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TR",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9687,Testability,benchmark,benchmark,9687," records were variant in at least 1 family member and checked for Mendelian constraints; 5066/143704 (3.53%) records had indeterminate consistency status due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TR",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9881,Testability,benchmark,benchmark,9881,"tus due to incomplete calls; 3886/143704 (2.70%) records contained a violation of Mendelian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:9971,Testability,benchmark,benchmark,9971,"ian constraints; ```. ### Perform analysis with hap.py against 4.2.1 truth set. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 112",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:10189,Testability,benchmark,benchmark,10189,"/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.3",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:10202,Testability,benchmark,benchmark,10202,"/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.3",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:10396,Testability,benchmark,benchmark,10396,"rk.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560; SNP PASS 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560.",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md:10486,Testability,benchmark,benchmark,10486,"mark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG002.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG003.output \; --engine=vcfeval \; --pass-only \; -l chr20. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG004.output.vcf.gz \; -f /benchmark/HG004_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/HG004.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. ```; Benchmarking Summary for HG002:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; INDEL PASS 11256 11208 48 21239 13 9586 7 4 0.995736 0.998884 0.451340 0.997308 NaN NaN 1.561710 2.047281; SNP ALL 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560; SNP PASS 71333 71087 246 88976 42 17795 5 4 0.996551 0.999410 0.199998 0.997979 2.314904 2.029984 1.715978 1.716560. Benchmarking Summary for HG003:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL Q",MatchSource.DOCS,docs/deeptrio-wgs-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deeptrio-wgs-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:390,Availability,down,downloads,390,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:660,Availability,down,downloads,660,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:330,Deployability,install,install,330,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:521,Deployability,install,install,521,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:774,Deployability,install,install,774,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:1390,Deployability,install,installed,1390,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:466,Integrability,depend,dependencies,466,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:977,Integrability,message,message,977,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:1052,Integrability,message,message,1052,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:1319,Integrability,depend,dependencies,1319,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:1555,Modifiability,config,configuring,1555,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:224,Security,access,access,224,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:843,Testability,test,tests,843,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:1014,Testability,test,tests,1014,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md:1024,Testability,test,tests,1024,"## Building DeepVariant from sources. DeepVariant comes with scripts to build it on Ubuntu 20.04. It can likely be; built and run on other unix-based systems with some minimal modifications to; these scripts. One way to get access to a machine running Ubuntu is through a; cloud computing platform like Google Cloud Engine. First install the [Google Cloud SDK](https://cloud.google.com/sdk/downloads),; because we will need to use its `gsutil` command to fetch some dependencies. The `build-prereq.sh` command below will install a number of system packages to; fulfill DeepVariant's prerequisites (using apt-get and pip, invoked via sudo).; This commands also downloads and builds TensorFlow and CLIF from source. First run `sudo su`, and then run the following commands to install; prerequisites, build the DeepVariant programs, and then run tests. ```shell; ./build-prereq.sh. ./build_and_test.sh; ```. At the end of the output of that last command, you should see a summary message; like ""Executed 55 out of 55 tests: 55 tests pass."" along with the message; ""Target //deepvariant:binaries up-to-date:"" followed by a list of the just-built; deepvariant binaries. ## Preparing a machine to run DeepVariant. The following command should be run on any machine on which you wish run; DeepVariant, since there are runtime dependencies, such as Python packages like; numpy and Tensorflow to be installed:. ```shell; ./run-prereq.sh; ```. ## Configuring the build. Advanced users may want to edit the settings.sh file before building. It; contains options for configuring TensorFlow, CUDA, GPU usage, etc.; ",MatchSource.DOCS,docs/deepvariant-build-test.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-build-test.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1655,Availability,avail,available,1655," curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvarian",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1145,Deployability,release,release,1145,"Variant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant ",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:2194,Deployability,pipeline,pipeline,2194,"38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type WGS`, you'll be using a model that is best suited; for Illumina Whole Genome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=tr",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:961,Testability,benchmark,benchmark,961,"# DeepVariant whole genome sequencing case study. In this case study, we describe applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPD",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1045,Testability,benchmark,benchmarks,1045,"ing case study. In this case study, we describe applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.de",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1085,Testability,benchmark,benchmark,1085,"e applying DeepVariant to a real WGS sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.n",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1279,Testability,benchmark,benchmark,1279,"vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You c",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1403,Testability,benchmark,benchmark,1403,"hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkd",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1519,Testability,benchmark,benchmark,1519,"eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:1840,Testability,test,testdata,1840,"t_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use HG003 Illumina WGS reads publicly available from the; [PrecisionFDA Truth v2 Challenge](https://doi.org/10.1101/2020.11.13.380741). ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam; curl ${HTTPDIR}/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai > input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20.bam.bai; ```. ## Running DeepVariant with one command. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can now run DeepVariant with one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.pcr-free.35x.dedup.grch38_no_alt.chr20",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:3991,Testability,benchmark,benchmark,3991,"el_type WGS`, you'll be using a model that is best suited; for Illumina Whole Genome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:4004,Testability,benchmark,benchmark,4004,"el_type WGS`, you'll be using a model that is best suited; for Illumina Whole Genome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:4198,Testability,benchmark,benchmark,4198,"ately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; SNP PASS 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.92",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md:4288,Testability,benchmark,benchmark,4288,"`--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; INDEL PASS 10628 10588 40 21099 19 10036 15 3 0.996236 0.998283 0.475662 0.997258 NaN NaN 1.748961 2.318182; SNP ALL 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; SNP PASS 70166 69917 249 84796 59 14782 13 3 0.996451 0.999157 0.174324 0.997802 2.296566 2.085786 1.883951 1.920577; ```; ",MatchSource.DOCS,docs/deepvariant-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:934,Deployability,release,release,934,"# DeepVariant Complete Genomics G400 case study. In this case study, we describe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/referenc",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:538,Testability,test,testdata,538,"# DeepVariant Complete Genomics G400 case study. In this case study, we describe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/referenc",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:874,Testability,benchmark,benchmark,874,"# DeepVariant Complete Genomics G400 case study. In this case study, we describe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/referenc",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:1065,Testability,benchmark,benchmark,1065,"ibe applying DeepVariant to a Complete Genomics G400; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvar",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:1189,Testability,benchmark,benchmark,1189,"th `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V3501517",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:1305,Testability,benchmark,benchmark,1305,"nment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics G400 HG002 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:1489,Testability,test,testdata,1489,"https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam > input/HG002.complete_g400.V350151728.grch38.chr20.bam. curl ${HTTPDIR}/HG002.complete_g400.V350151728.grch38.chr20.bam.bai > input/HG002.complete_g400.V350151728.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG002. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics G400 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.data-00000-of-00001 > input/weights-60-0.993753.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-g400/weights-60-0.993753.ckpt.index > input/weights-60-0.993753.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; `",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:2718,Testability,benchmark,benchmark,2718,"y machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:2731,Testability,benchmark,benchmark,2731,"y machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:2925,Testability,benchmark,benchmark,2925,"-v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; SNP PASS 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.75",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md:3015,Testability,benchmark,benchmark,3015,":""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG002.complete_g400.V350151728.grch38.chr20.bam \; --output_vcf /output/HG002.output.vcf.gz \; --output_gvcf /output/HG002.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-60-0.993753.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG002.output.vcf.gz \; -f /benchmark/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; INDEL PASS 11256 11130 126 20925 31 9340 26 4 0.988806 0.997324 0.446356 0.993047 NaN NaN 1.561710 2.049106; SNP ALL 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; SNP PASS 71333 70949 384 85736 50 14689 28 6 0.994617 0.999296 0.171328 0.996951 2.314904 2.102286 1.715978 1.753768; ```. To summarize:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRI",MatchSource.DOCS,docs/deepvariant-complete-g400-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-g400-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:940,Deployability,release,release,940,"# DeepVariant Complete Genomics T7 case study. In this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:532,Testability,test,testdata,532,"# DeepVariant Complete Genomics T7 case study. In this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:880,Testability,benchmark,benchmark,880,"# DeepVariant Complete Genomics T7 case study. In this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:1037,Testability,benchmark,benchmark,1037," this case study, we describe applying DeepVariant to a Complete Genomics T7; sample.; Then we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:1146,Testability,benchmark,benchmark,1146,"ity of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:1262,Testability,benchmark,benchmark,1262,"hromosome 20. For how to prepare environment, the steps are the same as; [this doc](deepvariant-case-study.md). ## Download Complete Genomics T7 HG001 chr20 BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:1444,Testability,test,testdata,1444," BAM. ```bash; mkdir -p input. HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam. curl ${HTTPDIR}/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai > input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam.bai; ```. ## Download Genome in a Bottle Benchmarks for HG001. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/NA12878_HG001/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ## Download Complete Genomics T7 model. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/complete-case-study-testdata. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.data-00000-of-00001 > input/weights-51-0.995354.ckpt.data-00000-of-00001. curl ${HTTPDIR}/complete-t7/weights-51-0.995354.ckpt.index > input/weights-51-0.995354.ckpt.index; ```. ## Running DeepVariant with one command. On a CPU-only machine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ``",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:2672,Testability,benchmark,benchmark,2672,"achine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:2685,Testability,benchmark,benchmark,2685,"achine:. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:2879,Testability,benchmark,benchmark,2879,"""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; SNP PASS 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; ```. To summarize:.",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md:2969,Testability,benchmark,benchmark,2969,"{BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WGS \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG001.complete_t7.E100030471QC960.grch38.chr20.bam \; --output_vcf /output/HG001.output.vcf.gz \; --output_gvcf /output/HG001.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir \; --customized_model /input/weights-51-0.995354.ckpt; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG001.output.vcf.gz \; -f /benchmark/HG001_GRCh38_1_22_v4.2.1_benchmark.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; INDEL PASS 9974 9947 27 21052 9 10750 3 5 0.997293 0.999126 0.510640 0.998209 NaN NaN 1.630447 2.156149; SNP ALL 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; SNP PASS 69175 68874 301 85030 44 16068 8 2 0.995649 0.999362 0.188969 0.997502 2.288757 2.084645 1.730097 1.781789; ```. To summarize:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_S",MatchSource.DOCS,docs/deepvariant-complete-t7-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-complete-t7-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:6717,Availability,avail,available,6717,"ootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.2. <a name=""vfootnote11"">(11)</a>: Between v1.1 and v1.2, we fixed an issue where; make_examples can generate fewer class 0 (REF) training examples than before.; This is the reason for more training examples in v1.2 when number of samples; didn't increase. <a name=""vfootnote12"">(12)</a>: In v1.2, we created BAM files with 100bp reads; and 125bp reads by trimming to augment the training data. ## Training data:. See ""[An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022)""; for a publicly available set of data we released. Data download information can; be found in the supplementary material.; ",MatchSource.DOCS,docs/deepvariant-details-training-data.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:6757,Availability,down,download,6757,"ootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.2. <a name=""vfootnote11"">(11)</a>: Between v1.1 and v1.2, we fixed an issue where; make_examples can generate fewer class 0 (REF) training examples than before.; This is the reason for more training examples in v1.2 when number of samples; didn't increase. <a name=""vfootnote12"">(12)</a>: In v1.2, we created BAM files with 100bp reads; and 125bp reads by trimming to augment the training data. ## Training data:. See ""[An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022)""; for a publicly available set of data we released. Data download information can; be found in the supplementary material.; ",MatchSource.DOCS,docs/deepvariant-details-training-data.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:4691,Deployability,update,update,4691,"-------- | ------------------------------; v1.6 | 3 HG001<br>1 HG004<br>1 HG005 | 534,302,654. ### HYBRID models. version | Replicates | #examples; ------- | -------------------------------------------------------- | -----------; v1.0 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 193,076,623; v1.1 | Same model as v1.0 |; v1.2 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 214,302,681; v1.3 | Same model as v1.2 |; v1.4 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2",MatchSource.DOCS,docs/deepvariant-details-training-data.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:5119,Deployability,release,release,5119,"G002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.",MatchSource.DOCS,docs/deepvariant-details-training-data.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:6742,Deployability,release,released,6742,"ootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vfootnote10"">(10)</a>: In v1.2, we include new PacBio training data; from Sequel II, Chemistry 2.2. <a name=""vfootnote11"">(11)</a>: Between v1.1 and v1.2, we fixed an issue where; make_examples can generate fewer class 0 (REF) training examples than before.; This is the reason for more training examples in v1.2 when number of samples; didn't increase. <a name=""vfootnote12"">(12)</a>: In v1.2, we created BAM files with 100bp reads; and 125bp reads by trimming to augment the training data. ## Training data:. See ""[An Extensive Sequence Dataset of Gold-Standard Samples for Benchmarking and Development](https://doi.org/10.1101/2020.12.11.422022)""; for a publicly available set of data we released. Data download information can; be found in the supplementary material.; ",MatchSource.DOCS,docs/deepvariant-details-training-data.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md:5065,Performance,tune,tune,5065,"04<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 214,302,681; v1.3 | Same model as v1.2 |; v1.4 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,645; v1.5 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,863,664; v1.6 | 10 HG002<br> 1 HG004<br> 1 HG005<br> 1 HG006<br> 1 HG007 | 215,353,081. <a name=""vfootnote1"">(1)</a>: In v0.5, we experimented with adding whole exome; sequencing data into training data. In v0.6, we took it out because it didn't; improve the WGS accuracy. <a name=""vfootnote2"">(2)</a>: The training data are from the same replicates as; v0.5. The number of examples changed because of the update in; [haplotype_labeler](https://github.com/google/deepvariant/tree/r0.6/deepvariant/labeler/haplotype_labeler.py). <a name=""vfootnote3"">(3)</a>: In v0.8, we used the; [Platinum Genomes Truthset](https://github.com/Illumina/PlatinumGenomes) to; create more training examples outside the GIAB confident regions. <a name=""vfootnote4"">(4)</a>: Previously, we split train/tune by leaving 3 WES; for tuning. Starting from this release, we leave out chr1 and chr20 from; training, and use chr1 for tuning. <a name=""vfootnote5"">(5)</a>: Starting from this version, we padded (100bps on; both sides) of the capture BED and used that for generating training examples.; We also added more `downsample_fraction`. <a name=""vfootnote6"">(6)</a>: (Before v1.0) PacBio is the only one we currently; uses HG002 in training and tuning. <a name=""vfootnote7"">(7)</a>: In v1.0, we train on HG002-HG004 for WGS as well,; but only using examples from the region of NIST truth confident region v4.2; subtracting v3.3.2. <a name=""vfootnote8"">(8)</a>: In v1.0, PacBio training data contains training; examples with haplotag sorted images and unsorted images. <a name=""vfootnote9"">(9)</a>: In v1.1, we exclude HG003 from training. And we; use all NIST truth confident regions for HG001-HG007 (except for HG003) for; training. We've always excluded chr20-22 from training. <a name=""vf",MatchSource.DOCS,docs/deepvariant-details-training-data.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details-training-data.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:610,Availability,checkpoint,checkpoint,610,"f# DeepVariant usage guide. ## Overview. DeepVariant is a set of programs used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Co",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:4883,Availability,checkpoint,checkpoint,4883," or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. ",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:7871,Availability,down,downsampled,7871,"; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study]",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:8038,Availability,error,error,8038,"ange was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant t",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:12099,Availability,mainten,maintenance-policy,12099,"u can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. NOTE: Having an instance up and running could cost you. Remember to delete the; instances you're not using. You can find the instances at:; https://console.cloud.google.com/compute/instances?project=YOUR_PROJECT. [exome case study]: deepvariant-exome-case-study.md; [whole genome case study]: deepvariant-case-study.md; [quick start]: deepvariant-quick-start.md; [Running DeepVariant on Google Cloud Platform]: https://cloud.google.com/life-sciences/docs/tutorials/deepvariant; [TensorFlow]: http://www.tensorflow.org/; [Colab example]: visualizing_examples.ipynb; ",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:5595,Deployability,configurat,configuration,5595,"ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:6874,Deployability,release,release,6874,"a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/dual core machine with sufficient memory. ## Updates on DeepVariant since precisionFDA truth challenge and bioRxiv preprint. The DeepVariant team has been hard at work since we first presented the method.; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our te",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:8812,Deployability,update,update,8812,"he ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:8829,Deployability,release,release,8829,"he ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:9013,Deployability,release,released,9013,"he held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 3",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:11360,Deployability,configurat,configuration,11360,"sampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-p",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:5491,Energy Efficiency,efficient,efficient,5491,"nd regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_varia",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:5584,Energy Efficiency,efficient,efficient,5584,"ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:6998,Energy Efficiency,reduce,reduced,6998,"a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/dual core machine with sufficient memory. ## Updates on DeepVariant since precisionFDA truth challenge and bioRxiv preprint. The DeepVariant team has been hard at work since we first presented the method.; Key changes and improvements include:. * Rearchitected with open source release in mind; * Built on [TensorFlow]; * Increased variant calling accuracy, especially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our te",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:8026,Energy Efficiency,reduce,reduced,8026,"ange was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant t",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:5595,Modifiability,config,configuration,5595,"ered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. So it doesn't improve; the speed if you get a multiple-GPU machine. ### postprocess_variants. `postprocess_variants` reads all of the output TFRecord files from; `call_variants`, sorts them, combines multi-allelic records, and writes out a; VCF file. When [gVCF output](deepvariant-gvcf-support.md) is requested, it also; outputs a gVCF file which merges the VCF with the non-variant sites. Because `postprocess_variants` combines and sorts the output of `call_variants`,; it needs to see all of the outputs from `call_variants` for a single sample to; merge into a final VCF. `postprocess_variants` is single-threaded and needs a; non-trivial amount of memory to run (20-30 GB), so it is best run on a; single/d",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:11360,Modifiability,config,configuration,11360,"sampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-64"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-platform ""Intel Skylake""; ```. ### Command for a GPU machine on Google Cloud Platform. ```shell; gcloud compute instances create ""${USER}-gpu"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""us-west1-b"" \; --min-cpu-p",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:3513,Performance,perform,performed,3513,"res its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. Duplicate marking may be performed, in our; analyses there is almost no difference in accuracy except at lower (<20x); coverages. Finally, we recommend that you do not perform BQSR. Running BQSR has; a small decrease on accuracy. It is not necessary to do any form of indel; realignment, though there is not a difference in DeepVariant accuracy either; way. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:3656,Performance,perform,perform,3656,"the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; version of the genome reference provided as the `--ref`. By compatible here we; mean the BAM and FASTA share at least a reasonable set of common contigs, as; DeepVariant will only process contigs shared by both the BAM and reference. As; an example, suppose you have a BAM file mapped to b37 + decoy FASTA and you; provide just the vanilla b37 fasta to `make_examples`. DeepVariant will only; process variants on the shared contigs, effectively excluding the hs37d5 contig; present in the BAM but not in the reference. The BAM file must be also sorted and indexed. It must exist on disk, so you; cannot pipe it into DeepVariant. Duplicate marking may be performed, in our; analyses there is almost no difference in accuracy except at lower (<20x); coverages. Finally, we recommend that you do not perform BQSR. Running BQSR has; a small decrease on accuracy. It is not necessary to do any form of indel; realignment, though there is not a difference in DeepVariant accuracy either; way. Third, if you are providing `--regions` or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzippe",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:9960,Performance,perform,performance,9960,"o see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10306,Performance,perform,performance-testdata,10306,"ified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studie",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10398,Performance,perform,performance-testdata,10398,"e; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only mach",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:1055,Testability,log,logical,1055,"ams used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM.",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:7928,Testability,test,testing,7928,"ially for indels; * Vastly faster with reduced memory usage. We have made a number of improvements to the methodology as well. The biggest; change was to move away from RGB-encoded (3-channel) pileup images and instead; represent the aligned read data using a multi-channel tensor data layout. We; currently represent the data as a 6-channel raw tensor in which we encode:. * The read base (A, C, G, T); * The base's quality score; * The read's mapping quality score; * The read's strand (positive or negative); * Does the read support the allele being evaluated?; * Does the base match the reference genome at this position?. These are all readily derived from the information found in the BAM file; encoding of each read. Additional modeling changes were to move to the inception-v3 architecture and to; train on many more independent sequencing replicates of the ground truth; training samples, including 50% downsampled versions of each of those read sets.; In our testing this allowed the model to better generalize to other data types. In the end these changes reduced our error rate by more than 50% on the held out; evaluation sample (NA24385 / HG002) as compared to our results in the; [PrecisionFDA Truth Challenge](https://precision.fda.gov/challenges/truth/results/):. DeepVariant April 2016 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 4175 | 2839 | 0.987882 | 0.991728 | 0.989802; SNP | 1689 | 832 | 0.999447 | 0.999728 | 0.999587. DeepVariant December 2017 (HG002, GIAB v3.2.2, b37):. Type | # FN | # FP | Recall | Precision | F1_Score; ----- | ---- | ---- | -------- | --------- | --------; INDEL | 2384 | 1811 | 0.993081 | 0.994954 | 0.994017; SNP | 735 | 363 | 0.999759 | 0.999881 | 0.999820. See the [whole genome case study], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; imag",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:9803,Testability,benchmark,benchmarking,9803,"], which we update with each release of; DeepVariant, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bas",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:9844,Testability,benchmark,benchmarks,9844,"nt, for the latest results. You can also see the [Colab example] to see how you can visualize the pileup; images. ## Training data over time. For the models we've released over time, you can find more details about the; training data in; [DeepVariant training data](deepvariant-details-training-data.md). ## CRAM support. As of v0.7, DeepVariant accepts CRAM files as input in addition to BAM files. As of v0.9.0, we changed the default to use the reference file specified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}""",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10318,Testability,test,testdata,10318,"ified by the; `--ref` flag, instead of the path to the original reference in the CRAM file; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studie",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10410,Testability,test,testdata,10410,"e; (encoded in the file's ""UR"" tag). For more information about CRAM, see the; [`samtools` documentation](http://www.htslib.org/doc/samtools.html) in general; but particularly the sections on; [Global Options](http://www.htslib.org/doc/samtools.html#GLOBAL_OPTIONS) and; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only mach",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:21,Usability,guid,guide,21,"f# DeepVariant usage guide. ## Overview. DeepVariant is a set of programs used to transform aligned sequencing reads into; variant calls. At the highest level, a user needs to provide three inputs:. 1. A reference genome in [FASTA](https://en.wikipedia.org/wiki/FASTA_format); format and its corresponding; [.fai index file](http://www.htslib.org/doc/faidx.html) generated using the; `samtools faidx` command. 1. An aligned reads file in [BAM](http://genome.sph.umich.edu/wiki/BAM) format; and its corresponding index file (.bai). The reads must be aligned to the; reference genome described above. 1. A model checkpoint for DeepVariant. The output of DeepVariant is a list of all variant calls in; [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Co",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:1777,Usability,learn,learning,1777,"ub.io/hts-specs/VCFv4.3.pdf) format. DeepVariant is composed of three programs: `make_examples`, `call_variants`, and; `postprocess_variants`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM. Since the; process of generating examples is embarrassingly parallel across the genome,; `make_examples` supports sharding of its input and output via the `--task`; argument with a sharded output specification. For example, if the output is; specified as `--examples examples.tfrecord@10.gz` and `--task 0`, the input to; the program will be 10% of the regions and the output will be written to; `examples.tfrecord-00000-of-00010.gz`. #### Input assumptions. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and c",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:1856,Usability,learn,learn,1856,"ts`. More details about each program are described in detail; in the [Inputs and outputs](#inputs-and-outputs) section. ## Inputs and outputs. ### General notes. * Sharded files are a single logical collection of files with a common naming; convention. For example, we talk about `filename@10` as a single 10-way; sharded file named `filename`. On most filesystems this actually looks like; 10 distinct files `filename-00000-of-00010`, ..., `filename-00009-of-00010`.; DeepVariant can write sharded files using their `filename@10`-style name and; can read sharded files using both that style as well as the glob form, such; as `filename-*` or `filename-*-of-00010`.; * Files with the `.gz` suffix are interpreted as being compressed with gzip; and are read/written accordingly. ### make_examples. `make_examples` consumes reads and the reference genome to create TensorFlow; examples for evaluation with our deep learning models. The tf.Example protos are; written out in TFRecord format. To learn more about tf.Example and TFRecord, see; the; [Using TFRecords and tf.Example](https://www.tensorflow.org/tutorials/load_data/tfrecord); Colab. `make_examples` is a single-threaded program using 1-2 GB of RAM. Since the; process of generating examples is embarrassingly parallel across the genome,; `make_examples` supports sharding of its input and output via the `--task`; argument with a sharded output specification. For example, if the output is; specified as `--examples examples.tfrecord@10.gz` and `--task 0`, the input to; the program will be 10% of the regions and the output will be written to; `examples.tfrecord-00000-of-00010.gz`. #### Input assumptions. `make_examples` requires its input files to satisfy a few basic requirements to; be processed correctly. First, the reference genome FASTA, passed in using the `--ref` flag, must be; indexed and can either be uncompressed or compressed with bgzip. Second, the BAM file provided to `--reads` should be aligned to a ""compatible""; versi",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:4868,Usability,learn,learning,4868," or other similar arguments these should; refer to contigs present in the reference genome. These arguments accept; space-separated lists, so all of the follow examples are valid arguments for; `--regions` or similar arguments:. * `--regions chr20` => only process all of chromosome 20; * `--regions chr20:10,000,000-11,000,000` => only process 10-11mb of chr20; * `--regions ""chr20 chr21""` => only process chromosomes 20 and 21. Fourth and finally, if running in training mode the `truth_vcf` and; `confident_regions` arguments should point to VCF and BED files containing the; true variants and regions where we are confident in our calls (i.e., calls; within these regions and not in the truth_vcf are considered false positives).; These should be bgzipped and tabix indexed and be on a reference consistent with; the one provided with the `--ref` argument. ### call_variants. `call_variants` consumes TFRecord file(s) of tf.Examples protos created; by `make_examples` and a deep learning model checkpoint and evaluates the model; on each example in the input TFRecord. The output here is a TFRecord of; CallVariantsOutput protos. `call_variants` doesn't directly support sharding its; outputs, but accepts a glob or shard-pattern for its inputs. `call_variants` uses around 4 GB per process and uses TensorFlow for evaluation.; When evaluating a model in CPU mode, TensorFlow can make use of multiple cores,; but scaling is sub-linear. In other words, `call_variants` on a 64 core machine; is less than 8x faster than running on a 8 core machine. When using a GPU, `call_variants` is both faster, more efficient, and needs; fewer CPUs. Based on a small number of experiments, currently the most efficient; configuration for `call_variants` on a GPU instance is 4-8 CPUs and 1 GPU.; Compared to our setting in the [whole genome case study], we noticed a 2.5x; speedup on the call_variants step using a single P100 GPU and 8 CPUs. Note that; currently `call_variants` can only use one GPU at most. ",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md:10659,Usability,feedback,feedback,10659,"d; [reference sequences in CRAM](http://www.htslib.org/doc/samtools.html#REFERENCE_SEQUENCES). `htslib` also hosts a nice page; [benchmarking CRAM](http://www.htslib.org/benchmarks/CRAM.html) with information; on the effect of different CRAM options on file size and encoding/decoding; performance. Here are some basic file size and runtime numbers for running a single; `make_examples` job on a 30x whole genome sample in BAM and CRAM format. Filetype | Size (Gb) | Runtime (min); -------- | --------- | -------------; BAM | 66.99 | 79m47.37307s; CRAM | 37.85 | 96m53.477s; Ratio | 56.50% | 121.43%. * BAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.bam`; * CRAM file:; `gs://deepvariant/performance-testdata/HG002_NIST_150bp_downsampled_30x.cram`. Runtime was measured on; [n1-standard-64](https://cloud.google.com/compute/docs/machine-types#n1_machine_types); machines. ## Starting from v1.2.0, we include `samtools` and `bcftools`. Based on user feedback ([GitHub issue #414](https://github.com/google/deepvariant/issues/414)),; we added samtools and bcftools in our Docker image:. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" samtools; ```. and. ```bash; docker run google/deepvariant:""${BIN_VERSION}"" bcftools; ```. You can read more about samtools and bcftools here: http://www.htslib.org/doc/. ## Commands for requesting machines used in case studies. We report runtime in our case studies documentation. In order to make sure the; results we report are reproducible without too much variation, we provide the; commands we used here to show you what kind of machines we ran the case studies; on. This is NOT the fastest or cheapest configuration. ### Command for a CPU-only machine on Google Cloud Platform. We used a 64-core (vCPU) machine with 240GiB of memory and no GPU, on the Google; Cloud Platform. Specifying the CPU platform also allows us to report the runtime; more consistently. ```shell; gcloud compute instances create ""${USER}-cpu""",MatchSource.DOCS,docs/deepvariant-details.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-details.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1079,Deployability,release,release,1079,"sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://sto",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:895,Testability,benchmark,benchmark,895,"# DeepVariant whole exome sequencing (WES) case study. Similar to the [case study on whole genome sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:979,Testability,benchmark,benchmarks,979,"# DeepVariant whole exome sequencing (WES) case study. Similar to the [case study on whole genome sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1019,Testability,benchmark,benchmark,1019,"whole exome sequencing (WES) case study. Similar to the [case study on whole genome sequencing data], in this; study we describe applying DeepVariant to a real exome sample using a single; machine. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` wil",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1213,Testability,benchmark,benchmark,1213,"vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_n",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1337,Testability,benchmark,benchmark,1337,"hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_di",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1453,Testability,benchmark,benchmark,1453,"eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:1632,Testability,test,testdata,1632,"00001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_a",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:2147,Testability,test,testdata,2147,"RCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 BAM. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam > input/HG003.novaseq.wes_idt.100x.dedup.bam; curl ${HTTPDIR}/HG003.novaseq.wes_idt.100x.dedup.bam.bai > input/HG003.novaseq.wes_idt.100x.dedup.bam.bai; ```. ### Download capture target BED file. In this case study we'll use `idt_capture_novogene.grch38.bed` as the capture; target BED file. For evaluation, `hap.py` will intersect this BED with the GIAB; confident regions. ```bash; HTTPDIR=https://storage.googleapis.com/deepvariant/exome-case-study-testdata. curl ${HTTPDIR}/idt_capture_novogene.grch38.bed > input/idt_capture_novogene.grch38.bed; ```. ## Running on a CPU-only machine. ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type WES \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003.novaseq.wes_idt.100x.dedup.bam \; --regions /input/idt_capture_novogene.grch38.bed \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type WES`, you'll be using a model that is best suited; for Illumina Whole Exome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to ",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:3894,Testability,benchmark,benchmark,3894,"ES`, you'll be using a model that is best suited; for Illumina Whole Exome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.9976",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:3907,Testability,benchmark,benchmark,3907,"ES`, you'll be using a model that is best suited; for Illumina Whole Exome Sequencing data. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.9976",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:4101,Testability,benchmark,benchmark,4101," `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; SNP PASS 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.7",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md:4191,Testability,benchmark,benchmark,4191,"Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on all chromosomes. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -T /input/idt_capture_novogene.grch38.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; INDEL PASS 1051 1022 29 1476 13 418 8 3 0.972407 0.987713 0.283198 0.980000 NaN NaN 1.747283 1.859406; SNP ALL 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; SNP PASS 25279 24987 292 27710 59 2662 34 2 0.988449 0.997645 0.096066 0.993025 2.854703 2.749729 1.623027 1.636078; ```. [case study on whole genome sequencing data]: deepvariant-c",MatchSource.DOCS,docs/deepvariant-exome-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-exome-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2334,Availability,down,downloads,2334,"e instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you c",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2655,Availability,avail,availability,2655,"tion](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2783,Availability,avail,availability,2783,"google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with gen",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4123,Availability,mainten,maintenance-policy,4123," email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4165,Availability,failure,failure,4165," email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2237,Deployability,install,install,2237,"oogle.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisi",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2272,Deployability,install,installation,2272,"oogle.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisi",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2368,Deployability,install,installation,2368,"e ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2877,Deployability,install,installation,2877,"ogle Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""co",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:3584,Energy Efficiency,allocate,allocate,3584," your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:1461,Security,authoriz,authorize,1461," you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation proc",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2420,Security,authenticat,authenticate,2420,"e ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:2894,Security,authenticat,authentication,2894,"ogle Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""co",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:3068,Security,access,access,3068,"nd `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --acce",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:730,Testability,log,log,730,"# Getting Started with GCP. DeepVariant doesn't require GCP, but if you want to use it, these are some; instructions that we found to be useful when getting started. ## Set up a Google Cloud account. To get started using DeepVariant on Google Cloud Platform (GCP), you first need; to set up an account and a project to contain your cloud resources. * If you do not have an account yet, you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on you",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4997,Testability,test,tested,4997,"hat you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instances/create); * [Reference to machine; sizes/types](https://cloud.google.com/compute/docs/machine-types); ",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:1365,Usability,simpl,simply,1365," you should create one at; [cloud.google.com](https://cloud.google.com). You should then [enable; billing for your; account](https://support.google.com/cloud/answer/6288653?hl=en) but note; that if your account is new, [you receive $300 of free; credit](https://cloud.google.com/free/). Once your cloud account is set up,; you should be able to log in to the [Cloud; Console](https://console.cloud.google.com) to view or administer your cloud; resources. * From the Cloud Console, [set up a; project](https://cloud.google.com/resource-manager/docs/creating-managing-projects); to house all of the cloud resources (storage, compute, services) that you; will associate with your use of DeepVariant. For example, if your; organization is AcmeCorp, you might call your project; `acmecorp-deepvariant`. * Finally, please visit the [""Compute Engine"" page on Cloud; Console](https://console.cloud.google.com/compute). You don't need to create; Compute Engine instances at this time, but simply visiting this page will; initialize your compute engine ""service account"" so that we can authorize; it. (As you progress in your use of Google Cloud Platform, you will likely find it; useful to create a [Cloud; Organization](https://cloud.google.com/resource-manager/docs/creating-managing-organization); to house your projects. Here are some [best; practices](https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations); for organizating cloud projects for an enterprise.). ## Install the Google Cloud SDK. The Google Cloud SDK comes with two very useful command line utilities that you; can use on your local workstation---`gcloud`, which lets you administer your; cloud resources, and `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation proc",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:3054,Usability,simpl,simple,3054,"nd `gsutil`, which lets you manage and transfer data to Google; Cloud Storage buckets. We will make use of these tools in the following; instructions. To install the Cloud SDK, [follow the installation instructions; here](https://cloud.google.com/sdk/downloads). The final step in the installation process (`gcloud init`) will have you; authenticate via your web browser and select a default [zone and; region](https://cloud.google.com/compute/docs/regions-zones/regions-zones) for; your cloud resources, which you can choose based on your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --acce",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:3571,Usability,guid,guide,3571," your location and regional; hardware availability. NOTE: Not all zones are equipped with GPUs, so if you want to use GPUs for your; project, please take note of the availability listing; [here](https://cloud.google.com/compute/docs/gpus/). To verify that the installation and authentication succeeded, run. ```shell; gcloud auth list; ```. and verify that your account email address is printed. ## Starting a Compute Engine instance. A simple way to access compute on GCP is Google Compute Engine. Compute Engine; instances can be sized to meet computational and storage needs for your project. Before we get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4222,Usability,simpl,simply,4222," get started, [ensure you have adequate quota; provisioned](https://cloud.google.com/compute/quotas) so that you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instan",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md:4854,Usability,guid,guide,4854,"hat you can get all; the CPUs/GPUs that you need. To start with, you might want to request quota for; 64 CPUs and 2 GPUs in your zone. DeepVariant can make use of multiple CPU cores and (currently, a single) GPU; device. For this ""quick start"" guide, let's allocate an 8-core non-preemptible; instance in your default zone with a single GPU, running Ubuntu 20.04, with a; disk of reasonable size for modest work with genomic data. From our local; command line, we do:. ```shell; gcloud beta compute instances create ""${USER}-deepvariant-quickstart"" \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --image-family ubuntu-2004-lts --image-project ubuntu-os-cloud \; --machine-type n1-standard-8 \; --boot-disk-size=200GB \; --zone us-west1-b \; --accelerator type=nvidia-tesla-k80,count=1 --maintenance-policy TERMINATE --restart-on-failure; ```. NOTE: To create an instance *without GPU*, simply omit the last line from the; command. Check that the instance has been created and started:. ```shell; gcloud compute instances list; ```. which should produce output like:. ```; NAME ZONE MACHINE_TYPE PREEMPTIBLE INTERNAL_IP EXTERNAL_IP STATUS; [USER]-deepvariant-quickstart us-west1-b n1-standard-8 10.138.0.4 35.185.203.59 RUNNING; ```. Then connect to your instance via SSH:. ```shell; gcloud compute ssh --zone us-west1-b ""${USER}-deepvariant-quickstart""; ```. You should land at a shell prompt in your new instance!. NOTE: All of these steps can also be completed from the Cloud Console, if you; prefer. Consult [this; guide](https://cloud.google.com/compute/docs/quickstart-linux), but be sure to; choose Ubuntu 20.04 as your image, as DeepVariant has not been tested on other; Linux distributions. For more information about getting started with Compute Engine, see:. * [Compute Engine instance creation in `gcloud`; manual](https://cloud.google.com/sdk/gcloud/reference/compute/instances/create); * [Reference to machine; sizes/types](https://cloud.google.com/compute/docs/machine-types); ",MatchSource.DOCS,docs/deepvariant-gcp-info.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gcp-info.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2981,Availability,error,error,2981,"ples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile """,MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:5571,Availability,down,downsampling,5571," the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent records; with GQ=0; GQ in [1, 5]; GQ in [6, 10]; GQ in [11, 15]; etc. are binned; together. A concrete example shown below has non-variant sites at each of positions 1-9 on; a hypothetical chromosome:. ```bash; Example input records:; Genome position | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 |; GQ of position | 8 | 10 | 9 | 27 | 47 | 50 | 50 | 45 | 33 |; ```. They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:67,Deployability,release,release,67,"# DeepVariant Genomic VCF (gVCF) support. Beginning with the 0.5.0 release, DeepVariant supports the creation of Genomic; VCF (gVCF) output. This has the same underlying format specification as the; [VCF format] but also includes additional records that distinguish regions that; have sequence coverage that appears to match the reference genome from regions; without sequence coverage, in which the genotype is unknown. gVCF files are required as input for analyses that create a set of variants in; a cohort of individuals, such as cohort merging or joint genotyping. ## Description of gVCF format. When run with gVCF output enabled, DeepVariant generates both the VCF output; containing only variant calls as well as an additional gVCF output file that; contains both variants and non-variant sites. The gVCF file includes both; variant calls and regions that are confidently called as matching the reference; genome. The non-variant sites compare the reference allele to an ""unspecified; alternate"" allele, represented by `<*>`. To minimize output file size, adjacent; records with equal (or similar, see discussion below) genotype qualities are; merged into a single record. Section 5.5 of the [VCF format] specification gives a description of the gVCF; format and example output, partially reproduced below. The gVCF output of; DeepVariant is syntactically and semantically equivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the ",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2185,Integrability,protocol,protocol,2185,"d into a single record. Section 5.5 of the [VCF format] specification gives a description of the gVCF; format and example output, partially reproduced below. The gVCF output of; DeepVariant is syntactically and semantically equivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-varian",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:3389,Integrability,protocol,protocol,3389,"all, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage)",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:4082,Integrability,depend,depends,4082," handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site genotype quality, and thus more records are created. To mitigate this effect, the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent records; with GQ=0; GQ in [1, 5]; GQ in [6, 10]; GQ in [11, 15]; etc. are binned; together. A concrete example sho",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:6445,Integrability,depend,depending,6445," They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP (minimum read; coverage seen in the block). To test it, you can follow the steps in [Quick Start], and in the step where; you run the one-step script `/opt/deepvariant/bin/run_deepvariant`, add this; flag:. ```bash; --make_examples_extra_args=""include_med_dp=true""; ```. Then, if you look at your output gVCF, you'll see the additional MED_DP; information, like:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878; chr20 10000000 . T <*> 0 . END=10000116 GT:GQ:MIN_DP:MED_DP:PL 0/0:50:45:58:0,135,1349; ```. [VCF format]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; [WGS case study]: deepvariant-case-study.md; [Quick Start]: deepvariant-quick-start.md; ",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2378,Modifiability,variab,variables,2378,"ivalent to this example. ```bash; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT Sample; 1 4370 . G <*> . . END=4383 GT:GQ 0/0:37; 1 4384 . C <*> . . END=4388 GT:GQ 0/0:41; 1 4389 . T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Var",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:3541,Modifiability,variab,variables,3541,"IR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site g",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2571,Testability,log,log,2571,"T TC,<*> 50 . . GT:GQ 0/1:50; 1 4390 . C <*> . . END=4390 GT:GQ 0/0:3; ```. ## Creating gVCF output with DeepVariant. The exact same three programs (`make_examples`, `call_variants`, and; `postprocess_variants`) are used when creating gVCF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:2793,Testability,log,log,2793,"CF output as in the [WGS case; study]. However, additional flags must be passed to the `make_examples` and; `postprocess_variants` steps. ### `make_examples`. The `make_examples` program is where the gVCF records are computed. One additional flag is required in `make_examples`, the `--gvcf <filename>`; flag. This specifies an additional output, which is a TFRecord file of Variant; protocol buffers. If running with multiple processes, the sharding applied to; this output filename must be the same as that applied to the `--examples`; output. A concrete example call, using variables defined in the [WGS case study]:. ```bash; GVCF_TFRECORDS=""${OUTPUT_DIR}/HG002.gvcf.tfrecord@${N_SHARDS}.gz"". ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --joblog ""${LOG_DIR}/log"" --res ""${LOG_DIR}"" \; python ""${BIN_DIR}""/make_examples.zip \; --mode calling \; --ref ""${REF}"" \; --reads ""${BAM}"" \; --examples ""${EXAMPLES}"" \; --gvcf ""${GVCF_TFRECORDS}"" \; --task {}; ) >""${LOG_DIR}/make_examples.log"" 2>&1`; ```. NOTE: gVCF outputs are only valid when `make_examples` is run in ""calling"" mode;; if attempted to run in ""training"" mode the program will exit and notify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --o",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:3951,Testability,log,log,3951,"otify the user; of the error. ### `postprocess_variants`. When run in gVCF mode, the `postprocess_variants` program handles the creation; of the final gVCF file that incorporates both the non-variant records and the; true variants discovered by the previous programs. Two additional flags are required in `postprocess_variants`, the input; `--nonvariant_site_tfrecord_path <filename>` which corresponds to the TFRecord; of Variant protocol buffers created in `make_examples`, and the output; `--gvcf_outfile <filename>` which is the final gVCF output. A concrete example call, using variables defined in the [WGS case study] and in; the above `make_examples` example:. ```bash; OUTPUT_GVCF=""${OUTPUT_DIR}/HG002.output.g.vcf.gz"". ( time python ""${BIN_DIR}""/postprocess_variants.zip \; --ref ""${REF}"" \; --infile ""${CALL_VARIANTS_OUTPUT}"" \; --outfile ""${OUTPUT_VCF}"" \; --nonvariant_site_tfrecord_path ""${GVCF_TFRECORDS}"" \; --gvcf_outfile ""${OUTPUT_GVCF}""; ) >""${LOG_DIR}/postprocess_variants.log"" 2>&1; ```. ## Storage and runtime considerations. The number of non-variant records created when running DeepVariant in gVCF; depends highly on the sequencing depth of the input sample. This is because the; gVCF records at adjacent sites are merged when the genotype qualities are equal,; and we limit the possible genotype quality seen to be at most 50. For; deeply-sequenced individuals (e.g. 30-50x coverage), many sites hit the GQ=50; cap and are merged into few records. Samples with lower sequencing depth have; more sites within the dynamic range of the binomial model used to estimate; non-variant site genotype quality, and thus more records are created. To mitigate this effect, the `make_examples` program has a flag; `--gvcf_gq_binsize <int>`. This flag allows the merging of adjacent records that; all have GQ values within a bin of the given size, and for each record emits the; minimum GQ value seen within the bin. For example, setting `--gvcf_gq_binsize 5` has the effect that adjacent ",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md:6692,Testability,test,test,6692," They would create five resultant gVCF record values with `--gvcf_gq_binsize 5`,; with relevant values of:. ```bash; start | INFO | GQ; ------------------; 1 | END=3 | 8; 4 | END=4 | 27; 5 | END=7 | 47; 8 | END=8 | 45; 9 | END=9 | 33; ```. By synthetically downsampling a 50x coverage whole genome and applying different; GQ binning strategies, we see how the size of the resultant data varies as the; two factors change. The below figure shows the size of output (measured as the; number of records generated relative to the baseline of a 50x whole genome with; `--gvcf_gq_binsize 1`) at different coverage levels, for GQ bins of size 1, 3,; 5, and 10. The value of each bar is written in blue font above it for clarity. ![gVCF size](images/DeepVariant-gvcf-sizes-figure.png?raw=true ""DeepVariant gVCF sizes""). ### Runtime. Despite the creation of many additional records, the running time of; `make_examples` increases minimally when gVCF support is enabled. The; single-threaded `postprocess_variants` program is more adversely affected, with; observed runtimes increasing on the [WGS case study] from ~25 minutes to 5-7; hours depending on genome coverage. ### New option to include MED_DP. Starting in v1.2.0, we added a flag to enable adding MED_DP (median read; coverage seen in the block) in addition to the default MIN_DP (minimum read; coverage seen in the block). To test it, you can follow the steps in [Quick Start], and in the step where; you run the one-step script `/opt/deepvariant/bin/run_deepvariant`, add this; flag:. ```bash; --make_examples_extra_args=""include_med_dp=true""; ```. Then, if you look at your output gVCF, you'll see the additional MED_DP; information, like:. ```; #CHROM POS ID REF ALT QUAL FILTER INFO FORMAT NA12878; chr20 10000000 . T <*> 0 . END=10000116 GT:GQ:MIN_DP:MED_DP:PL 0/0:50:45:58:0,135,1349; ```. [VCF format]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; [WGS case study]: deepvariant-case-study.md; [Quick Start]: deepvariant-quick-start.md; ",MatchSource.DOCS,docs/deepvariant-gvcf-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-gvcf-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:1246,Availability,down,download,1246,"Variant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozyg",MatchSource.DOCS,docs/deepvariant-haploid-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:879,Testability,test,testdata,879,"# DeepVariant support for variant calling in chromosome X and Y. ## Case study. A case study on how to use the parameters mentioned here are described in; [DeepVariant X, Y calling case study](deepvariant-xy-calling-case-study.md). ## Haploid calling support. As DeepVariant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1`",MatchSource.DOCS,docs/deepvariant-haploid-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:1035,Testability,test,testdata,1035,"iant calling in chromosome X and Y. ## Case study. A case study on how to use the parameters mentioned here are described in; [DeepVariant X, Y calling case study](deepvariant-xy-calling-case-study.md). ## Haploid calling support. As DeepVariant is a diploid variant caller, it assigns genotypes as {Hom-ref,; Het, Hom-alt} for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consi",MatchSource.DOCS,docs/deepvariant-haploid-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:1357,Testability,test,testdata,1357,"for each candidate allele it observes. For samples with karyotype; XY, the chromosome X and Y are effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozygous alternate: likelihood(REF,ALT1); Homozygous alternaate: likelihood(ALT1,ALT1); ```",MatchSource.DOCS,docs/deepvariant-haploid-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md:1455,Testability,test,testdata,1455,"effectively haploid. So, we are introducing two; flags to re-adjust the genotypes in regions that are considered to be haploid; for those samples. You can use `--haploid_contigs` and `--par_regions_bed` parameters to readjust; the genotypes in haploid regions. For samples with XY karyotype, it is expected; that users will set `--haploid_contigs=""chrX,chrY""` for; [GRCh38](https://storage.googleapis.com/deepvariant/case-study-testdata/GCA_000001405.15_GRCh38_no_alt_analysis_set.fa); and `--haploid_contigs=""X,Y""` for; [GRCh37](https://storage.googleapis.com/deepvariant/case-study-testdata/hs37d5.fa).; You can also provide a PAR region bed file with; `--par_regions_bed=""/input/GRCh3X_par.bed""` parameter. The regions in the PAR; bed file will be skipped from genotype readjustment. You can download the PAR; bed files from here:; [GRCh38_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh38_PAR.bed),; [GRCh37_par.bed](https://storage.googleapis.com/deepvariant/case-study-testdata/GRCh37_PAR.bed). ## How it works. The genotype re-adjustment is implemented in the `postprocess_variants` stage of; DeepVariant. For any variant, that is in the`--haploid_contigs` regions and; **not** in the `--par_regions_bed` regions, the genotype likelihoods of; heterozygous variants are set as 0 and the genotypes are normalized again after; re-adjusting the likelihoods. After that the most-likely genotype is assigned to; the allele which excludes any heterozygous calls. For example, suppose we observe an alternate allele `ALT1` at a position that we; consider to be haploid. So the observed alleles at that position are:; `Candidates: {REF, ALT1}` The neural network generates likelihoods for the; genotypes for this candidate as such:. ```; Homozygous reference: likelihood(REF,REF); Heterozygous alternate: likelihood(REF,ALT1); Homozygous alternaate: likelihood(ALT1,ALT1); ```. So the likelihood vector looks like: `L={L[(REF, REF)], L[(REF, ALT1)], L[(ALT1,; ALT1)]}` In th",MatchSource.DOCS,docs/deepvariant-haploid-support.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-haploid-support.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:1212,Deployability,release,release,1212,"; [pbmm2](https://github.com/PacificBiosciences/pbmm2).; 2. Illumina NovaSeq on HG003 aligned with; [BWA MEM](https://github.com/lh3/bwa). The FASTQ files come from the; [PrecisionFDA Truth challenge v2](https://precision.fda.gov/challenges/10/view). They are merged together into a single bam file using `samtools merge`, and then; a new index is created for this hybrid bam using `samtools index`. Note that the; two original bam files must have the same sample name. Finally, we assess the quality of the DeepVariant variant calls with `hap.py`. To make it faster to run over this case study, we run only on chromosome 20. ## Background on the hybrid model. This is what the pileup image looks like: The longer PacBio reads are shown at; the top, followed by the shorter Illumina reads at the bottom. ![Example of a hybrid pileup for one variant](images/hybrid_pileup.png). A DeepVariant hybrid model was first trained for the PrecisionFDA Truth; Challenge V2, and this release model is similar except it has been re-trained; with additional datasets including the HG004 truth set that was held out during; the challenge. Interestingly, DeepVariant didn't strictly need any code changes to work on; hybrid data -- it worked the first time we tried. But we knew from many previous; experiments that Illumina reads benefit from being realigned to a haplotype; graph, which is too time consuming and unnecessary for the PacBio long reads. We; added a small code change to specifically realign all the short reads to the; haplotype graph, while leaving longer reads with their original alignments. This; created a small but measurable improvement, and was the only code change we made; to enable the hybrid model, aside from training a dedicated hybrid model and; exposing it for easy use through the --model_type parameter in; `run_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating th",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3179,Deployability,release,release,3179,"better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:4245,Deployability,pipeline,pipeline,4245,"nchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""HYBRID_PACBIO_ILLUMINA"" \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--mo",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:2995,Testability,benchmark,benchmark,2995," it for easy use through the --model_type parameter in; `run_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3079,Testability,benchmark,benchmarks,3079,"un_deepvariant.py`. Much of the work we put into DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3119,Testability,benchmark,benchmark,3119,"to DeepVariant is in; experimenting with different approaches, training on more and better data, and; carefully evaluating the models before releasing them. We did the same with this; hybrid model. ## Prepare environment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3313,Testability,benchmark,benchmark,3313,"vironment. ### Tools. [Docker](https://docs.docker.com/get-docker/) will be used to run DeepVariant; and [hap.py](https://github.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3437,Testability,benchmark,benchmark,3437,"hub.com/illumina/hap.py),. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only mac",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3553,Testability,benchmark,benchmark,3553,"eference. FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study wi",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:3916,Testability,test,testdata,3916,"h38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --mode",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:4591,Testability,test,tested,4591,"nchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 BAM. We'll use a HG003 BAM file that contains pacbio and illumina data merged; together using `samtools merge`. See the top of this page for more information; on those two datasets. ```bash; mkdir -p input; HTTPDIR=https://storage.googleapis.com/deepvariant/hybrid-case-study-testdata. curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam; curl ${HTTPDIR}/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai > input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam.bai; ```. ## Running DeepVariant. DeepVariant pipeline consists of 3 steps: `make_examples`, `call_variants`, and; `postprocess_variants`. You can run DeepVariant with just one command using the; `run_deepvariant` script. ### Running on a CPU-only machine. Here we specify `--regions chr20` to run on just chromosome 20, saving time so; you can run this case study within about half an hour (tested on 64 CPUs). ```bash; mkdir -p output; mkdir -p output/intermediate_results_dir. BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ""HYBRID_PACBIO_ILLUMINA"" \; --ref /reference/GRCh38_no_alt_analysis_set.fasta \; --reads /input/HG003_hybrid_35x_ilmn_35x_pacb.grch38.phased.chr20.bam \; --output_vcf /output/HG003.output.vcf.gz \; --output_gvcf /output/HG003.output.g.vcf.gz \; --num_shards $(nproc) \; --regions chr20 \; --intermediate_results_dir /output/intermediate_results_dir; ```. By specifying `--model_type HYBRID_PACBIO_ILLUMINA`, you'll be using a model; that is best suited for (and trained on) the combination of PacBio Hifi long; reads and Illumina short reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:6547,Testability,benchmark,benchmark,6547,"fferent model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:6560,Testability,benchmark,benchmark,6560,"fferent model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:6754,Testability,benchmark,benchmark,6754,"termediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; SNP PASS 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 ",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md:6844,Testability,benchmark,benchmark,6844,"rectory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. To see the pileup images visually, check out [show_examples](show-examples.md). For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). Just make sure to use `--model_type; HYBRID_PACBIO_ILLUMINA` when running on combined PacBio and Illumina data. ## Benchmark with hap.py. See [hap.py](https://github.com/illumina/hap.py) documentation for more details; on the parameters and outputs. ```bash; mkdir -p happy. sudo docker run \; -v ""${PWD}/benchmark"":""/benchmark"" \; -v ""${PWD}/input"":""/input"" \; -v ""${PWD}/output"":""/output"" \; -v ""${PWD}/reference"":""/reference"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; /output/HG003.output.vcf.gz \; -f /benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r /reference/GRCh38_no_alt_analysis_set.fasta \; -o /happy/happy.output \; --engine=vcfeval \; --pass-only \; -l chr20; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; INDEL PASS 10628 10602 26 23385 63 12212 10 51 0.997554 0.994361 0.522215 0.995955 NaN NaN 1.748961 2.721448; SNP ALL 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; SNP PASS 70166 70138 28 105564 43 35354 16 16 0.999601 0.999388 0.334906 0.999494 2.296566 1.812971 1.883951 2.187440; ```. Notice that F1 scores are above 0.999 for SNPs and above 0.995 for indels!",MatchSource.DOCS,docs/deepvariant-hybrid-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-hybrid-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:1095,Availability,down,download,1095," we describe applying DeepVariant to Oxford Nanopore R10.4.1; duplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: Chr20; Chemistry: ONT R10.4.1 Duplex; Basecaller: Dorado v0.1.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDI",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:4394,Deployability,release,release,4394,"chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg002.duplex.r104.ont.chr20.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:2254,Modifiability,variab,variables,2254,"eate local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:2430,Modifiability,variab,variable,2430,"1/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. ",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:4149,Performance,perform,performance,4149,"gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; j",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:878,Testability,benchmark,benchmarking,878,"# DeepVariant with Oxford Nanopore R10.4.1 Duplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; duplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: Chr20; Chemistry: ONT R10.4.1 Duplex; Basecaller: Dorado v0.1.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:1885,Testability,test,testdata,1885,"want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study-duplex"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG002 Duplex chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam; curl ${HTTPDIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai > ${INPUT_DIR}/HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002_R1041_Duplex_all_Dorado_v0.1.1_400bps_pass_2_GRCh38.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG002_R1041_Duplex_Dorado_v0.1.1_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:4230,Testability,benchmark,benchmark,4230,"s_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \;",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md:4314,Testability,benchmark,benchmarks,4314,"pecifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true` to; the command above to figure out what flags you need in each step. Based on the; different model types, different flags are needed in the `make_examples` step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG002 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG002. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG002_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}",MatchSource.DOCS,docs/deepvariant-ont-r104-duplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-duplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:1063,Availability,down,download,1063,"implex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R10",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:4190,Deployability,release,release,4190,"mistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg003.ul.r104.ont.chr20.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${RE",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:2091,Modifiability,variab,variables,2091,"nt-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifyi",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:2235,Modifiability,variab,variable,2235,"ure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you ",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:3945,Performance,perform,performance,3945,"vcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:846,Testability,benchmark,benchmarking,846,"# DeepVariant with Oxford Nanopore R10.4.1 Simplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:1850,Testability,test,testdata,1850,". If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam.bai > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam.bai. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG003_R104_sup_merged.80x.chr20.bam""; THREADS=$(nproc); REGION=""chr20"". # Set up output variable; OUTPUT_VCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.chr20.output.vcf.gz""; OUTPUT_GVCF=""HG003_UL_R1041_Guppy6_sup_2_GRCh38.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type ONT_R104 \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:4026,Testability,benchmark,benchmark,4026,"_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. By specifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}""",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:4110,Testability,benchmark,benchmarks,4110,"ecifying `--model_type ONT_R104`, you'll be using a model that is best; suited for Oxford Nanopore R10.4.1 chemistry Simplex and Duplex reads. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. `--intermediate_results_dir` flag is optional. By specifying it, the; intermediate outputs of `make_examples` and `call_variants` stages can be found; in the directory. After the command, you can find these files in the directory:. ```; call_variants_output.tfrecord.gz; gvcf.tfrecord-?????-of-?????.gz; make_examples.tfrecord-?????-of-?????.gz; ```. ## Benchmark HG003 chr20 output from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > ${INPUT_DIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi. TRUTH_VCF=""HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz""; TRUTH_BED=""HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${R",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md:139,Usability,simpl,simplex,139,"# DeepVariant with Oxford Nanopore R10.4.1 Simplex reads. In this case study, we describe applying DeepVariant to Oxford Nanopore R10.4.1; simplex reads. Then we assess the quality of the DeepVariant variant calls with; `hap.py`. To make it faster to go over this case study, we run only on chromosome 20. The dataset used in this case-study has following attributes:. ```bash; Sample: HG003; Region: Chr20; Chemistry: ONT R10.4.1; Coverage: 80x; ```. **Model note:**. * The model is trained with Guppy 6+ ""SUP"" Simplex and Dorado v0.1.1 Duplex; reads. * The model is trained on both Ultra-long and sheared reads with varying read; N50 and coverage. ## Prepare environment. In this case-study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/ont-case-study"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input/data""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download HG003 Ultra-long chr20 bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/ont-case-study-testdata; curl ${HTTPDIR}/HG003_R104_sup_merged.80x.chr20.bam > ${INPUT_DIR}/HG003_R104_sup_merged.80x.chr20.bam; curl ${HTTPDIR}/HG003_R104_sup_merged",MatchSource.DOCS,docs/deepvariant-ont-r104-simplex-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-ont-r104-simplex-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:268,Availability,avail,available,268,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1319,Availability,down,download,1319," PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Downloa",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2411,Availability,avail,available,2411,"38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regio",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2547,Availability,down,downloads,2547,"erence/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:816,Deployability,install,installed,816,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:930,Deployability,configurat,configuration,930,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1091,Deployability,install,install,1091,"this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchm",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1187,Deployability,install,install,1187,"this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchm",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1891,Deployability,release,release,1891,d channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stac,MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:747,Integrability,depend,dependencies,747,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1099,Integrability,depend,dependencies,1099,"this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchm",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:930,Modifiability,config,configuration,930,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:951,Modifiability,config,config,951,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:989,Modifiability,config,config,989,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1027,Modifiability,config,config,1027,"# Using DeepVariant for small variant calling from PacBio HiFi reads. #### Author: William Rowell <wrowell@pacificbiosciences.com>. In this case study we describe applying DeepVariant to PacBio HiFi reads to call; variants. We will call small variants from a publicly available whole genome; HiFi dataset from PacBio. Starting in v1.4.0, PacBio calling uses one-step variant calling. If you're; looking for documentation for the two-step process, please look at v1.3.0. ## Prepare environment. ### Tools. [Singularity](https://sylabs.io/docs/) will be used to run DeepVariant and; [hap.py](https://github.com/illumina/hap.py), and we'll use; [miniconda](https://docs.conda.io/en/latest/miniconda.html) and a conda; environment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchma",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1707,Testability,benchmark,benchmark,1707,ment to handle the other dependencies for the case study and samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HT,MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1791,Testability,benchmark,benchmarks,1791,nd samtools. - singularity (must be installed by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG0,MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:1831,Testability,benchmark,benchmark,1831,by `root` user; outside of the scope of this; case study); - samtools. ```bash; # add channels to conda configuration; conda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## R,MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2025,Testability,benchmark,benchmark,2025,dd channels conda-forge. # create the environment and install dependencies; conda create -y -n deepvariant_env; conda activate deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BI,MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2149,Testability,benchmark,benchmark,2149,"e deepvariant_env; conda install -y samtools==1.10; ```. ### Download Reference. We will be using GRCh38 for this case study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BI",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:2265,Testability,benchmark,benchmark,2265,"ase study. ```bash; mkdir -p reference. # download and decompress; curl ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta. # index reference; samtools faidx reference/GRCh38_no_alt_analysis_set.fasta; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG003. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download HG003 chr20 HiFi alignments. We'll use HG003 chr20 HiFi reads publicly available from the [PrecisionFDA Truth v2 Challenge](https://precision.fda.gov/challenges/10). ```bash; mkdir -p input; HTTPDIR=https://downloads.pacbcloud.com/public/dataset/HG003/deepvariant-case-study. curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam > input/HG003.GRCh38.chr20.pFDA_truthv2.bam; curl ${HTTPDIR}/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai > input/HG003.GRCh38.chr20.pFDA_truthv2.bam.bai; ```. ## Run DeepVariant on chromosome 20 alignments. ```bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:3880,Testability,benchmark,benchmark,3880,"``bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. ## Benchmark output. ```bash; mkdir -p happy. singularity exec docker://jmcdani20/hap.py:v0.3.12 \; /opt/hap.py/bin/hap.py \; --threads $(nproc) \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -f benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -o happy/giab-comparison.v4.2.first_pass \; --engine=vcfeval \; --pass-only \; -l chr20 \; benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; deepvariant_output/output.vcf.gz; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10551 77 22590 69 11527 39 29 0.992755 0.993763 0.510270 0.993259 NaN NaN 1.748961 2.275319; INDEL PASS 10628 10551 77 22590 69 11527 39 29 0.992755 0.993763 0.510270 0.993259 NaN NaN 1.748961 2.275319; SNP ALL 70166 70141 25 98780 23 28559 5 11 0.999644 0.999672 0.289117 0.999658 2.296566 1.823452 1.883951 1.913585; SNP PASS 70166 70141 25 98780 23 28559 5 11 0.999644 0.999672 0.289117 0.999658 2.296566 1.823452 1.883951 1.913585; ```; ",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md:4038,Testability,benchmark,benchmark,4038,"``bash; ulimit -u 10000 # https://stackoverflow.com/questions/52026652/openblas-blas-thread-init-pthread-create-resource-temporarily-unavailable/54746150#54746150; BIN_VERSION=""1.6.1""; mkdir -p deepvariant_output. singularity exec --bind /usr/lib/locale/ \; docker://google/deepvariant:${BIN_VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref reference/GRCh38_no_alt_analysis_set.fasta \; --reads input/HG003.GRCh38.chr20.pFDA_truthv2.bam \; --output_vcf deepvariant_output/output.vcf.gz \; --num_shards $(nproc) \; --regions chr20; ```. NOTE: If you want to run each of the steps separately, add `--dry_run=true`; to the command above to figure out what flags you need in each step. Based on; the different model types, different flags are needed in the `make_examples`; step. ## Benchmark output. ```bash; mkdir -p happy. singularity exec docker://jmcdani20/hap.py:v0.3.12 \; /opt/hap.py/bin/hap.py \; --threads $(nproc) \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -f benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -o happy/giab-comparison.v4.2.first_pass \; --engine=vcfeval \; --pass-only \; -l chr20 \; benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; deepvariant_output/output.vcf.gz; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 10628 10551 77 22590 69 11527 39 29 0.992755 0.993763 0.510270 0.993259 NaN NaN 1.748961 2.275319; INDEL PASS 10628 10551 77 22590 69 11527 39 29 0.992755 0.993763 0.510270 0.993259 NaN NaN 1.748961 2.275319; SNP ALL 70166 70141 25 98780 23 28559 5 11 0.999644 0.999672 0.289117 0.999658 2.296566 1.823452 1.883951 1.913585; SNP PASS 70166 70141 25 98780 23 28559 5 11 0.999644 0.999672 0.289117 0.999658 2.296566 1.823452 1.883951 1.913585; ```; ",MatchSource.DOCS,docs/deepvariant-pacbio-model-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-pacbio-model-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:356,Availability,down,download,356,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1915,Availability,down,downloaded,1915,"ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:3592,Availability,down,download,3592,"r20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:825,Deployability,release,release,825,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1359,Deployability,update,update,1359,"a through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; ",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1383,Deployability,install,install,1383,"a through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; ",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:3544,Deployability,release,released,3544,"tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; -",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:3666,Deployability,release,releases,3666,"fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.bam; NA12878_S1.chr20.10_10p1mb.bam.bai; test_nist.b37_chr20_100kbp_at_10mb.bed; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; ucsc.hg19.chr20.unittest.fasta; ucsc.hg19.chr20.unittest.fasta.fai; ucsc.hg19.chr20.unittest.fasta.gz; ucsc.hg19.chr20.unittest.fasta.gz.fai; ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. ### Model location (optional). Starting from r0.8, we put the model files inside the released Docker images.; So there is no need to download model files anymore. If you want to find the; model files of all releases, you can find them in our bucket on the Google Cloud; Storage. You can view them in the browser:; https://console.cloud.google.com/storage/browser/deepvariant/models/DeepVariant. ## Run DeepVariant with one command. DeepVariant consists of 3 main binaries: `make_examples`, `call_variants`, and; `postprocess_variants`. To make it easier to run, we create one entrypoint that; can be directly run as a docker command. If you want to see the details, you can; read through [run_deepvariant.py]. ```bash; OUTPUT_DIR=""${PWD}/quickstart-output""; mkdir -p ""${OUTPUT_DIR}""; ```. You can run everything with the following command:. ```bash; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=/input/ucsc.hg19.chr20.unittest.fasta \; --reads=/input/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=/output/output.vcf.gz \; --output_gvcf",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:5846,Deployability,install,install,5846,"ry_run=true` to the command above, which will print out all the commands; but not execute them. This will generate 5 files and 1 directory in `${OUTPUT_DIR}`:. ```bash; ls -1 ${OUTPUT_DIR}; ```. outputting:. ```; intermediate_results_dir; output.g.vcf.gz; output.g.vcf.gz.tbi; output.vcf.gz; output.vcf.gz.tbi; output.visual_report.html; ```. The directory ""intermediate_results_dir"" exists because; `--intermediate_results_dir /output/intermediate_results_dir` is specified. This; directory contains the intermediate output of make_examples and call_variants; steps. For more information about `output.visual_report.html`, see the; [VCF stats report documentation](deepvariant-vcf-stats-report.md). ## Notes on GPU image. If you are using GPUs, you can pull the GPU version, and make sure you run with; `--gpus 1`. `call_variants` is the only step that uses the GPU, and can only use; one at a time. `make_examples` and `postprocess_variants` do not run on GPU. For an example to install GPU driver and docker, see [install_nvidia_docker.sh]. ```; sudo docker run --gpus 1 \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}:/output"" \; google/deepvariant:""${BIN_VERSION}-gpu"" \; /opt/deepvariant/bin/run_deepvariant \; ...; ```. ## Notes on Singularity. ### CPU version. ```; # Pull the image.; singularity pull docker://google/deepvariant:""${BIN_VERSION}"". # Run DeepVariant.; singularity run -B /usr/lib/locale/:/usr/lib/locale/ \; docker://google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \ **Replace this string with exactly one of the following [WGS,WES,PACBIO,ONT_R104,HYBRID_PACBIO_ILLUMINA]**; --ref=""${INPUT_DIR}""/ucsc.hg19.chr20.unittest.fasta \; --reads=""${INPUT_DIR}""/NA12878_S1.chr20.10_10p1mb.bam \; --regions ""chr20:10,000,000-10,010,000"" \; --output_vcf=""${OUTPUT_DIR}""/output.vcf.gz \; --output_gvcf=""${OUTPUT_DIR}""/output.g.vcf.gz \; --intermediate_results_dir ""${OUTPUT_DIR}/intermediate_results_dir"" \ **Optional.; --num_shards=1 \ *",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:170,Integrability,depend,depend,170,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:187,Testability,test,test,187,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:273,Testability,test,test,273,"# DeepVariant quick start. This is an explanation of how to use DeepVariant. ## Background. To get started, you'll need the DeepVariant programs (and some packages they; depend on), some test data, and of course a place to run them. We've provided a Docker image, and some test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; ",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1284,Testability,test,test,1284," test data in a bucket on Google Cloud; Storage. The instructions below show how to download the data through the; corresponding public URLs from these data. This setup requires a machine with the AVX instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1474,Testability,test,test,1474," instruction set. To see if your; machine meets this requirement, you can check the `/proc/cpuinfo` file, which; lists this information under ""flags"". If you do not have the necessary; instructions, see the next section for more information on how to build your own; Docker image. ### Use Docker to run DeepVariant in one command. Starting from the 0.8 release, we introduced one convenient command that will; run through all 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_D",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1851,Testability,test,test,1851,"ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1978,Testability,test,test,1978,"You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current d",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:2030,Testability,test,testdata,2030,"Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:2110,Testability,test,testdata,2110,"urself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.gzi; ```. This should create a subdirectory in the current directory containing the actual; data files:. ```bash; ls -1 ${INPUT_DIR}; ```. outputting:. ```; NA12878_S1.chr20.10_10p1mb.",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md:1896,Usability,guid,guide,1896,"ll 3 steps that are required to go from a BAM file to the VCF/gVCF; output files. You can still read about the r0.7 approach in; [Quick Start in r0.7]. If you want to compile the DeepVariant binaries for yourself, we also have a; [Dockerfile] that you can use to build your own Docker image. You can read the; [docker build] documentation on how to build. ## Get Docker image, models, and test data. ### Get Docker image. ```bash; BIN_VERSION=""1.6.1"". sudo apt -y update; sudo apt-get -y install docker.io; sudo docker pull google/deepvariant:""${BIN_VERSION}""; ```. ### Download test data. Before you start running, you need to have the following input files:. 1. A reference genome in [FASTA] format and its corresponding index file; (.fai). 1. An aligned reads file in [BAM] format and its corresponding index file; (.bai). You get this by aligning the reads from a sequencing instrument,; using an aligner like [BWA] for example. We've prepared a small test data bundle for use in this quick start guide that; can be downloaded to your instance from the public URLs. Download the test bundle:. ```bash; INPUT_DIR=""${PWD}/quickstart-testdata""; DATA_HTTP_DIR=""https://storage.googleapis.com/deepvariant/quickstart-testdata"". mkdir -p ${INPUT_DIR}; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/NA12878_S1.chr20.10_10p1mb.bam.bai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.bed; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/test_nist.b37_chr20_100kbp_at_10mb.vcf.gz.tbi; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.chr20.unittest.fasta.gz.fai; wget -P ${INPUT_DIR} ""${DATA_HTTP_DIR}""/ucsc.hg19.ch",MatchSource.DOCS,docs/deepvariant-quick-start.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-quick-start.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:625,Availability,down,downloaded,625,"# DeepVariant RNA-seq Case Study. This case study will demonstrate how to run DeepVariant using the RNA-seq model,; and evaluate the result using `hap.py`. ## Overview. ### Tools. We will use the following tools:. * [Docker](https://docs.docker.com/get-docker/) - Used to run DeepVariant.; * [mosdepth](https://github.com/brentp/mosdepth) - For calculating coverage.; * [bedtools](https://bedtools.readthedocs.io) - Used to intersect bedfiles.; * [hap.py](https://github.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:2281,Availability,down,download,2281,"/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai > data/hg005_gm26107.mrna.grch38.bam.bai; ```. ### Generate a 3x coverage file. RNA-seq data is only observed in ",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:2906,Availability,avail,available,2906,". curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai > data/hg005_gm26107.mrna.grch38.bam.bai; ```. ### Generate a 3x coverage file. RNA-seq data is only observed in regions that are expressed in a given sample.; Therefore, we will restrict our evaluation to regions of the BAM file that reach; a minimum threshold of 3x in our truth dataset intersected with the confident; GIAB regions. This allows us to better evaluate the accuracy of the model when; it is feasible for a variant to be called from RNA-seq data. ```bash; # Generate a coverage file, and filter for 3x.; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/mosdepth:0.3.1--h4dc83fb_1 \; mosdepth \; --threads $(nproc) \; data/hg005_coverage \; data/hg005_",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:5326,Availability,down,download,5326,"thin the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta; ```. ### Directory Structure. After you have run the steps above, your directory structure should look like; this:. ```; .; ├── benchmark; │   ├── chr20_CDS_3x.benchmark_regions.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.bed;",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:8169,Availability,avail,available,8169,"DeepVariant RNA-seq model and produce an output; VCF (`output/out.vcf.gz`). ```bash; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=data/hg005_gm26107.mrna.grch38.bam \; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fas",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1818,Deployability,release,release,1818," create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:8546,Energy Efficiency,efficient,efficiently,8546,"output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:8221,Performance,perform,perform,8221,"gz`). ```bash; BIN_VERSION=""1.4.0"". sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; google/deepvariant:""${BIN_VERSION}"" \; run_deepvariant \; --model_type=WES \; --customized_model=model/model.ckpt \; --ref=reference/GRCh38_no_alt_analysis_set.fasta \; --reads=data/hg005_gm26107.mrna.grch38.bam \; --output_vcf=output/HG005.output.vcf.gz \; --num_shards=$(nproc) \; --regions=data/chr20_CDS_3x.bed \; --make_examples_extra_args=""split_skip_reads=true,channels=''"" \; --intermediate_results_dir output/intermediate_results_dir; ```. **Flag summary**. * `--model_type` - Sets the model and options, but we will override the model; with `--customized model`.; * `--customized_model` - Points to a model trained using RNA-seq data.; * `--ref` - Specifies the reference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --ta",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:768,Testability,benchmark,benchmark,768,"# DeepVariant RNA-seq Case Study. This case study will demonstrate how to run DeepVariant using the RNA-seq model,; and evaluate the result using `hap.py`. ## Overview. ### Tools. We will use the following tools:. * [Docker](https://docs.docker.com/get-docker/) - Used to run DeepVariant.; * [mosdepth](https://github.com/brentp/mosdepth) - For calculating coverage.; * [bedtools](https://bedtools.readthedocs.io) - Used to intersect bedfiles.; * [hap.py](https://github.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:896,Testability,benchmark,benchmark,896,"# DeepVariant RNA-seq Case Study. This case study will demonstrate how to run DeepVariant using the RNA-seq model,; and evaluate the result using `hap.py`. ## Overview. ### Tools. We will use the following tools:. * [Docker](https://docs.docker.com/get-docker/) - Used to run DeepVariant.; * [mosdepth](https://github.com/brentp/mosdepth) - For calculating coverage.; * [bedtools](https://bedtools.readthedocs.io) - Used to intersect bedfiles.; * [hap.py](https://github.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1450,Testability,benchmark,benchmark,1450,"ithub.com/illumina/hap.py) - Used to evaluate the results.; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_h",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1534,Testability,benchmark,benchmarks,1534,"; We will use Docker to run `hap.py`. ### Data. We will use these data in our analysis. Files will be downloaded in subsequent; steps. * HG005 RNA-seq BAM; * Model Checkpoint Files; * GRCh38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > dat",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1660,Testability,benchmark,benchmarks,1660,"h38 Reference + Index; * CDS bedfile (chr20 only); * GIAB benchmark data. ## Prepare Data. ### Setup directories. Lets first create directories to organize files. ```bash; mkdir -p data benchmark reference model output happy; ```. ### Download the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 ",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:1934,Testability,benchmark,benchmark,1934,"ownload the GRCh38 Reference. We will be using GRCh38 for this case study. ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://stora",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:2046,Testability,benchmark,benchmark,2046,".gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg0",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:2165,Testability,benchmark,benchmark,2165,"001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > reference/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > reference/GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v4.2.1 of the Genome in a Bottle; small variant benchmarks for HG005. We will also restrict analysis to CDS; regions on chromosome 20 to make this demonstration quicker. The benchmarks consist of a bedfile containing confident regions, a VCF of; 'true' variants, and a VCF index. ```bash; FTPDIR=ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/ChineseTrio/HG005_NA24631_son/NISTv4.2.1/GRCh38. curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.bed > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl -L ${FTPDIR}/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ### Download and extract a CDS bedfile. Next, we will download a [gencode](https://www.gencodegenes.org/) gff3; annotation and extract a bed file of chr20 CDS regions. ```bash; curl -L https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_41/gencode.v41.basic.annotation.gff3.gz > data/gencode.v41.basic.annotation.gff3.gz. # Extract chr20 CDS regions and convert to bed file.; gzip -dc data/gencode.v41.basic.annotation.gff3.gz | \; awk -v OFS='\t' '$1 == ""chr20"" && $3 == ""CDS"" && $4 < $5 { print $1, $4, $5, ""CDS"" }' | \; awk '!dup[$0]++' > data/chr20_CDS.bed; ```. ### Download HG005 BAM. We'll use HG005 poly-A selected Illumina RNA-seq reads that are publicly; available. ```bash; HTTPDIR=https://storage.googleapis.com/brain-genomics-public/research/sequencing/grch38/bam/rna/illumina/mrna. curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam > data/hg005_gm26107.mrna.grch38.bam; curl -L ${HTTPDIR}/hg005_gm26107.mrna.grch38.bam.bai >",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:4975,Testability,benchmark,benchmark,4975,"er interactively to execute a series of; commands. Run the following command to launch a bedtools container. ```bash; sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/bedtools:2.23.0--h5b5514e_6 \; /bin/bash; ```. ### Extract regions with 3x coverage, and filter out unused contigs. We will restrict our analysis to regions with a minimum of 3x coverage. ```bash; # (Run within the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:5054,Testability,benchmark,benchmark,5054," sudo docker run \; -v ""$(pwd):$(pwd)"" \; -w $(pwd) \; -it quay.io/biocontainers/bedtools:2.23.0--h5b5514e_6 \; /bin/bash; ```. ### Extract regions with 3x coverage, and filter out unused contigs. We will restrict our analysis to regions with a minimum of 3x coverage. ```bash; # (Run within the bedtools container); min_coverage=3; gzip -dc data/hg005_coverage.per-base.bed.gz | \; egrep -v 'HLA|decoy|random|alt|chrUn|chrEBV' | \; awk -v OFS=""\t"" -v min_coverage=${min_coverage} '$4 >= min_coverage { print }' | \; bedtools merge -d 1 -c 4 -o mean -i - > data/hg005_3x.bed; ```. ### Intersect coverage with CDS regions. Now we will intersect our 3x bedfile with the CDS bed file:. ```bash; # (Run within the bedtools container); bedtools intersect \; -a data/hg005_3x.bed \; -b data/chr20_CDS.bed > data/chr20_CDS_3x.bed. # We will also intersect this file with confident GIAB regions; bedtools intersect \; -a benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.bed \; -b data/chr20_CDS_3x.bed > benchmark/chr20_CDS_3x.benchmark_regions.bed; ```. We now have a bed file of CDS regions intersected with 3x coverage regions; called `data/chr20_CDS_3x.bed`. You can exit the docker container now. Type; `exit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-r",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:6247,Testability,benchmark,benchmark,6247,"xit` and hit enter. ### Download the RNA-seq model. Finally, lets download the RNA-seq model that we will use to call variants. ```bash; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.data-00000-of-00001 > model/model.ckpt.data-00000-of-00001; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.example_info.json > model/model.ckpt.example_info.json; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.index > model/model.ckpt.index; curl https://storage.googleapis.com/deepvariant/models/DeepVariant/1.4.0/DeepVariant-inception_v3-1.4.0+data-rnaseq_standard/model.ckpt.meta > model/model.ckpt.meta; ```. ### Directory Structure. After you have run the steps above, your directory structure should look like; this:. ```; .; ├── benchmark; │   ├── chr20_CDS_3x.benchmark_regions.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.bed; │   ├── HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; │   └── HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ├── data; │   ├── chr20_CDS_3x.bed; │   ├── chr20_CDS.bed; │   ├── gencode.v41.basic.annotation.gff3.gz; │   ├── hg005_3x.bed; │   ├── hg005_coverage.mosdepth.global.dist.txt; │   ├── hg005_coverage.mosdepth.summary.txt; │   ├── hg005_coverage.per-base.bed.gz; │   ├── hg005_coverage.per-base.bed.gz.csi; │   ├── hg005_gm26107.mrna.grch38.bam; │   └── hg005_gm26107.mrna.grch38.bam.bai; ├── happy; ├── model; │   ├── model.ckpt.data-00000-of-00001; │   ├── model.ckpt.index; │   └── model.ckpt.meta; ├── output; └── reference; ├── GRCh38_no_alt_analysis_set.fasta; └── GRCh38_no_alt_analysis_set.fasta.fai; ```. ### Running DeepVariant RNA-seq on a CPU-only machine. The command below will run the DeepVariant RNA-seq model and produce an output; VCF (`output/out.vcf.gz`). ```bash; BIN_VERSION=""1.4.0"". ",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:8976,Testability,benchmark,benchmark,8976,"ference sequence.; * `--reads` - Specifies the input bam file.; * `--output_vcf` - Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:9064,Testability,benchmark,benchmark,9064,"Specifies the output variant file.; * `--num_shards` - Sets the number of shards to the number of available; processors (`$(nproc)`). This is used to perform parallelization.; * `--regions` - Restricts analysis to 3x chr20 CDS regions only.; * `--make_examples_extra_args=` - Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUT",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:9324,Testability,benchmark,benchmark,9324,"Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; INDEL PASS 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; SNP ALL 287 275 12 314 6 33 3 2 0.958188 0.978648",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:9380,Testability,benchmark,benchmark,9380,"Passes additional arguments to; make_examples.; * `split_skip_reads=true` - *Important!* This flag is critical for RNA-seq; variant calling to work properly. It enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; INDEL PASS 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; SNP ALL 287 275 12 314 6 33 3 2 0.958188 0.978648",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md:9628,Testability,benchmark,benchmarking,9628," enables RNA-seq data to be; processed efficiently.; * `channels=''` - Resets the channel list to be appropriate for the; RNA-seq model.; * `--intermediate_results_dir` - Outputs results to an intermediate directory. For running on GPU machines, or using Singularity instead of Docker, see; [Quick Start](deepvariant-quick-start.md). ## Benchmark on chr20. ```bash; sudo docker run \; -v $(pwd):$(pwd) \; -w $(pwd) \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; benchmark/HG005_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; output/HG005.output.vcf.gz \; -f benchmark/chr20_CDS_3x.benchmark_regions.bed \; -r reference/GRCh38_no_alt_analysis_set.fasta \; -o happy/happy.output \; --engine=vcfeval \; --pass-only \; --target-regions=data/chr20_CDS_3x.bed \; --threads=$(nproc); ```. **Flag summary**. * `-f` - Sets the benchmark regions (regions of interest that we want to; benchmark.); * `-r` - Sets the reference genome.; * `-o` - Specifies the output location.; * `--engine` - Sets the variant comparison engine. See; [hap.py documentation](https://github.com/Illumina/hap.py) for details.; * `--pass-only` - Restricts benchmarking to variants that have passed all; filters.; * `--target-regions` - Restricts analysis to given regions only.; * `--threads` - Level of parallelization to use. **Output:**. The above command should output the following results:. ```; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; INDEL PASS 9 6 3 11 1 4 1 0 0.666667 0.857143 0.363636 0.75000 NaN NaN 0.800000 1.200000; SNP ALL 287 275 12 314 6 33 3 2 0.958188 0.978648 0.105096 0.96831 4.125 3.984127 1.141791 1.093333; SNP PASS 287 275 12 314 6 33 3 2 0.958188 0.978648 0.105096 0.96831 4.125 3.984127 1.141791 1.093333; ```; ",MatchSource.DOCS,docs/deepvariant-rnaseq-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-rnaseq-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:1635,Availability,mainten,maintenance-policy,1635," the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13687,Availability,checkpoint,checkpoints,13687,"_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have te",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14526,Availability,checkpoint,checkpoint,14526,"une_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14580,Availability,checkpoint,checkpoint,14580,"une_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14612,Availability,checkpoint,checkpoint,14612,".init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14903,Availability,checkpoint,checkpoints,14903,"red \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCK",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14952,Availability,checkpoint,checkpoints,14952,"; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTP",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14980,Availability,checkpoint,checkpoint,14980,"r training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:15092,Availability,checkpoint,checkpoints,15092,"epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:15104,Availability,checkpoint,checkpoint,15104,"epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:15199,Availability,checkpoint,checkpoints,15199,"dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Prev",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:17437,Availability,error,error,17437,"n` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at chr20:60402030 for sample 0; [W] Variants that overlap on the reference allele: 1; [I] Total VCF records: 132914; [I] Non-reference VCF records: 96273;",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:21194,Availability,down,downsamples,21194,"IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:21317,Availability,robust,robust,21317,"IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:125,Deployability,pipeline,pipeline,125,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:624,Deployability,pipeline,pipeline,624,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:746,Deployability,configurat,configuration,746,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:1298,Deployability,pipeline,pipeline,1298,"eepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:3907,Deployability,update,update,3907,"E100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:3927,Deployability,install,install,3927,"E100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:7832,Deployability,configurat,configuration,7832,"OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:8954,Deployability,install,install,8954,"training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 instal",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9002,Deployability,install,install,9002,"reprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfre",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9761,Deployability,update,update,9761,"e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9783,Deployability,install,install,9783,"e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9809,Deployability,install,install,9809,"e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9819,Deployability,upgrade,upgrade,9819,"e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9837,Deployability,install,install,9837,"e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9858,Deployability,upgrade,upgrade,9858,"e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9872,Deployability,install,install,9872,"e other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resourc",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9943,Deployability,install,install,9943,"tall beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the d",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:11775,Deployability,pipeline,pipeline,11775,"taflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:12841,Deployability,configurat,configuration,12841,"ffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${T",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:21051,Deployability,release,released,21051,"IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:17267,Integrability,message,messages,17267,"Port.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at c",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:17539,Integrability,message,message,17539,"ance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can; ignore the message. Once this is done, we have the final callset in VCF format here:; `${OUTPUT_DIR}/test_set.vcf.gz`. Next step is to run `hap.py` to complete the; evaluation on chromosome 20:. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. time sudo docker run -it \; -v ""${DATA_DIR}:${DATA_DIR}"" \; -v ""${OUTPUT_DIR}:${OUTPUT_DIR}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/test_set.vcf.gz"" \; -f ""${TRUTH_BED}"" \; -r ""${REF}"" \; -o ""${OUTPUT_DIR}/chr20-calling.happy.output"" \; -l chr20 \; --engine=vcfeval \; --pass-only; ```. The output of `hap.py` is here:. ```; [I] Total VCF records: 3775119; [I] Non-reference VCF records: 3775119; [W] overlapping records at chr20:60402030 for sample 0; [W] Variants that overlap on the reference allele: 1; [I] Total VCF records: 132914; [I] Non-reference VCF records: 96273; 2023-10-14 20:09:55,773 WARNING Creating template for vcfeval. You can speed this up by supplying a SDF template that corre; spond",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:746,Modifiability,config,configuration,746,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:2104,Modifiability,variab,variables,2104,"aseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:7832,Modifiability,config,configuration,7832,"OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:12783,Modifiability,config,config,12783,"e is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:12841,Modifiability,config,configuration,12841,"ffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${T",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:12983,Modifiability,config,config,12983,"dation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13424,Modifiability,config,config,13424,"t""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perf",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13454,Modifiability,config,config,13454,"54; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13539,Modifiability,config,config,13539,"frecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13622,Modifiability,config,config,13622,"_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoi",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13729,Modifiability,config,config,13729,"???.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 G",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13755,Modifiability,config,config,13755,"examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13788,Modifiability,config,config,13788,"fig file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; --",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13885,Modifiability,config,config,13885,"nfiguration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once tr",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14290,Modifiability,config,config,14290,"this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14334,Modifiability,config,config,14334,"us 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualiz",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14638,Modifiability,config,configured,14638,".init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14658,Modifiability,config,config,14658,".init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:416,Performance,optimiz,optimized,416,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:1307,Performance,optimiz,optimization,1307,"eepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:4815,Performance,perform,performance,4815,"-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:4910,Performance,perform,performance,4910," apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHAR",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9261,Performance,perform,performs,9261,"et.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_patt",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:10779,Performance,perform,performance,10779," install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; tim",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13211,Performance,optimiz,optimized,13211,"; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_DIR/validation_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_DIR/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 59401; ```. ### Fetch a config file. Before we can begin training, we will need a configuration file containing; training parameters. Parameters within this training file can be overridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoc",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14117,Performance,tune,tune,14117,"iant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DI",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14189,Performance,tune,tune,14189,"this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14362,Performance,perform,perform,14362," -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14435,Performance,tune,tune,14435,"; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a T",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14490,Performance,tune,tune,14490,"; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a T",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14569,Performance,perform,performing,14569,"une_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14601,Performance,perform,performing,14601,".init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cl",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:16486,Performance,perform,performed,16486," the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can;",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:16522,Performance,perform,performance,16522," the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can;",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:20430,Performance,tune,tune,20430,"IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:20663,Performance,tune,tune,20663,"IC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9811 | 212 | 155 | 0.978849 | 0.985044 | 0.981937 |; | SNP | 66180 | 57 | 70 | 0.999139 | 0.998944 | 0.999042 |. The baseline we're comparing to is to directly use the WGS model to make the; calls, using this command:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE}-gpu \; run_deepvariant \; --model_type WGS \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/baseline.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. Baseline:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 9620 | 403 | 823 | 0.959792 | 0.924112 | 0.941615 |; | SNP | 66159 | 78 | 83 | 0.998822 | 0.998748 | 0.998785 |. ### Additional things to try. #### Parameters to tune. Starting from the default setting of this tutorial is a good starting point, but; this training case study is by no means the best setting. Training is both a; science and an art. There are many knobs that we could potentially tune. Users; might be able to use different parameters to train a more accurate model even; with the same data, such as `batch_size`, `learning_rate`,; `learning_rate_decay_factor` in modeling.py. #### Downsampling the BAM file to generate more training examples. When generating the training set, we can make some adjustment to create more; training data. For example, when we train the released WGS model for; DeepVariant, for each BAM file, we created an extra set of training examples; using `--downsample_fraction=0.5`, which downsamples the reads and creates; training examples with lower coverage. We found that this makes the trained; model more robust. [GPU machine]: deepvariant-details.md#command-for-a-gpu-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:8147,Safety,avoid,avoid,8147,"tion set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the vir",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:4143,Security,validat,validation,4143,"_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_*"" ""${DATA_DIR}""; ```. ### Download extra packages. ```bash; sudo apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized traini",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:9666,Security,access,access,9666,"n/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machine. ```bash; sudo apt install -y python3.8-venv; # Create a virtualenv; python3 -m venv beam. # Activate the virtualenv; . beam/bin/activate; ```. Consult the instructions at https://beam.apache.org/get-started/quickstart-py/; if you run into any issues. Then, get the script that performs shuffling:. ```bash; mkdir -p ${SHUFFLE_SCRIPT_DIR}; wget https://raw.githubusercontent.com/google/deepvariant/r1.6.1/tools/shuffle_tfrecords_beam.py -O ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py; ```. Next, we shuffle the data using DataflowRunner. Before that, please make sure; you enable Dataflow API for your project:; http://console.cloud.google.com/flows/enableapi?apiid=dataflow. To access `gs://` path, make sure you run this in your virtual environment:. ```bash; sudo apt -y update && sudo apt -y install python3-pip; pip3 install --upgrade pip; pip3 install setuptools --upgrade; pip3 install apache_beam[gcp]==2.50.0 # 2.51.0 didn't work in my run.; pip3 install tensorflow # For parsing tf.Example in shuffle_tfrecords_beam.py.; ```. Shuffle using Dataflow. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_BUCKET}""/training_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_BUCKET}/training_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DataflowRunner \; --staging_location=""${OUTPUT_BUCKET}/staging"" \; --temp_location=""${OUTPUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://cons",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:11554,Security,validat,validation,11554,"PUT_BUCKET}/tempdir"" \; --save_main_session \; --region us-east1; ```. Then, you should be able to see the run on:; https://console.cloud.google.com/dataflow?project=YOUR_PROJECT. In order to have the best performance, you might need extra resources such as; machines or IPs within a region. That will not be in the scope of this case; study here. The output path can be found in the dataset_config file by:. ```bash; gsutil cat ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; ```. In the output, the `tfrecord_path` should be valid paths in gs://. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 44516; # class1: 173673; # class2: 124569; #; # --input_pattern_list=OUTPUT_BUCKET/training_set.with_label.tfrecord-?????-of-00016.gz; # --output_pattern_prefix=OUTPUT_BUCKET/training_set.with_label.shuffled; #. name: ""HG001""; tfrecord_path: ""OUTPUT_GCS_BUCKET/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; num_examples: 342758; ```. We can shuffle the validation set locally using; [DirectRunner](https://beam.apache.org/documentation/runners/direct/). Adding; `--direct_num_workers=0` sets the number of threads/subprocess to the number of; cores of the machine where the pipeline is running. ```bash; time python3 ${SHUFFLE_SCRIPT_DIR}/shuffle_tfrecords_beam.py \; --project=""${YOUR_PROJECT}"" \; --input_pattern_list=""${OUTPUT_DIR}""/validation_set.with_label.tfrecord-?????-of-00016.gz \; --output_pattern_prefix=""${OUTPUT_DIR}/validation_set.with_label.shuffled"" \; --output_dataset_name=""HG001"" \; --output_dataset_config_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --job_name=shuffle-tfrecords \; --runner=DirectRunner \; --direct_num_workers=0; ```. Here is the validation_set:. ```bash; cat ""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt""; ```. ```; # Generated by shuffle_tfrecords_beam.py; # class0: 5591; # class1: 31854; # class2: 21956; #; # --input_pattern_list=OUTPUT_DIR/validation_set.with_label.tfrecord-?????-of-00016.gz; # --output",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14178,Security,validat,validation,14178,"this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOI",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:2085,Testability,log,logged,2085,"aseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:2808,Testability,log,logs,2808,"ct ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; following command:. ```bash; gcloud compute ssh ${host} --zone ${zone}; ```. Once you have logged in, set the variables:. ```bash; YOUR_PROJECT=REPLACE_WITH_YOUR_PROJECT; OUTPUT_GCS_BUCKET=REPLACE_WITH_YOUR_GCS_BUCKET. BUCKET=""gs://deepvariant""; VERSION=""1.6.1""; DOCKER_IMAGE=""google/deepvariant:${VERSION}"". MODEL_BUCKET=""${BUCKET}/models/DeepVariant/${VERSION}/DeepVariant-inception_v3-${VERSION}+data-wgs_standard""; GCS_PRETRAINED_WGS_MODEL=""${MODEL_BUCKET}/model.ckpt"". OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir"". BASE=""${HOME}/training-case-study""; DATA_BUCKET=gs://deepvariant/training-case-study/BGISEQ-HG001. INPUT_DIR=""${BASE}/input""; BIN_DIR=""${INPUT_DIR}/bin""; DATA_DIR=""${INPUT_DIR}/data""; OUTPUT_DIR=""${BASE}/output""; LOG_DIR=""${OUTPUT_DIR}/logs""; SHUFFLE_SCRIPT_DIR=""${HOME}/deepvariant/tools"". REF=""${DATA_DIR}/ucsc_hg19.fa""; BAM_CHR1=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr1.bam""; BAM_CHR20=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr20.bam""; BAM_CHR21=""${DATA_DIR}/BGISEQ_PE100_NA12878.sorted.chr21.bam""; TRUTH_VCF=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_PGandRTGphasetransfer_chrs_FIXED.vcf.gz""; TRUTH_BED=""${DATA_DIR}/HG001_GRCh37_GIAB_highconf_CG-IllFB-IllGATKHC-Ion-10X-SOLID_CHROM1-X_v.3.3.2_highconf_nosomaticdel_chr.bed"". N_SHARDS=16; ```. ## Download binaries and data. ### Create directories:. ```bash; mkdir -p ""${OUTPUT_DIR}""; mkdir -p ""${BIN_DIR}""; mkdir -p ""${DATA_DIR}""; mkdir -p ""${LOG_DIR}""; ```. ### Copy data. ```bash; gsutil -m cp ${DATA_BUCKET}/BGISEQ_PE100_NA12878.sorted.chr*.bam* ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/ucsc_hg19.fa*"" ""${DATA_DIR}""; gsutil -m cp -r ""${DATA_BUCKET}/HG001_GRCh37_GIAB_",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:4902,Testability,test,testing,4902," apt -y update; sudo apt -y install parallel; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/install_nvidia_docker.sh; bash -x install_nvidia_docker.sh; ```. ## Run make_examples in “training” mode for training and validation sets. Create examples in ""training"" mode (which means these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHAR",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:6110,Testability,log,log,6110," training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting using the; `--channels ""insert_size""` flag. And, the make_examples step creates; `*.example_info.json` files. For example, you can see it here:. ```; cat ""${OUTPUT_DIR}/training_set.with_label.tfrecord-00000-of-00016.gz.example_info.json""; ```. ```json; {; ""version"": ""1.6.1"",; ""shape"": [100, 221, 7],; ""channels"": [1, 2, 3, 4, 5, 6, 19]; }; ```. Depending on your data type, you might want to tweak the flags for the; `make_examples` step, which can result in different shape of the output; examples. We will want to shuffle this on Dataflow later, so we copy the data to GCS; bucket first:. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time s",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:7610,Testability,log,log,7610," for the; `make_examples` step, which can result in different shape of the output; examples. We will want to shuffle this on Dataflow later, so we copy the data to GCS; bucket first:. ```; gsutil -m cp ${OUTPUT_DIR}/training_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. NOTE: If you prefer shuffling locally, please take a look at this user-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:7957,Testability,log,logic,7957,"-provided; shuffler option:; https://github.com/google/deepvariant/issues/360#issuecomment-1019990366. ### Validation set. ```; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v /home/${USER}:/home/${USER} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR21}"" \; --examples ""${OUTPUT_DIR}/validation_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr21'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/validation_set.with_label.make_examples.log""; ```. This took: 5m31.122s. Copy to GCS bucket:. ```bash; gsutil -m cp ${OUTPUT_DIR}/validation_set.with_label.tfrecord-?????-of-00016.gz* \; ${OUTPUT_BUCKET}; ```. ## Shuffle each set of examples and generate a data configuration file for each. Shuffling the `tensorflow.Example`s is an important step for training a model.; In our training logic, we shuffle examples globally using a preprocessing step. First, if you have run this step before, and want to rerun it, you might want to; consider cleaning up previous data first to avoid confusion:. ```bash; # (Optional) Clean up existing files.; gsutil -m rm -f ""${OUTPUT_BUCKET}/training_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt""; gsutil -m rm -f ""${OUTPUT_BUCKET}/validation_set.with_label.shuffled-?????-of-?????.tfrecord.gz""; gsutil rm -f ""${OUTPUT_BUCKET}/validation_set.dataset_config.pbtxt""; gsutil rm -f ""${OUTPUT_BUCKET}/example_info.json""; ```. Here we provide examples for running on; [Cloud Dataflow Runner](https://beam.apache.org/documentation/runners/dataflow/); and also [DirectRunner](https://beam.apache.org/documentation/runners/direct/).; Beam can also use other runners, such as; [Spark Runner](https://beam.apache.org/documentation/runners/spark/). First, create a virtual environment to install beam on your machi",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:13932,Testability,log,log,13932,"rridden when; we run `train` by passing `--config.<param>=<value>`. ```bash; curl https://raw.githubusercontent.com/google/deepvariant/r1.6.1/deepvariant/dv_config.py > dv_config.py; ```. ### Start `train`. NOTE: all parameters below are used as an example. They are not optimized for; this dataset, and are not recommended as the best default either. ```bash; ( time sudo docker run --gpus 1 \; -v /home/${USER}:/home/${USER} \; -w /home/${USER} \; ${DOCKER_IMAGE}-gpu \; train \; --config=dv_config.py:base \; --config.train_dataset_pbtxt=""${OUTPUT_BUCKET}/training_set.dataset_config.pbtxt"" \; --config.tune_dataset_pbtxt=""${OUTPUT_DIR}/validation_set.dataset_config.pbtxt"" \; --config.init_checkpoint=gs://deepvariant/models/DeepVariant/1.6.1/checkpoints/wgs/deepvariant.wgs.ckpt \; --config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRA",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:14698,Testability,test,tested,14698,"config.num_epochs=10 \; --config.learning_rate=0.0001 \; --config.num_validation_examples=0 \; --experiment_dir=${TRAINING_DIR} \; --strategy=mirrored \; --config.batch_size=512 \; ) > ""${LOG_DIR}/train.log"" 2>&1 &; ```. Once training starts, you should see a summary of your training dataset:. ```; Training Examples: 342758; Batch Size: 512; Epochs: 10; Steps per epoch: 669; Steps per tune: 116; Num train steps: 6690; ```. As training runs, the validation/tune dataset will be evaluated at the end of; each epoch, and every n training steps specified by `--config.tune_every_steps`.; You can lower `--config.tune_every_steps` to perform evaluation more frequently. Checkpoints are stored whenever the `tune/f1_weighted` metric improves when; evaluating the tune dataset. In this way, the last checkpoint stored will always; be the best performing checkpoint. The best performing checkpoint metric can be; configured using `--config.best_checkpoint_metric`. We have tested training with 1 and 2 GPUs and observed the following runtimes:. n GPUs | Time; ------ | ----------; 1 | 89m39.451s; 2 | 54m8.163s. Once training is complete, the following command can be used list checkpoints:. ```bash; gsutil ls ${TRAINING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:15983,Testability,log,logdir,15983,"ING_DIR}/checkpoints/; ```. The best checkpoint can be retrieved using the following command:. ```bash; BEST_CHECKPOINT=$(gsutil cat ${TRAINING_DIR}/checkpoints/checkpoint | sed -n 's/model_checkpoint_path: ""\(.*\)""/\1/p'); BEST_CHECKPOINT=${TRAINING_DIR}/checkpoints/${BEST_CHECKPOINT}; ```. ### (Optional) Use TensorBoard to visualize progress. We can start a TensorBoard to visualize the progress of training better. This; step is optional. You'll want to let `train` run for a while before you start a TensorBoard. (You; can start a TensorBoard immediately, but you just won't see the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:16513,Testability,test,test,16513," the metrics summary; until later.); We did this through a Google Cloud Shell from https://console.cloud.google.com,; on the top right:. ![Shell](images/ActivateShell.png?raw=true ""Activate Google Cloud Shell""). This opens up a terminal at the bottom of the browser page, then run:. ```bash; # Change to your OUTPUT_BUCKET from earlier.; OUTPUT_BUCKET=""${OUTPUT_GCS_BUCKET}/customized_training""; TRAINING_DIR=""${OUTPUT_BUCKET}/training_dir""; tensorboard --logdir ${TRAINING_DIR} --port=8080; ```. After it started, I clicked on the “Web Preview” on the top right of the mini; terminal:. ![WebPreview](images/WebPreview.png?raw=true ""Web Preview""). And clicked on ""Preview on port 8080"":. ![PreviewOnPort](images/PreviewOnPort.png?raw=true ""Preview on Port 8080""). Once it starts, you can see many metrics, including accuracy, speed, etc. You; will need to wait for `train` to run for a while before the plots will appear. ### Test the model. Now that we have performed training, we can test the performance of the new; model using our holdout dataset (chr20). The following one-step command can be used to call DeepVariant and run our newly; trained model:. ```bash; sudo docker run --gpus all \; -v /home/${USER}:/home/${USER} \; ""${DOCKER_IMAGE}-gpu"" \; run_deepvariant \; --model_type WGS \; --customized_model ""${BEST_CHECKPOINT}"" \; --ref ""${REF}"" \; --reads ""${BAM_CHR20}"" \; --regions ""chr20"" \; --output_vcf ""${OUTPUT_DIR}/test_set.vcf.gz"" \; --num_shards=${N_SHARDS}; ```. In v1.4.0, by using `--model_type WGS`, `run_deepvariant` will automatically add; `insert_size` as an extra channel in the `make_examples` step. So we don't need; to add it in `--make_examples_extra_args`. When the `call_variants` step is run, you might see messages like:. ```; E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. You can use `nvidia-smi` to confirm whether the GPUs are used. If so, you can;",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:385,Usability,learn,learning,385,"# Advanced Case Study: Train a customized SNP and small indel variant caller for BGISEQ-500 data. DeepVariant is an analysis pipeline that uses a deep neural network to call; genetic variants from next-generation DNA sequencing (NGS) data. While; DeepVariant is highly accurate for; [many types of NGS data](https://rdcu.be/7Dhl), some users may be interested in; training custom deep learning models that have been optimized for very specific; data. This case study describes one way to train such a custom model using a GPU, in; this case for BGISEQ-500 data. Please note that there is not yet a production-grade training pipeline. This is; just one example of how to train a custom model, and is neither the fastest nor; the cheapest possible configuration. The resulting model also does not represent; the greatest achievable accuracy for BGISEQ-500 data. ## High level summary of result. We demonstrated that by training on 1 replicate of BGISEQ-500 whole genome data; (everything except for chromosome 20-22), we can significantly improve the; accuracy comparing to the WGS model as a baseline:. * Indel F1 `94.1615%` --> `98.1937%`; * SNP F1: `99.8785%` --> `99.9042%`. This tutorial is meant as an example for training; all the other processing in; this tutorial were done serially with no pipeline optimization. ## Request a machine. For this case study, we use a [GPU machine] with 16 vCPUs. You can request this; machine on Google Cloud using the following command:. ```bash; host=""${USER}-deepvariant-vm""; zone=""us-west1-b"". gcloud compute instances create ${host} \; --scopes ""compute-rw,storage-full,cloud-platform"" \; --maintenance-policy ""TERMINATE"" \; --accelerator=type=nvidia-tesla-p100,count=1 \; --image-family ""ubuntu-2004-lts"" \; --image-project ""ubuntu-os-cloud"" \; --machine-type ""n1-standard-16"" \; --boot-disk-size ""300"" \; --zone ""${zone}"" \; --min-cpu-platform ""Intel Skylake""; ```. After a minute or two, your VM should be ready and you can ssh into it using the; followi",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:5192,Usability,learn,learning,5192,"ns these `tensorflow.Example`s will; contain a `label` field). In this tutorial, we create examples on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md:5298,Usability,learn,learning,5298,"es on one replicate of HG001 sequenced by; BGISEQ-500 provided on the; [Genome In a Bottle FTP site](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/NA12878/BGISEQ500/standard_library/readme.txt). In this tutorial, we will split the genome up into the following datasets:. | chrom | Name | Description |; | ----- | --------------------- | -------------------------------------------- |; | chr1 | Training Set | Examples used to train our model. |; | chr21 | Validation / Tune Set | Examples used to evaluate the performance of our model during training.|; | chr20 | Test Set | Examples reserved for testing performance of our trained model. |. Note that normally, the training dataset will be much larger (e.g. chr1-19),; rather than just a single chromosome. We use just chr1 here to demonstrate how; customized training works. For the definition of these 3 sets in commonly used machine learning; terminology, please refer to; [Machine Learning Glossary](https://developers.google.com/machine-learning/glossary/). ### Training set. First, to set up, lets pull the docker images. ```bash; sudo docker pull ${DOCKER_IMAGE} # Standard CPU Docker Image.; sudo docker pull ${DOCKER_IMAGE}-gpu # GPU-enabled Docker image.; ```. The `make_examples` step doesn't use GPU, so we will not require the GPU-enabled; image. ```bash; ( time seq 0 $((N_SHARDS-1)) | \; parallel --halt 2 --line-buffer \; sudo docker run \; -v ${HOME}:${HOME} \; ${DOCKER_IMAGE} \; make_examples \; --mode training \; --ref ""${REF}"" \; --reads ""${BAM_CHR1}"" \; --examples ""${OUTPUT_DIR}/training_set.with_label.tfrecord@${N_SHARDS}.gz"" \; --truth_variants ""${TRUTH_VCF}"" \; --confident_regions ""${TRUTH_BED}"" \; --task {} \; --regions ""'chr1'"" \; --channels ""insert_size"" \; ) 2>&1 | tee ""${LOG_DIR}/training_set.with_label.make_examples.log""; ```. This took `20m14s`. Starting in v1.4.0, we added an extra channel in our WGS setting using the; `--channels ""insert_size""` flag. And, the make_examples step creates; `*.example_info",MatchSource.DOCS,docs/deepvariant-training-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-training-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vcf-stats-report.md:4745,Testability,log,log,4745,"0/2 both; become Het (0/x). 1/1 and 3/3 are Hom (x/x). Het - both variants (x/y) includes; all calls with two different alternate alleles, such as 1/2 or 3/5. ### Biallelic base changes. Of all biallelic SNPs, this shows the counts from a particular REF (along the; top labeling the four charts) to a particular ALT (each bar within the charts; labeled at the bottom). See the Ti/Tv section for a brief explanation of why; some of these base changes tend to be more frequent than others. RefCalls and; multi-allelic variants are not included. ### Biallelic Ti/Tv ratio. Transition (Ti) count is the number of biallelic SNPs going from purine->purine; or pyrimidine->pyrimidine, where purines are A and G, pyrimidines are C and T.; Transversions (Tv) are purine->pyrimidine or pyrimidine->purine. Transitions; are biologically more likely to occur than transversions due to the molecular; structure of the bases, so a ratio well above one is desirable. This; [article](https://gatkforums.broadinstitute.org/gatk/discussion/6308/evaluating-the-quality-of-a-variant-callset); on the GATK forums has a good discussion of how to interpret the ratio. These; include all biallelic SNPs, excluding RefCalls. ### Biallelic indel size distribution. The sizes of all biallelic insertions and deletions are shown as histograms. The; top and bottom plots show the same data, just on a linear scale on top and on a; log scale on the bottom. RefCalls and multi-allelic variants are not included. ## Examples. ### WGS case study HG002 (DeepVariant v0.10.0). ![visual report of WGS HG002 case study v0.10.0](images/WGS_HG002.v0.10.0_visual_report.png). ### WES case study HG002 (DeepVariant v0.10.0). ![visual report of WES HG002 case study v0.10.0](images/WES_HG002.v0.10.0_visual_report.png). ### PacBio case study HG002 (DeepVariant v0.10.0). ![visual report of PacBio HG002 case study v0.10.0](images/PacBio_HG002.v0.10.0_visual_report.png). [VCF specification]: https://samtools.github.io/hts-specs/VCFv4.3.pdf; ",MatchSource.DOCS,docs/deepvariant-vcf-stats-report.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vcf-stats-report.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vcf-stats-report.md:2902,Usability,guid,guiding,2902,"lls. Any entries without a DP are ignored. ### Quality score. This is the QUAL column of the VCF file. See the [VCF specification] for how; this is calculated, but in short, a high QUAL score indicates a low probability; that the call shown in ALT is wrong. This chart shows quality scores from all; rows of the VCF file, including RefCalls. ### Genotype quality. These numbers come from the GQ sub-column, listed in the FORMAT column of the; VCF file. An important distinction here is that if the variant-caller is very; sure that the variant is there, but not sure if it is heterozygous or; homozygous, the QUAL score can be very high, but the genotype quality score (GQ); can be low. GQ is on a Phred scale, calculated as -10*log10(probability that the; genotype is wrong). Any entries without a GQ are ignored. ### Variant allele frequency for all genotypes. The histograms show the variant allele frequency (VAF) distributions for; different genotypes. Black guiding lines are shown to indicate the theoretical; VAF for the main genotypes. For example heterozygous variants should have about; as many variant-supporting as reference-supporting reads, for a VAF of 0.5. The; reference calls will not usually show a VAF as low as 0 because otherwise they; wouldn’t have been flagged as candidates in the first place. The genotypes are; based on the GT sub-column and consolidated. For example, 0/1 and 0/2 both; become Het (0/x). 1/1 and 3/3 are Hom (x/x). Het - both variants (x/y) includes; all calls with two different alternate alleles, such as 1/2 or 3/5. ### Biallelic base changes. Of all biallelic SNPs, this shows the counts from a particular REF (along the; top labeling the four charts) to a particular ALT (each bar within the charts; labeled at the bottom). See the Ti/Tv section for a brief explanation of why; some of these base changes tend to be more frequent than others. RefCalls and; multi-allelic variants are not included. ### Biallelic Ti/Tv ratio. Transition (Ti) count is t",MatchSource.DOCS,docs/deepvariant-vcf-stats-report.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vcf-stats-report.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:917,Availability,down,download,917,"# Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR ",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:1071,Availability,down,download,1071,"This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${D",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:1226,Availability,down,download,1226,"h 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${DATA_DIR}/kmc -k29 -m128 -okff -t$(nproc) @HG003.fq.paths ${DATA_DIR}/HG003.fq $TMPDIR; ```. Output on the terminal:. ```; **************************************",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:496,Deployability,update,update,496,"# Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR ",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:523,Deployability,install,install,523,"# Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR ",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:908,Deployability,release,releases,908,"# Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR ",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:1062,Deployability,release,releases,1062,"This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR for a; scratch directory:. ```bash; TMPDIR=$(mktemp -d); time ${D",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6270,Deployability,release,release,6270,"RSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Ben",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:3549,Testability,log,log,3549,"ique counted k-mers : 2753735220; Total no. of k-mers : 103092565745; Total no. of reads : 838385300; Total no. of super-k-mers : 9929565346. real 24m11.431s; user 142m37.817s; sys 8m14.566s; ```. Run `giraffe`` on the graph, haplotype index, kmers and reads:. ```bash; ${DATA_DIR}/vg paths \; -x ${DATA_DIR}/hprc-v1.1-mc-grch38.gbz \; -L -Q GRCh38 > ${DATA_DIR}/GRCh38.path_list.txt; ```. ```bash; time ${DATA_DIR}/vg giraffe --progress \; --read-group ""ID:1 LB:lib1 SM:HG003 PL:illumina PU:unit1"" \; --sample ""HG003"" \; -o BAM --ref-paths ${DATA_DIR}/GRCh38.path_list.txt \; -P -L 3000 \; -f ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz \; -f ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz \; -Z ${DATA_DIR}/hprc-v1.1-mc-grch38.gbz \; --kff-name ${DATA_DIR}/HG003.fq.kff \; --haplotype-name ${DATA_DIR}/hprc-v1.1-mc-grch38.hapl \; -t $(nproc) > reads.unsorted.bam; ```. NOTE: No need to sort this yet, because we'll need to sort it in the next step. ## Runtime. On my machine, the last few lines of the log showed:. ```; Mapped 838385300 reads across 64 threads in 14093.4 seconds with 3.25431 additional single-threaded seconds.; Mapping speed: 929.496 reads per second per thread; Used 896175 CPU-seconds (including output).; Achieved 935.515 reads per CPU-second (including output); Memory footprint: 61.0703 GB. real 283m10.368s; user 15260m35.845s; sys 214m57.882s; ```. File size:. ```; $ ls -lh reads.unsorted.bam; -rw-rw-r-- 1 pichuan pichuan 69G Nov 1 23:56 reads.unsorted.bam; ```. Then, clean up contig names, and sort:. ```bash; INBAM=reads.unsorted.bam; BAM=reads.sorted.chrfixed.bam; time samtools view -h $INBAM | sed -e ""s/GRCh38#0#//g"" | samtools sort --threads 10 -m 2G -O BAM > ${BAM}; # Index the BAM.; samtools index -@$(nproc) ${BAM}; ```. The step with `time` above took:. ```; real 73m19.172s; user 178m59.088s; sys 24m36.986s; ```. File size:. ```; $ ls -lh reads.sorted.chrfixed.bam; -rw-rw-r-- 1 pichuan pichuan 40G Nov 2 02:09 reads.sorted.chrfixed.bam; ```. #",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:5129,Testability,test,test,5129,"38#0#//g"" | samtools sort --threads 10 -m 2G -O BAM > ${BAM}; # Index the BAM.; samtools index -@$(nproc) ${BAM}; ```. The step with `time` above took:. ```; real 73m19.172s; user 178m59.088s; sys 24m36.986s; ```. File size:. ```; $ ls -lh reads.sorted.chrfixed.bam; -rw-rw-r-- 1 pichuan pichuan 40G Nov 2 02:09 reads.sorted.chrfixed.bam; ```. ## Run DeepVariant With `min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true`. Get the same reference we used for; [DeepVariant Case Study](deepvariant-case-study.md). ```bash; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids. curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; samtools faidx ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna; ```. And then, run DeepVariant. (If you want to test on one smaller chromosome first, you can add; `--regions chr20` like what we did in; [DeepVariant Case Study](deepvariant-case-study.md).). ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6210,Testability,benchmark,benchmark,6210,"at we did in; [DeepVariant Case Study](deepvariant-case-study.md).). ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". time sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o $",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6404,Testability,benchmark,benchmark,6404,""" \; -v ""${PWD}:${PWD}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WGS \; --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Pr",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6528,Testability,benchmark,benchmark,6528," --ref=${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; --reads=${PWD}/${BAM} \; --output_vcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.h",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6644,Testability,benchmark,benchmark,6644,"min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; --output_gvcf=${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.g.vcf.gz \; --make_examples_extra_args=""min_mapping_quality=1,keep_legacy_allele_counter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:6912,Testability,benchmark,benchmark,6912,"nter_behavior=true,normalize_reads=true"" \; --num_shards=$(nproc); ```. Stage | Time (minutes); -------------------------------- | -----------------; make_examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; INDEL PASS 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; SNP ALL 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:7069,Testability,benchmark,benchmark,7069,"examples | 116m37.385s; call_variants | 214m37.055s; postprocess_variants (with gVCF) | 30m59.968s. ### Run hap.py. ```bash; mkdir -p benchmark. FTPDIR=ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh38. curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz; curl ${FTPDIR}/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi > benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz.tbi; ```. ```bash; mkdir -p happy. sudo docker pull jmcdani20/hap.py:v0.3.12. sudo docker run --rm \; -v ""${DATA_DIR}"":""${DATA_DIR}"" \; -v ""${PWD}:${PWD}"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark.vcf.gz \; ${PWD}/min_mapping_quality-keep_legacy_allele_counter_behavior-normalize_reads-vg.vcf.gz \; -f ${PWD}/benchmark/HG003_GRCh38_1_22_v4.2.1_benchmark_noinconsistent.bed \; -r ${DATA_DIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna \; -o ${PWD}/happy/happy.output \; --engine=vcfeval \; --pass-only; ```. Output:. ```; Benchmarking Summary:; Type Filter TRUTH.TOTAL TRUTH.TP TRUTH.FN QUERY.TOTAL QUERY.FP QUERY.UNK FP.gt FP.al METRIC.Recall METRIC.Precision METRIC.Frac_NA METRIC.F1_Score TRUTH.TOTAL.TiTv_ratio QUERY.TOTAL.TiTv_ratio TRUTH.TOTAL.het_hom_ratio QUERY.TOTAL.het_hom_ratio; INDEL ALL 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; INDEL PASS 504501 502199 2302 960061 1526 434935 906 371 0.995437 0.997094 0.453029 0.996265 NaN NaN 1.489759 1.952023; SNP ALL 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436586; SNP PASS 3327496 3316515 10981 3858659 5550 534709 2104 475 0.996700 0.998330 0.138574 0.997514 2.102576 1.970783 1.535137 1.436586; ```. | Type | TRUT",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md:143,Usability,simpl,simplicity,143,"# Using graph genomes: VG Giraffe + DeepVariant case study; ---. This is an example to run `vg giraffe`, so we can go from FASTQs --> BAM. For simplicity and consistency, we run the following with a; [Google Cloud instance with 64 cores](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). I added more disks because 300G is not enough for the example below. I changed; it to `--boot-disk-size ""1000""`. ## Install softwares that will be used later. ```bash; sudo apt update -y; sudo apt-get -y install aria2 docker.io samtools; ```. ## Download input FASTQ files. ```bash; DATA_DIR=${PWD}/data; mkdir -p ${DATA_DIR}; gcloud storage cp gs://brain-genomics-public/research/sequencing/fastq/novaseq/wgs_pcr_free/35x/HG003.novaseq.pcr-free.35x.R?.fastq.gz ${DATA_DIR}/; ```. ## Download VG files. Get binaries `vg` 1.51.0 and `kmc`:. ```bash; wget https://github.com/refresh-bio/KMC/releases/download/v3.2.2/KMC3.2.2.linux.x64.tar.gz; tar zxf KMC3.2.2.linux.x64.tar.gz bin/kmc; mv bin/kmc ${DATA_DIR}/; wget https://github.com/vgteam/vg/releases/download/v1.51.0/vg -O ${DATA_DIR}/vg; chmod +x ${DATA_DIR}/vg ${DATA_DIR}/kmc; ```. Get the graph (.gbz) and haplotype index (.hapl).; I used `aria2c` to download these files. You can use other approaches as well. ```bash; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.gbz; aria2c -c -x10 -s10 -d ""${DATA_DIR}"" https://s3-us-west-2.amazonaws.com/human-pangenomics/pangenomes/freeze/freeze1/minigraph-cactus/hprc-v1.1-mc-grch38/hprc-v1.1-mc-grch38.hapl; ```. ## Run `vg giraffe` with one command to get from FASTQs to BAM. Put the paths name into a file named HG003.fq.paths:. ```bash; cat > HG003.fq.paths <<- EOM; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R1.fastq.gz; ${DATA_DIR}/HG003.novaseq.pcr-free.35x.R2.fastq.gz; EOM; ```. Run `kmc`` on this file. I used -t$(nproc) to use all cores, and $TMPDIR ",MatchSource.DOCS,docs/deepvariant-vg-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-vg-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:917,Availability,down,download,917,"# Calling variants in non-autosomal contigs. For details about the support for haploid contigs, please read; [DeepVariant haploid support](deepvariant-haploid-support.md). In this case study, we describe how to call variants in non-autosomal regions; like X, Y chromosomes. Then we assess the quality of the DeepVariant variant; calls with `hap.py`. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: ChrX, ChrY; Platform: PacBio; Sample Karyotype: X, Y; ```. ## Prepare environment. In this case study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:3610,Deployability,release,release,3610,"cker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""${OUTPUT_DIR}/hg002.chrXY.happy.output"" \; --engine=vcfeval \; --pass-only \; -l ""${REGION}""; ```. Output:. ```; Benchmark",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:2063,Modifiability,variab,variables,2063,"reate local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLO",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:2268,Modifiability,variab,variable,2268,"05.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVaria",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:3346,Performance,perform,performance,3346,"; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:700,Testability,benchmark,benchmarking,700,"# Calling variants in non-autosomal contigs. For details about the support for haploid contigs, please read; [DeepVariant haploid support](deepvariant-haploid-support.md). In this case study, we describe how to call variants in non-autosomal regions; like X, Y chromosomes. Then we assess the quality of the DeepVariant variant; calls with `hap.py`. The dataset used in this case-study has following attributes:. ```bash; Sample: HG002; Region: ChrX, ChrY; Platform: PacBio; Sample Karyotype: X, Y; ```. ## Prepare environment. In this case study, we will use [Docker](https://docs.docker.com/get-docker/) to; run DeepVariant for variant calling and; [hap.py](https://github.com/illumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:1680,Testability,test,testdata,1680,"lumina/hap.py) for benchmarking. If you want to run on GPU machines, or use `Singularity` instead of `Docker`,; please follow [Quick Start](deepvariant-quick-start.md) documentation. ### Create input and output directory structures and download inputs. ```bash; BASE=""${HOME}/XY-walkthrough"". # Set up input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker ",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:1976,Testability,test,testdata,1976,"p input and output directory data; INPUT_DIR=""${BASE}/input""; OUTPUT_DIR=""${BASE}/output"". ## Create local directory structure; mkdir -p ""${INPUT_DIR}""; mkdir -p ""${OUTPUT_DIR}/data"". # Download reference to input directory; FTPDIR=ftp://ftp.ncbi.nlm.nih.gov/genomes/all/GCA/000/001/405/GCA_000001405.15_GRCh38/seqs_for_alignment_pipelines.ucsc_ids; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.gz | gunzip > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta; curl ${FTPDIR}/GCA_000001405.15_GRCh38_no_alt_analysis_set.fna.fai > ${INPUT_DIR}/GRCh38_no_alt_analysis_set.fasta.fai. # Download bam file to input directory; HTTPDIR=https://storage.googleapis.com/deepvariant/xy-case-study-testdata; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam; curl ${HTTPDIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai > ${INPUT_DIR}/HG002.pfda_challenge.grch38.chrXY.bam.bai. HTTPDIR=https://storage.googleapis.com/deepvariant/case-study-testdata; curl ${HTTPDIR}/GRCh38_PAR.bed > ${INPUT_DIR}/GRCh38_PAR.bed. # Set up input variables; REF=""GRCh38_no_alt_analysis_set.fasta""; BAM=""HG002.pfda_challenge.grch38.chrXY.bam""; THREADS=$(nproc); REGION=""chrX chrY""; HAPLOID_CONTIGS=""chrX,chrY""; PAR_BED=""GRCh38_PAR.bed"". # Set up output variable; OUTPUT_VCF=""HG002_pacbio_hifi.chrXY.output.vcf.gz""; OUTPUT_GVCF=""HG002_pacbio_hifi.chrXY.output.g.vcf.gz""; INTERMEDIATE_DIRECTORY=""intermediate_results_dir"". mkdir -p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --outpu",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:3427,Testability,benchmark,benchmark,3427,"-p ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Run DeepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${IN",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md:3509,Testability,benchmark,benchmarks,3509,"eepVariant. We will run DeepVariant from docker using the `run_deepvariant` script. ```bash; BIN_VERSION=""1.6.1"". sudo docker pull google/deepvariant:""${BIN_VERSION}"". sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/run_deepvariant \; --model_type PACBIO \; --ref ""${INPUT_DIR}/${REF}"" \; --reads ""${INPUT_DIR}/${BAM}"" \; --output_vcf ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; --output_gvcf ""${OUTPUT_DIR}/${OUTPUT_GVCF}"" \; --num_shards ""${THREADS}"" \; --haploid_contigs ""${HAPLOID_CONTIGS}"" \; --par_regions_bed ""${INPUT_DIR}/${PAR_BED}"" \; --regions ""${REGION}"" \; --intermediate_results_dir ""${OUTPUT_DIR}/${INTERMEDIATE_DIRECTORY}""; ```. ## Benchmark X, Y outputs from DeepVariant. We will use Genome-in-a-Bottle (GIAB) dataset to evaluate the performance of; DeepVariant. ### Download Genome in a Bottle Benchmarks. We will benchmark our variant calls against v1.0 of the Genome in a Bottle; small variant benchmarks for HG002_chrXY. ```bash; FTPDIR=https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/AshkenazimTrio/HG002_NA24385_son/chrXY_v1.0/GRCh38/SmallVariant. curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.bed; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz; curl ${FTPDIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi > ${INPUT_DIR}/HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz.tbi. TRUTH_VCF=""HG002_GRCh38_chrXY_smallvar_v1.0.vcf.gz""; TRUTH_BED=""HG002_GRCh38_chrXY_smallvar_v1.0.bed""; ```. ```bash; sudo docker pull jmcdani20/hap.py:v0.3.12. REGION=""chrX,chrY""; sudo docker run \; -v ""${INPUT_DIR}"":""${INPUT_DIR}"" \; -v ""${OUTPUT_DIR}"":""${OUTPUT_DIR}"" \; -v ""${PWD}/happy:/happy"" \; jmcdani20/hap.py:v0.3.12 /opt/hap.py/bin/hap.py \; ""${INPUT_DIR}/${TRUTH_VCF}"" \; ""${OUTPUT_DIR}/${OUTPUT_VCF}"" \; -f ""${INPUT_DIR}/${TRUTH_BED}"" \; -r ""${INPUT_DIR}/${REF}"" \; -o ""$",MatchSource.DOCS,docs/deepvariant-xy-calling-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/deepvariant-xy-calling-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:4632,Availability,down,downsamples,4632,"on-human variant calling using species-specific DeepVariant models](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/)”. If these reasons seem applicable, there could be some other reason DeepVariant; determined the position is not variant. You can catalog the variant position and; its support. The way to improve variant calling for these positions is to train; new models, but be aware that training is already a balance between reducing; false negatives and positives, and it may not be possible to call variants like; the one you are seeing without increasing overall false positives by a greater; amount. ## How does DeepVariant use pileup images to call variants?. See this; [blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). ## What happens if I change the pileup_image_height?. If the actual depth in a particular region is greater than the pileup image; height, DeepVariant randomly downsamples reads until the image has been filled; up. For the default DeepVariant models (height 100), an image can accommodate at; most 95 reads in a given region (5 rows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for s",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:5743,Availability,avail,available,5743,"ows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8237,Availability,echo,echo,8237,"hanges to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8876,Availability,error,error,8876,"ide the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## ",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:9204,Availability,error,error,9204,"so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:10494,Availability,error,error-tolerant,10494,"current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ):. > Mapped reads are preprocessed using an error-tolerant, local; > De-Bruijn-graph-based read assembly procedure that realigns them according to; > their most likely derived haplotype. Candidate windows across the genome are; > selected for reassembly by looking for any evidence of possible genetic; > variation, such as mismatching or soft clipped bases. The selection criteria; > for a candidate window are very permissive so that true variation is unlikely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > select",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:12545,Availability,down,down,12545,"ost likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ##",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8486,Deployability,release,release,8486,"use the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:11882,Deployability,update,updates,11882,"likely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partiti",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:1355,Energy Efficiency,reduce,reduces,1355,"is; important to first determine whether a candidate variant was proposed by; DeepVariant. A potential variant requires at least 2 reads to support a variant; and a minimum fraction of reads supporting the variant (0.12 for SNPs and PacBio; Indels, 0.06 for Illumina Indels). All sites that have been generated as; candidates are written in the VCF file, so if you do not see a row in the VCF; file for the variant in question, it means that a candidate was not made.; However, within these sites certain possible alleles may have been pruned in; reporting. To see all alleles, you may add: `--debug_output_all_candidates=ALT`; in the postprocess_variants step. To increase the sensitivity of DeepVariant to these sites, you may add the; following parameters, here shown with their defaults:. ```; --make_examples_extra_args=""vsc_min_count_snps=2,vsc_min_fraction_snps=0.12,vsc_min_count_indels=2,vsc_min_fraction_indels=0.06""; ```. It is sometimes also the case that realignment of the reads within DeepVariant; changes or reduces the evidence supporting the variant. To check for this, try; using the `--norealign_reads` flag to turn off realignment temporarily. Note; that we don't recommend turning off the realigner for Illumina data in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In additi",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:12701,Energy Efficiency,efficient,efficiently,12701,"nally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provid",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:13455,Energy Efficiency,reduce,reduces,13455," variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provided by [Paul Grosu](https://github.com/pgrosu) in; this [issue thread](https://github.com/google/deepvariant/issues/684). We thank; Paul for providing a detailed description and reasoning. ## Singularity related questions:. ### `TMPDIR`. If you have issues with `TMPDIR` when running with Singularity, try adding this; to your command:. ```bash; export TMPDIR=""$PWD/tmp_dir""; ```. See https://github.com/google/deepvariant/issues/524#issuecomment-1067597987. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our; Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid thi",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:4998,Integrability,depend,depending,4998,"dels, but be aware that training is already a balance between reducing; false negatives and positives, and it may not be possible to call variants like; the one you are seeing without increasing overall false positives by a greater; amount. ## How does DeepVariant use pileup images to call variants?. See this; [blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). ## What happens if I change the pileup_image_height?. If the actual depth in a particular region is greater than the pileup image; height, DeepVariant randomly downsamples reads until the image has been filled; up. For the default DeepVariant models (height 100), an image can accommodate at; most 95 reads in a given region (5 rows are reserved for the reference; sequence). You may be able to successfully run our pretrained models with a different; pileup image height (via `--pileup_image_height` in `make_examples.py`),; depending on the new height, but we generally do not recommend using different; image heights at training and inference time. If you wish to use a different; pileup image height, we recommend retraining a new model with images of that; height. If you are working with extremely high coverage sequencing data for applications; such as somatic sequencing, we recommend using a somatic caller instead of; DeepVariant, which is a germline caller. ## Can I use DeepVariant for somatic (non-germline) calling?. We do not recommend using DeepVariant for somatic calling. We do have a; prototype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8943,Integrability,message,message,8943,"OUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for runn",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:9370,Integrability,message,message,9370," \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKy",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:11699,Integrability,depend,depend,11699,"cross the genome are; > selected for reassembly by looking for any evidence of possible genetic; > variation, such as mismatching or soft clipped bases. The selection criteria; > for a candidate window are very permissive so that true variation is unlikely; > to be missed. All candidate windows across the genome are considered; > independently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:6630,Performance,perform,perform,6630,"ype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output di",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:11972,Performance,perform,performed,11972,"pendently. De Bruijn graphs are constructed using multiple fixed k-mer; > sizes (from 20 to 75, inclusive, with increments of 5) out of the reference; > genome bases for the candidate window, as well as all overlapping reads. Edges; > are given a weight determined by how many times they are observed in the; > reads. We trim any edges with weight less than three, except that edges found; > in the reference are never trimmed. Candidate haplotypes are generated by; > traversing the assembly graphs and the top two most likely haplotypes are; > selected that best explain the read evidence. The likelihood function used to; > score haplotypes is a traditional pair HMM with fixed parameters that do not; > depend on base quality scores. This likelihood function assumes that each read; > is independent. Finally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:12713,Performance,perform,perform,12713,"nally, each read is then realigned to its most likely; > haplotype. This procedure updates both the position and the CIGAR string for; > each read. Local realignment is not performed for long reads (PacBio, and other similar; technologies). The realigner step can optionally be switched off using; `--norealign_reads`. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. This can be done by passing the following parameters:; `--make_examples_extra_args=""emit_realigned_reads=true,realigner_diagnostics=/output/realigned_reads""`. Note that this is meant for debugging and produces a bam file for every; candidate variant, which can result in millions of tiny bam files, so when using; this, narrow down the DeepVariant run using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provid",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:14451,Safety,avoid,avoid,14451,"un using `--regions` to just the variants you; want to inspect more closely. ## How are `AD` and `DP` values calculated?. In order to efficiently perform variant calling, DeepVariant partitions the; genome into chunks (set by `--partition_size`), and will read in a max number of; reads into each partition (set by `--max_reads_per_partition`). By default,; `--partition_size` is set to 1000 and `--max_reads_per_partition` is set to; 1500. The `AD` and `DP` values are based on the read depths constrained by; `--max_reads_per_partition`. For example, if you have a depth of 2000x at a given site, DeepVariant will; subsample 1500 reads, and `DP` or `AD` will be capped at 1500. If you want to; calculate the true `AD` and `DP` values at high-depth regions, you can set; `--max_reads_per_partition=0` to calculate `AD` and `DP` using all reads. In; practice, capping reads per partition reduces runtimes with little/no impact on; accuracy. ## Missing variant calls near the edge of a contig. This is a known issue that we don't currently address. Please see:; https://github.com/google/deepvariant/issues/505 for more context. ## Why does DeepVariant PASS variants that have such a low read depth ~2 ?. Please see the answers provided by [Paul Grosu](https://github.com/pgrosu) in; this [issue thread](https://github.com/google/deepvariant/issues/684). We thank; Paul for providing a detailed description and reasoning. ## Singularity related questions:. ### `TMPDIR`. If you have issues with `TMPDIR` when running with Singularity, try adding this; to your command:. ```bash; export TMPDIR=""$PWD/tmp_dir""; ```. See https://github.com/google/deepvariant/issues/524#issuecomment-1067597987. ### Issues with `/mnt/`. User reported that sometimes their setup uses `/mnt/`, which exists in our; Docker image, and it has caused an issue in Singularity. You can use `-B` in Singularity to avoid this issue. See:; https://github.com/google/deepvariant/issues/530#issuecomment-1076923302 for; more details. ",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:9225,Testability,test,tested,9225,"hat your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:9448,Testability,test,test,9448,"eepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: CUDA_ERROR_NOT_INITIALIZED: initialization error; ```. We; have tested and confirmed that this does not affect GPU usage or inference. So; you can continue running DeepVariant without being worried about this message. ## How much GPU memory is needed for the Keras models?. 16GB. In our test, we observe the model occupying 16GB GPU memory. ## Do models from before r1.6.0 work with current inference code?. No. We have moved from Slim to Keras. All models before `1.6.0` were trained in; Slim platform. So they are not compatible with `1.6.0` anymore. ## Can call_variants be run on multiple GPUs?. No. Although possible, we have not implemented the multi-GPU capability in GPU; inference yet. ## Can model_train be run on multiple GPUs?. No. TensorFlow's Estimator API does provide support for running training on; multiple GPUs through the use of a DistributionStrategy. However,; DistributionStrategy cannot be used with exponential moving average (EMA), which; is present in the DeepVariant codebase. ## What is the realigner and how does it work?. From the; [DeepVariant 2018 manuscript](https://www.nature.com/articles/nbt.4235.epdf?author_access_token=q4ZmzqvvcGBqTuKyKgYrQ9RgN0jAjWel9jnR3ZoTv0NuM3saQzpZk8yexjfPUhdFj4zyaA4Yvq0LWBoCYQ4B9vqPuv8e2HHy4vShDgEs8YxI_hLs9ov6Y1f_4fyS7kGZ):. > Mapped reads are",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:2032,Usability,learn,learned,2032,"se sites, you may add the; following parameters, here shown with their defaults:. ```; --make_examples_extra_args=""vsc_min_count_snps=2,vsc_min_fraction_snps=0.12,vsc_min_count_indels=2,vsc_min_fraction_indels=0.06""; ```. It is sometimes also the case that realignment of the reads within DeepVariant; changes or reduces the evidence supporting the variant. To check for this, try; using the `--norealign_reads` flag to turn off realignment temporarily. Note; that we don't recommend turning off the realigner for Illumina data in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify ",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:2635,Usability,clear,clearly,2635,"in general; cases because the realigner improves accuracy overall. There is also the option to output the realigned reads, e.g. to inspect the new; alignments in IGV. See the ""What is the realigner and how does it work?"" section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify more than 2 haplotypes present by overlapping the reads. If you can, it; suggests that the region may have a copy number variation. Some analysis of this; was presented at AGBT as a poster; “[Uncaptured segmental duplication creates artifacts in workflows using GRCh37](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096)”. This pattern of undercalling positions at high variant density may affect; variant-dense non-human species (those with a variant density of >1 in 40; positions). For an analysis of this",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:2692,Usability,learn,learned,2692,"section; for instructions. **Missing variants where a candidate is generated:**. If a candidate is made, but is called as reference (either 0/0 or ./.) it means; that the neural network processed the genomic region, but based on all of its; learned experience from training data, it decided the highest probability for; the position was as non-variant. Some of the reasons that DeepVariant may; suspect a false positive are: strand-bias in reads, low mapping quality in; reads, low base quality in reads, and overall low coverage. In addition, there is another pattern that causes DeepVariant to suspect variant; positions which can initially seem counterintuitive to human observers. This; occurs when a dense set of variants appears on one haplotype while the other; haplotype is fully reference, and humans often perceive this as missing a; clearly heterozygous position. DeepVariant seems to have learned that this; signature often indicates a region which is a segmental duplication, copy number; variant, or structural variant where multiple copies of similar genomic regions; are mapping to the same reference location. In this case, it may be worthwhile; to inspect the region to see if it has elevated coverage, and whether you can; identify more than 2 haplotypes present by overlapping the reads. If you can, it; suggests that the region may have a copy number variation. Some analysis of this; was presented at AGBT as a poster; “[Uncaptured segmental duplication creates artifacts in workflows using GRCh37](https://pbs.twimg.com/media/ERe2bSyWsAcE00h?format=jpg&name=4096x4096)”. This pattern of undercalling positions at high variant density may affect; variant-dense non-human species (those with a variant density of >1 in 40; positions). For an analysis of this, please see our blog; “[Improved non-human variant calling using species-specific DeepVariant models](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepva",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:6602,Usability,clear,clear,6602,"ype implementation for somatic calling, which can take a tumor and normal; BAM and call subclonal variants. However, we don't yet have enough confidence in; the available truth sets, and that they come from a diverse enough sampling of; cancers with mutational profiles, for us to be certain in releasing something of; high quality. We're watching developments in the area of these truth sets and; hope to be able to further develop the somatic caller in the future. ## Can I use DeepVariant on plant genomes?. DeepVariant has previously been applied to plant species. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output di",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:7172,Usability,simpl,simplest,7172,"ies. In the case of rice,; there was good evidence of high accuracy. You can see; [some results in this blog post](https://cloud.google.com/blog/products/data-analytics/analyzing-3024-rice-genomes-characterized-by-deepvariant).; However, these rice genomes were diploid and with a similar variant density of; humans. DeepVariant is currently written to be a diploid variant caller. So if the plant; species you are working with is polyploid, it is not yet clear how DeepVariant; will perform. That is because even with re-training, DeepVariant can only; produce variant calls that are homozygous alternate, heterozygous, or homozygous; reference, which don't have much meaning in a tetraploid genome, for example. ## Can I use DeepVariant on other non-human species?. See this; [blog post](https://google.github.io/deepvariant/posts/2018-12-05-improved-non-human-variant-calling-using-species-specific-deepvariant-models/). ## How do I build/run DeepVariant?. In general, we recommend running DeepVariant using Docker for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides ",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md:8118,Usability,simpl,simpler,8118,"r for the simplest; setup. If you are building from source because you want to experiment with; changes to the codebase, we still recommend Docker. You can clone the; DeepVariant repo, modify the source code, and build a Docker image with your; changes using the provided Dockerfile. ## Why can't it find one of the input files? E.g., ""Could not open"". This often happens because the way Docker works, input and output directories; have to be mounted and then files are referred to by their mounted location,; which can be confusing. To check that files are visible inside the Docker; container, you can `ls` inside the container. For example, using the setup shown; in the README and looking inside the `/input` volume:. ```; BIN_VERSION=""1.6.1""; docker run \; -v ""YOUR_INPUT_DIR"":""/input"" \; -v ""YOUR_OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; ls /input; ```. Mounting directories with Docker can be confusing. One trick to make this; simpler is to set both sides as your `$HOME`, so the paths are the same inside; and outside the Docker container. ```; echo $HOME # see what your home directory is first.; ls $HOME; BIN_VERSION=""1.6.1""; sudo docker run \; -v ""${HOME}"":""${HOME}"" \; google/deepvariant:""${BIN_VERSION}"" \; ls $HOME; ```. ## How do I run multi-sample calling?. Since the DeepVariant v0.9 release, we recommend; ""[Best practices for multi-sample variant calling with DeepVariant](https://github.com/google/deepvariant/blob/r0.9/docs/trio-merge-case-study.md)"". For specifically calling on duos or trios, we introduced; [DeepTrio](https://github.com/google/deepvariant/blob/r1.6.1/docs/deeptrio-details.md); in v1.1. ## Why am I seeing ""CUDA_ERROR_NOT_INITIALIZED: initialization error"" while running on GPU?. We have been observing the following message while running on GPU since we moved; platform from slim to keras:. ```bash; 2023-10-20 22:21:03.818638: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:1278] could not retrieve CUDA device count: C",MatchSource.DOCS,docs/FAQ.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/FAQ.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:48,Deployability,release,release,48,"# DeepTrio runtime and accuracy metrics for all release models. ## WGS (Illumina). ### Runtime. Runtime is on HG002/HG003/HG004 (all chromosomes). Stage | Wall time (minutes); -------------------------------- | -----------------; make_examples | ~439m; call_variants for HG002 | ~351m; call_variants for HG003 | ~355m; call_variants for HG004 | ~361m; postprocess_variants (parallel) | ~61m; total | ~1567m = ~26.12 hours. ### Accuracy. We report hap.py results on HG002/HG003/HG004 trio (chr20, using NIST v4.2.1; truth), which was held out while training. #### HG002:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 11208 | 48 | 13 | 0.995736 | 0.998884 | 0.997308 |; | SNP | 71087 | 246 | 42 | 0.996551 | 0.99941 | 0.997979 |. #### HG003:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 10584 | 44 | 20 | 0.99586 | 0.998192 | 0.997024 |; | SNP | 69975 | 191 | 55 | 0.997278 | 0.999215 | 0.998246 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 10945 | 55 | 27 | 0.995 | 0.997643 | 0.99632 |; | SNP | 71446 | 213 | 52 | 0.997028 | 0.999273 | 0.998149 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WGS/HG004.output.visual_report.html). ## PacBio (HiFi). In v1.6.1, we introduced read haplo",MatchSource.DOCS,docs/metrics-deeptrio.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:6121,Deployability,configurat,configuration,6121," ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/metrics-deeptrio.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:6121,Modifiability,config,configuration,6121," ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/metrics-deeptrio.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:6164,Testability,log,log,6164," ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/metrics-deeptrio.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:6583,Testability,log,log,6583," ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform; ",MatchSource.DOCS,docs/metrics-deeptrio.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md:5850,Usability,simpl,simplicity,5850,"-------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 29 | 0 | 0 | 1.0 | 1.0 | 1.0 |; | SNP | 683 | 2 | 0 | 0.99708 | 1.0 | 0.998538 |. #### HG004:. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 32 | 1 | 1 | 0.969697 | 0.969697 | 0.969697 |; | SNP | 677 | 2 | 0 | 0.997054 | 1.0 | 0.998525 |. * See VCF stats report (for all chromosomes); - [HG002](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG002.output.visual_report.html); - [HG003](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG003.output.visual_report.html); - [HG004](https://storage.googleapis.com/deepvariant/visual_reports/DeepTrio/1.6.1/WES/HG004.output.visual_report.html). ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); For bigger datasets (WGS and PACBIO), we used bigger disk size (900G).; This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deeptrio.sh. # WGS; bash inference_deeptrio.sh --model_preset WGS. # WES; bash inference_deeptrio.sh --model_preset WES. # PacBio; bash inference_deeptrio.sh --model_preset PACBIO. ```. Runtime metrics are taken from the resulting log after each stage of; DeepTrio. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mac",MatchSource.DOCS,docs/metrics-deeptrio.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics-deeptrio.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:4553,Availability,avail,available,4553,"). ## Hybrid (Illumina + PacBio HiFi). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -------------------; make_examples | ~172m; call_variants | ~211m; postprocess_variants (with gVCF) | ~24m; total | ~407m = ~6.78 hours. ### Accuracy. Evaluating on HG003 (all chromosomes, using NIST v4.2.1 truth), which was held; out while training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HY",MatchSource.DOCS,docs/metrics.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:39,Deployability,release,release,39,"# Runtime and accuracy metrics for all release models. ## WGS (Illumina). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | ------------------; make_examples | ~103m; call_variants | ~196m; postprocess_variants (with gVCF) | ~27m; total | ~326m = ~5.43 hours. ### Accuracy. hap.py results on HG003 (all chromosomes, using NIST v4.2.1 truth), which was; held out while training. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 501683 | 2818 | 1265 | 0.994414 | 0.997586 | 0.995998 |; | SNP | 3306788 | 20708 | 4274 | 0.993777 | 0.99871 | 0.996237 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/WGS/deepvariant.output.visual_report.html). ## WES (Illumina). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -----------------; make_examples | ~6m; call_variants | ~1m; postprocess_variants (with gVCF) | ~1m; total | ~8m. ### Accuracy. hap.py results on HG003 (all chromosomes, using NIST v4.2.1 truth), which was; held out while training. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 1022 | 29 | 13 | 0.972407 | 0.987713 | 0.98 |; | SNP | 24987 | 292 | 59 | 0.988449 | 0.997645 | 0.993025 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/WES/deepvariant.output.visual_report.html). ## PacBio (HiFi). ### Runtime. Runtime is on HG003 (all chromosomes). Stage | Time (minutes); -------------------------------- | -------------------; make_examples | ~149m; call_variants | ~217m; postprocess_variants (with gVCF) | ~33m; total | ~399m = ~6.65 hours. ### Accur",MatchSource.DOCS,docs/metrics.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:4980,Deployability,configurat,configuration,4980,"training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mach",MatchSource.DOCS,docs/metrics.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:4980,Modifiability,config,configuration,4980,"training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-mach",MatchSource.DOCS,docs/metrics.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:5023,Testability,log,log,5023,"pe | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ",MatchSource.DOCS,docs/metrics.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:5619,Testability,log,log,5619,"pe | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 runs produced the same output. [CPU instance with 64 CPUs]: deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform. ",MatchSource.DOCS,docs/metrics.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md:4781,Usability,simpl,simplicity,4781,"urs. ### Accuracy. Evaluating on HG003 (all chromosomes, using NIST v4.2.1 truth), which was held; out while training the hybrid model. | Type | TRUTH.TP | TRUTH.FN | QUERY.FP | METRIC.Recall | METRIC.Precision | METRIC.F1_Score |; | ----- | -------- | -------- | -------- | ------------- | ---------------- | --------------- |; | INDEL | 503014 | 1487 | 2767 | 0.997053 | 0.994781 | 0.995916 |; | SNP | 3323624 | 3871 | 2273 | 0.998837 | 0.999317 | 0.999077 |. [See VCF stats report.](https://storage.googleapis.com/deepvariant/visual_reports/DeepVariant/1.6.1/HYBRID/deepvariant.output.visual_report.html). ## Inspect outputs that produced the metrics above. The DeepVariant VCFs, gVCFs, and hap.py evaluation outputs are available at:. ```; gs://deepvariant/case-study-outputs; ```. You can also inspect them in a web browser here:; https://42basepairs.com/browse/gs/deepvariant/case-study-outputs. ## How to reproduce the metrics on this page. For simplicity and consistency, we report runtime with a; [CPU instance with 64 CPUs](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform); This is NOT the fastest or cheapest configuration. Use `gcloud compute ssh` to log in to the newly created instance. Download and run any of the following case study scripts:. ```; # Get the script.; curl -O https://raw.githubusercontent.com/google/deepvariant/r1.6.1/scripts/inference_deepvariant.sh. # WGS; bash inference_deepvariant.sh --model_preset WGS. # WES; bash inference_deepvariant.sh --model_preset WES. # PacBio; bash inference_deepvariant.sh --model_preset PACBIO. # ONT_R104; bash inference_deepvariant.sh --model_preset ONT_R104. # Hybrid; bash inference_deepvariant.sh --model_preset HYBRID_PACBIO_ILLUMINA; ```. Runtime metrics are taken from the resulting log after each stage of; DeepVariant. The runtime numbers reported above are the average of 5 runs each.; The accuracy metrics come from the hap.py summary.csv output file.; The runs are deterministic so all 5 r",MatchSource.DOCS,docs/metrics.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/metrics.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:32,Deployability,release,release,32,# Documentation. * [DeepVariant release notes](https://github.com/google/deepvariant/releases). ## Quick start and Case studies. * [DeepVariant quick start](deepvariant-quick-start.md); * [DeepVariant whole genome case study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing De,MatchSource.DOCS,docs/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:85,Deployability,release,releases,85,# Documentation. * [DeepVariant release notes](https://github.com/google/deepvariant/releases). ## Quick start and Case studies. * [DeepVariant quick start](deepvariant-quick-start.md); * [DeepVariant whole genome case study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing De,MatchSource.DOCS,docs/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:1991,Testability,test,testing,1991,e study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing DeepVariant](deepvariant-build-test.md); * [DeepVariant Genomic VCF (gVCF) support](deepvariant-gvcf-support.md); * [Getting Started with GCP](deepvariant-gcp-info.md) (It is not required to; run DeepVariant on GCP.); ,MatchSource.DOCS,docs/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:2030,Testability,test,test,2030,e study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing DeepVariant](deepvariant-build-test.md); * [DeepVariant Genomic VCF (gVCF) support](deepvariant-gvcf-support.md); * [Getting Started with GCP](deepvariant-gcp-info.md) (It is not required to; run DeepVariant on GCP.); ,MatchSource.DOCS,docs/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:421,Usability,simpl,simplex,421,# Documentation. * [DeepVariant release notes](https://github.com/google/deepvariant/releases). ## Quick start and Case studies. * [DeepVariant quick start](deepvariant-quick-start.md); * [DeepVariant whole genome case study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing De,MatchSource.DOCS,docs/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:462,Usability,simpl,simplex-case-study,462,# Documentation. * [DeepVariant release notes](https://github.com/google/deepvariant/releases). ## Quick start and Case studies. * [DeepVariant quick start](deepvariant-quick-start.md); * [DeepVariant whole genome case study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing De,MatchSource.DOCS,docs/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md:1943,Usability,guid,guide,1943,e study](deepvariant-case-study.md); * [DeepVariant exome case study](deepvariant-exome-case-study.md); * [DeepVariant PacBio case study](deepvariant-pacbio-model-case-study.md); * [DeepVariant ONT R10.4 simplex case study](deepvariant-ont-r104-simplex-case-study.md); [DeepVariant ONT R10.4 duplex case study](deepvariant-ont-r104-duplex-case-study.md); * [DeepVariant hybrid (PacBio and Illumina) case study](deepvariant-hybrid-case-study.md); * [DeepVariant Complete Genomics T7 case study](deepvariant-complete-t7-case-study.md); * [DeepVariant Complete Genomics G400 case study](deepvariant-complete-g400-case-study.md); * [Runtime and accuracy metrics for all DeepVariant models](metrics.md); * [Best practices for multi-sample variant calling](trio-merge-case-study.md); * [Using graph genomes: VG Giraffe + DeepVariant case study](deepvariant-vg-case-study.md). ## Visualization and analysis. * [show_examples: Saving human-readable images from DeepVariant examples](show-examples.md); * [VCF stats report](deepvariant-vcf-stats-report.md); * [Runtime by region for make_examples](runtime-by-region.md). ### Colab notebooks. * [Colab example: visualizing pileup images/tensors](visualizing_examples.ipynb); * [Can you beat DeepVariant?: A look inside the classification task](cybdv_notebook.ipynb); * [Google Developer Codelab: Variant Calling on a Rice genome with DeepVariant](https://codelabs.developers.google.com/codelabs/genomics-deepvariant). ## (Advanced) Training. * [Advanced Case Study: Train a customized SNP and small indel variant caller; for BGISEQ-500 data](deepvariant-training-case-study.md); * [DeepVariant training data](deepvariant-details-training-data.md). ## More details. * [DeepVariant usage guide](deepvariant-details.md); * [Building and testing DeepVariant](deepvariant-build-test.md); * [DeepVariant Genomic VCF (gVCF) support](deepvariant-gvcf-support.md); * [Getting Started with GCP](deepvariant-gcp-info.md) (It is not required to; run DeepVariant on GCP.); ,MatchSource.DOCS,docs/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/README.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:2302,Availability,avail,available,2302,"ript, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against th",MatchSource.DOCS,docs/runtime-by-region.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:5893,Modifiability,variab,variability,5893,"own on the y-axis, as a percentage of the total runtime for the; given task. If all regions had the same runtime, the curve would be a; straight diagonal line. The extent to which the curve bends to the upper; left corner shows how much some regions take disproportionately longer than; others. Hover the cursor over the lines to see the exact percentages.; 3. ""Total runtime for each task"": Each point is a task. Hover over each point; to see the runtime calculated into hours, minutes, and seconds. Drag a; rectangle around some of the tasks to see them highlighted in the Pareto; curve. Often the tasks with longer runtimes in the chart will be the same; tasks with Pareto curves leaning to the upper left, indicating that for; tasks than run longer than others, the cause is with a subset of the regions; not with an overall slowdown of all regions.; 4. ""Stage runtimes for each task"": A histogram of how long each stage takes for; the different tasks. Often the `make pileup images` stage will show more; variability here than other stages.; 5. ""Top runtime regions"" and ""Median runtime regions"": This shows some; individual regions to give more context for some of the trends seen in other; charts. Pay attention especially to the differences between the y-axis; limits in these two charts. The long-running regions are often taking; hundreds of times longer than median regions, with the runtime also taken up; by different stages.; 6. ""The longest-running regions that produced no examples"": This profiles some; individual regions that yielded zero output examples. Also look at the; subtitle to see what percentage of the total runtime is taken up by; processing these zero-example regions.; 7. ""Runtime by stage for ..."": When there are more than 5000 regions, there; will be two charts here, one for the bottom 99% of regions and one for the; top 100 regions (both by total runtime). If fewer than 5000 regions, there; will only be one chart showing all the regions. This is similar to the;",MatchSource.DOCS,docs/runtime-by-region.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:1502,Testability,log,logs,1502," file; per task if the examples are sharded. This TSV file can then be visualized using the `runtime_by_region_vis` script,; creating a visual report. ![Sample runtime profile from a WGS run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runti",MatchSource.DOCS,docs/runtime-by-region.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:1600,Testability,log,logs,1600,"` script,; creating a visual report. ![Sample runtime profile from a WGS run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/mak",MatchSource.DOCS,docs/runtime-by-region.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:1687,Testability,log,logs,1687," run](images/runtime_by_region_wgs.png). Example reports for typical runs:. * [WGS](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wgs.html); * [WES](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_wes.html); * [PacBio](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_pacbio.html); * [Hybrid](https://storage.googleapis.com/deepvariant/example-reports/runtime_report_hybrid.html). ## How to enable runtime profiling during a DeepVariant run. ### Using the run_deepvariant script. When using the one-step `run_deepvariant` script, supply a `--logging_dir`; directory and set `--runtime_report`. For example, when following the; [quick start](deepvariant-quick-start.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; `",MatchSource.DOCS,docs/runtime-by-region.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:2492,Testability,log,logs,2492,"rt.md), including `--runtime_report; --logging_dir=/output/logs` would produce the following runtime profiling output; file and visual report:. ```; /output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against thresholds; (see `--vsc_min*` parameters for the thresholds). 3. Make pileup images: Represent the reads as a; [pileup image tensor](https://",MatchSource.DOCS,docs/runtime-by-region.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:2590,Testability,log,logs,2590,"_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv; /output/logs/make_examples_runtime_by_region_report.html; ```. ### Running stages individually. If you are using make_examples itself without the run_deepvariant script, supply; make_examples with `--runtime_by_region=` a filename (.tsv). If the examples are; sharded then the runtime profile should be sharded into the same number of; shards, e.g. when using `examples@64` then runtimes could be `runtimes@64.tsv`. Then use the `runtime_by_region_vis` script to create a visual report of the; make_examples runtime by region data. Continuing from the quick start, it looks; like this:. ```bash; BIN_VERSION=""1.6.1"" # Only available in v1.1+.; docker run \; -v ""INPUT_DIR"":""/input"" \; -v ""OUTPUT_DIR:/output"" \; google/deepvariant:""${BIN_VERSION}"" \; /opt/deepvariant/bin/runtime_by_region_vis \; --input=/output/logs/make_examples_runtime_by_region/make_examples_runtime-00000-of-00001.tsv \; --output=/output/logs/make_examples_runtime_by_region/make_examples_runtime_vis.html \; --title=""Quick start runtime profiling""; ```. ## Interpreting the runtime report. ### Where the data comes from. `make_examples`, the first stage of DeepVariant, starts from a BAM file and; corresponding reference FASTA file. One 1000 bp region at a time (set by; `--partition_size`), `make_examples` will:. 1. Get reads: Query the bam files for all the reads in the region. Optionally; (`--realign_reads`) do a local assembly of the reads and realign the reads; to the resulting haplotype graph. 2. Find candidates: Catalogue all the putative alternate alleles for all those; reads, and compare the accumulated evidence for each alt against thresholds; (see `--vsc_min*` parameters for the thresholds). 3. Make pileup images: Represent the reads as a; [pileup image tensor](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/).; When `--alt_aligned_pileup` is enabled, those alignments are included in; this ste",MatchSource.DOCS,docs/runtime-by-region.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md:7150,Testability,log,logic,7150," same; tasks with Pareto curves leaning to the upper left, indicating that for; tasks than run longer than others, the cause is with a subset of the regions; not with an overall slowdown of all regions.; 4. ""Stage runtimes for each task"": A histogram of how long each stage takes for; the different tasks. Often the `make pileup images` stage will show more; variability here than other stages.; 5. ""Top runtime regions"" and ""Median runtime regions"": This shows some; individual regions to give more context for some of the trends seen in other; charts. Pay attention especially to the differences between the y-axis; limits in these two charts. The long-running regions are often taking; hundreds of times longer than median regions, with the runtime also taken up; by different stages.; 6. ""The longest-running regions that produced no examples"": This profiles some; individual regions that yielded zero output examples. Also look at the; subtitle to see what percentage of the total runtime is taken up by; processing these zero-example regions.; 7. ""Runtime by stage for ..."": When there are more than 5000 regions, there; will be two charts here, one for the bottom 99% of regions and one for the; top 100 regions (both by total runtime). If fewer than 5000 regions, there; will only be one chart showing all the regions. This is similar to the; ""Stage runtimes for each task"" except that regions are shown individually; here instead of being combined into tasks. This shows the spread of runtimes; across regions for the different stages.; 8. ""Trends for ..."": This is in one or two sets of charts by the same logic as; the ""Runtime by stage"" charts. This shows a grid of charts intersecting; counts of reads, candidates, and examples (rows) with the runtime for the; four stages (columns) in seconds. It is common that some of these runtimes; will correlate nicely with the counts. For example, the `write outputs`; runtime is closely tied to the number of examples, which is not surprising.; ",MatchSource.DOCS,docs/runtime-by-region.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/runtime-by-region.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:858,Availability,avail,available,858,"# show_examples: Saving human-readable images from DeepVariant examples. This is a short guide to using the show_examples tool to view the pileup images; used within DeepVariant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```ba",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:2172,Availability,avail,available,2172,"tart guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded exam",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:3328,Availability,down,down,3328,"ng from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; mapping quality, many errors, nearby variants, or any other concepts.; Filtering can be done any way you want, `grep` would be an easy option (the; TSV's header is not needed).; * Write out example tfrecords using `--write_tfrecords` after applying any; filtering using the options above.; ",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:3747,Availability,error,errors,3747,"ng from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; mapping quality, many errors, nearby variants, or any other concepts.; Filtering can be done any way you want, `grep` would be an easy option (the; TSV's header is not needed).; * Write out example tfrecords using `--write_tfrecords` after applying any; filtering using the options above.; ",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:2723,Energy Efficiency,power,powerful,2723," documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; ma",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:3000,Energy Efficiency,power,powerful,3000,"ng from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate them in more; depth.; * Stop after a certain number of examples, e.g. 10? Use `--num_records 10`.; * Sharded examples? Use for example, `--examples make_examples.tfrecord@64.gz`; to search through them all. This is best paired with `--regions` or `--vcf`; to narrow down to a small number of examples of interest. You can also use; the actual filename of a single make_examples file to only read that one, as; shown in the sample code above.; * Use `--curate` to create a TSV file with concepts for each pileup. Then; filter that TSV in any way you want and read that filtered TSV in using; `--filter_by_tsv` to e.g. get pileup images only for examples with low; mapping quality, many errors, nearby variants, or any other concepts.; Filtering can be done any way you want, `grep` would be an easy option (the; TSV's header is not needed).; * Write out example tfrecords using `--write_tfrecords` after applying any; filtering using the options above.; ",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:2083,Testability,test,testdata,2083,".gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is available only in version 1.0.0 and later.; sudo docker run \; -v ""${INPUT_DIR}"":""/input"" \; -v ""${OUTPUT_DIR}"":""/output"" \; google/deepvariant:""${BIN_VERSION}"" /opt/deepvariant/bin/show_examples \; --examples=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz \; --example_info_json=/output/intermediate_results_dir/make_examples.tfrecord-00000-of-00001.gz.example_info.json \; --output=/output/pileup \; --num_records=20 \; --curate. # And then your images are here:; ls ""${OUTPUT_DIR}""/pileup*.png; ```. ## Try it with these powerful optional parameters. * Filter to regions? Use e.g. `--regions chr20:1-3000000` or paths to BED or; BEDPE files.; * Filter to records from a VCF? Use `--vcf variants.vcf`. This can be a piece; of a VCF, e.g. grepping a hap.py output VCF for false positives. This is a; powerful way to pick out variants of interest and investigate th",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:89,Usability,guid,guide,89,"# show_examples: Saving human-readable images from DeepVariant examples. This is a short guide to using the show_examples tool to view the pileup images; used within DeepVariant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```ba",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:496,Usability,guid,guide,496,"# show_examples: Saving human-readable images from DeepVariant examples. This is a short guide to using the show_examples tool to view the pileup images; used within DeepVariant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```ba",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md:1180,Usability,guid,guide,1180,"iant and save them as PNG image files. This tool is; particularly useful when you want to try to understand how a candidate variant; of interest was represented when it was passed into the neural network. ![An example pileup image](images/example_1.4.0.png). This example was generated with the data from the; [quick start guide](deepvariant-quick-start.md) and the example commands below. For more information on the pileup images and how to read them, please see the; [""Looking through DeepVariant's Eyes"" blog post](https://google.github.io/deepvariant/posts/2020-02-20-looking-through-deepvariants-eyes/). The `show_examples` tool is introduced in DeepVariant 1.0.0, so it is not; available in older versions, but it will work with make_examples output files; from older versions of DeepVariant. ## Finding the make_examples output tfrecord files. First, find the make_examples.tfrecord.gz files output by DeepVariant during the; make_examples (first) stage. If you followed along with the [quick start guide](deepvariant-quick-start.md); and case studies that used the Docker version, then these files are usually; hidden inside the Docker container. But you can get them exported into the same; output directory where the VCF file appears by adding the following setting in; the `run_deepvariant` command. ```bash; # Add the following to your run_deepvariant command.; --intermediate_results_dir=/output/; ```. Then the make_examples file should appear in the directory docker mounted as; `/output/`. For example, if you followed the; [quick-start documentation](deepvariant-quick-start.md), it looks like this:; `${OUTPUT_DIR}/make_examples.tfrecord-00000-of-00001.gz`. ## Running show_examples. Once you have a make_examples output tfrecord file, then you can run; `show_examples` to see the pileup images inside:. ```bash; # Continuing from the quick start linked above:; INPUT_DIR=""${PWD}/quickstart-testdata""; OUTPUT_DIR=""${PWD}/quickstart-output"". BIN_VERSION=""1.6.1"" # show_examples is a",MatchSource.DOCS,docs/show-examples.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/show-examples.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:1822,Availability,down,downloading,1822,"s; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; ",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:1888,Availability,down,download,1888,"-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncb",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:3613,Availability,down,downloading,3613,"ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:3930,Availability,down,downloading,3930,"loUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_fathe",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4158,Availability,down,downloading,4158,"hkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh3",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8480,Availability,avail,available,8480,"he `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex phen; 1 Sample_Diag-excap51-HG002-EEogPU Sample_Diag-excap51-HG003-EEogPU Sample_Di",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:1983,Deployability,install,install,1983,"ts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:2028,Deployability,install,install,2028,"ts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you can run: `sudo apt-get -y install aria2`. ```; DIR=""${PWD}/trio""; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bam -o HG002.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG002_NA24385_son/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG002-EEogPU_v02-KIT-Av5_AGATGTAC_L008.posiSrt.markDup.bai -o HG002.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4274,Deployability,release,release,4274,"GAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4472,Deployability,release,release,4472,"001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4678,Deployability,release,release,4678,"ge.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DI",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4903,Deployability,release,release,4903,"and for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-tra",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5104,Deployability,release,release,5104,"_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools an",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5313,Deployability,release,release,5313,"/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; doc",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5541,Deployability,release,release,5541,"hmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5742,Deployability,release,release,5742,"_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:5951,Deployability,release,release,5951,"_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \;",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:6104,Deployability,install,install,6104,"ther/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUT",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:6159,Deployability,install,installed,6159,"ther/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG003_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUT",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:6262,Deployability,install,install,6262,".nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is ",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:6320,Deployability,install,install,6320,".nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG003_truth.bed; ```. HG004:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG004_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG004_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG004_NA24143_mother/NISTv4.2.1/GRCh37/HG004_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG004_truth.bed; ```. (No need to install bcftools and other tools, because they are now installed in; the DeepVariant images.). ## Run DeepVariant on trio to get 3 single sample VCFs. First, install docker if you don't have it yet: `sudo apt-get -y install; docker.io`. With the example command below, it runs DeepVariant on the trio one by one. This; is for demonstration only. If you're running this on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is ",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:7455,Deployability,pipeline,pipeline,7455,"is on a large cohort, running; serially is not the most effective approach. ```; N_SHARDS=$(nproc) # Or change to the number of cores you want to use; CAPTURE_BED=agilent_sureselect_human_all_exon_v5_b37_targets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfil",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:981,Modifiability,extend,extended,981,"# Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration). ## Overview. This document outlines all the steps and considerations for calling and merging; a trio using DeepVariant and [GLnexus](https://github.com/dnanexus-rnd/GLnexus).; These best practices were developed and evaluated as described in the article; published in _Bioinformatics_:; [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://doi.org/10.1093/bioinformatics/btaa1081); (2021). The process involves 3 major stages: running DeepVariant to create individual; genome call sets, running GLnexus to merge call sets, and analyzing the merged; call set. NOTE: This case study demonstrates an example of how to run DeepVariant; end-to-end on one machine. The steps below were done on a machine with this; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:7690,Modifiability,config,config,7690,"ets.bed; VERSION=1.6.1. declare -a trio=(HG002 HG003 HG004); for SAMPLE in ""${trio[@]}""; do; BAM=${SAMPLE}.bam. OUTPUT_VCF=${SAMPLE}.vcf.gz; OUTPUT_GVCF=${SAMPLE}.g.vcf.gz. time sudo docker run \; -v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) fo",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:7860,Modifiability,config,config,7860,"v ""${DIR}"":""/data"" \; google/deepvariant:${VERSION} \; /opt/deepvariant/bin/run_deepvariant \; --model_type=WES \; --ref=""/data/hs37d5.fa"" \; --reads=""/data/${BAM}"" \; --regions=""/data/${CAPTURE_BED}"" \; --output_vcf=""/data/${OUTPUT_VCF}"" \; --output_gvcf=""/data/${OUTPUT_GVCF}"" \; --num_shards=${N_SHARDS}; done; ```. Note: The BAM files should provide unique names for each sample in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. C",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8384,Modifiability,config,config,8384,"ique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #sex: 1=male; 2=female; 0=unknown; #phenotype: -9=missing, 0=missing; 1=unaffected; 2=affected; #; #fam-id ind-id pat-id mat-id sex ph",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:391,Performance,scalab,scalable,391,"# Best practices for multi-sample variant calling with DeepVariant (WES trio demonstration). ## Overview. This document outlines all the steps and considerations for calling and merging; a trio using DeepVariant and [GLnexus](https://github.com/dnanexus-rnd/GLnexus).; These best practices were developed and evaluated as described in the article; published in _Bioinformatics_:; [Accurate, scalable cohort variant calls using DeepVariant and GLnexus](https://doi.org/10.1093/bioinformatics/btaa1081); (2021). The process involves 3 major stages: running DeepVariant to create individual; genome call sets, running GLnexus to merge call sets, and analyzing the merged; call set. NOTE: This case study demonstrates an example of how to run DeepVariant; end-to-end on one machine. The steps below were done on a machine with this; [example command to start a machine](deepvariant-details.md#command-for-a-cpu-only-machine-on-google-cloud-platform). The steps in this document can be extended to merge larger cohorts as well. See this workflow:. ![workflow](images/cohort-workflow.png?raw=true ""DeepVariant+GLnexus cohort workflow""). A few things to note before we start:. * It is recommended to use BAM files with original quality scores. In the case; that BAM files went through recalibration, optional DV flags can be used in; order to use original scores: `--parse_sam_aux_fields`,; `--use_original_quality_scores`.; * DeepVariant optionally allows gVCF output. This option is required for; further GLnexus analysis in this document. ## Dataset. The Whole Exome Sequencing (WES) dataset we're using is from:. [ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/](https://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/). * HG002_NA24385_son; * HG003_NA24149_father; * HG004_NA24143_mother. ### Commands for downloading the input BAMs. Just for convenience, we use aria2 to download our data. You can change it to; whatever other tools (wget, curl) that you prefer. To install aria2, you",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8239,Performance,perform,performance,8239," in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #se",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8266,Performance,perform,performance,8266," in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #se",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:3742,Testability,test,testdata,3742,"59_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bam -o HG003.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:3885,Testability,test,testdata,3885,"cbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG003_NA24149_father/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG003-EEogPU_v02-KIT-Av5_TCTTCACA_L008.posiSrt.markDup.bai -o HG003.bai; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:4074,Testability,test,testdata,4074,"2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bam -o HG004.bam; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/data/AshkenazimTrio/HG004_NA24143_mother/OsloUniversityHospital_Exome/151002_7001448_0359_AC7F6GANXX_Sample_HG004-EEogPU_v02-KIT-Av5_CCGAAGTA_L008.posiSrt.markDup.bai -o HG004.bai; ```. ### Command for downloading the reference file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.gz; gunzip ${DIR}/hs37d5.fa.gz; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/hs37d5.fa.fai; ```. ### Command for downloading the input capture region BED file. ```; aria2c -c -x10 -s10 -d ""${DIR}"" https://storage.googleapis.com/deepvariant/exome-case-study-testdata/agilent_sureselect_human_all_exon_v5_b37_targets.bed; ```. ### Command for downloading the truth files. HG002:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG002_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark.vcf.gz.tbi -o HG002_truth.vcf.gz.tbi; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG002_NA24385_son/NISTv4.2.1/GRCh37/HG002_GRCh37_1_22_v4.2.1_benchmark_noinconsistent.bed -o HG002_truth.bed; ```. HG003:. ```; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.nih.gov/giab/ftp/release/AshkenazimTrio/HG003_NA24149_father/NISTv4.2.1/GRCh37/HG003_GRCh37_1_22_v4.2.1_benchmark.vcf.gz -o HG003_truth.vcf.gz; aria2c -c -x10 -s10 -d ""${DIR}"" ftp://ftp-trace.ncbi.nlm.",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md:8278,Usability,guid,guide,8278," in their `SM`; header tag, which is usually derived from a command-line flag to the read; aligner. If your BAM files don't have unique `SM` tags (and if it's not feasible; to adjust the alignment pipeline), add the `--sample_name=XYZ` flag to; `run_deepvariant` to override the sample name written into the gVCF file header. ## Merge the trio samples using GLnexus. ### Run GLnexus to merge 3 gVCFs. And then run GLnexus with this config:. ```; sudo docker pull quay.io/mlin/glnexus:v1.2.7. time sudo docker run \; -v ""${DIR}"":""/data"" \; quay.io/mlin/glnexus:v1.2.7 \; /usr/local/bin/glnexus_cli \; --config DeepVariantWES \; --bed ""/data/${CAPTURE_BED}"" \; /data/HG004.g.vcf.gz /data/HG003.g.vcf.gz /data/HG002.g.vcf.gz \; | sudo docker run -i google/deepvariant:${VERSION} bcftools view - \; | sudo docker run -i google/deepvariant:${VERSION} bgzip -c \; > ${DIR}/deepvariant.cohort.vcf.gz; ```. When we ran on this WES trio, it took only about 13 seconds. For more details on; performance, see; [GLnexus performance guide](https://github.com/dnanexus-rnd/GLnexus/wiki/Performance). For a WGS cohort, we recommend using `--config DeepVariantWGS` instead of; `DeepVariantWES`. Another preset `DeepVariant_unfiltered` is available in; `glnexus:v1.2.7` or later versions for merging DeepVariant gVCFs with no QC; filters or genotype revision (see; [GitHub issue #326](https://github.com/google/deepvariant/issues/326) for a; potential use case). The details of these presets can be found; [here](../deepvariant/cohort_best_practice). ## Annotate the merged VCF with Mendelian discordance information using RTG Tools. Create an SDF template from our reference file:. ```; sudo docker run \; -v ""${DIR}"":""/data"" \; realtimegenomics/rtg-tools format \; -o /data/hs37d5.sdf /data/hs37d5.fa; ```. Create a PED file `$DIR/trio.ped` that looks like this (with the sample name; of the trio):. ```; FILE=""${DIR}/trio.ped""; cat <<EOM >$FILE; #PED format pedigree; #; #fam-id/ind-id/pat-id/mat-id: 0=unknown; #se",MatchSource.DOCS,docs/trio-merge-case-study.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/docs/trio-merge-case-study.md
https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md:650,Testability,test,test,650,"---; name: 'Problem encountered while running DeepVariant'; about: 'Tell us what happened, so we can try to help'; title: ''; labels: ''; assignees: ''. ---. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. ",MatchSource.DOCS,.github/ISSUE_TEMPLATE/problem_report.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md
https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md:686,Testability,test,test,686,"---; name: 'Problem encountered while running DeepVariant'; about: 'Tell us what happened, so we can try to help'; title: ''; labels: ''; assignees: ''. ---. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. ",MatchSource.DOCS,.github/ISSUE_TEMPLATE/problem_report.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md
https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md:280,Usability,clear,clear,280,"---; name: 'Problem encountered while running DeepVariant'; about: 'Tell us what happened, so we can try to help'; title: ''; labels: ''; assignees: ''. ---. **Have you checked the FAQ? https://github.com/google/deepvariant/blob/r1.6.1/docs/FAQ.md**:. **Describe the issue:**; (A clear and concise description of what the issue is.). **Setup**; - Operating system:; - DeepVariant version:; - Installation method (Docker, built from source, etc.):; - Type of data: (sequencing instrument, reference genome, anything special that is unlike the case studies?). **Steps to reproduce:**; - Command:; - Error trace: (if applicable). **Does the quick start test work on your system?**; Please test with https://github.com/google/deepvariant/blob/r1.6/docs/deepvariant-quick-start.md.; Is there any way to reproduce the issue by using the quick start?. **Any additional context:**. ",MatchSource.DOCS,.github/ISSUE_TEMPLATE/problem_report.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/.github/ISSUE_TEMPLATE/problem_report.md
https://github.com/google/deepvariant/tree/v1.6.1/deepvariant/environment_tests/README.md:32,Testability,test,tests,32,This is for lightweight (smoke) tests that we generally want to run; before anything else.; ,MatchSource.DOCS,deepvariant/environment_tests/README.md,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/deepvariant/environment_tests/README.md
https://github.com/google/deepvariant/tree/v1.6.1/third_party/nucleus/pip_package/egg_files/requires.txt:36,Testability,mock,mock,36,contextlib2; intervaltree; absl-py; mock; numpy; six; protobuf==3.19.4; Pillow==9.5.0; ipython; apache-beam; ,MatchSource.DOCS,third_party/nucleus/pip_package/egg_files/requires.txt,google,deepvariant,v1.6.1,,https://github.com/google/deepvariant/tree/v1.6.1/third_party/nucleus/pip_package/egg_files/requires.txt
